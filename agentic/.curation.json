"{\"title\":\"\",\"courses\":[{\"course_title\":\"SQL Essential Training\",\"course_admin_id\":2501656,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":2501656,\"Project ID\":null,\"Course Name\":\"SQL Essential Training\",\"Course Name EN\":\"SQL Essential Training\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"The ubiquity of big data means that now, more than ever, there is a burning need to warehouse, access, and understand the contents of massive databases quickly and efficiently. Attaining proficiency in SQL is essential to prepare for our data-driven present and future. It is also crucial to furthering your skills as a data analyst or application developer. In this course, Walter Shields teaches you: the basic structure of databases\u00e2\u20ac\u201dwhat they are, how they work, and how to successfully navigate them; how to use SQL to retrieve and understand data no matter the scale of a database; and how to master the most important SQL query syntax, along with how and when to use it best.&lt;p&gt;This course includes Code Challenges powered by CoderPad. Code Challenges are interactive coding exercises with real-time feedback, so you can get hands-on coding practice alongside the course content to advance your programming skills.&lt;/p&gt;\",\"Course Short Description\":\"Prepare for the data-driven future by learning how to manage relational databases with SQL.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":20168005,\"Instructor Name\":\"Walter  Shields\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Tech Educator and Best-Selling Author\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2023-02-07T00:00:00\",\"Course Updated Date\":\"2024-05-24T00:00:00\",\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/sql-essential-training-20685933,https://www.linkedin.com/learning/sql-essential-training-revision-q1-2023\",\"Series\":\"Essential Training\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Beginner\",\"LI Level EN\":\"Beginner\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Databases\",\"Primary Software\":\"SQL\",\"Media Type\":\"Interactive\",\"Has CEU\":\"Yes\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":16459.0,\"Visible Video Count\":79.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":302,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4414008\",\"duration\":42,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"The data-driven world\",\"fileName\":\"2501656_en_US_00_01_WX30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about the general concept of data accumulation, why there is a need for the skill of analyzing it; the instructor's background and motivation for teaching you this skill; and the depth of learning in this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2844729,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- In the time it takes me to finish this sentence\\nover 500,000 Google searches would've taken place\\nand in the next minute\\nover 300 hours of content will be uploaded to YouTube.\\nOnly half of a percent of all data collected\\nis ever analyzed.\\nThere's an incredible demand\\nfor skilled data handling professionals.\\nAnd guess what?\\nThat's where you come in.\\nI invite you to join me\\nin my LinkedIn learning course on the essentials of SQL\\nwhere you'll gain SQL skills\\nthat help businesses increase profitability\\nand realize more opportunities.\\n\\nLet's dive in.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4414009\",\"duration\":142,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"2501656_en_US_00_02_MM30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about the location of the associated sample database for the course. Also, discover what knowledge you should have to complete the course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7139643,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- So here's a quick rundown of what we're going to cover,\\nand a good way to start\\nis saying what we're not going to do.\\nMany SQL courses spend lots of time explaining the history\\nand computer science theory\\nbehind databases and query language.\\nWe're not going to do that.\\nInstead,\\nwe'll focus on learning just enough of the essentials\\nto understand how relational databases function.\\nThen you can apply the skills you learn in this course\\nto complete the final project.\\n\\nAs you apply your skills in real world scenarios,\\nuse this course as a reference guide when needed.\\nYou see,\\nas great as a course might be,\\nthe truth is learning SQL requires a commitment\\nto regular study and practice.\\nThere's just no way around it,\\nbut this course will help you adopt\\na resilient, can-do mindset\\nas you continue your learning path.\\nYou know,\\nit's a pleasure to spend my working hours\\nventuring into oceans of data\\nbut it is my privilege\\nthat you've allowed me to help you navigate the skill of SQL\\nwithout any fear of getting lost.\\n\\nNow, let's talk about what you should know.\\nIf you don't have any database knowledge, that's okay.\\nWe'll be stepping through the basics\\nall the way to advanced SQL topics.\\nIf you do have prior database or SQL experience,\\nit's helpful,\\nbut you don't need it to be successful in this course.\\nWe'll be using an SQLite database for this course\\nin addition to the DB Browser for SQLite database software.\\n\\nSQLite can be easily installed on Windows and Mac machines.\\nLite is not to be confused with lightweight\\nas this software is robust\\nand handles large volumes of data.\\nLater,\\nI'll walk you through downloading the course exercise file,\\na sample database.\\nWe'll also install the database software,\\nDB Browser for SQLite.\\nOkay, as they say in Jamaica, enough talk.\\n\\nLet's get to work.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3900220\",\"duration\":118,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"CoderPad tour\",\"fileName\":\"2501656_en_US_00_03_FY24Q4_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"updateType\":\"ADDED\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":285,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":null,\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3151162,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] This course includes\\nautomated code challenges that appear\\nwhen you click on the challenge links\\nin the course's table of contents.\\nNow, each challenge includes instructions\\nand a code editor you can use to create\\nand test your own solutions to the challenge.\\nThese challenges appear in the same area as the course page\\nwhere you watch the courses videos.\\nWe recommend using a desktop browser for the best experience\\nwith code challenges,\\nbut you can also use the LinkedIn Learning mobile\\napp if you prefer.\\n\\nThe code environment has three areas,\\ninstructions on the top left,\\na console for output on the bottom left,\\nand a code editor for your answer on the right.\\nYou can use these drag handles\\nto allocate spaces as you like.\\nTo get even more horizontal space\\nfor the code editor, you can collapse the courses table\\nof contents on the left.\\nEach technical interview question has instructions\\nthat include a description\\nand the question's desired result.\\n\\nCreate your answer in the code editor.\\nWhen you click test my code, in the bottom right,\\nyou'll see a message indicating whether your code returned a\\ncorrect result and a text-based version of the return data.\\nIf your answer doesn't create the correct result,\\nyou'll see a message telling you the code is incorrect\\nand showing the data that was returned.\\nIf any messages are too long to fit in the console,\\nyou can scroll sideways to see all of the text.\\n\\nWhen you've finished each code challenge,\\nreturn to the course's table of contents,\\nand click the next video to see a walkthrough\\nof my solution.\\n\"}],\"name\":\"Introduction\",\"size\":13135534,\"urn\":\"urn:li:learningContentChapter:4400958\"},{\"duration\":348,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4400947\",\"duration\":166,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Relational database theory\",\"fileName\":\"2501656_en_US_01_01_MM30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about basic database terminology and structure, and a hands-on section designed to set up the specific database software. This section explains the specific SQL tools, methods, and strategies.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7796269,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Okay!\\nTo start, let's get familiar with some basic vocabulary.\\nWhen we're talking about relational databases,\\nthere are a few key terms you just need to know.\\nFirst up, data.\\nWhat is it exactly?\\nWell, if we're being proper,\\ndata is the plural of the word datum,\\nand a datum is defined as a piece of information.\\nSo data are pieces of information\\nlike your last social media post,\\nwhether that's text, images, or video,\\nor the last phrase you search for on Google.\\n\\nSecond, a database.\\nIt's simply a collection of data.\\nThat data can be organized in many different ways,\\nbut it's usually organized in tables.\\nNow, what are tables?\\nWell, that's the third key term you need to know.\\nYou know, in Trinidad, where I'm from,\\nthe word lime used as a term to mean you're hanging out.\\nYou're going to chill out with some friends.\\nYou're going to lime.\\n\\nWhen used like this,\\nlime has absolutely nothing to do with the fruit.\\nSimilarly, in SQL,\\na table has nothing to do\\nwith the type you eat your food on.\\nIn the world of SQL,\\ntables are made up of rows and columns.\\nRows run left to right\\nand columns up and down,\\nthink Excel spreadsheet.\\nEach row represents a single piece\\nof data called a record.\\n\\nAnd yes, (upbeat music)\\nI'm thinking of some great reggae records right now,\\nbut it's not that type of record we're talking about.\\nEach column represents a specific attribute of that data,\\nsuch as name or address.\\nIn order to get the data, take a look at it, and analyze it,\\nwe use a special language called SQL,\\nshort for Structured Query Language.\\nBy the way,\\nSQL and SQL are terms used interchangeably\\nthat mean exactly the same thing.\\n\\nSQL includes many different commands or keywords.\\nOne of the most common is SELECT.\\nThe SELECT statement allows us\\nto specify the columns we want to get from a table\\nand include in our results.\\nBy understanding these basic concepts,\\nyou're on your way to mastering relational databases.\\nSo let's lime with some SQL.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4408108\",\"duration\":182,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"The SQLite database engine\",\"fileName\":\"2501656_en_US_01_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about the basics of RDBMS (relational database management systems); the various types, SQLite, why it's different; the nuances in SQL syntax from system to system; and why SQLite is selected for this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4856832,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The software system that's used\\nto compose SQL statements is called\\na Relational Database Management System or RDBMS for short.\\nNow, there are quite a wide variety\\nof relational database management systems that are out there\\nand the analogy I like to use is that of sneakers.\\nNow, if you're a sneaker head like me,\\nyou're probably into a number of different brands\\nlike Nike or Reebok or Adidas.\\n\\nAnd much like a relational database management system,\\nregardless of the brand,\\nthose sneakers perform a single function\\nand that is to protect our feet.\\nSo an RDBMS, though there are many different varieties,\\nthe singular function is to allow us an area\\nto compose SQL statements.\\nNow, here are a few popular\\nrelational database management systems\\nwhich you can research a bit for yourself\\nsimply by popping in the phrase\\nrelational database management system into Google\\nlike I have here.\\n\\nSo there are quite a wide variety,\\nbut it makes some sense to just get a bit familiar\\nwith what they are.\\nWithin a relational database management system,\\nthere are areas that allow us to compose a SQL statement.\\nSo like the SQL statement that's in front of us here,\\nthere are some main components\\nthat we should just take a high level note of.\\nSo firstly, there are a number of different clauses\\nin any given SQL statement.\\n\\nIn this one, we have the SELECT clause,\\nthe FROM clause, and the ORDER BY clause.\\nNow, within the SELECT clause,\\nthere can be one or more fields\\nthat are within this particular clause.\\nIn the FROM clause, you can have one or many tables\\ninside of this particular clause.\\nAnd lastly, the ORDER BY clause\\nalso houses one or more fields.\\nTogether, all three clauses\\ncomprise of a single SQL statement.\\n\\nNow, with regard to the number of different\\nrelational database management systems that are out there,\\nthere is a slight nuance of difference\\nfrom one to the other when it comes to the syntax\\nor the SQL statement that you compose within each.\\nSo for example, in SQLite,\\nthe RDBMS that we've chosen for our course,\\nthis is a perfectly legal statement.\\nBut if we were trying to recreate\\nthe same statement within Oracle,\\nthere would be about a 5% difference\\nwith regard to the syntax\\nto make that legal within an Oracle RDBMS.\\n\\nOkay, great.\\nUp next, let's take a look at the sample database\\nwe'll be using throughout the duration of this course.\\n\"}],\"name\":\"1. The Basics of Database Structures\",\"size\":12653101,\"urn\":\"urn:li:learningContentChapter:4413020\"},{\"duration\":386,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4409075\",\"duration\":110,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"WSDA Music (sample database)\",\"fileName\":\"2501656_en_US_02_01_MM30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about the course structure and the significance of the sample database which is used.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5149496,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- As a kid growing up,\\nI lived in a tough neighborhood called San Juan\\nwhich is in the eastern part of Trinidad.\\nAnd even though the environment had its share of turbulence,\\nmy mom was my rock that kept things steady.\\nYou know, she set an example for me to study\\nand keep my focus on education.\\nThat experience plays a major role\\nin why I'm motivated to see you succeed at this course.\\nBeside helping you understand the technical stuff,\\nmy purpose is to keep you focused, motivated,\\nand steady through any turbulence\\nin your own environment as you move through the course.\\n\\nIn the last chapter,\\nwe covered quite a bit of theory, and now that that's done,\\nlet's get into some SQL practice.\\nIt's time to get familiar\\nwith the exercise files for the course\\nand some strategies to help you win with learning SQL.\\nSetting up your environment is a critical first step\\nin learning any new coding language\\nand SQL is no different.\\nThe course exercise file is a relational database\\nthat belongs to WSDA music,\\nan online retail music company selling popular music.\\n\\nIt contains product information like songs and albums,\\npersonal customer information, employee records,\\nand sales data.\\nNow, the company management wants to know\\nif their database contains any useful information\\nabout sales, customer demographics,\\nand any ways the company can improve\\nor expand their services.\\nYou've been given the task of analyzing their data\\nand presenting insights you discover to management.\\n\\nAre you up for the challenge?\\nLet's get started.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4408109\",\"duration\":70,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"The DB Browser software\",\"fileName\":\"2501656_en_US_02_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to locate the free downloadable SQL software (SQLite). Get familiar with the DB Browser software and its context with regard to available RDBMS.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2578605,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] As a first step for analyzing\\nour WSDA database\\nlet's take a look\\nat the relational database management system\\nspecifically built for SQLite.\\nNow, DB Browser for SQLite is a high quality,\\nvisual open source tool that's used to create,\\ndesign and edit database files compatible with SQLite.\\nYou may not realize it, but if you have a mobile phone\\nyou are already using SQLite.\\n\\nNow, here are some fun facts\\nabout this particular database software.\\nIt can be found on every Android device,\\nevery iPhone and iOS device,\\nevery Mac and every Windows machines.\\nIf you want to learn a bit more, navigate to SQLite.org.\\nOkay, now that we have a high level understanding\\nabout what DB Browser for SQLite is,\\nlet's ready our cells and get it installed on our machines.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4414010\",\"duration\":112,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to install DB Browser\",\"fileName\":\"2501656_en_US_02_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to install DB Browser.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3809225,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, let's get DB browser\\nfor SQLite installed on your machine.\\nThe first step to do this is to navigate\\nto the website sqlitebrowser.org,\\nand once you're there\\nyou want to navigate\\nto the download option from the menu choices up top.\\nI'm going to cover the installation\\nfor both Windows and Mac machines.\\nI'm on a Mac machine\\nso I'll walk you through the entire way.\\nFor Windows, it's very straightforward\\nand I'll walk you through the install\\nfor the Windows version as well.\\n\\nOkay. Starting with the Windows folks,\\nyou want to navigate\\nto where it says Windows on the download page\\nand select the option Standard installer for 64-bit Windows.\\nThis is most likely the version of Windows that you have\\nif you've purchased your machine recently.\\nOnce you click on that file, it'll download a MSI file\\nand you simply open that file and step through the options\\nonce you're presented with it.\\n\\nFor the Mac folks, you want to scroll down\\non this same download page to the macOS option\\nand the only link that is there is\\nDB Browser for SQLite.\\nYou want to click on that option\\nand that's going to trigger the download of the DMG file.\\nOnce that DMG file is downloaded, all you're simply doing is\\nat that point, selecting the DB browser\\nfor SQLite icon on the left\\nand dragging it over to the folder on the right.\\n\\nWith that step taken\\nyou've completely installed the software\\nDB Browser for SQLite.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4412027\",\"duration\":94,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Learning tips\",\"fileName\":\"2501656_en_US_02_04_MM30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to recall some tips that can be applied to enhance your learning experience and gain a deeper understanding of the concepts introduced. Tips such as writing rather than copying and pasting examples are emphasized.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3721802,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Before we go any further,\\nI'd .like to point out some tips or strategies\\nthat's going to help you succeed in learning SQL.\\nThe first one is to type everything out,\\nall of the SQL queries\\nthat you are going to have introduced to you\\nthroughout the course.\\nIt's going to be important to not just simply look at it\\nand move on, but pause and write out all of the syntax\\nthat you get introduced to.\\nThe reason for this is,\\nit's going to help you reinforce some of the concepts\\nthat's going to be introduced\\nas well as learn how to work through the inevitable errors\\nthat you will encounter as you start to get more familiar.\\n\\nSecondly, I'd like to point out\\nthe importance of taking natural language questions\\nand converting those into the equivalent SQL statement.\\nSo if you get a request from a manager\\nor someone who's asking you\\nabout the data that you have stored, for instance\\nhow many customers do we have in our database?\\nYou would then take that natural question\\nand convert it to the equivalent SQL query.\\n\\nThat's an important job\\nas a data analyst or an SQL practitioner\\nand it helps you to take natural English language questions\\nand then convert it to the equivalent SQL statement.\\nThese two tips, if you employ them\\nwill help you significantly as you learn and master SQL.\\n\"}],\"name\":\"2. The SQL Stack\",\"size\":15259128,\"urn\":\"urn:li:learningContentChapter:4413021\"},{\"duration\":642,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4411039\",\"duration\":66,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to access the DB Browser for SQLite software\",\"fileName\":\"2501656_en_US_03_01_MM30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to navigate the SQLite environment.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3419994,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Let's get to know DB Browser for SQLite.\\nIt's our relational database management system\\nor RDBMS we'll be using for this course.\\nThere are a few reasons for choosing this software,\\nbut some of the main reasons are:\\nSQLite is a free and open source\\ndatabase management system.\\nOpen source just means that the source code\\nfor this software is open to the public.\\nDB Browser for SQLite is a visual tool\\nthat's used to create, design and edit database files\\ncompatible with SQLite.\\n\\nWe'll be using DB Browser for SQLite for this course.\\nAgain, because it's free, open source\\neasy to install and easy to use.\\nNow let's walk through the main areas of this software\\nso you can become a bit more familiar with it.\\nWe'll be stepping through the database structure\\nbrowse data, and execute SQL tabs.\\nLet's go.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4413012\",\"duration\":139,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Load the sample database file\",\"fileName\":\"2501656_en_US_03_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to access the sample database and load it within the SQLite database environment.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4018661,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, we are now ready\\nto load the sample database\\nthat is going to accompany us throughout this course,\\nand to do so, we want to navigate\\nto the overview area of the course,\\nand go down to the related to this course section.\\nOnce there, you're going to click\\non the Show All link next to exercise files.\\nThat's going to pull up the folder\\nwhich inside of that folder you're going\\nto find the WSDA sample database.\\n\\nYou're simply going to download that file,\\nand once you trigger that download,\\nthat file will most likely end up\\nin your downloads folder,\\nand that's it.\\nThe next step we're going to take is firing up the DB Browser\\nfor SQLite software, which we installed earlier.\\nNow, if you are on a Windows machine,\\nyou want to navigate to Start, Windows,\\nand then simply click\\non the DB Browser for SQLite application.\\n\\nIf you're on a Mac machine, like myself,\\nyou want to navigate to the Applications folder,\\nthen look for that same application DB Browser for SQLite.\\nOnce there, double click and fire up that software.\\nOkay, with that software fired up,\\nyou're going to get presented\\nin front of you a screen exactly like this.\\nWhat we're going to do now is take the sample database\\nwhich we downloaded and load it to this environment.\\n\\nTo do so, we want to click on Open Database\\nfrom the options up top, and you simply navigate\\nto your download folder and find that WSDA_Music.db.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4414011\",\"duration\":167,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Getting familiar with a database\",\"fileName\":\"2501656_en_US_03_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to take an internal look at the sample database\u2014or any new database encountered\u2014and the general approach to becoming familiar with the general database objects.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7685959,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that we have loaded our sample database\\nWSDA Music to our database environment,\\nwe can now take a look at some of the contents\\nor the available information that's presented to us.\\nStarting with the Database Structure tab.\\nThis is one of the main areas that tells us\\nand allows us to get a little more familiar\\nwith our database.\\nSo, we can tell that there are 11 tables within our database\\nand here they are listed out.\\n\\nOne of the key features of this area\\nis the ability to expand these various tables\\nand see what the contents are of each.\\nSo with the example of the Album table,\\nI've just expanded that and it shows\\nthat there are three fields or columns\\nwithin this particular table.\\nI can also tell the type of fields\\nthat each one of these are\\nby looking under the Type column here.\\nSo, the album ID is an integer field\\nand that simply means numeric\\nor numbers are stored in this field.\\n\\nThe Title column or field has an envar char\\nor character type.\\nAnd last but not least is the Artist ID\\nwhich again has an integer stored.\\nSo quite a bit of information is housed\\nwithin the database structure.\\nUnder the Schema, we can tell even more information,\\nand that is that the album ID is not allowed\\nto hold nulls or it's not allowed to be blank.\\n\\nThe same thing holds for the title\\nand the same thing holds for the Artist ID.\\nJust from this database structure alone\\nand expanding the table,\\nI can tell quite a bit of information about our database.\\nSo this is actually the exact same method\\nthat I have employed over the past couple of decades\\nwhen I enter a new environment\\nand I'm trying to get familiar with a brand new database.\\nI first look at the structure, I see how many tables,\\nI look at the types of tables that are there\\nand the types of content that's housed within each.\\n\\nAnd this gives me a very high-level overview\\nand allows me to get a lot more familiar with the database.\\nSo here, the database structure,\\nI would highly encourage you\\nto get familiar with your database with this method.\\nIt gives you the ability to understand\\nwhat is within the database, how many tables\\nand gives you a general good sense\\nof starting to get familiar with a new database.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4409076\",\"duration\":125,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"The browse data area\",\"fileName\":\"2501656_en_US_03_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to examine the contents of various tables, what to look for, and how to become familiar with this concept.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4258616,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Host] Okay, we're doing fantastic.\\nWe've just taken a look at the database structure area.\\nNow let's take a look\\nat the other main area of our DB browser\\nfor SQL Light database management software\\nthat is going to be important to get familiar with.\\nSo in the browse data tab, what does it allow us to do?\\nWell, we do see here, under the table dropdown\\nwe have listed all 11 of the tables\\nthat we observed in the database structure.\\n\\nSo one of the main differences here is,\\nthe database structure shows us the structure of the house\\nso to speak, the framing, how it's built out.\\nNow, the browse data tab is allowing us\\nto see what is within that structure.\\nSo if you recall, we did take a look\\nat the album table and saw\\nthat within the database structure there were three fields\\nalbum ID, title, and artist ID.\\n\\nIn the browse data tab,\\nwe're able to access that same album table\\nand now instead of just seeing the structure\\nwe're seeing what's housed within that table.\\nSo we're actually seeing the data that's\\nstored within there.\\nSo every role here that we see is a record\\nof information that's stored within the album table.\\nFor instance, record number one the album ID\\nhas an ID of one.\\n\\nThe title is Vector Vixens Volume two\\nand the artist ID is one.\\nSo browse data allows us to access these tables\\nand get another point of view,\\na point of reference of what's housed within them.\\nIt's the exact same method I employ still to this day.\\nThat gets me up to speed quickly\\nin understanding a new database\\nand what might be housed within it.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4413013\",\"duration\":145,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"The query writing area\",\"fileName\":\"2501656_en_US_03_05_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about how SQL queries are run or executed within the relational database management system. This chapter explains how to go about opening any new database in an SQL browser and exploring its contents. Learn how to familiarize yourself with using an SQL browser to navigate the overall structure of a new database, view data on individual database tables, and access the Execute SQL tab.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3833589,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Let's take a look at the third important area\\nthat we should get familiar with\\nin our DB Browser for SQLite database software.\\nSo, we took a look at the Database Structure.\\nWe also took a look at the Browse Data.\\nNow, let's take a look at the Execute SQL tab.\\nNow, at the moment, it looks pretty blank,\\nbut, I can tell you, we are going to be spending\\nquite a significant number of time within this area.\\n\\nThis is where the magic happens.\\nThis is where we compose our SQL or S Q L statements\\nand discover the results of those queries.\\nNow, let me point out some main components\\nthat we should take a note of at the moment.\\nUp here, we have two play buttons,\\nand this allows us to execute,\\nor run, the code that we will be writing within this area.\\nNow, where will we actually be writing or composing?\\nWell, you see where my cursor is blinking here?\\nI'm just typing in random words at the moment,\\nbut this is where we'll be composing our queries,\\nand it is referred to as the query pane.\\n\\nQuite simple, but that's the name, the query pane.\\nDown here, once we have a query composed,\\nwe will have an area to observe the results of that query.\\nAnd in this area here,\\nthe second box is called the result pane,\\nand this is where you can observe the result\\nof executing a query.\\nThe last area here, where it says,\\n\\\"Execution finished without errors,\\\"\\nthis is referred to as the messages pane.\\n\\nVery important, because if we compose a query,\\nobserve the result, the messages pane tells us\\nif that query has run without error.\\nIt tells us how long that query took to run,\\nand a lot of other vital information that's going to be useful\\nto us as SQL practitioners.\\nSo, this Execute SQL tab, again, is the area\\nthat we will be spending quite a significant amount\\nof time as we start practicing\\nand getting to sharpen our skills as SQL practitioners.\\n\\n\"}],\"name\":\"3. The SQLite Database Environment\",\"size\":23216819,\"urn\":\"urn:li:learningContentChapter:4411051\"},{\"duration\":1731,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4412028\",\"duration\":114,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Queries\",\"fileName\":\"2501656_en_US_04_01_MM30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to identify the various best practices and why comments are one of the cornerstone best practices that should not be overlooked.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5967844,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- You're probably starting to notice\\nthat SQL is a simple\\nyet powerful language with many different commands.\\nGetting familiar with some of these SQL commands is\\none of the key components\\nto getting the hang of working with databases.\\nOne of the best ways to get familiar\\nwith these commands is to simply type it out.\\nTyping out the commands\\nas you encounter them helps to retain\\nand understand their use.\\n\\nYou'll get more insight\\nabout how they allow you to access and control your data.\\nWhether you're searching for specific pieces of information,\\ncreating new tables, or adding new records,\\nthere are definitely more SQL commands\\nthan I can ever show you in this course\\nor you should bother to retain.\\nBeside a few that are simple and easy to use,\\nit's really more important to understand the principle\\nbehind finding and using the appropriate SQL command.\\n\\nIn this chapter, I'll cover basic query writing etiquette.\\nSo you may need a knife, a fork, and a plate.\\nI'm just kidding.\\nBut we will understand the basics\\nof writing a proper query and formatting the results.\\nSome of the topics we'll discuss are: creating comments,\\nSQL query best practices, using an alias,\\nalong with sorting and limiting your query results.\\n\\nBy the end of this chapter,\\nyou'll be able to select individual columns,\\nalso known as fields in SQL world, from a specific database\\nand display those fields in alphabetical order.\\nOkay, enough talk.\\nLet's get started.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4400948\",\"duration\":225,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Query commenting\",\"fileName\":\"2501656_en_US_04_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video you will be able to discuss the Query concept, how to complete SQL query, and how to approach 'questioning' a repository of data\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6120171,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Before we get into our actual\\nquery writing journey,\\nlet's take a look at something that is simple,\\nyet powerful, and we should not omit it.\\nWhat I'm referring to is comments.\\nComments are a plain or natural language way\\nof describing a query that we have written.\\nAnd there are two main ways of writing comments.\\nThere is a comment line\\nwhich is written by starting with two dashes,\\nfollowed by a comment, like this.\\n\\nSo whatever follows these two dashes becomes a comment.\\nThe other way to write a comment is a comment block\\nby doing a front slash star, then going a couple lines down,\\nthen do star front slash.\\nWhatever falls between here becomes a comment.\\nAnd I can go several lines down\\nand anything falls between these blocks becomes a comment.\\nI don't need to have two dashes\\nas I do in the single line comment.\\n\\nNow, the way that you would format a comment is as follows.\\nI would say, first, who created this?\\nSo you do created by,\\nthen you want to say, when was this created?\\nAnd you'd put a month, followed by a day,\\nfollowed by a year.\\nAnd last but not least, but probably the most significant,\\nyou would want to have a description\\nand description would say what your query is intended to do\\nor what your query is doing.\\n\\nAnd in my case here, I'm going to show in my description\\nthat this is the structure of a basic query.\\nNow, this is a very standard format of putting a comment\\nbefore any SQL statement that you intend to compose.\\nIt's very helpful because often as a practitioner,\\nyou write several SQL statements\\nand you may have found yourself forgetting\\nwhat exactly any given one may do.\\n\\nThis is a very simple, but effective way of reminding you\\nof what exactly your query might be doing.\\nAdded to this, when others are working with queries\\nthat you have written, it gives them a very helpful clue\\ninto what exactly the query is doing.\\nAnother important point is queries can often be very long.\\nYou can have several hundreds of lines\\nin terms of what's referred to as a SQL script,\\nwhich can have one or many queries.\\n\\nAnd again, the comments is a really good way\\nfor you to clue yourself or others in\\nas to what that query is doing.\\nOnce we have this done,\\nthe other thing that we want to take a look at\\nor take a note of is just a general high-level way\\nof progressing through the composition of a query.\\nSome of the things that we might want to consider\\nare the following:\\nWhat table within the database are we requesting data from?\\nWhat fields within that table are we interested in?\\nAnd do we want to exclude any data\\nor filter or omit any range or time period?\\nAnd last but not least, what does our query do?\\nSo again, these are some just general high-level questions\\nto start shaping our minds\\nand getting ourselves ready to compose our query.\\n\\nUp next, we'll take a look\\nat actually composing our first query,\\nwhere we'll take a look at the structure\\nof a basic SQL query.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4413014\",\"duration\":409,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Query composition\",\"fileName\":\"2501656_en_US_04_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to create your first SQL query.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13694797,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, the moment we have been waiting for\\nis here.\\nWe get to compose our first SQL statement.\\nSo let's do that by first starting up a brand new tab.\\nThis icon all the way to the left here is Open Tab.\\nBy clicking on that, that gives us a brand new tab.\\nThe existing tab that I just had is right here still,\\nbut I just have now created a brand new tab\\nand gives us a clean space\\nto start composing our query.\\n\\nNow, jumping right into our best practice,\\nI'm going to start with front slash star, star front slash,\\nand in there I'm going to put my comments, as such,\\ncreated by,\\nand that will be me, by Walter Shields.\\nYou put the create date,\\nand that would be month, followed by day, followed by year.\\nAnd last but not least, we'll put the description\\nand let's think about what description we're going to put.\\n\\nWell, let's say that we were tasked or asked\\nby management of WSDA Music\\nto give us a list of all the customers in the database,\\nand they want to have their first names,\\ntheir last names, and email addresses.\\nSo let's put that as a description.\\nFirst, last,\\nand we should just say properly first and last names.\\nOkay.\\n\\nSo we want to have a query\\nthat's going to display\\nall customer's first and last names,\\ntogether with their email addresses.\\nHow would we begin to compose such a query?\\nWell, if we start by going to the Database Structure tab,\\nwe can remind ourselves about the entire structure\\nof this database.\\nAnd if we recall, there are eleven tables.\\nSo one of the key things is to look at the names\\nof these tables,\\nwhich gives us some clue\\nas to what might be housed within it.\\nAnd our current request is asking for information or data\\nfrom our customers.\\n\\nAnd as we go down the line here,\\nwe do see that we have a customer table.\\nSo when we expand this particular table,\\nwe can now see the fields or the columns\\nthat are the part of this particular table.\\nNow in examining it, we do see that we have a first name\\nwhich is what was requested,\\na last name,\\nand if we go down here, we do have an email address.\\nSo by going to the Database Structure tab,\\nwe've now given ourselves some more information\\nas to where we can actually zero in to get this data.\\n\\nNow, if we go to the Browse Data tab\\nand we take a look at that same customer table,\\nwe can see the contents of that particular table\\nand confirm, yes, indeed,\\nthis does seem to have what it says it houses: first name,\\nlast names, and email addresses,\\nif we scroll on over here.\\nSo how would we now compose a SQL statement\\nthat's going to give us this result?\\nNow let's go below our comment\\nand start with the keyword \\\"from.\\\"\\nNow, once I go to keyword \\\"from,\\\"\\nand I want to specify from where.\\n\\nSo now we have just confirmed\\nthat the customer table is the table that actually houses\\nthat information that we're after.\\nNow I'm going to pause here and take a look at this popup\\nthat you'll probably have noticed as we were typing before.\\nThis is referred to as IntelliSense\\nand this actually gives us the option to not type as much.\\nAnd it's the job of IntelliSense\\nto guess what we're actually typing,\\nagain with the aim of saving us some typing time.\\n\\nWe're after the customer table\\nwhich is denoted here by this symbol.\\nThe other option here is Customer Id\\nwhich is actually a field within the customer table,\\nbut in the from clause\\nwe want to specify what table we're after.\\nSo, this effectively is saying go to the customer table.\\nAll right, let's remind ourselves there again.\\nInside the customer table\\nnow we have all of these choices that we can choose\\nto include in our query result.\\n\\nBack to our execute SQL tab again.\\nWe want to start above that From clause\\nwith the keyword Select.\\nNow under Select,\\nwe can now select the columns or the fields\\nthat we're interested in.\\nOur request specifically is asking for first name.\\nAnd there it is.\\nYou can now select that item.\\nAnd it's also asking for last name.\\nBecause another column is going to be included\\nin our result,\\nI separate column names with a comma.\\n\\nI go down to the second line, and then I put last name\\nand I select that option from our helpful IntelliSense.\\nFinally, I want to see email addresses,\\nso I repeat the process, add a comma,\\nand add email, and select that item.\\nSo with this done,\\nI have effectively built a basic SQL statement\\nand now it is ready to be run or executed.\\nI go up here and I select this particular item, execute all,\\nand when I do that, I see that we have a few results\\ndown here in our Results pane.\\n\\nSo our results pane is now populated\\nwith the result of our constructed SQL statement,\\nand it does give us the request\\nthat we have been tasked with: first name, last name,\\nand email address.\\nDown here in the messages pane,\\nwe see we have some important information as well.\\nWe can tell now that we have 60 rows or 60 customers\\nthat have been returned in our result.\\nWe can tell the time that it took\\nfor that result to be generated,\\nwhich was 19 milliseconds.\\n\\nAnd we also have the repeat of our SQL statement\\nthat we've composed here.\\nSo a lot of pertinent information.\\nAnd together we have just done our very first SQL statement,\\ngotten a result,\\nand this is something to be very, very proud of.\\nWell done.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4400949\",\"duration\":168,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Query composition best practices\",\"fileName\":\"2501656_en_US_04_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to structure your SQL syntax according to the coding convention.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5713776,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Before we add on to what we have just learned\\nwith regard to our basic SQL statement,\\nlet's point out some things\\nthat may not have been that obvious.\\nSo in the composition of our SQL statement,\\nwe've done a few particular things\\nthat I wanted to draw to your attention.\\nSo I started with the FROM clause\\nand what this does is point me\\nin the direction of where the data is.\\nSo FROM Customer effectively says,\\ngo to the Customer table.\\n\\nAnd by putting SELECT up here\\nand specifying the columns, it's effectively saying,\\nwell now that I'm at the Customer table,\\nI'd like to see the first name,\\nlast name and email address displayed.\\nAnd that's exactly what has happened.\\nSo let's talk a little bit about how this statement\\nis structured or laid out.\\nWe have some indentation taking place\\nand this way of formatting\\na SQL statement\\nis what's considered part of that best practice.\\n\\nThe same best practice that has us putting a comment\\nbefore we start composing any SQL statement.\\nThere is nothing preventing this statement\\nfrom being written all in one line.\\nThis will be perfectly legal.\\nThere'll be no syntax errors,\\nif you put SELECT FirstName, LastName,\\nEmail FROM Customer\\nall in one line.\\nNothing stops you from doing this,\\nexcept a best practice.\\n\\nSo why don't we put the sequel statement\\neven though it works in one line?\\nIt seems like it might save some space.\\nWell, the reason for this is readability.\\nIt's important the way that this syntax\\nis laid out for the human eye.\\nWhen it's laid out like this,\\nwe can clearly tell the distinct clauses,\\nthe SELECT clause, the FROM clause.\\nWe can clearly tell what's housed\\nwithin each of those clauses.\\nThe SELECT houses three fields,\\nthe FROM has one particular table listed\\nand this is very readable for not only ourselves\\nwho compose this SQL statement,\\nbut for others who's going to actually\\ninherit this SQL statement from us.\\n\\nIt also holds true for SQL statements\\nthat we'll inherit from others.\\nWhen it's formatted according to this best practice,\\nit helps us be that much more\\nof a professional practitioner of SQL.\\nHelps us to read what we have composed\\nthat much more clearly\\nand helps others\\nwho actually are going to inherit our work after us.\\nSo these are some important points\\nthat's worth pausing to point out\\nand it is something that you should employ early\\nin your learning journey of SQL.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4412029\",\"duration\":249,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Column custom names\",\"fileName\":\"2501656_en_US_04_05_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about the significance of communication as a data analyst and how aliases are used to do this.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8707827,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Lecturer] Let's get into something that's\\npretty important with regard to communication of\\nyour query results.\\nNow, often the technical language of a database will differ\\nfrom the common language of a business.\\nIf we look at our example here\\nthe way that our WSDA music database stores\\nthe first name in the customer table is not\\nin the most readable way.\\n\\nNow what I'm referring to is the column name.\\nFirst name is all one word\\neven though it really is two distinct words\\nthe same goals for last name.\\nIn SQL, we have ways of improving the communication\\nof our results to our business audience.\\nAnd we do this via something called aliases.\\nAliases allow us to rename our columns\\nto names that are more appropriate to the business.\\n\\nSo in our example here\\nwe have first name, last name, email,\\nand WSDA Music Management usually refers to these\\nparticular columns as customer first name,\\ncustomer last name and customer email.\\nNow, how would we go about actually communicating\\nthese particular column names when we present\\nour final query results?\\nWell we introduce aliasing by way of the 'as' keyword\\nwe shall all place after the column name, first name.\\n\\nThen I will put a square brackets,\\nopen and closed square brackets.\\nAnd between those square brackets, I'm going to put\\nthe proper first name, which will be customer first name.\\nNow let's see what happens when I rerun this query.\\nBefore I do that, let's just take a look at the names\\nas they are displayed in our result pane.\\nNow I'm going to run this query, and what has happened?\\nFirst name has now changed to what we have aliased\\nthat column to be, which is customer first name\\nmuch more readable.\\n\\nAnd when we present this to management, there's no questions\\nto be asked because we're speaking in their language.\\nLet's do the same for last name.\\nNow, slightly different than putting square brackets.\\nI can also put open and closed quotes\\nand put the name I'd like it to be, customer last name.\\nRunning this query again, let's observe the result.\\nNow, our second column has now had its name updated to\\ncustomer last name.\\n\\nLast but not least, let us do one more thing.\\nI will now alias the email as uppercase email.\\nLet's keep it at one word.\\nBut let's say that our management just prefers to\\nsee that email in upper case, in which case\\nbecause it's just a single word.\\nI don't need any square brackets, I don't need any quotes.\\nI just simply put the one word that it altered\\nfrom the original and I'm going to rerun this.\\n\\nAnd again, my email column is now capitalized\\nas I have aliased it.\\nAgain, aliasing is very important\\nbecause it is our responsibility\\nas an SQL practitioner to understand\\nand speak in the language of the business.\\nSo we take what our columns are originally stored as and\\ntranslate that to a language that's more readable\\nand understandable by the business.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4410116\",\"duration\":238,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Sorting query results\",\"fileName\":\"2501656_en_US_04_06_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to use the ORDER BY clause to achieve and present results the correct results.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8820856,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We are doing really great.\\nWe have just formatted our brand new query\\nand have aliased the columns,\\nand now we're speaking in the language of the business.\\nSo our query looks pretty good,\\nbut we can still do a few things\\nto improve our query result.\\nNow WSDA Music Management loves our report,\\nbut they would like to have the results sorted by last name.\\nHow would we go about achieving this in SQL?\\nWell, our first step is to include a brand new clause\\nwhich comes after the from clause\\ncalled the order by clause.\\n\\nBy placing that clause there,\\nand then specifying the column, we now want to order by\\nwhich in this case is the last name,\\nwe can now sort our results.\\nNow when it comes to sorting,\\nwe can go in one of two different ways.\\nWe can either sort ascending\\nwhich is A to Z or descending order.\\nAnd the ascending order sort is actually the default sort\\nin SQL.\\n\\nSo by me clicking on run,\\nat this point, let's just take a note of the first record,\\nwhich is Luis.\\nNow I'm going to run with our order by included.\\nOur first records becomes Roberto\\nbecause this customer's last name starts with A,\\nand if we observe down the customer last name column,\\nwe'll see that all of our results are now sorted\\nin ascending order.\\nAnd just the same, I can include the keyword ASC\\nafter the last name field, which we now have sorted,\\nand that does the exact same thing,\\nspecifies that this column is now sorted in ascending order.\\n\\nWhat if we wanted to sort this column in descending order?\\nWell, that's a simple change\\nof including the keyword DESC after the last name.\\nAnd now let's run this query.\\nCustomer Finn is now in position number one,\\nand that is because his last name starts with a Z.\\nAnd if you look at the customer last name,\\nit's now sorted in descending order from Z through A.\\nAll right, great.\\n\\nNow we also have the ability\\nto sort not only by one column but by multiple columns.\\nSo what if we had a scenario\\nwhere we wanted to sort by first name in ascending order?\\nIn addition to this, we want to also sort by last name.\\nTo do that, I'm going to include a comma,\\ngo to the next line and specify the last name.\\nAnd this is to be sorted in descending order.\\n\\nSo I add that DESC.\\nAnd now let's run this query.\\nLooking at the results,\\nit may not jump out at us right away,\\nbut let's find a record that looks the same.\\nSo here in row number 16 and 17, we have two Franks.\\nBut when we look at the last name,\\nwe first have R appearing first and then H.\\nAnd that is because of our last name descending sort\\nwhich is sorting this customer's last name\\nin descending order.\\n\\nLet's take a look at one more record here,\\nJohn in 30 and 31.\\nTwo Johns, but when we look at the last name,\\nG appears before D, and again,\\nthat's because of our descending order sort.\\nSo with the addition of the order by clause,\\nwe now have enhanced our ability to deliver reports\\nin accordance to what our audience would expect.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4411040\",\"duration\":180,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Limiting query results\",\"fileName\":\"2501656_en_US_04_07_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about limiting query result output.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6684995,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Our report has been greatly enhanced.\\nWe have aliased our columns,\\nsorted our results.\\nBut now, we can even refine our results further.\\nSo thus far, if we take a look\\nat our last query that we've just written,\\nwe see all of the results displayed here.\\nIf we look further in the messages pane,\\nwe can see that our results show\\nthat 60 rows returned in 14 milliseconds.\\nSo these are all 60 rows\\nor all of the records\\nthat are contained in the customer table.\\n\\nOften, management may be interested\\nin not all 60 rows\\nbut maybe a subset.\\nOften you may want to look at the top 10 records\\nof a result set.\\nSo we are refining our results\\nin that we are only specifying three columns\\nfrom the customer table,\\nbut we could go ahead\\nand refine even further the results\\nso that we don't display all 60\\nbut instead an amount that we specify in our query.\\n\\nTo do so, we introduce a new keyword called LIMIT,\\nand that comes after the ORDER BY clause\\nhere in our instance.\\nSo if I now include LIMIT\\nand I simply put the amount of rows\\nI'd like to limit my results by,\\nand in this case, let's assume 10,\\nI now simply are going to run this particular query.\\nAnd before that, again,\\nwe have 60 rows,\\nwe'll go all the way down to 60,\\nbut now with the limit 10 included,\\nlet's see what we get.\\n\\nWhen I run this,\\nnow we have 10 rows being displayed instead of 60.\\nAnd if we look down here in our results pane,\\nwe see that is also reflected:\\n10 rows returned in 21 milliseconds.\\nSo this is often a very real-world request\\nwhere you're asked to give\\nthe top 10 best performing sales, for instance,\\nor top 10 customers.\\nYou are now enabled, with the use of the LIMIT keyword,\\nto actually refine and limit\\nthe results that you present\\nback to the requester of this information.\\n\\nAlright, we're doing really great\\nwith the SQL syntax\\nthat we've been introduced to thus far.\\nWe've looked at how to alias columns.\\nWe've also looked at how to order our results.\\nAnd thirdly, we've looked at how to limit our results\\nusing the LIMIT keyword.\\nAll of these skills combined\\ngives you the ability to significantly perform\\na great deal of analysis.\\nGoing forward, we'll now equip ourselves\\nwith even more SQL commands\\nwhereby we can perform even deeper analysis.\\n\\n\"},{\"urn\":\"urn:li:learningContentAssessment:66478693498eefbaf27243e2\",\"duration\":480,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Concise track pricing report\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1031869\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:3900219\",\"duration\":148,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Concise track pricing report\",\"fileName\":\"2501656_en_US_04_09_FY24Q4_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"updateType\":\"ADDED\",\"editingNotes\":\"#EDITING New movie - CP Solution\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":165,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3701669,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] In this challenge,\\nyou've been asked by the management of Red 30 Tech\\nto supply them with a report that lists\\nboth track names alongside their unit prices.\\nNow, let's take a look at how we would respond\\nto this request, starting with a comment block.\\nWe have within this block who's created the query,\\nwhen it was created,\\nand a short description of what the query does.\\n\\nNext, let's go down to the from clause\\nwhere we've specified the track table.\\nThat's our first step.\\nWe've aliased this track table with the letter T.\\nNow that we know what table we're going to\\nwithin the select clause,\\nwe've specified the fields in this table\\nwe're interested in seeing in our result,\\nand those fields are t.Name,\\nwhich we have aliased as \\\"Track Name.\\\"\\nI want to point out that we've put a pair\\nof double quotes around \\\"Track Name\\\"\\nbecause this alias has a space between track and name.\\n\\nNext, we have a comma separating\\nthe next column we're interested in seeing,\\nwhich is t.UnitPrice,\\nand we've aliased this column as Price.\\nNow, I'd like to point out again\\nthat in this case we're omitting any quotes\\naround the alias Price\\nbecause there's no space in this alias.\\nWith the select clause done,\\nlet's move on down to the order by clause\\nwhere we've specified the name column\\nas the column we wish to order are results by,\\nand that is the t.Name.\\n\\nLast but not least, we have the limit clause,\\nand we're limiting our results, or our rows, to 20.\\nNow, let's hit test my code and observe our result.\\nWe can see here in our console output\\nthe 20 rows that we've expected,\\nand we can now supply the management of Red 30 Tech\\nwith this report, which gives them what they've requested:\\na list of track names alongside their unit prices.\\n\\n\"}],\"name\":\"4. Composing Queries\",\"size\":59411935,\"urn\":\"urn:li:learningContentChapter:4412037\"},{\"duration\":3265,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4408110\",\"duration\":155,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Types of SQL operators\",\"fileName\":\"2501656_en_US_05_01_MM30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to identify the different types of operators that you use to perform data analysis.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5604840,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- All right.\\nSo now, we are pretty dangerous\\nwith our ability to display and order fields\\nbut we can become even more stealth\\nby giving ourselves precise tools\\nthat allow us to ask specific questions.\\nHere's what I mean by that.\\nWith what we've learned so far,\\nto answer a question like\\nhow many customers' last names begin with B?\\nYou could write a SQL query to select records\\nfrom the customer table,\\nsort the data alphabetically by last name,\\nthen, just manually count the names ending in B.\\n\\nManually counting your results\\nmay be fine for a few records,\\nbut what if our database was much larger\\nand contained millions of customers?\\nWould you really want to manually count\\nall the B names then?\\nI don't think so.\\nFortunately,\\nSQL has some tools\\nthat not only allow us to narrow down the results\\nof our query to very specific data,\\nbut also to order and filter our data\\nby conditions you specify.\\n\\nLet's see what these new statements can do.\\nOkay, we've done quite a lot.\\nWhat we're about to do next\\nis enhance our ability to write queries\\nby introducing a new clause called the where clause.\\nBefore getting into the where clause,\\nit's important that we pause and take note\\nof some of the common operators that will be used\\nin conjunction with the where clause.\\nSo, there are three main operator types\\nwhich fall under arithmetic, comparison,\\nand logical operators.\\n\\nUnder arithmetic operators,\\nthere is add, subtract,\\nmultiply, divide, and modulo.\\nSome common comparison operators\\nare equal to, not equal to,\\ngreater than, less than,\\ngreater than or equal to,\\nand less than or equal to.\\nAnd there are also logical operators\\nwhich are and, or,\\nin, like, and between.\\n\\nUp next,\\nlet's take a look at how these common operators\\nwill be used in conjunction with the where clause\\nand thereby give ourselves the ability\\nto greatly refine\\nand respond to inquiries about our data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4412030\",\"duration\":236,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filter and analyze numeric data\",\"fileName\":\"2501656_en_US_05_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about numeric data and the ways to start analyzing this data type.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8197113,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Here's the scenario.\\nManagement at WSDA Music has asked,\\nhow many customers purchased two songs at 99 cents each?\\nWell, if we zero in on this question,\\nhow many customers purchased two songs at 99 cents each,\\nand when a customer purchases a track,\\nthat generates a transaction in the invoices table.\\nSo let's go over to the Browse Data tab\\nand start by taking a look at the track table.\\n\\nNow, if we look over at the UnitPrice field,\\nwe do see that there are tracks at 99 cents each,\\nwhich is part of our question,\\nhow many customers purchased two songs at 99 cents each?\\nNow, two songs purchased at 99 cents each\\nwill total to $1.98.\\nSo let's take a look at our invoice table\\nand if we scroll over to the total,\\nwe do see that there are invoices totaling $1.98.\\n\\nSo to respond to this question,\\nwe would need to present all of the records\\nin the invoice table that total $1.98.\\nHow would we achieve this with SQL?\\nWell, with what we've learned thus far,\\nwe can select all of the records from the invoice table,\\nthen sort by the total column,\\nand simply manually count all of the records\\nthat are $1.98.\\n\\nWith the inclusion of the WHERE clause,\\nwe're able to enhance our abilities\\nand avoid counting manually\\nto present the answer to management.\\nLet's take a look\\nat how we would go about doing this in SQL.\\nStarting with the FROM,\\nwe're going to target the invoice table.\\nSo effectively, we're saying go to the invoice table.\\nNow I'm going to include the SELECT clause above the FROM\\nand include a few fields we want to see in our result.\\n\\nLet's include the InvoiceDate,\\nthe BillingAddress,\\nthe BillingCity,\\nand the Total.\\nOkay, at this point, we're on familiar ground,\\nbut now let's include our new clause\\ncalled the WHERE clause.\\nThe WHERE clause comes after the FROM\\nand we're going to specify that we want to see records\\nwhere the total is equal to $1.98.\\n\\nWe can also include our ORDER BY,\\nwhich comes after the WHERE clause,\\nand we want to ORDER BY the InvoiceDate.\\nNow let's run this SQL statement and observe our result.\\nIf we take a look at our result here,\\nlet's jump down first to the messages window\\nand we do see that we have 111 rows\\nthat satisfy our new criteria\\nwhich is specified in the WHERE clause\\nwhere the total is equal to $1.98.\\n\\nNow, if we take a look at our results\\nand look at the total column,\\nwe only are displaying records\\nthat satisfy the criteria of $1.98.\\nAnd this is a power of the WHERE clause\\nthat we've now included in our SQL statement\\nwhich gives us the ability to filter records\\nto the ones that we specify.\\nIn this case, it's $1.98.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4408111\",\"duration\":245,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"BETWEEN and IN operators\",\"fileName\":\"2501656_en_US_05_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about BETWEEN and IN operators.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8987892,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now, let's say there was a twist\\nto our scenario in that WSDA Music Management\\nis pretty happy with us finding out\\nhow many customers purchased two songs at 99 cents each,\\nbut now they'd like to know how many invoices exist\\nbetween $1.98 and $5.\\nLet's see how we would use SQL\\nto respond to such a question.\\nWell, to respond to this request,\\nwe would simply have to make an alteration\\nto our WHERE clause.\\n\\nCurrently, it's saying total equal to $1.98.\\nBut since the request wants to find records\\nthat exist between $1.98 and $5,\\nwe're simply going to include the logical operator\\nbetween $1.98 and $5.\\nBy altering our WHERE clause to include the logical operator\\nbetween the other keyword AND,\\nwe're effectively responding to this question,\\nhow many invoices exist between $1.98 and $5?\\nNow before I run this, let's take a look at our total\\nwhich is currently 111 rows and let's see our result\\nnow that I'm going to hit Execute.\\n\\nWe have now gone to 178 rows\\nand we do see in our total,\\nwe have our results which satisfy our criteria,\\nand that is all invoices that exist between $1.98 and $5.\\nYou can effectively tell WSDA Management\\nthat that number is 178 invoices.\\nNow, the management at WSDA Music\\nis so impressed with your work so far\\nwith the previous reports you've presented to them.\\n\\nThey've now asked for even more reports.\\nSo their latest request is now asking for,\\nhow many invoices do we have\\nthat are exactly $1.98 or $3.96?\\nLet's see how we would alter our current SQL statement\\nto retrieve this information.\\nNow again, with the WHERE clause,\\nlet's update this to include a new logical operator.\\nI'm going to remove everything that we have so far\\nand instead say IN, which is a new logical operator.\\n\\nI'm going to open parenthesis and include $1.98\\nwhich is the first value that they're interested in seeing.\\nI'm going to put a comma and put the second value, $3.96,\\nwhich is the second value\\nthat management is interested in seeing.\\nThen I'm closing my parenthesis.\\nAnd examining what we have here now,\\nwe're effectively saying where total in $1.98, $3.96.\\n\\nBy formatting our SQL statement now this way,\\nwe're effectively responding to our request,\\nhow many invoices do we have\\nthat are exactly $1.98 or $3.96?\\nBefore running again,\\nlet's take a look at how many records we have displayed.\\n178 in our previous request.\\nNow I'm going to run our new SQL statement\\nand we've gone to 168 records.\\nAnd if we take a look at our result,\\nwe can see the values in our total column\\nare only $1.98 or $3.96.\\n\\nAnd we can effectively respond to WSDA Management\\nand say we have a total of 168 invoices\\nthat are either $1.98 or $3.96.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4408112\",\"duration\":298,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filter and analyze text data\",\"fileName\":\"2501656_en_US_05_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about text (string) data and ways to start analyzing this data type.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10171189,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Management at WSDA Music is very impressed\\nand they've been able to discover quite a bit\\nof insight based on the reports\\nthat we have presented to them.\\nThus far, we've filtered quite a bit\\nof numeric data and in very much the same way,\\nwe can use operators to return specific text data.\\nSo the latest request from WSDA management\\nis to now find out how many invoices\\nwere billed to the city of Brussels.\\n\\nHow would we go about altering our SQL query\\nto respond to this request?\\nWell, first, let's go down\\nto our WHERE clause and take a look.\\nWe're now interested in filtering records\\nto where the BillingCity is equal to Brussels.\\nSo let's make this alteration.\\nI'm going to remove our current WHERE clause criteria\\nand say BillingCity equal to Brussels.\\n\\nI'm going to pause here.\\nI've just included a single quote\\nand this is one of the significant differences\\nof numeric versus text data when it comes to filtering.\\nUnlike numeric data, text data must be surrounded in quotes.\\nSo I'm going to put Brussels\\nbetween a pair of single quotes like this.\\nNow, before running our statement here, as we did before,\\nlet's take a look at our current record count\\nwhich satisfied our previous request.\\n\\nAnd now, we're going to run our current\\nupdated SQL statement and we've gone to seven rows.\\nAnd if we take a look\\nat our results and look at the BillingCity,\\nwe see all seven of these records are now pointing\\nto Brussels, exactly like what we've specified\\nin our WHERE criteria.\\nSo we can effectively tell WSDA management\\nthat there are seven invoices that were billed to Brussels.\\n\\nAnd once again, when it comes to text,\\nwe must include single quotes\\naround the text criteria that we are searching for.\\nManagement at WSDA Music is very excited.\\nThey've been able to uncover quite a bit of insight\\nand with the last query that we delivered to them,\\nthey've been able to see what's happening\\nin the city of Brussels.\\nNow, with that insight,\\nthey would like to have a little more information\\naround more cities that they are doing business in.\\n\\nSo the current request is how many invoices were billed\\nto Brussels, Orlando, or Paris.\\nHow would we go about satisfying\\nthis particular request with SQL?\\nWell, very much in the same way\\nthat we were able to employ\\nthe IN clause while we were searching for numeric data.\\nWe can do the same thing with text data.\\nSo let's go ahead and alter our current SQL statement.\\n\\nSo we want to now include\\nthe IN clause and I'm going to replace\\nthe equal operator with IN.\\nAnd when we include the IN clause,\\nyou want to make sure and start with an open parenthesis.\\nAnd because this is text data, we're surrounding one\\nof the criteria that we're looking for,\\nwhich is Brussels, in single quotes.\\nNow, we want to include the other two criteria\\nthat we're looking for, which is Orlando,\\nand just like Brussels, that must be surrounded\\nin single quotes, and I'll put that in lowercase.\\n\\nAnd the third city we're interested in is Paris.\\nSo once again, I put a comma, I put a pair of single quotes\\nand I put Paris in between there.\\nI end my IN clause with the closure of the parentheses.\\nAnd now, let's see what we get.\\nBefore running, we have seven rows,\\nwhich were the seven records that belong\\nto the city of Brussels.\\nNow, we've included Orlando and Paris\\nand we're going to run our SQL statement.\\n\\nWe've gone from seven records to 28 rows\\nand if we take a look again at the BillingCity,\\nwe have Brussels, we have Paris, we have Orlando\\nas the only criteria that is being satisfied,\\nwhen we have altered our SQL statement.\\nOnce again, we can return to WSDA management\\nand let them know that when it comes to billing activity\\nin the cities of Brussels, Orlando, or Paris,\\nthe company has 28 records that satisfies this criteria.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4400950\",\"duration\":379,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Search records without an exact match\",\"fileName\":\"2501656_en_US_05_05_MM30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about wildcard searching and how it extends your analyst abilities\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13629286,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now, so far, we've been responding\\nto quite a number of requests from WSDA Music management\\nand we have been using the EQUAL operator quite a bit\\nbecause we were searching for exact values.\\nNow in SQL, it is possible to search for partial values.\\nAnd when we're searching for partial values,\\nwe include a new operator called the LIKE operator.\\nNow, this comes in quite handy\\nwhen we are searching for values\\nthat we're not quite sure of\\nor a value is quite likely incorrectly spelled\\nin our database,\\nand we have to resort to such a search to get our results.\\n\\nNow, the management of WSDA Music\\nis interested in how many invoices were billed\\nin the cities that start with B.\\nHow will we go about responding\\nto such a request with SQL?\\nSo the LIKE operator always works in conjunction\\nwith something that's referred to\\nas the wild card character.\\nSo above our SQL statement,\\nI'm just going to put a note\\nin the form of a single line comment that says,\\n\\\"The % is the wild card character.\\\"\\nSo in SQL, the % is referred to as a wild card character,\\nand it always works in conjunction with the LIKE operator.\\n\\nSo I'm just going to make a note here\\nthat the % is equivalent of saying,\\n\\\"I don't care what comes next.\\\"\\nNow, let's respond to our request.\\nHow many invoices were billed\\nin the cities that start with B?\\nI'm going to remove our current criteria\\nthat we have in the WHERE clause,\\nand I'm going to say WHERE BillingCity,\\nand our new operator, the LIKE operator.\\n\\nAnd because we are interested in searching for BillingCity,\\nwhich is a text value,\\nI'm going to put single quotes as we do with text values.\\nAnd I'm interested in only the BillingCities\\nthat start with B.\\nSo I put a B there.\\nAnd I don't care what comes after.\\nI do care that the city starts with B,\\nbut whatever else comes after, I really don't care.\\nAnd if we look up here,\\nour %, which is the wild card character,\\neffectively says, \\\"I don't care what comes next.\\\"\\nSo let's put that % right after the B.\\n\\nAnd now, before running as we usually do,\\nlet's take a look at our current record count, which is 28.\\nI'm going to run our newly updated SQL statement.\\nAnd now, we went to 62 records.\\nAnd if we take a look at our BillingCity,\\nall 62 records now satisfy\\nthe one key criteria that we've specified,\\nand that is that that BillingCity starts with a B.\\nWhat comes after, we really don't care,\\nbut we do care that it starts with a B.\\n\\nAnd this is effectively performing a partial search\\nand responding effectively to the current request\\nby WSDA Music management,\\nhow many invoices were billed in cities\\nthat start with B?\\nAnd that number is 62 invoices.\\nNow, management is getting very excited\\nby the informative reports\\nthat you've been supplying to them.\\nIn a new twist, they have a brand new request.\\n\\nThey'd like to know how many invoices were billed in cities\\nthat have a B anywhere in its name.\\nHow will we go about altering our current SQL statement\\nto satisfy this request?\\nWell, let's remind ourselves.\\nThe % or the wild card character\\nis effectively saying, \\\"I don't care.\\\"\\nSo to respond to this current request,\\nlet's alter our current criteria,\\nwhich is searching for BillingCities that start with a B.\\n\\nNow, if we alter this current SQL statement\\nand enter another wild card character before the B,\\nwe're effectively saying this,\\n\\\"I don't care what comes before the B,\\nI don't care what comes after the B,\\nI do care that there is a B somewhere in the BillingCity.\\\"\\nSo let's run our current altered SQL statement\\nand observe the result.\\nWe have currently 62 rows.\\n\\nI'm going to run this statement and we've gone to 83 rows.\\nAnd if we take a look at the BillingCity,\\nwe do see that there are some records that start with a B\\nbut some that do not, like Dublin.\\nBut what it does have is a B somewhere in this name,\\nwhich is what our criteria has specified\\nand our results are satisfying.\\nDublin, D-U-B, has a B in the name.\\nLet's go down, check some other records.\\nWe do see there's a B obviously starting here.\\n\\nBut again, Lisbon, we do see a B in between the name.\\nIt doesn't start, doesn't end, it does have a B in there.\\nAnother quick note.\\nYou see that by specifying your criteria\\nwith a capitalized B, it returns all the records\\nwhether or not it's capitalized.\\nSo it is case insensitive.\\nSo our SQL statement,\\nwhich we have now altered, effectively satisfies\\nWSDA Music management's latest request,\\nhow many invoices were billed in cities\\nthat have a B anywhere in its name.\\n\\nAnd we could respond to management and say,\\n\\\"There are 83 invoices that satisfy this criteria.\\\"\\n\"},{\"urn\":\"urn:li:learningContentVideo:4414012\",\"duration\":348,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filter and analyze using dates\",\"fileName\":\"2501656_en_US_05_06_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about the power functions that are specific to working with date data types.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12709195,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We have done some excellent work\\nand performed quite a bit of analysis on numeric data.\\nWe've also performed analysis using text data,\\nand in very much the same way,\\nwe can continue to perform analysis using a new data type,\\nwhich is date data.\\nNow, management at WSDA Music,\\nthey're interested in specific invoices\\nin this part of their analysis.\\nSo they'd like to know how many invoices were billed\\non May 22, 2010.\\n\\nBefore we start diving into the equivalent SQL\\nto achieve such a request, we need to familiarize ourselves\\nwith how dates are stored in this particular database.\\nSo let's jump over to our Browse Data tab\\nand take a look at any table that has the dates.\\nWe have our invoice table specified here.\\nAnd if we take a look, dates are stored by year,\\nfollowed by month, followed by day.\\n\\nIt's very important to take a look as our first step\\nas to how the current database you're working in\\nis storing dates.\\nDates particularly can be stored\\nin a number of different formats.\\nAs we see right here, it's stored year,\\nfollowed by month, followed by day.\\nBut dates can easily be stored in other formats\\nsuch as month, followed by day, followed by year.\\n\\nOr if stored as they do in Europe,\\nwe are looking at day, followed by month, followed by year.\\nAnother twisted date as we have in our current database\\nis not only are our dates stored by year,\\nfollowed by month, followed by day,\\nbut we also have the time included.\\nNow let's jump over to our Database Structure tab\\nand take a look at the invoice table again\\nand look at our invoice date.\\n\\nIf we look at the type, we see that the type\\nthat this data type is stored is date/time,\\nand that is it is also including the time\\nas part of what is stored within this field.\\nSo this first step is important\\nbecause when it comes to us responding in SQL,\\nwe need to format the date\\nexactly like it is stored within our database.\\nSo we always make that the first step\\nbefore responding in a SQL statement.\\n\\nSo now our current task,\\nhow many invoices were billed on May 22, 2010?\\nNow let's go down to our WHERE clause.\\nNow we want to know\\nhow many invoices were billed on this date.\\nSo I'm going to remove the current criteria in my WHERE clause\\nand I want to say WHERE InvoiceDate,\\nbecause now we're looking at dates, is equal to,\\nand just like text,\\ndates are also stored between single quotes.\\n\\nAnd I'm going to format that date\\nexactly as it is stored in our database\\nand that is year dash 05 dash 22.\\nThen I'm going to put the time, 00:00:00.\\nNow let us take a look at the result\\nof altering our WHERE clause with this particular criteria.\\nWe have 329 rows from our last query.\\nNow let's run this statement and observe.\\n\\nWe went down to a single record\\nand that is that particular invoice\\nthat had the transaction date of May 22, 2010.\\nAnd we have effectively responded to WSDA Music Management,\\ntheir latest request, which is wondering how many invoices\\nwere billed to this particular date.\\nQuite a common request\\nas investigations into what you might be researching\\nmay require you to perform analysis on date-related fields\\nand this is how you would do it.\\n\\nWell done.\\nNow, working with a date/time field can be quite cumbersome\\nwhen you have to enter this many characters\\ninto your WHERE clause.\\nSo as a preview to our upcoming chapter on functions,\\nlet's include a slight alteration\\nwhich will save us some typing.\\nWe could include in our WHERE clause\\na date function called Date.\\n\\nNow let's do this.\\nLet's put Date open parenthesis,\\nthen close the parenthesis around the field InvoiceDate.\\nAnd now we can simply remove\\nthe time portion of our criteria and rerun our query.\\nLet's see our result.\\nExactly the same.\\nThe only difference is we no longer have to include\\nthe time portion,\\nand that is the impact of having included a Date function,\\nwhich effectively says\\ndon't bother about including the time.\\n\\nI now, with this function,\\ncan proceed to only process the date\\nand the date function effectively does this.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4412031\",\"duration\":216,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filter records based on more than one condition\",\"fileName\":\"2501656_en_US_05_07_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to identify two common, but powerful and essential operators, that allow you to perform advanced analytics on data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7651922,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] So far, we've responded\\nto a number of managerial requests.\\nAnd in our responses,\\nwe have only been looking at a single field.\\nNow let's take a look at our last query that we constructed.\\nAnd if we look at the WHERE clause,\\nwe've specified that the invoice date field\\nis the field that we are currently filtering on\\nand we've specified this May 22nd, 2010 date\\nas the date that we are interested in.\\n\\nOur result shows the results of this particular criteria.\\nNow in SQL, it is possible for us to use the AND\\nand the OR operators to specify criteria\\nfrom multiple fields.\\nNow let's consider this latest managerial request\\nfrom WSDA Music.\\nGet a list of all invoices that were billed\\nafter May 22nd, 2010\\nand have a total of less than $3.\\n\\nNow, let's alter our SQL statement to satisfy this criteria.\\nI'm going to switch out this equal sign for a greater than\\nand that is because the first part of our request\\nis asking for invoices that were billed after\\nor greater than May 22nd, 2010.\\nAdded to this, we would also like to have invoices\\nthat are less than $3.\\nSo I'm going to include the AND operator\\nand the total of less than $3.\\n\\nBy adding this criteria,\\nI'm now satisfying the complete request\\nand we are looking effectively at two separate fields.\\nLet's now run this query and observe the result.\\nWe've gone from a single record to 124 rows.\\nThat's now satisfying this criteria.\\nIf we take a look at the first record,\\nbecause we have sorted by invoice date in ascending order,\\nthat means the smallest or the earliest date\\nis going to be seen as the first record.\\n\\nAnd that earliest date does satisfy our criteria\\nbecause it is saying that it is May 30th, 2010\\nand we want all our invoice dates\\nthat are greater than May 22nd, 2010.\\nSo May 30th is definitely after May 22nd.\\nSecondly, our invoices need to be less than $3.\\nAnd if we take a look at our total column,\\nwe do see that every single one of our invoices\\nis less than $3.\\n\\nSo effectively, we have now specified two columns\\nor two fields within our WHERE clause\\nand responded effectively to our latest management request\\nfrom WSDA Music which is to get all invoices\\nthat are after May 22nd, 2010\\nand also have an invoice amount of less than $3.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4410117\",\"duration\":159,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Logical operator OR\",\"fileName\":\"2501656_en_US_05_08_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to identify how the OR operator differs from AND in its operation nod results.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5926960,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Now let's look\\nat a scenario where we would have to employ the OR operator\\nin our response to a request.\\nManagement at WSDA music would\\nnow like us to get all invoices\\nwhose billing city starts with P\\nor the billing city starts with D.\\nEither one of these would satisfy the criteria.\\nNow let's go down to our current criteria\\nand replace it to reflect our current ask.\\n\\nWe want to get the billing city\\nso let's specify that field.\\nSo the billing city that starts with a P.\\nNow, because this is a partial search\\nwe want to include our like operator\\nand we are searching a text field.\\nSo we surround that in single quotes\\nand we want the cities that start\\nwith P and we don't care what comes after.\\nAnd that calls for our wild card character\\nwhich is placed after the P.\\n\\nThis is effectively going to give us all billing\\ncities that begin with P.\\nWe're also interested in billing cities that start with a D.\\nSo either or of these particular cities\\nwould satisfy the criteria.\\nWe would now include a or billing city like,\\nand just like the Ps\\nall we're doing is searching for the Ds now, which is D.\\n\\nAnd I don't care what comes after.\\nSo in looking at our complete criteria\\nbilling city like P or billing city like D\\nincluding our wild card characters for each\\nwhich tells us we are interested in D\\nand we don't care what comes after.\\nNow let's run our statement and see what happens.\\nWe have gone to 56 rows and we have, if we take a look\\nat our billing city, all of our results, all 56 rows\\neither have a city that starts\\nwith P or a city that starts with D\\nand we again can respond to WSDA's managerial requests\\nby saying that there are 56 records\\nor 56 invoices that satisfy your request\\nof cities that either start with P or start with D.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4400951\",\"duration\":361,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Brackets and order\",\"fileName\":\"2501656_en_US_05_09_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about the importance of the order of operations when musing multiple operators like the AND and OR.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13485457,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now the management has tasked us\\nwith a new request\\nand they would like us to get a list of all invoices\\nthat are greater than $1.98 from any cities\\nwhose names start with P or D.\\nNow, if we recall from our last query,\\nwhich we still have here,\\nwe've satisfied the criteria for cities\\nthat start with P or cities that start with D.\\nThe one twist in the current request is to have all invoices\\nthat are greater than $1.98.\\n\\nSo let's respond to this request\\nby altering our WHERE clause, and saying:\\ntotal\\ngreater than\\n$1.98.\\nIn addition to this,\\nyou want to include the AND operator and say\\nthe complete WHERE clause criteria,\\nwhich is now saying: total is greater than $1.98\\nAND BillingCity LIKE P\\nOR BillingCity LIKE D.\\n\\nNow let's run our current statement and observe the result.\\nWe've gone to 43 rows,\\nand let's take a look.\\nIf we really pay attention to our results here,\\nwe might find that we have some things\\nthat don't look quite right.\\nOne of the criteria is that all invoices\\nmust be greater than a dollar and 98 cent.\\nAnd if that's the case,\\nif we take a look at record number 4,\\nwe have an invoice item that is less than $1.98.\\n\\nWhat has gone wrong?\\nThis is one of the key things we should pay attention to\\nwhen we're responding to a query\\nthat's asking for multiple criteria,\\nlike the one that we're doing now.\\nWhen it comes to multiple criteria,\\nwe should employ something that's referred to as PEMDAS.\\nMost likely, if you're in the European part of the world,\\nit's referred to as this,\\nor BEMDAS if you're in the US.\\n\\nThis stands for parenthesis, exponents,\\nmultiplication, and division, addition, and subtraction.\\nThe only difference with BEMDAS\\nis we've replaced parenthesis with brackets.\\nNow, what does this mean?\\nThis refers to the order of operation.\\nThe order in which a query is processed.\\nNow, if we take a look at our result here,\\nwhat has been happening is,\\nwe are processing the query, like this.\\n\\nSQL is saying, \\\"Give me all the records\\nthat are greater than $1.98\\nAND the BillingCity LIKE P.\\\"\\nOkay?\\nFor all the records that are satisfying\\nthis particular part of the criteria,\\nif we observe,\\nthe billing cities that start with P\\nall satisfy this criteria and that they're all above $1.98.\\n\\nBut the billing cities that do not start with P,\\nlike Dublin here,\\ndoes not satisfy this request.\\nAs it is,\\nthe request is saying just include any city regardless\\nof what that invoice amount is once it starts with D,\\nand that includes even the ones that are below $1.98.\\nWe are able to control our results by including brackets\\nthat satisfy the PEMDAS or BEMDAS rule.\\n\\nSo now let's alter our SQL statement here, and do this.\\nInstead of leaving it at as is,\\nwe're going to put brackets around this part of our criteria.\\nNow, what this does is that it directs SQL to say,\\n\\\"First, bring us all of the cities\\nthat satisfy this criteria.\\\"\\nThat is, all of the cities that start with P\\nor start with D.\\n\\nOnce we have satisfied this criteria from this subset,\\nthen give me all of the invoice totals\\nthat are greater than $1.98.\\nSo again, we're at 43 records.\\nLet's rerun this altered statement and observe the result.\\nWe went down to 35 records,\\nand if we take a look at the BillingCity,\\nthey all start with P or D.\\nGreat.\\nThe other part of the criteria is\\nthat the total must be greater than $1.98.\\n\\nAnd if we take a look,\\nwe have every one of our criteria being above this amount.\\n1.99 seems to be the lowest so far,\\nand I'm scrolling down all the way to the end,\\nand we see that none of our records are below that $1.98.\\nAnd this is a very important concept\\nwhen it comes to the order of operation.\\nWhen we have multiple criteria\\nthat's being asked for in our request,\\nwe must consider the order\\nin which it is going to be executed.\\n\\nAnd by specifying our brackets around the area\\nthat we want to execute first,\\nwe employ the PEMDAS or the BEMDAS rule\\nand we get the correct result.\\nSo effectively we can tell WSDA Music Management\\nthat all of the invoices\\nthat are greater than a dollar and 98 cent\\nfrom cities that start with P or D are 35 invoices.\\nAnd here they are.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4411041\",\"duration\":592,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"IF THEN logic with CASE\",\"fileName\":\"2501656_en_US_05_10_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about the CASE statement and its ability to add custom-defined groups to your result set.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":23117674,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The managerial team at WSDA Music\\nhas been greatly benefiting from the reports\\nthat you have supplied to them thus far.\\nIn fact, so much so they're now in a position\\nto set some new sales goals.\\nThe managerial team now wants as many customers as possible\\nto spend between 7 and $15\\nwhen purchasing music from their online store.\\nAs a result, they've created the following categories;\\na baseline purchase, which is anything that falls\\nbetween 99 cent and 1.99,\\na low purchase which falls between $2 and 6.99,\\nthe target purchase, which is the sales goal of $7 and $15\\nand a top performer category\\nwhich is anything that is above $15.\\n\\nNow, based on these categories,\\nthe WSDA Music Sales Department wants to see\\nif there's any information that they can decipher\\nor glean from the database concerning the sales\\nin all of the listed categories.\\nNow this is a great scenario to introduce the CASE statement\\nwhich will help us perform this level of analysis.\\nAll right, we have our basic SQL statement in front of us\\nand all this is doing is searching our invoice table\\nand displaying these fields.\\n\\nNow, we would also like to include the CASE statement,\\nwhich actually allows us to create a new temporary field\\nin our database that serves as a label\\nfor the data based on user-specified conditions.\\nAnd this is exactly a great case\\nfor these user-specified categories\\nthat WSDA management has laid out for us.\\nSo let's see how we would implement this using SQL.\\n\\nAs a first step, we would need to include a comma\\nafter our last field, and we include the keyword CASE.\\nNow in a new line, we want to say the first condition\\nand the keyword WHEN total is less than $2\\nthen we want to label this as baseline purchase.\\n\\nNow let's pause and take a look at this.\\nAll we have done from the comma here\\nis just included another field so far\\nand we're in the midst of building our CASE statement.\\nSo we've started with the keyword CASE\\nand the first condition or category we'd like\\nto include is WHEN the total is less than $2\\nwhich satisfies this criteria here\\nwhich is anything that falls between 99 cent and 1.99.\\n\\nSo anything less than $2,\\nwe can safely put into this category,\\nwhich is the baseline purchase.\\nOkay, now let's go to another bucket of information.\\nTo do that, let's include another keyword WHEN\\nso when the total is between $2 and 6.99,\\nthen this bucket of category\\nis going to be called low purchase.\\n\\nAnd I'm just going to grab it over here and pop it in here.\\nSo let's take a look at this criteria here.\\nWhen the total is between the value $2 and 6.99,\\nwhich is exactly the low purchase,\\nwe would like to call this bucket low purchase.\\nAll right, in very much the same way,\\nwe're going to repeat the same syntax.\\nWHEN the total is between 7 and $15,\\nthen this becomes our target.\\n\\nThat's the target purchase that our sales team\\nat WSDA Music has set for the sales team.\\nJust one more bucket here that we'd like to include,\\nand because if anything is above our target purchase,\\nthat means every other combination, we can say ELSE.\\nLet's just call anything else outside\\nof the categories we've specified top performer.\\n\\nOkay, one final thing that we must do\\nto end off our CASE statement is the keyword END.\\nAnd we can also label this entire field\\nas we say as a purchase type.\\nOkay, now let's take a look at the entire statement here.\\nWhat have we done?\\nI'm going to highlight the CASE portion\\nand this is simply a calculated field\\nand the CASE statement really allows you a lot\\nof flexibility by doing this.\\n\\nWe've said CASE to indicate that there's a CASE statement.\\nNow, when this particular condition is met,\\nwhen the total is less than two,\\nwe want to label this as baseline purchase.\\nSecond condition, when the total is between $2 and 6.99,\\nthen we want to label this as low purchase.\\nWhen the total is between 7 and 15,\\nthen this is our target purchase, and everything else\\nthat's not in any of the previous categories,\\nwe can label that as a top performer.\\n\\nLast but not least, we call an end\\nto our CASE statement with the keyword END.\\nAnd just as we were aliasing any other column,\\nwe're calling this CASE statement column a purchase type.\\nNow let's run our statement and observe the result.\\nWe have a brand new column that has been created,\\nand they bucket each one of those categories\\naccording to what we have specified.\\n\\nSo we have top performer, which is anything above 15\\nand we tell, we have this here.\\nThe target purchase which is between 7 and 15.\\nWe have $8.91, and that is pretty accurate.\\nWe also have a baseline purchase of $1.98,\\nwhich does satisfy this criteria.\\nAnd then, we have a low purchase,\\nwhich is between that $2 and $6 range,\\nand we see that we have this being satisfied as well.\\n\\nAnd that applies to every single record in our database.\\nNow with the inclusion of the CASE statement,\\nwe now enabling management to perform even deeper analysis.\\nSo as mentioned before, if our managerial team now wishes\\nto see which cities do their top-performing sales come from,\\nthey can actually perform this type of analysis\\nby including even more of a refinement to our SQL statement.\\n\\nSo let's see how we can respond to this question\\nby including the WHERE clause in our current SQL statement.\\nThe WHERE clause comes after the FROM\\nand before the ORDER BY.\\nSo we want to say WHERE,\\nand we can actually filter using our calculated field\\nor our CASE statement, which we've labeled as purchase type.\\nSo we can now say WHERE Purchase Type,\\nand I'll just keep it \\\"apples to apples\\\"\\nwith capitalizing purchase\\nwhere purchase type is equal to\\nand we are interested in the top performers.\\n\\nNow again, looking at our total record count, which is 413,\\nlet's now run our altered statement\\nwith the WHERE clause included,\\nwhich is actually filtering on our calculated column\\nor our CASE statement, and let's look at the result.\\nWe've gone to 12 rows,\\nand if we take a look at the purchase type column\\nin our result, they're filtered to only the top performers.\\n\\nAnd now, management can see\\nif they take a look at the billing city\\nin response to their question\\nwhich city accounts for our top performers.\\nMost of the cities that are accounting\\nfor the top performers seem to be outside\\nof the United States.\\nAnd with the inclusion of something like this\\nwith regard to the WHERE clause,\\nwe are now effectively performing very deep levels\\nof data analysis and enabling the management team\\nat WSDA Music with some powerful new tools\\nto add to their ability to generate sales.\\n\\n\"},{\"urn\":\"urn:li:learningContentAssessment:664786b23450318606cee2d7\",\"duration\":480,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Categorize tracks by price\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1031870\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:3901055\",\"duration\":276,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Categorize tracks by price\",\"fileName\":\"2501656_en_US_05_12_FY24Q4_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"updateType\":\"ADDED\",\"editingNotes\":\"#EDITING New movie - CP Solution\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":346,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":null,\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8503873,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] In this challenge,\\nyou've been tasked by the management at Red30 Tech\\nto produce a query which selects track names,\\ncomposers, unit prices,\\nand categorizes each track based on its price.\\nLet's take a look at our response,\\nstarting with our comment block,\\nwhich we've specified the name, create date,\\nas well as a short description of what our query is doing.\\n\\nNow, going down to the from clause\\nwhere we are specifying the table\\nwe're interested in getting our data from,\\nwhich is the track table in this case.\\nNow up to the SELECT clause,\\nwe are now going to specify the fields of the track table\\nthat we're interested in seeing in our result.\\nAnd those fields are starting with name,\\nwhich we've aliased as Track Name.\\nThe Track Name alias is surrounded with double quotes\\nbecause of the space between the word Track and Name.\\n\\nNext we have a comma separating our next field,\\nwhich is Composer.\\nThe third field we're interested in seeing is UnitPrice,\\nand we're aliasing this field\\nwith the word Price.\\nNow, we're not putting quotes around this\\nbecause there's no space in this particular alias.\\nNow onto our case statement,\\nwhich is referred to as a calculated column.\\n\\nA way to dissect this case statement\\nis start with the beginning, first, the keyword case,\\nand go down to the end with the keyword end.\\nAnd we are aliasing this calculated column as PriceCategory.\\nWith the start and the end done,\\nlet's now go one line up from the end\\nand specify the else clause,\\nwhich is the condition which holds true\\nwhen all others have failed.\\n\\nAnd in our case we're saying else exclusive.\\nNow starting with the first condition,\\nwhich is when UnitPrice is less than or equal to 99 cent,\\nthen Budget,\\nwhich is when this particular value\\nwithin the UnitPrice field holds true,\\nthat is, it's less than or equal to 99 cent,\\nthen label these rows as Budget.\\n\\nContinuing with this logic in mind,\\ngo down to our next condition,\\nwhich is when the UnitPrice is greater than 99 cents,\\nand UnitPrice is less than or equal to $1, 49 cents,\\nthen label the records that hold true to this condition\\nas Regular.\\nThird condition is stating\\nthat when UnitPrice is greater than $1.49,\\nand UnitPrice is less than or equal to $1.99,\\nthen label these records as Premium.\\n\\nAnd as we stated before, we said,\\nwhen all those conditions don't hold true,\\nlabel those records as Exclusive.\\nGoing down to the ORDER BY clause,\\nwe've now specified that we want to see our results\\nordered by UnitPrice in ascending order.\\nNow let's take a look at what happens\\nwhen we click on Test my code.\\n\\nWe do see our output\\nand we do see the rows that we are expecting to see,\\nwhich is Track Name, the COMPOSER, the PRICE,\\nand taking a look at the PRICE category,\\nwe can see that the various labels\\nthat we have specified in our case statement\\nis being applied in Budget, Regular, Premium,\\nand we do not seem to have any exclusives,\\nbut we certainly see that our labels are being applied.\\n\\nSo we can go back to the management at Red30 Tech\\nand present them with our report.\\n\"}],\"name\":\"5. Discovering Insights in Data\",\"size\":117985401,\"urn\":\"urn:li:learningContentChapter:4409081\"},{\"duration\":2569,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4411042\",\"duration\":101,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Joins explained\",\"fileName\":\"2501656_en_US_06_01_MM30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about JOINs on a high level. It's crucial to understand JOIN statements which allow you to return and compare data from multiple tables using INNER JOIN, LEFT JOIN, and RIGHT\\nJOIN. This is a core skill set that needs to be mastered in order to work with relational database structures.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4337845,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- You know, losing my mom back in 2011 was tough,\\nbut it got me very clear on how rewarding it is\\nto live a life in line with your purpose and passion.\\nI also became very clear on what my own passion is,\\nand I love helping you learn and succeed at data analytics.\\nThat pep talk is needed\\nfor what we are about to get into next.\\nSo far, we've written a lot of queries\\nand can do quite a bit with the SQL we've learned.\\n\\nBut as we sharpen our skills,\\nwe've not yet tapped into maximizing the power of SQL.\\nYou see, we've only been getting data\\nfrom one table at a time thus far.\\nAnd in relational databases,\\ndata is stored across multiple tables.\\nEach table contains some, but not all of the information\\nabout our fictional company WSDA Music.\\nSo, for instance, if the company's management wanted a list\\nof the support reps or employees\\nthat were assigned to each customer,\\nto answer this request,\\nwe need the data stored\\nin both the customer and employee tables.\\n\\nThis is a common scenario where knowing how to get data\\nfrom multiple tables comes in handy.\\nIn this chapter, you'll become comfortable getting data\\nfrom two or more tables with a single SQL query\\nby using powerful tools called joins.\\nA join is a command that connects the fields\\nfrom two or more tables of a relational database.\\nLet's take a look.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4409077\",\"duration\":353,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How tables share a relationship, part 1\",\"fileName\":\"2501656_en_US_06_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about relational database architecture as a premise for understanding JOINs.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12696571,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now, as we mentioned,\\na join is a command that combines the fields\\nfrom two or more tables in a relational database.\\nLet's take a look at a very simple example\\nusing the invoices table\\nof our WSDA music database.\\nNow, in the browse data tab where we are,\\nif we take a look,\\nwe see that this table has nine fields.\\nEach invoice has an identification number\\nknown as the invoice ID,\\nand each customer that generated an invoice\\nhas an identification number known as the customer ID.\\n\\nLet's say the marketing department\\nof our WSDA music database wants to get to know\\nthe customer base a bit better.\\nThey ask us for a full list of customer names,\\nfirst and last names,\\nnext to all the invoices generated by that customer.\\nHow can we write a query using only the customer table?\\nWe would not be able to answer this question in one query\\nusing the skills we've learned thus far.\\n\\nOur invoice table doesn't contain the names\\nof our customers.\\nInstead, the invoice table contains\\na field called customer ID.\\nTo discover how to display a list of invoices\\nthat include the customers who generated those invoices,\\nwe need to look at the invoice\\nand customer tables side by side.\\nIf we take a look at the customer table,\\nwe can see that this table contains the information we want,\\nthe first and last names of our WSDA music customers.\\n\\nThis table also has a customer ID field,\\nand if we look closely at the icons in the customer table,\\nwe can see that this version of the customer ID field\\nhas a small key icon next to it.\\nThis key icon is the symbol for a primary key.\\nIt's the unique identifying field for that particular table.\\nNow, since the customer ID is the primary key\\nof the customer table,\\nthere is a similar field in the invoice table.\\n\\nThere is also, as we can see,\\na field called customer ID in the invoice table.\\nNow, these two customer ID fields\\nprovide us with the link we need\\nto access both of these tables simultaneously.\\nWe now have all the information required\\nto join these two fields together\\nand produce a list of invoices\\nwith the customer names that generated them.\\n\\nLet's take a look at the SQL syntax to do this.\\nNow, in front of us, we have a basic SQL statement.\\nLet's continue to build it out so that we create a join\\nthat would now connect our customer and invoice tables\\nvia what's referred to as an inner join.\\nSo after the from clause,\\nwe're now going to include the keyword inner join,\\nand below this, we're going to say the name\\nof the table we'd like to join.\\n\\nIn this case, it's customer.\\nNow that we have the two tables specified,\\nwe can now say on what columns or fields\\nwe would like to join the invoice to the customer table.\\nTo do so, we put in a particular syntax, which is called on,\\nand then, we're going to specify these tables.\\nInvoice followed by the field customer ID,\\nand we want to say equal to customer,\\nand we want to specify the field within the customer table,\\nwhich is the customer ID.\\n\\nNow, let's run this query and observe the result.\\nOkay.\\nWhen we look at this query,\\nthere's a lot that we already know.\\nThe query begins with select,\\nas all our other queries have so far.\\nWe also have a star symbol, and what this is is,\\ninstead of specifying a column that we want to return,\\nthis returns every single column\\nthat is in both the customer and invoice table.\\n\\nWe are selecting all the fields from the invoice table\\nand joining them to all the fields in the customer table.\\nWe use the keyword on to provide the query\\nwith the link between these two tables,\\nwhich are the customer ID field in the invoice table\\nto the customer ID field in the customer table.\\nSince there are two versions of the customer ID field,\\none in each table, we have to use this particular notation,\\nwhich is table name followed by field name\\nto specify exactly which field in which table\\nwe're interested in joining.\\n\\nAll together, we now have a new syntax,\\nwhich represents a join between the invoice\\nand the customer table.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4410118\",\"duration\":262,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How tables share a relationship, part 2\",\"fileName\":\"2501656_en_US_06_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about relational database architecture as a premise for understanding JOINs.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9510240,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Host] Now let's take a closer look\\nat the result that our join has produced.\\nThere are a few things that we can observe,\\nnow that we've combined\\nthe invoice table to the customer table.\\nNow, if we take a look at the invoice ID field\\nfrom the invoice section of the results set,\\nwe can see that the first seven records\\nare linked to the same customer.\\nThis is customer id number one.\\nThis link tells us that customer id number one\\nis responsible for generating all seven of these invoices.\\n\\nIf we scroll over to the customer portion of our result,\\nwe can tell who customer id number one is.\\nAnd in this case, it's Mr. Lewis G.\\nOne customer is linked to many invoices.\\nIn the language of relational databases,\\nwe can say that the customer table\\nhas a one to many relationship with the invoice table.\\nNow this makes sense when you think about it,\\nseeing that a single customer can generate\\nmany invoices simply by purchasing multiple songs.\\n\\nWe can observe the relationship between two tables\\nof a relational database by looking at another way.\\nAnd this is looking at what's referred to\\nas a entity relationship diagram.\\nLet's now take a look at the ERD.\\nNow, if we take a look at the diagram in front of us,\\nit's referred to as an ERD,\\nor an entity relationship diagram.\\nAnd what it is, is a graphical representation\\nof the relationship between our tables.\\n\\nNow, if we take a look at the customer table\\nand the invoice table, we can observe that there is indeed\\na relationship that is demonstrated by this line\\nthat is connecting them.\\nIf we take a look inside of the customer table,\\nwe do see that primary key field,\\ndesignated by that key symbol next to the customer id field.\\nIn the invoice table,\\nwe see the primary key is the invoice id,\\nbut we do see that customer id field\\nalso in the invoice table.\\n\\nNow, to officially refer to this particular field,\\nthe customer id field, it is referred to as a foreign key.\\nSo the customer table is connected to the invoice table\\nby means of a primary foreign key connection.\\nAnd again, this symbol here shows that one customer\\ncan have many invoices.\\nNow this is how an ERD demonstrates the relationship\\nbetween multiple tables.\\n\\nIf we take a look at the rest of our ERD,\\nwe can see many other instances of primary\\nand foreign key relationships.\\nNow joins would not be necessary\\nif the invoice table had included all of the customer names.\\nNow, why was this not done?\\nIf we go one step further,\\ninstead of a database with 13 tables,\\nwe can have just one gigantic table\\nwith all of the information that we have displayed here.\\n\\nWhy don't we do this?\\nWell, the answer to this is, in a relational database,\\nthe process of distributing fields across related tables\\nis known as normalization.\\nAnd normalization keeps the size of our database small,\\nas it reduces the need to duplicate\\nor make mention of data in multiple places.\\nOkay, now this becomes significant\\nwhen saving seconds of processing time\\nwhen we construct our queries.\\n\\nConsidering the staggering size of some databases,\\nevery second does indeed count.\\nImagine if your Google search took five minutes\\nrather than a few seconds.\\nNow that we have identified the common link\\nbetween the two related fields\\nin both the invoice and customer table,\\nlet's take a closer look at how we would write join syntax\\nthat would be a little more efficient.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4414013\",\"duration\":341,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Simplifying JOINs\",\"fileName\":\"2501656_en_US_06_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to create the JOIN syntax and leverage aliases to maximize efficiency.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13209208,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] So we've seen from our example\\nthat's in front of us, our first example\\nthat joins have a special syntax\\nwhen referring to field names.\\nSince two tables in any given database\\nmay have fields with the identical names,\\nwhen creating joins, we must specify the table name\\nfollowed by the column name that we would like to join\\nso that the SQL browser knows\\nexactly which version of the field we are referencing.\\n\\nNow this syntax requires that the table name be listed first\\nfollowed by the column or field name.\\nNow joins are often used with aliases\\njust as we visited prior.\\nNow we introduce aliasing\\nto reduce the amount of typing that's required\\nwhen we're building out a SQL statement with a join.\\nLet's visit how we would do this.\\n\\nNow I'm going to make some alterations\\nthat's going to be more in line with a real world query\\nwhen we are building a SQL statement with a join.\\nHere we have the invoice, and I'm just going to do\\nas we do a column name and introduce the keyword as,\\nand because this is invoice, let's call this I.\\nDown here in the customer area,\\nlet's alias this customer table as C.\\n\\nLet's revisit our syntax that comes after the on keyword.\\nWe have the invoice table followed by the customer ID field.\\nBecause we have just aliased the invoice table as I,\\ninstead of fully typing out invoice,\\nI simply have to say I at this point.\\nThe same thing can occur for the customer table,\\nwhich is also aliased as C.\\nSo instead of typing out full customer, I just simply put C.\\n\\nAs you would suspect down here in the order by,\\nI would also say C.customerid.\\nNow as we mentioned before,\\nthe star returns all of the records from all of the tables.\\nAnd while this is great for our demonstration\\nof the concept, in a real world scenario,\\nwe would want to specify the fields that we are interested in.\\nSo our request wants to show the invoices\\nalong with the customers that generated them.\\n\\nSo let's start introducing some of the fields\\nthat are relevant to our request.\\nLet's start with the customer last and first name.\\nAt this point, I just need to put a C,\\nmeaning I would like to start referencing fields\\nfrom my customer table.\\nAnd because I've aliased this table as C, in my select\\nI'm also going to just specify C to mean customer table.\\n\\nI put a period and then I say last name.\\nI'm going to put a comma,\\nand I also want the customer's first name.\\nOkay, I'm going to introduce another comma,\\nand I'm going to say I want a few fields\\nfrom my invoice table now.\\nTo reference fields from my invoice table,\\nI place my alias, which is I,\\nand then I want to see my invoice id.\\n\\nI'll specify this.\\nI also want to include the customer id,\\nbut this time from the invoice table.\\nAnd I'm also going to include the invoice date.\\nSo I do, I.invoicedate.\\nAnd last but not least, I want the total\\nwhich also resides in the invoice table.\\nNow in looking at the full syntax here,\\nI have now aliased the syntax that we previously had.\\n\\nAnd it's very important because we have two tables,\\nand as mentioned, in any given two tables,\\nwe may have a field that's named exactly the same thing.\\nAnd I have done so by introducing\\nas I next to the invoice, as C, and everywhere else\\nthat I'm referencing a field\\nfrom either the customer or invoice table,\\nI'm proceeding that with a C or an I,\\nfollowed by a period\\nand then listing out the name of that field.\\n\\nNow let's now rerun our query and observe the result.\\nWe have a much cleaner looking query\\nthat now includes only the fields that we are interested in.\\nWe have fields from both the customer and invoice tables.\\nWe're satisfying the request that says\\nplease list all of the customers\\ntogether with their associated invoices.\\nWe can tell when those invoices were generated\\nand the amounts of each.\\n\\nAnd hereby we have effectively responded to a request\\nthat is now giving us a view into both the customer\\nand invoice tables.\\nWell done.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4411043\",\"duration\":175,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Types of JOINs\",\"fileName\":\"2501656_en_US_06_05_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to recognize the different types of JOINs available and the different results they produce.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5088473,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] So far, we've used joins to access the fields\\nof multiple tables.\\nWe've identified the primary key of the customer table,\\nidentified a similar foreign key in the invoice table,\\nand then we used the ON keyword\\nto link the two tables together,\\nhoping that the data matches up.\\nNow consider this:\\nwhat happens if the data from the tables we join\\ndoes not match up completely?\\nAnd here's what I mean by this.\\n\\nWhat if we had a customer, let's call him customer 6,\\ndelete his WSDA music account\\nand was subsequently removed\\nfrom the customer table altogether.\\nBecause WSDA music is required to keep financial records,\\nthere's still evidence from the invoice table\\nthat customer 6 made a purchase at some point.\\nIt's not unusual to find discrepancies in a database\\nand we must decide whether we want our query\\nto include data that does not match up\\nor exclude it altogether.\\n\\nTo handle discrepancies between tables,\\ndifferent types of joins are used.\\nTo understand this concept, it's helpful to look\\nat join types using a slightly simplified version\\nof our invoice and customer table.\\nAs we look at our simplified invoice and customer tables,\\nwe can identify a few discrepancies.\\nFirst, our invoice table shows\\nthat someone with a customer ID of 6 made a purchase\\non May 1st, 2017\\nbut this customer does not appear in our customer table.\\n\\nAlso, it appears that customer 1\\nand customer 5 never made a purchase at all.\\nSince those customer ID numbers do not show up\\nin the invoice table, customer 2 shows up twice\\nso we can infer that this customer made two purchases.\\nSince the records for customer 1\\nand customer 5 exist in the customer table\\nbut they do not exist in the invoice table\\nand customer 6 only exist in the invoice table,\\neach table contains at least one unique record\\nthat does not exist in the other table.\\n\\nNow we can attempt to join these two tables together\\nand observed how the output is handled depending\\non which type of join we use.\\nLet's start with our familiar inner join.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4400952\",\"duration\":140,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"The INNER JOIN\",\"fileName\":\"2501656_en_US_06_06_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to be flexible with JOIN types and how to extend your coding skill beyond system limits with the INNER JOIN.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4023125,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] So let's talk about the inner join.\\nAn inner join only returns matching records.\\nAny unmatched data from either table is ignored.\\nJoins are often described with the use of Venn diagrams.\\nAs our Venn diagram shows,\\nan inner join represents\\nonly the overlapping section\\nof the Venn diagram.\\nNow, if we take a look\\nat our simplified invoice and customer tables,\\nwe can see that our inner join\\nwill ignore invoice number five.\\n\\nAnd that is because this invoice refers to customer six,\\nand this customer does not appear in our customer table.\\nLikewise, customer one\\nand customer five\\nfrom the customer table did not generate any invoices.\\nIn other words,\\nthey do not have any records in the invoice table.\\nSo that record is ignored as well.\\nAs the Venn diagram shows,\\nonly the overlapping data is included.\\n\\nNow, if we revisit the code\\nthat we generated to produce this inner join,\\nwe can see that this is the exact SQL statement\\nthat we created when we first visited our inner join.\\nNow, if we take a look at the output of this query,\\nwe can see that only four records were returned.\\nInvoice number five,\\ncustomer number one,\\nand customer number five are all omitted.\\n\\nCustomer number two is responsible for two records.\\nNow, the inner join is the most common type\\nof join that's used,\\nand the main use of the inner join is\\nto bring corresponding data together\\nfrom different tables in a relational database.\\nNow, let's move on to another join type\\ncalled the left outer join.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4412032\",\"duration\":179,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"The LEFT JOIN\",\"fileName\":\"2501656_en_US_06_07_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to be flexible with JOIN types and how to extend your coding skill beyond system limits with the LEFT JOIN.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5247693,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] A left outer join combines all the records\\nfrom the left table\\nwith any matching records from the right table.\\nAs shown in our Venn diagram, the concept\\nof left table and right table depends entirely on the order\\nthese tables are listed in the join statement.\\nSo for example, our SQL statement here\\nhas listed invoices first.\\n\\nThen after the left outer join, it lists the customer table.\\nAnd in this instance, our invoice table is the left table\\nand our customer table is the right table.\\nWith this type of join,\\neverything in our invoice table will be displayed.\\nSince customer one did not order any songs,\\nthat particular record is omitted.\\nHowever, as we can see\\nin our simplified invoice and customer table,\\nwe are combining all five records from the invoice table\\nwith only three records from the customer table.\\n\\nRemember, there is no record for customers number one\\nor number five in the invoice table,\\nand customer number two has produced\\nor purchased two songs and created two invoice records.\\nUnlike the inner join,\\nwhich matched an equal number of records between each table,\\nlet's now take a look\\nat the output of this query to understand\\nhow the SQL browser handles it.\\n\\nNow, this SQL query or syntax\\nfor the left outer join is very similar\\nto what we've used for the inner join.\\nWe're simply replacing inner join\\nwith left outer join instead.\\nNow, when we take a look\\nat the output of the left outer join,\\nwe see that the SQL browser has added nulls\\nor null data to our results set.\\nRemember that we have no information\\nin the customer table about customer six.\\n\\nAdding null data is how the SQL browser handles the fact\\nthat we are trying to match five records\\nfrom the invoice table to only four records\\nfrom the customer table.\\nLeft joins are useful\\nbecause they allow us to see discrepancies in our data.\\nWe can produce lists\\nof customers that have not generated invoices\\nor search for data that has been removed in the right table\\nbut still exist in the left.\\n\\nNow let's move on to take a look at right outer joins.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4400953\",\"duration\":209,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"The RIGHT JOIN\",\"fileName\":\"2501656_en_US_06_08_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to be flexible with JOIN types and how to extend your coding skill beyond system limits with the RIGHT JOIN.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6050832,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now before getting into our right outer join,\\nI wanted to remind you about something we mentioned\\nin the beginning of the course\\nwith regard to relational database management systems.\\nIf you recall, we did mention there are various types\\nof relational database management systems,\\nsuch as the one we're using for this course, SQL Light.\\nBut there are others like Microsoft SQL Server and Oracle.\\nAnd we mentioned that there is about a 5% difference\\nwith regard to the syntax that you would write\\nin one RDBMS versus another.\\n\\nNow, I'm calling this out\\nbefore getting into the right outer join,\\nbecause right outer joins are not supported in SQL Light.\\nBut since right joins are still very popular\\nin other relational database management systems\\nI think it's appropriate to include it\\nin our discussion here.\\nNow, the right outer join returns the entire right table\\nas well as matching information from the left table.\\n\\nThe right join is a mirror image of the left join,\\nand functions in a very similar way.\\nNow, similar to what occurred in the left join,\\nthe right join takes all fields from the right table,\\nin this case the customer's table,\\nand matches that data with any corresponding data\\nfrom the invoice table or the left table.\\nNow again, if we look at our simplified invoice\\nand customer table, since customer six does not exist\\nin the customer table, this record is simply ignored.\\n\\nThe SQL statement required to create a right join\\nis not surprisingly, similar to the other two joins\\nwe've shown thus far.\\nIf we take a look at the syntax here, all we have done\\nto create a right join again is replace left outer join,\\nas we did previously, with right outer join.\\nNow, this particular join returned the most records out\\nof the three joins we've demonstrated thus far.\\n\\nCustomer number one and customer number five\\ndid not have corresponding data in the invoice table.\\nSo null values were assigned to those records.\\nTwo records from the invoice table corresponded\\nto customer number two, so to join resulted in data\\nfrom customer two being listed twice.\\nNow right joins are used less frequently than left joins,\\nand since SQL Light does not recognize the right join,\\nit is a best practice to reverse the order of the tables\\nin your query which would actually give you the same result.\\n\\nWe'll demonstrate this a little later.\\nNow, so far we've taken a look at inner join, left join,\\nand right join.\\nAnd in all of our examples thus far,\\nwe've been joining just two tables.\\nWell, it is possible to perform joins\\non more than two tables, so let's take a look\\nat how we would generate the sequel syntax to do this.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4409078\",\"duration\":281,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Tables and Entity Relationship diagrams\",\"fileName\":\"2501656_en_US_06_09_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about Tables and Entity Relationship diagrams.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10359385,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now, as we mentioned,\\njoins can be used to combine more than just two tables.\\nAdding additional tables using joins\\nis pretty straightforward.\\nAll we need to do is follow the same pattern\\nas the inner joins we have already demonstrated.\\nA great place to start when planning to write a join\\nis the ERD or entity relationship diagram\\nto first understand how tables may be related.\\n\\nWe can see that in addition to the relationship\\nbetween the invoice and customer tables,\\nthere is also a relationship\\nbetween the support rep ID from the customer table\\nand the employee ID from the employees table.\\nNow, even though these two fields\\nare called completely different names,\\nwhen you think about the context of these names\\nin the tables that they reside, they make sense.\\n\\nA support rep in the customer table\\nis internally referred to as a support representative.\\nAnd in the employee table\\nthat same support rep is referred to as an employee.\\nSo when referring to customers,\\nthat same person is referred to\\nas the customer's support rep.\\nBut internally,\\nthat same support rep can be referred to as an employee.\\n\\nOkay, now let's consider the following scenario.\\nLet's say that WSDA Music customer service department\\nwants to reward the employees\\nthat are responsible for the 10 highest individual sales.\\nCustomer service wants to create a plaque for each employee\\nwith a list of the customers that they've helped.\\nNow that we have an operational scenario,\\nwe can look at the ERD diagram\\nto determine what fields we need in our query.\\n\\nNow, sometimes when we are writing out complex query\\nthat reference or access multiple tables,\\nit's helpful to think through what fields are needed\\nand what tables those fields come from.\\nIn the following diagram,\\nwe have laid out some of the fields that were needed\\njust to conceptually visualize\\nwhat we're going to do\\nwhen it comes to writing our statement.\\nSo under the fields we need\\nwe've identified that we need from the employee table\\nthe employee first name, last name, and the employee ID.\\n\\nOn the right here,\\nwe have a little preview\\nof how we're going to alias those particular column names\\nfrom the employee table.\\nAll of them are proceeding with a E dot\\nto first reference that table\\nand then the column or field name from that table.\\nWe also see that we do need the customer table\\nand the fields from this table\\nwill be the customer first name, last name,\\nand the support rep ID.\\n\\nWe do see that the fields\\nalso are preceded with the customer table C dot\\nfollowed by the field from that table.\\nFirst name, last name, support rep ID.\\nThe third table that we need is the invoice table,\\nand we have also identified the fields\\nthat we need from this table\\nwhich are customer ID and total purchase amount.\\nThe columns, as we will format it in our SQL statement\\nis I to represent the invoice table dot field name,\\ncustomer ID and total.\\n\\nWe also want to explain\\nhow we are going to present these records,\\nand we can say that we'll sort the query\\nby highest invoice value and limit to the top 10 invoices.\\nOkay. With what we have here,\\nwe have everything that we need\\nto start composing our SQL statement.\\nSo let's jump over to our SQL browser and take a look\\nat how we would start composing this statement.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4412033\",\"duration\":345,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Joining many tables\",\"fileName\":\"2501656_en_US_06_10_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to be flexible with JOIN types and how to extend your coding skill beyond system limits with many tables.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9432693,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that we have identified\\nthe fields that we need to respond to the current request\\nfrom WSDA Music Management,\\nwhich is what employees are responsible\\nfor the 10 highest individual sales.\\nLet's go ahead now and start building\\nout our SQL query that's going to respond to this request.\\nNow, starting with the FROM clause,\\nwe can identify the first table that we need\\nwhich is the Invoice table.\\n\\nWe want to alias this Invoice table as i.\\nNext, we want to now join this table\\nto the first table that we need to respond to this inquiry\\nwhich is the Customer table.\\nSo we're now going to put our keyword INNER JOIN\\nand then we specify the Customer table\\nbecause that's the table we're going to join to.\\n\\nWe want to alias this table as c for customer.\\nAnd now, we are ready for ON keyword, which is going\\nto tell us the exact fields that we're going\\nto be joining the Invoice to the Customer tables.\\nNow, we're going to say i.CustomerId\\nis equal to c, that customer ID.\\n\\nAnd again, we're using our notation here\\nwhich is table name, followed by the field from that table.\\nAnd what we have done here, we've effectively joined\\nour Invoice to our Customer table\\nby way of these two fields.\\nNow, if you recall, we did identify that we needed\\none more table to add to our current join\\nand that is the Employee table.\\nTo do this join, we're just simply going to repeat\\nthe INNER JOIN keyword and then include the new table\\nwe intend to join, which is now the Employee table.\\n\\nWe're going to alias this table as e, and now, we are ready\\nfor our ON keyword and to now specify the exact\\ncolumns that we need to join on.\\nSo we are going say, c dot support representative\\nequal to e.EmployeeId.\\nAnd again, within the Customer table, the support rep\\ndoes mean the exact same thing as the employee\\nwhich is in the Employee table.\\n\\nAnd via these two fields, we are now effectively\\njoining our third table, which is the Employee table.\\nNow, a couple more things to add on\\nto our query here is, first we want to include\\nthe ORDER BY and we want to order our results\\nby the invoice total.\\nSo we specify the Invoice table\\nwith i dot and specify the total\\nand we are going to order our results in descending order.\\n\\nSo we include the keyword, DESC.\\nLast but not least, we want to reward the top 10 employees.\\nSo let's limit our results to 10.\\nNow, we've done everything that we need to do\\nfrom our FROM clause.\\nNow, let's include the fields that we want to see displayed.\\nTo do this, we include the SELECT above the FROM\\nand now we're going to use our table\\nand column notation to include the various fields,\\nstarting with the employee first name.\\n\\nThen we'll do the employee last name.\\nNext, we'll place the employee ID.\\nNext, we'll put the customer's first name,\\nfollowed by the customer's last name.\\nThen we are going to include the support rep ID\\nwhich helped the customer.\\nAnd last but not least,\\nwe're going to include the invoice, customer ID as well.\\n\\nAnd finally, one more field, and that's the total,\\nand we get that from the Invoice table.\\nNow, let's take a look at our complete SQL statement.\\nIt's pretty involved here\\nbut we are effectively joining our Invoice\\nto our Customer to our Employee table.\\nThen we are ordering our results, limiting it by 10,\\nand we've specified the various fields we want\\nto include in our result.\\n\\nNow, let's go ahead and run our complete statement.\\nNow, we do see, we have a nice result.\\nWe have the employees' first names,\\nthe customer first names,\\nthe support reps who helped them,\\nand the total amount\\nof invoice that that employee generated for the transaction.\\nWe can now return to WSDA Music Management\\nand effectively respond to their inquiry\\nwhich is what employees are responsible\\nfor the top 10 highest individual sales.\\n\\nAnd we can submit this report and effectively\\nrespond to this inquiry.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:664786e23450b22d8e07636a\",\"duration\":600,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Analyzing customer support interactions\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1031873\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:3899163\",\"duration\":183,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution:  Analyzing customer support interactions\",\"fileName\":\"2501656_en_US_06_12_FY24Q4_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"updateType\":\"ADDED\",\"editingNotes\":\"#EDITING New movie - CP Solution\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":248,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":null,\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4865754,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(lively music)\\n- [Instructor] In this challenge,\\nyou've been asked by the management\\nof Red30 Tech to supply them with a report\\nthat lists each customer\\nalongside their assigned support representative.\\nSo let's take a look at our response.\\nStarting with our comment block, we've specified\\nwho created our query, when it was created,\\nand a short description of what it's doing.\\n\\nNow, down to the from clause,\\nwe've specified the customer table,\\naliased with the letter c,\\nand we are joining onto the employee table,\\nwhich is aliased with the letter e.\\nNow, why these two tables?\\nWell, referencing our task here, we've been asked\\nto supply our customers,\\nalongside with their support representatives.\\nAnd our customer table obviously has our customers\\nand our employee table houses our support representatives.\\n\\nNow that we have both customer\\nand employee table specified, using the ON keyword,\\nwe're supplying the columns from these two tables\\nthat we are connecting,\\nand that is from our customer table, the SupportRepId,\\nwhich is going to be equal to the e.EmployeeId table.\\nNow, let's go up to our SELECT clause\\nwith our FROM clause completed.\\nIn our SELECT clause,\\nwe're supplying various fields from each table\\nthat we'd like to see in our result set,\\nstarting with two fields from the customer table\\nwith the notation c.,\\nwe're referencing the customer table\\nvia the alias that we supplied.\\n\\nAnd we're also aliasing this FirstName column\\nas CustomerFirstName.\\nNext, we move on to our CustomerLastName,\\naliased as CustomerLastName.\\nLet's not forget our comma between.\\nAnd then we're also supplying two fields\\nfrom our employee table, which is also the EmployeeFirst\\nand LastName, aliased as SupportRepFirstName\\nand SupportRepLastName respectively.\\n\\nLast but not least, we go down to our ORDER BY clause,\\nwhich is the order we'd like to see our results,\\nand in this case here, we'd like\\nto see our results ordered first by EmployeeLastName,\\nfollowed by CustomerLastName.\\nNow, let's run our query here by hitting test my code\\nand observe our result.\\nWe can see that we now have a query\\nthat does indeed generate a list\\nof each customer alongside their support representative.\\n\\nWe can now take this report back\\nto the management of Red30 Tech.\\n\"}],\"name\":\"6. Accessing Data from Multiple Tables\",\"size\":84821819,\"urn\":\"urn:li:learningContentChapter:4411052\"},{\"duration\":1807,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4409079\",\"duration\":103,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Calculating with functions\",\"fileName\":\"2501656_en_US_07_01_MM30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about the significance of calculations and how to leverage functions to perform them with SQL queries. It is important to recognize how to enhance your query writing ability with a powerful collection of calculation tools known as functions, including aggregate, string, and date functions.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4695387,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- I think before we say another word about SQL\\nlet's recognize what you've accomplished so far.\\nYou just got through joins.\\nThat's a job well done, and I know\\nbecause joins were my biggest pain point\\nwhen I first got started with SQL.\\nNow think about the following request.\\nHow many customers do we have whose last name starts with S?\\nTo answer this request with SQL\\none approach would be to select the last name's field\\nfrom the customer's table, order by last name,\\nscroll down to the S names,\\nand then count those names manually.\\n\\nAnother approach would be to filter our results\\nto return only Last names starting with S,\\nusing the where clause\\nwhich would make the counting process easier.\\nBoth these methods require some sort of manual counting\\nto get the answer to the request.\\nIn this chapter\\nI'll show you how to use something called functions\\nto simplify your calculations\\nand eliminate the need for you to do any manual counting,\\nfunctions are one of the most powerful features of SQL.\\n\\nWith them, you can do complex calculations\\non your data with a single line of code.\\nLet's take a look\\nat some of the most commonly used functions in SQL\\nincluding aggregate, string, date and time functions.\\nLet's go.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4411044\",\"duration\":39,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"String, date, and aggregate function types\",\"fileName\":\"2501656_en_US_07_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to identify the different types of functions available to you in SQL.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1009007,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now let's head over to our execute SQL tab\\nand take a look at the following function.\\nNow, when I type in the keyword or the function UPPER,\\nthen open parenthesis, we do see this popup in front of us\\nwhich is telling us a little bit more\\nabout the UPPER function and what it expects.\\nNow, most functions in DB Browser\\nhave popup information that's very helpful in determining\\nwhat arguments the function takes\\nand what the function actually does.\\n\\nNow let's start exploring some of the various functions\\nstarting with string function.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4408113\",\"duration\":297,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Connecting strings\",\"fileName\":\"2501656_en_US_07_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about string-specific functions and how to use them in SQL, as well as how to apply string-manipulating techniques.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10102867,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now, to demonstrate the use\\nof string functions,\\nwe can take a look at the following scenario.\\nNow, let's say our managerial team at WSDA Music\\nis interested in sending out personalized postcards\\nto each one of their US-based customers.\\nNow, to do this, we would need a single field\\nthat comprises of the customer's first name, last name,\\naddress and that would be their full address\\nwhich would include the street,\\nthe city, the state, and the zip code.\\n\\nLet's see how we could use our SQL string functions\\nto build this field.\\nNow, when it comes to joining separate fields,\\nthis is referred to as concatenating them in SQL.\\nAnd to achieve this,\\nwe use the double pipe operator in SQLite.\\nI'm going to make a quick single line comment\\njust to demonstrate that the double pipe\\nis just simply the pipe symbol on your keyboard two times.\\n\\nOkay, now let's start to build our new field\\nwhich is going to be our mailing address.\\nLet's put a new comma in our SELECT clause\\nand then start building our new field.\\nFirst, we need the first name, so I'm going to put FirstName,\\nthen I'm going to use the double pipe operator like this,\\nthen I'm going to put the LastName.\\n\\nSo by doing this,\\nwe'll effectively join two separate fields together.\\nLet's now observe the result here.\\nOkay, we now have a new field\\nand this field has the first and last name fields\\nin one single field.\\nThe only issue is we don't have it looking very readable\\nwithout a space between the first and the last name.\\nSo let's go ahead and modify our current statement.\\n\\nTo do this, we would need to now put a pair of single quotes\\nwith a space in between.\\nWe want to make sure and have that space in between.\\nAnd then we're going to put another pair of double pipe\\nand make sure that LastName is there.\\nAnd now let's rerun our statement.\\nWhat we've done is created a space\\nbetween the first and last name\\nand that looks a lot more readable to the human eye.\\nOkay, let's continue building out our mailing address.\\n\\nWe have first and last name with a space in between.\\nWe do need the street address.\\nSo let's follow this with address,\\nwhich we'll start with our double pipe.\\nWe put a space again\\nbecause we want a space after that last name.\\nAnd then we put another pair of double pipes\\nand we put our address.\\nNow let's rerun our statement and slowly we're building out.\\nWe have name, which is first and last, plus the address.\\n\\nThis time, we're going to put a pair of double pipes.\\nAnd then between the double pipes,\\nwe want not just a space, but we want also a comma there.\\nSo we want it formatted slightly different\\nby putting a comma in between.\\nAnd now let's put city and let's run our query.\\nOkay, now, if we extend our result here,\\nwe do see we have the street,\\nfollowed by a comma then the city.\\n\\nLet's continue and then put the state.\\nSo by now, we're just repeating the pattern.\\nNo comma this time.\\nAnd we put state.\\nAnd finally, let's put the zip code.\\nRepeat the pattern once more.\\nAnd the zip code is referred to as the postal code\\nin our customer table.\\nNow let's run our full concatenated address.\\nAnd we do have a complete address\\nthat looks nicely formatted.\\n\\nFirst and last name, followed by street,\\ncity, state, and zip.\\nLast but not least, let's give our new column a nice title\\nand call it mailing address.\\nOne more time, we will rerun this.\\nAnd here again, we have now a nicely formatted address,\\ncomplete with a nice name\\nthat we have aliased as mailing address.\\nWell done.\\n\\nand a very common action with regard to manipulating data\\nin an existing database via string functions.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4413015\",\"duration\":259,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Separating text\",\"fileName\":\"2501656_en_US_07_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to identify more common string manipulation functions.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9035595,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Lecturer] In addition to joining or\\nconcatenating strings of text we can do the reverse of this\\nwhich is to truncate or shorten a string of text.\\nNow, we can achieve shortening\\nor truncating a string of text by the use\\nof understanding how text is stored in a text field.\\nOne important concept to understand is that text\\nhave positions in a stored field.\\n\\nSo for instance, our first name field that has Frank\\nthe letter F sits in position number one\\nand R two, A three, and so forth.\\nNow this positioning matters when it comes\\nto manipulating strings of text.\\nNow, let's say our management\\nat WSDA Music is really happy\\nabout our mailing list, however, they want to\\nactually have a little more precise postal codes.\\nIf you notice, the current postal codes all have a dash\\nand some numbers following\\nand these numbers are usually referred\\nto as the zip+4 code.\\n\\nAnd this isn't necessary for mail to find their way\\nto the intended recipient.\\nSo let's go ahead and remove this zip+4 code\\nand let's start our removal of this\\nby employing one new string function.\\nSo first, let's take a look at the length\\nof this postal code.\\nAnd to do this, we're going to employ the length function.\\nI'm going to say open parenthesis and include the field\\nthat we want to find out the length of.\\n\\nAnd in this case, it's the postal code.\\nNow I'm just going to hit run and we have a brand new column.\\nAnd what this column displays is the length\\nof our postal codes.\\nNow, if we take a look at these lengths, they all vary\\nand that is because they don't always have\\nthe equal number of digits.\\nIf we take a look at row number five,\\nthe zip+4 code is actually three digits,\\nwhereas in other cases it's four digits.\\n\\nSo we can actually start our removal of parts\\nof the zip code by employing the sub string function.\\nI'm going to go into a new line here and start typing,\\nS, U, B, S, T, R and open parenthesis.\\nAnd this sub string function actually removes the\\nextra US postal codes from this particular field.\\n\\nTo do this, we're going to say first,\\nthe field that we want to manipulate or change\\nwhich is the postal code.\\nThen we specify the first digit that we want to\\nstart the removal, which is\\nfrom digit number one or position number one.\\nAnd then we want to keep the five postal code digits that\\nwill allow our mailing address or letters.\\nWe want to keep the five digits\\nor the five zip codes that are going to allow our packages\\nor postcards to get to their intended recipients.\\n\\nSo let's indicate that we want to keep five digits\\nby putting a comma in five\\nand then we're going to close our sub string function\\nand we can also alias this as five digit postal code.\\nOkay, now let's go ahead\\nand run our updated statement and check out the result.\\nHere we now have a new postal code\\nand this time it's nicely formatted\\nto only the first five digits, which as we can verify here\\nis exactly what we wanted to do.\\n\\nNow we could respond to WSDA Music Management and say\\nhere is the updated mailing list\\nwith a nicely updated and cleaner five digit postal code.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4412034\",\"duration\":119,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"UPPER and LOWER string functions\",\"fileName\":\"2501656_en_US_07_05_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to recognize a wider range of functions that can enhance your abilities as a DA.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4469541,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] So, let's take a look\\nat a couple more string functions\\nthat will be pretty helpful to know.\\nFirst, let's take a look at the UPPER function,\\nand the UPPER function simply takes a string of text\\nand converts it to upper case.\\nSo, let's take the FirstName field\\nand make this field upper case.\\nSo, just simply putting the field name\\nbetween the parenthesis of the upper function\\nlet's make sure and include our extra comma\\nto accommodate this new field,\\nand let's even alias this to call it First Name All caps.\\n\\nNow, let's run this newly updated statement.\\nWe have our new field First Name All caps,\\nand if you take a look at the contents,\\nit now has all of our first names in upper case,\\nand we can verify the original value\\nin our FirstName field here\\nand in now newly updated uppercase values.\\nOkay, one more string function\\nwhich is the opposite of UPPER, which is LOWER.\\n\\nThe LOWER function simply converts a string\\nof text to all lowercase.\\nNow, let's do this for our LastName field\\nand let's alias this as Last Name All lower.\\nNow, let's run this updated SQL statement\\nand we now have our newly created field,\\nLast Name All lower,\\nand the values show, as it says,\\nall values are now all lowercase.\\n\\nSo, two other powerful string functions\\nto add to our knowledge base,\\nand with that, we've now wrapped up\\nour discussion on string functions.\\nNext, let's take a look at date functions.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4400954\",\"duration\":342,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Date functions\",\"fileName\":\"2501656_en_US_07_06_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to identify date-specific functions and how to use them in SQL.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10156057,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Speaker] Now date functions allow us to manipulate data\\nthat's stored in various date and time formats.\\nNow, if you recall within our WSDA Music database\\nthe dates that we have stored in this database\\nare stored in both date and time.\\nAnd that is year, month, day\\nfollowed by hour, minute, and second.\\nNow, let's say that we were tasked by the management\\nat WSDA Music\\nto calculate the\\nemployees ages.\\n\\nThat is, we need to leverage the birth date field\\nto calculate each employee's age.\\nNow, to do this,\\nwe need to find out the difference\\nbetween the current date and the employee's birthdate.\\nTo perform such a calculation,\\nwe can employ a new date function\\nwhich is called the strftime.\\nAnd, the strftime function converts date and time strings\\ninto another format.\\n\\nSo, to operate this function properly,\\nyou need to pass into it two arguments,\\nand that is,\\nfirst, you need to pass in your desired format,\\nthat is the format you'd like to format the date in\\ntogether with the time string to be formatted.\\nSo,\\nlet's now\\ntake our current SQL statement here,\\nwhich is simply selecting a few fields\\nfrom our employee table,\\nand modify it to now include the strftime function\\nto calculate our employee's birthdays.\\n\\nNow, our first step would be to include a brand new column\\nand then introduce the strftime function.\\nNow we need to supply the format that we intend\\nto format this particular field as,\\nand we want to format this as year, month, day.\\nTo do that, we place a pair of single quotes,\\nthen inside the single quotes,\\nwe put a percent,\\ncapital Y for year,\\nthen a dash, percent,\\nM for the month,\\nand then another dash, percent, D for the date.\\n\\nThis tells our field how we want it formatted.\\nNext, we place a comma and we place the time string.\\nIn this case,\\nwe are placing the birth date for our employee.\\nNow, we can also go ahead one step further\\nand alias this particular column as birth date,\\nno time code.\\nNow let's go ahead and run our query here.\\n\\nNow, if you take a look at our result here,\\nwe now have a newly added column,\\nand this particular column has now performed\\nwhat the strftime does.\\nIn this case,\\nand converted it to simply just a date format.\\nOkay.\\nbut let's respond to the managerial request\\nfrom WSDA management\\nwho does want us to calculate the ages of all employees.\\n\\nSo, let's now introduce yet another column,\\nand to do this calculation,\\nwe simply need to introduce a subtraction\\nwhere we are subtracting the present date\\nfrom the employee's birth date.\\nLet's start again by introducing the strftime function,\\nand we're going to format things the exact same way,\\nand that is year,\\nfollowed by month,\\nfollowed by day.\\nNow, instead of the birth date,\\nwe want the present date.\\n\\nTo get the present date,\\nwe introduce a new function\\nin between a pair of single quotes as well,\\nand that is the now function.\\nAnd the now function simply reads the present date\\nthat it receives from your computer\\nand presents this date to be calculated.\\nNext, we want to subtract this\\nfrom the\\nemployee's birthdate.\\nSo, we're simply going to repeat\\nthe previous strftime function we did for birth date,\\nno time code.\\n\\nSo, let's repeat strftime,\\nand now, present the same formatting,\\nwhich is percent y,\\npercent month,\\nand percent day.\\nThis time, we're going to supply that birth date field again,\\nand now we can alias this as\\nthe age column.\\nSo now, let's run our updated query and look at the result.\\nNow we have yet another column, which is our age column.\\n\\nAnd this column is performing\\nthis mathematical calculation here,\\nwhich is simply taking the difference\\nbetween the present date and the employee's birth date,\\nand generating the actual age for each employee.\\nSo with this completed,\\nwe can now return to the management at WSDA Music and say,\\nhere is our latest report that now calculates the ages\\nof every employee.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4411045\",\"duration\":266,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Aggregate functions\",\"fileName\":\"2501656_en_US_07_07_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how aggregate functions like SUM, MIN, MAX, and more are used to turn a range of numbers into a single data point.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7511317,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's take a look at another type of function\\ncalled aggregate functions.\\nAggregate functions turn a range of numbers\\ninto a single point of data based on a variety\\nof mathematical operations.\\nNow let's take a look at an example\\nwhere we would use our invoice table\\nto calculate the grand total.\\nIn other words, let's say we were tasked\\nby WSDA Music Management to get our total global sales\\nof all time.\\n\\nHow would we go about constructing a SQL statement\\nto do this?\\nWell, let's first start with our usual from clause,\\nand because we're interested in the total sales,\\nthis information is held inside of our invoice table.\\nSo let's specify the invoice table after our from,\\nthen let's go above our from and specify the select.\\nNow we can introduce our sum aggregate function\\nto calculate the grand total for us.\\n\\nSo let's specify sum,\\nand then within parentheses,\\nwe want to specify the actual column or field\\nwe intend to apply this aggregation to.\\nAnd in this case, in the invoice table,\\nwe want to apply this to the total field.\\nNow we could alias this as well and say total sales.\\nNow let's run our newly constructed statement\\nand observe the result.\\n\\nIf we notice here, we have a total value of $3,329.46.\\nNow this is now effectively responding\\nto WSDA Music Management, their latest request,\\nwhich is to generate the total global sales of all time.\\nAnd here we have a report\\nthat is responding to this question.\\nNow there are a number of other popular aggregate functions\\nthat we should get familiar with as well.\\n\\nSo let's go ahead and modify this present statement\\nto include a few more aggregate functions.\\nLet's introduce the average function, which is AVG,\\nand then we'll perform the average on the same column\\nwhich is the total.\\nLet's alias this as average sales.\\nNow let's take a look at this.\\nLet's rerun our statement,\\nand we now see we have a new column with the average sales.\\n\\nOkay, let's introduce a few more aggregates.\\nLet's go with the max aggregate function,\\nand we'll do this to the same field again,\\nand we could call this the maximum sale.\\nNow let's run this statement and we have now $1,000.86\\nas our maximum sale amount.\\nLet's include the minimum aggregate function,\\nand we'll call this minimum sale.\\n\\nLast but not least,\\nlet's also introduce one more aggregate function\\nwhich is the count.\\nWith the count, we can count every single row\\nwithin our invoice table.\\nTo do this, we're going to do something\\na little slightly different\\nwhich is to just put a star in between the parentheses,\\nand we will call this the sales count.\\nOkay, with this, let's go ahead and now run our statement.\\n\\nAnd now we have the various values that are being triggered\\nvia the use of our various aggregate functions.\\nWe have total sales, average sales, maximum sale,\\nminimum sale, and a total count of sales.\\nNow this is a very good use of aggregate functions.\\nWe're going to take a look at one more use of our functions\\nand see how we can make them a little more efficient\\nby doing what's referred to as nesting them.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4409080\",\"duration\":190,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Nesting functions\",\"fileName\":\"2501656_en_US_07_08_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to identify how to nest functions.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5448244,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now let's take a look\\nat another useful aspect of functions.\\nWhat I'm referring to is nesting of functions.\\nA nested function is a function that's contained\\nwithin another function.\\nNow if you recall from our last example\\nwhich is still in front of us here,\\nwe used a number of different aggregate functions\\nto perform different calculations.\\nAnd if you take a look at the average function,\\nwhen we look at the result for this,\\nwe do see that we have quite a number of digits\\nafter our decimal place.\\n\\nNow this is a great candidate to employ nesting,\\nand we could clean this actual number up\\nby nesting our average function.\\nNow before we get into the nesting,\\nwe want to nest this in yet another function.\\nAn appropriate function here is a new function\\ncalled the round function,\\nand what the round function does\\nis that it returns a floating point value\\nrounded to the number of digits that you specify.\\n\\nSo let's go ahead and employ this new function\\nand nest our current average function.\\nTo start doing this, I'm going to create some space\\nin our average function line here\\nand introduce this round function.\\nI'm going to say open parenthesis,\\nand all that I'm going to do\\nis go completely past our current average function,\\nand now I'm going to say a comma at this point.\\n\\nAnd then I'm going to apply how many digits\\ndo I want to round this value to?\\nThen I'm going to close off this function.\\nNow before running this, let's take a look here.\\nWe have our original average function\\nand now we have our newly added round function\\nwhich actually surrounds it.\\nAnd this round function expects two parameters\\nwhich is the value to be rounded\\nwhich is our average value now\\ncurrently with many digits after the decimal point.\\n\\nAnd now we are going to limit it to just two decimal places\\nwith the use of this second argument in our round function.\\nNow let's go ahead and run our statement here.\\nNow we went from a very long average sales number to 8.06\\nwhich is actually a lot more presentable\\nand helps us emphasize the use of nesting\\nand shows a great example where you can employ this\\nin your SQL result.\\n\\nNow when we present this report back to management\\nat WSDA Music, they're a lot more pleased\\nand there are no more questions coming back\\nas you've supplied this report\\nin a quite efficient format.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:664786f7498eb93efc27e607\",\"duration\":480,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Customer postal code transformation\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1031874\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:3903001\",\"duration\":192,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Customer postal code transformation\",\"fileName\":\"2501656_en_US_07_10_FY24Q4_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"updateType\":\"ADDED\",\"editingNotes\":\"#EDITING New movie - CP Solution\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":276,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":null,\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5054596,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] In this challenge,\\nyou've been tasked by the management at Red30 Tech\\nto assist them in updating their customer database\\nby means of a query, which gives the customer's full name,\\nand that is their first and last name,\\ntogether with their postal code,\\nwhich is standardized in a five-digit format.\\nSo let's take a look at how we would respond\\nto such a request.\\n\\nStarting with the From clause,\\nwe have specified the customer table\\nand aliased this table with a C.\\nWith this table specified,\\nwe can go up to our Select clause\\nand include the fields of this table\\nthat we're interested in seeing.\\nAnd those fields are the first name,\\nfollowed by a double pipe symbol,\\nfollowed by a pair of single quotes.\\nNow, the double pipe effectively concatenates\\nthe first name field to our pair of single quotes,\\nand this is then followed by another pair of double pipes,\\nwhich, again, is concatenating our last name field,\\nwhich follows this pair of double pipes.\\n\\nThis is all aliased with the name CustomerFullName.\\nNow moving on to the second field,\\nwhich is our standardized postal code.\\nTo create our standardized postal code,\\nwe employ the *subs string function,\\nwhich we are effectively trimming\\nor creating a *sub string\\nout of the postal code field.\\nAnd in our case, we're specifying that we want\\nto start from position one\\nand go to position number five,\\neffectively giving us a five-digit postal code\\nfrom this field.\\n\\nNext, we go to our Where clause,\\nwhere we've specified our country as USA.\\nAnd that is to say we are filtering our results\\nto only the country USA.\\nFinally, we go down to our Order By clause,\\nand here we are ordering our results\\nby customer full name.\\nIn our Order By clause, we have not specified *ASC,\\nbut without it specified,\\nit defaults to a ascending order sort.\\n\\nWith this query now complete,\\nlet's hit Test my code and observe the result.\\nUnder our console output, we can see\\nthat we have effectively concatenated our customer first\\nand last name with a space in between,\\nAlice Johnson, Ava Clark, et cetera,\\nand we have also included\\nthe standardized five-digit postal code,\\neffectively responding to management's request.\\n\\n\"}],\"name\":\"7. SQL Functions\",\"size\":57482611,\"urn\":\"urn:li:learningContentChapter:4414018\"},{\"duration\":1285,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4411046\",\"duration\":57,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Grouping your query results\",\"fileName\":\"2501656_en_US_08_01_MM30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about grouping, its significance in DA, and how to apply it with SQL. It is crucial to recognize how to perform grouping on SQL queries you write in order to effectively answer common questions about data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2473312,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Here's the situation.\\nWe need to get a list of the average amount customers spend\\nby billing Citi.\\nManagement at WSDA Music needs to know\\nwhich cities have the best sales.\\nThis kind of information is useful\\nwhen planning their marketing budget and knowing\\nwhich cities to target their advertising dollars on.\\nIn this chapter, I'll introduce the Group By clause,\\nwhich comes in handy for requests like this.\\n\\nThe Group By clause is often used\\nwith aggregate functions like Count, Max, Min, and Average.\\nFor example, our current request by WSDA Music Management\\nto list the average amount customers spend by billing Citi.\\nLet's take a look at how we would use the Group By clause\\nto answer this question.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4413016\",\"duration\":348,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filtering with a grouped condition\",\"fileName\":\"2501656_en_US_08_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how aggregates apply to groups of data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9659197,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that we've taken a look\\nat some of the most commonly used aggregate functions,\\nnow is a great time to mention\\nthat a useful feature of aggregate functions\\nis their ability to calculate subtotals\\nor aggregates for different groups of data.\\nNow, looking at the invoice table\\nof our WSDA Music database,\\nwe know that we can get the average amount\\nfor an invoice very easily\\nwith the use of a AVG function.\\n\\nNow, let's say that we had a new request\\nfrom the WSDA Music Company management\\nand they have asked us to calculate\\nthe average invoice amount by BillingCity.\\nNow, let's take a look at how we would respond\\nto such a request with SQL.\\nAs we usually do, let's start with the FROM clause.\\nAnd we are interested in invoice amounts\\nso let's go to the Invoice table.\\nNow, above the FROM, I'm going to include our SELECT.\\n\\nAnd now, let's include the fields that we want to display.\\nSo that would be, if we take a look at our question,\\nwhat are the average invoice totals by city?\\nSo let's include the BillingCity.\\nNow, we would also want to include\\nthe average invoice totals\\nso we know that we can include our AVG function.\\nAnd let's include the column\\nor the field that we'd like to apply the AVG function to.\\nIn this case, it's the total.\\n\\nNow, this seems to respond to this question\\nwhen we take a look,\\nwhat are the average invoice totals by city?\\nSo we've included the BillingCity,\\nas well as the average total by means of our AVG function.\\nLet's now run this and observe the result.\\nOkay, when we take a look at this result here,\\nsomething seems not quite right.\\nNow, we wanted to view the average invoice amount\\nfor each individual city in our invoice table.\\n\\nEven though we included the BillingCity\\nin our SELECT statement,\\nthe query is still only giving us the global average\\nfor all invoices.\\nNow, why isn't our query returning the average total\\nfor every city in our invoice table?\\nTo solve this puzzle,\\nlet's take a look at what we are really asking\\nthe SQL Browser to do.\\nThe question we were asked was,\\nwhat is the average invoice total by city?\\nNow, if we take a look at our query here,\\nwe ask the SQL Browser for two items in the invoice table.\\n\\nFirst, we asked the Browser to list every city\\nthat's in the BillingCity field.\\nNext, we asked the Browser to give us the average\\nof the total field.\\nThe first request would produce\\nmultiple lines for each city,\\nand the second request would give us a single line answer,\\nthat is the average amount.\\nIn other words, we're asking the Browser\\nto display both an aggregate and a nonaggregate field\\nat the same time.\\n\\nNow, we didn't get the information we wanted\\nbecause we didn't quite construct our query the right way.\\nWe can fix this issue\\nby including the GROUP BY clause to our query.\\nLet's do that now.\\nSo the GROUP BY clause always follows the FROM.\\nSo I'm going to include GROUP BY.\\nAnd we want to group by the field or the column\\nthat is not aggregated.\\n\\nSo if we take a look in our SELECT clause here,\\nwe have an aggregated value, the total,\\nand we also have a nonaggregated, the BillingCity.\\nAnd the rule of thumb here,\\nwhen we're including a GROUP BY clause here,\\nwe want to apply the GROUP BY clause\\nto the nonaggregated field,\\nwhich in this case is BillingCity.\\nOne more thing we'll do\\nis include the ordering of our results.\\n\\nSo let's ORDER BY the same field, which is BillingCity.\\nAnd we're going to order in ascending order,\\nand that is a default so we don't have to specify that.\\nNow, let's take a look at running our updated query\\nand observe the result.\\nNow we have 54 rows.\\nAnd if we take a look,\\nwe have an average amount for every single city.\\nIf I scroll down here, our city is sorted.\\n\\nAnd we can now tell\\nevery single city's average invoice amount,\\nwhich is indeed what we have been asked by WSDA management.\\nOne more improvement we can make to our result\\nis nesting our AVG function in a ROUND function\\nas we did before.\\nSo let's put round open parenthesis.\\nAnd after this AVG function,\\nyou're going to put a comma and the value we want to round to,\\nwhich is two decimal places.\\n\\nAnd now, let's rerun our query with our nested function.\\nAnd we have a nicely formatted average amount\\nfor each billing city.\\nSo now, we can return to WSDA Music management\\nwith our updated report\\nand present the answer to our request.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4413017\",\"duration\":127,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Grouping with the WHERE clause\",\"fileName\":\"2501656_en_US_08_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about grouping with the WHERE clause.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3771305,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now it is possible to add criteria\\nto queries that we've applied grouping to.\\nAdding criteria to a group query works in the same way\\nas with other queries that we've seen before.\\nUsing the Where clause allows us to add new criteria.\\nSo let's consider a alteration\\nof our original request by WSDA Music Management.\\nThey have now asked us what are the average invoice totals\\nby city for only the cities that start with L.\\n\\nNow we can achieve the result of this request\\nby making some alterations to our existing SQL statement.\\nLet's go ahead and add a Where clause to our query.\\nAnd the Where clause always comes\\nafter the From clause and before the Group By.\\nSo let's go ahead and add Where\\nand now we are interested in a subset of this data.\\nWe are now only interested in the billing cities\\nthat start with L.\\n\\nSo let's include BillingCity\\nand we just want to perform a partial search.\\nSo to do this,\\nwe include the Like keyword\\nand we are performing a search\\non a text data type,\\nwhich is the BillingCity\\nwhich holds the city names,\\nwhich are held in text.\\nTo do that,\\nwe also need to include our single quotes\\nand we want a partial search\\nof all the cities that start with L.\\nSo we put an L between our single quotes\\nand we put our wild card character,\\nwhich says, \\\"I don't care what comes after the L.\\\"\\nNow with this included,\\nlet's observe the result of our query.\\n\\nWe're starting off with 54 rows.\\nI'm going to execute this query.\\nAnd we've gone down to 3 rows\\nand we have only the Billing Cities\\nthat start with L\\nand their associated average invoice amount.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4410119\",\"duration\":257,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Grouping with the HAVING clause\",\"fileName\":\"2501656_en_US_08_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about grouping with the HAVING clause.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7698273,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now, in our last example,\\nwe were able to include the WHERE clause\\nand apply filtering to our non-aggregated field.\\nIn other words, you were able to filter down\\nto only the cities that started with L\\nby way of our WHERE clause\\nwhich was applied to the billing city\\nwhich in our example here is the non-aggregated field.\\nNow, at the same time,\\nthere are times where we will need to use criteria\\non fields that have been aggregated,\\nfor instance, like our average total here.\\n\\nSo let's say we have a new twist\\nin the request from WSDA Music Management.\\nThey have now asked us to find all average totals\\nthat are greater than $5.\\nHow would we go about doing this?\\nBut first, let's update our question.\\nWe want all average invoice totals greater than five bucks\\nand let's include our dollar sign there.\\nOkay, what are the average invoice totals\\nthat are greater than $5?\\nNow, this is just an alteration of our present WHERE clause,\\nwhich is presently searching for only the billing cities\\nthat begin with L.\\n\\nSo let's remove this current criteria\\nand instead include the new criteria\\nwhich is the average total or the average invoice amount\\nand we want only the ones that are greater than $5.\\nNow let's observe our query result with this update.\\nWhat has happened?\\nWe have this new error message being displayed to us.\\nWe have no result.\\nWe have a lot of red under the lines\\nthat we've placed here in our query editor.\\n\\nAnd if we look at our messages window,\\nwe do see a clear error.\\n\\\"Execution finished with errors. Misuse of aggregate.\\\"\\nAnd it specifies the average aggregate function.\\nSo this error message tells us, at least in this case,\\nthat we cannot use the WHERE clause\\nto create a condition based on an aggregate function.\\nNow, if we want additional filtering\\nbased on the aggregate function,\\nwe do need to include a secondary filtering clause\\nand this clause is referred to as the HAVING clause.\\n\\nNow, the HAVING clause\\nalways comes after the GROUP BY clause.\\nSo let's now modify our query to reflect this.\\nSo instead of having the WHERE clause,\\nand let me just create some space\\nso that we see things a little more clearly,\\nlet's now remove the WHERE clause altogether.\\nAnd instead, we will include the HAVING clause.\\nSo one thing to keep in mind\\nis that the HAVING clause always comes after the GROUP BY.\\n\\nSo I'm going to close the gap of the GROUP BY,\\nI'm going to create some space under it\\nbetween the GROUP BY and the ORDER BY clause,\\nand now I'm going to include the keyword HAVING.\\nAnd now I'm going to include exactly what we had\\nin our WHERE clause attempting to get our new filter,\\nwhich in this case is the aggregate average\\nof the invoice amount, which is the total.\\nAnd we want only those values that were greater than $5.\\n\\nSo here we have our updated statement.\\nA little tough to see,\\nbut let's run it now and observe the result.\\nWe now have a new result here. No errors.\\nAnd if we take a look at the average amount\\nbeing displayed here, it's all over the $5.\\nNow let me close the gaps in our SQL statement\\nand take a look at the full SQL statement.\\nSo we have now a SELECT clause.\\nWe do have our FROM clause,\\nbut we do have our grouping as well.\\n\\nAnd our new clause called the HAVING clause is now included\\nand we have now our aggregate function\\nin this particular clause.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4400955\",\"duration\":169,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Grouping with the WHERE and HAVING clause\",\"fileName\":\"2501656_en_US_08_05_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to recognize the distinction between HAVING vs. WHERE, when to use each, and the associated syntax.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5150785,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Now a simple way of explaining\\nthe difference between the WHERE\\nversus the HAVING clause\\nis the WHERE clause is\\nfor filtering non aggregate data\\nand the HAVING is for filtering results\\nthat contain aggregates.\\nNow, a more detailed way\\nto describe this difference is\\nthat two types of filtering occur\\nwhen both a WHERE\\nand a HAVING clause are included in the same query.\\nThe WHERE clause tells the query\\nwhat information to include from the table.\\n\\nThen once the information is filtered\\nand aggregate functions are applied to the fields\\nthe HAVING clause acts as a further filter\\nfor the aggregated data.\\nNow, let's take a look at this\\nand add a little twist to our current request\\nfrom WSDA Music management.\\nThey would now want the exact result\\nthat we have here but only for the cities\\nthat start with B.\\n\\nSo let's alter our question here.\\nWhat are the average invoice totals greater than $5\\nfor\\ncities\\nstarting with B?\\nNow we already have our query\\nfiltering for\\nthe average totals that are greater than five.\\nNow we need to include the filter\\nfor only cities that start with B.\\nSo first let's go ahead and include the WHERE clause\\nwhich always comes after the FROM\\nand before the GROUP BY.\\n\\nSo I'm going to say WHERE\\nand now\\nI want where\\nthe billing cities that start with B.\\nSo I'm going to say billing city\\nand I want to include my LIKE\\nit's a text field.\\nI'm including my single quotes\\nand I want the ones that start with B.\\nI include my wild card\\nbecause I don't care what comes after B.\\nI do care that it starts with a B.\\nNow, let's now run our query.\\nWe now have updated results eight rows\\nand as we can see all our billing cities start with B\\nand all of our average invoice amounts\\nare greater than five.\\n\\nNow, in this query we just performed\\nwe have done the same task as we did before\\nbut this time we added\\nthe WHERE clause to filter results\\nto only the cities that start with B.\\nThis filtering step is performed\\nbefore the HAVING and ORDER BY clauses are processed.\\nSince we must filter before we can group the order\\nof these filtering clauses is important\\nand the WHERE is most always coming before the HAVING.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4400956\",\"duration\":178,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Grouping by many fields\",\"fileName\":\"2501656_en_US_08_06_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to apply multiple levels of grouping to your SQL query.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5459322,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's wrap up our look at aggregate functions\\nused in conjunction with the group by clause\\nand take a look at one more scenario.\\nNow it is possible to group by more than one aggregate field\\nat a time.\\nLet's say we wanted a more detailed breakdown\\nof our average invoices.\\nWe could write our query\\nso that our aggregate data is grouped first by country\\nthen by city.\\nSo let's take a look at modifying our existing SQL statement\\nto respond to WSDA Music's latest request.\\n\\nAnd here we have it.\\nWhat are the average invoice totals\\nby billing country and city?\\nAs we have our query set up now\\nwe are responding to a grouped by billing city result.\\nThe current request\\nalso wants us to include the billing country.\\nSo let's do that.\\nIn our select\\nwe're going to also include the billing country.\\nLet's put our comma.\\n\\nAnd now, because we now went from one non aggregated field\\nto two, we must now include\\nboth of these non aggregated fields in our group by clause.\\nSo let's go ahead and include billing country.\\nAnd we are going to separate the two fields with a comma\\nand include the billing city which has already been there.\\nWith this alteration\\nlet's go ahead and now run our statement.\\n\\nNow, if we take a look at our result\\nwe now have some more detailed information.\\nWe now have our billing city\\nas well as billing country\\nand the associated average amounts.\\nLet's make one slight alteration to our existing query\\nbecause we are starting with countries\\nand we'd like to see what cities fall in between them,\\nlet's order our results by billing country\\ninstead of billing city.\\nNow let's rerun our result, and here we have it.\\n\\nWhat we're going to see here is categories of information.\\nSo where we see Brazil appearing many times,\\nif we look over in the city,\\nthere are the various cities within Brazil.\\nAnd over here we have the associated average invoice amount\\nper city.\\nSo we have a much more granular look into our data.\\nWe could see the same for Canada.\\nWe have the various cities within Canada being displayed\\nalong with their associated average invoice amounts.\\n\\nSo by doing this simple alteration\\nto our existing SQL statement,\\nwe have now given ourselves\\nan even deeper level of analysis.\\nAnd WSDA Music Management is thrilled\\nby the level of detail\\nthat they can now conduct some proper research.\\nWell done.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:66478714345007be26491530\",\"duration\":600,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge:  Calculate average spend per city\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1031875\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:3900218\",\"duration\":149,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Calculate Average spend per city\",\"fileName\":\"2501656_en_US_08_07_FY24Q4_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"updateType\":\"ADDED\",\"editingNotes\":\"#EDITING New movie - CP Solution\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":180,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":null,\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3663091,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] In this challenge,\\nyou've been asked to calculate\\nthe average spending amount of customers in each city.\\nLet's now take a look at our query for responding to this.\\nSo starting with our From clause,\\nwe're interested in invoice data.\\nSo we've specified the invoice table\\nand alias this table with the letter I.\\n\\nWith this table specified, we can go up to our select clause\\nand tap into the fields\\nof this table we're interested in seeing.\\nAnd those fields are billing city, which we've alias\\nwith the word city.\\nAnd the other field we're interested in seeing\\nis the average spend.\\nNow we've employed a couple of functions\\nto arrive at our average spend,\\nand that is first, the average function,\\nwhich is being applied to our total field.\\n\\nAnd with this average applied,\\nwe're then nesting that average of function\\ninside of a round function, which is effectively going\\nto round our result to two decimal places.\\nWith this done, let's go down to our group by clause\\nand specify our non-aggregated field.\\nAnd again, if we reference our task,\\nwe're after calculating the average spend amount per city.\\n\\nSo we're grouping by the billing city with our group\\nby applied to our non-aggregated field billing city,\\nwe can go down to our last clause, which is our order\\nby clause, which is specifying that our results be ordered\\nby city in ascending order.\\nWith that done, let's now click on test my code\\nand take a look at our result.\\nWe can see that we have our two columns, a city together\\nwith the average spend,\\nand we have our results rounded to two decimal places.\\n\\nWe can now effectively respond to management\\nwith our result.\\n\"}],\"name\":\"8. Grouping\",\"size\":37875285,\"urn\":\"urn:li:learningContentChapter:4400959\"},{\"duration\":1726,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4411047\",\"duration\":82,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Subqueries and aggregate functions\",\"fileName\":\"2501656_en_US_09_01_MM30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about subqueries, what they are, and why and how to use them. It's critical to recognize the concept of nesting one query inside of another query, resulting in what is called a subquery. Learn how to use subqueries with a variety of SQL keywords you have already learned. Also, become more familiar with the DISTINCT keyword.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3160671,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Have you ever heard of tree hugging?\\nWhile a subquery is kind of like that\\nit's one query hugging another.\\nA subquery is simply one query written inside of another.\\nWhen a query is wrapped inside of another\\nit's called a \\\"Nested query.\\\"\\nThe wrapping is just an open and closed parenthesis\\nor brackets that surround the query.\\nLet's say we were asked by WSDA management\\nhow each individual city\\nwas performing against the global average sales?\\nLet's think about that.\\n\\nTo respond to this request\\nwe would need to display the global average sales amount\\nand at the same time,\\ndisplay the average sales per city,\\nmanagement can then see what cities are on par,\\nover and underperforming.\\nSubqueries are useful in this type of scenario.\\nInstead of writing one query,\\nfor instance to get the global average,\\nthen another query to get the average sales per city,\\nwe can just use a subquery\\nthat performs both operations at once.\\n\\nLet's begin our first look at subqueries\\nby looking at how they are used with aggregate functions.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4408114\",\"duration\":329,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"SELECT clause subquery\",\"fileName\":\"2501656_en_US_09_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to apply subqueries to a SELECT statement and what situations would warrant its use.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9768943,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] To demonstrate the use of subqueries,\\nlet's begin with a simple SELECT statement.\\nThis is the same SELECT statement\\nwe used in our last chapter\\nwhere we found the average invoice total\\nin the invoice table.\\nWe can see that the average invoice value is $8.06.\\nNow, let's say that we were asked by WSDA Music Management\\nto gather data about all invoices\\nthat were lower than this average.\\nWell, below here, I've started a partially constructed\\nSQL statement that is going to attempt\\nto respond to this question.\\n\\nNow, we would need a SELECT statement\\nthat displays some of the invoice fields\\nsuch as invoice date, billing address, billing city,\\nand, of course, we need the total.\\nWe would then want to filter our results\\nby comparing them to an aggregate function.\\nWe would want a WHERE clause\\nthat compares the total to the average total.\\nNow, if you recall in our last chapter,\\nwe did learn that attempting a direct comparison\\nwithin the WHERE clause would generate an error.\\n\\nIn fact, if we attempted to put our aggregate average\\nin the WHERE clause,\\nwe saw that we got a misuse of aggregate function error.\\nSo we need to find a way to take our original query,\\nthe one that gave us our average total of $8.06,\\nand insert all of it inside of this new query\\nthat we're attempting to construct.\\nNow, fortunately, in SQL,\\nthere is an easy way to accomplish this\\nand it is by use of what's referred to as a subquery.\\n\\nNow, let's continue to build out our statement here\\nand include a subquery.\\nSo the last piece we need to include here is our filtering\\nand we want to say where the total\\nis less than the average total amount.\\nTo do this, let's first start by including a pair\\nof open and close parentheses in our WHERE clause.\\nNow, we're just simply going to repeat the statement\\nor the SQL query that has produced this particular result.\\n\\nAnd if you recall, this is our query.\\nSo in other words,\\nwe're simply going to include this query inside of this one.\\nLet's do this.\\nWe're going to say select and we want the average total\\nand this is going to be from our invoice table.\\nNow, by doing this, we're saying we want the average total\\nfrom the invoice table.\\n\\nIn other words,\\nthe exact same result from our original statement.\\nIn fact, I'm going to just highlight this portion of our query\\nand rerun this.\\nWithout the round, we still have our $8.06.\\nNow, if I run our entire query here,\\nlet's observe the result.\\nNow we have our query producing exactly what we want.\\nWe have the results displaying\\nall of the invoice information\\nwhere that invoice total is less than the average amount,\\nwhich we know to be $8.06.\\n\\nIf we take a look at all of our totals here,\\nit is actually less than that $8.06.\\nWe have sorted in descending order by our total,\\nwhich means the highest value appears first,\\nand it does indeed confirm that this is below the $8.06\\nSo this full query is what we needed to respond effectively\\nto the latest request by WSDA Management.\\n\\nAnd we can respond and say, \\\"Here are all of the invoices\\nthat are less than the average amount.\\\"\\nNow let's just take a look at a diagram\\nthat's going to reinforce the components of the subquery\\nso that we are fully understanding how it's laid out.\\nNow, here we have our subquery and various parts of it\\nthat it's referred to as.\\nNow, the inner part of the subquery\\nis simply the part that is between the parentheses.\\n\\nSo when we created our WHERE clause\\nand then said total less than\\nand these open and close parentheses,\\nthis particular area as indicated here\\nis referred to as the inner query.\\nAnd again, that's simply the part\\nthat's inside of our parentheses.\\nEvery other part of the query,\\nand that is this area as well as this area,\\nis referred to as the outer query.\\nSo in other words,\\nthe original query that we started off with\\nis referred to as the outer query\\nand the part that goes inside of the parentheses\\nis referred to as the inner query.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4411048\",\"duration\":309,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Aggregated subqueries\",\"fileName\":\"2501656_en_US_09_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to filter your query results using subqueries within the WHERE clause.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8860555,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now if you recall\\nin our last chapter on functions,\\nwe used the group by statement\\nto show the average subtotals of invoices by city.\\nNow, what if we had a new ask\\nabout how each individual city was performing\\nagainst the global average sales?\\nNow, one way to answer this question\\nwould be to write a query that was able to display\\nthe average sales of each city right next\\nto the global average.\\n\\nThe query we write to display the average sales\\nby billing city is identical\\nto the one we wrote previously, with one exception.\\nWe also need to include a subquery in the select clause\\nto calculate the global average.\\nThis way, we can compare the two values.\\nLet's go ahead and build out our query.\\nAs we usually do,\\nlet's start with the from clause.\\nAnd we want the invoice table.\\n\\nI'm going to go above the from and include our select.\\nAnd let's now include the fields we want to see.\\nWe want to see the billing city.\\nAnd now that I have included the billing city,\\nlet me go ahead and introduce the group by clause,\\nwhich always comes after the from, group by.\\nAnd let's group by that billing city,\\nseeing that we're not going to aggregate this field.\\n\\nThis order by billing city as well.\\nAnd now, I'm going to return back up\\nto our select statement here,\\nor select clause and include the rest of our query.\\nSo, so far, we would like to display the billing city,\\nas well as the average for each billing city.\\nSo let's go ahead and introduce our aggregate function here,\\nwhich is the average.\\n\\nAnd we want to see the average total.\\nOkay, so this takes care of some of the ask.\\nSo we want to display the average total\\nfor each billing city.\\nHowever, the last twist is we want to see\\nhow each average is performing against\\nthe global average sales.\\nSo now, we need to include the global average sale\\nas part of our result.\\n\\nSo let's include one more column here.\\nI'm going to put a comma,\\nand let's include our subquery inside of our select.\\nNow, as we said before with subqueries,\\nwe're going to start with an open and closed parenthesis.\\nAnd in between that comes a full select statement.\\nAnd in this case, we want the average.\\nSo let's go ahead and say select the average,\\nand that's going to be the total from the invoice table.\\n\\nLet's take a look at our result.\\nWe now have our billing city.\\nWe now have an aggregated column,\\nwhich is displaying our average,\\nand now we have a subquery\\nwhich also has an aggregate function,\\nwhich is displaying our global average.\\nLet's run this query.\\nOkay.\\nIf we take a look, we now see that we have some values\\ndisplaying for our average,\\nand some other values displaying for our global average.\\n\\nNow, let's label this a little better\\nso we can be a little cleaner in our result.\\nSo let's call our average total the city average.\\nAnd I'm going to say as.\\nAnd for our subquery, let's also alias this\\nby calling it the global average.\\nNow, let's rerun our statement.\\nAnd we have a little cleaner titles,\\nand makes a little more sense to read this now.\\n\\nSo we have each city,\\nand we have the city average that is for each city.\\nNow, because we wanted to compare the global average\\nagainst each city, we have the global average being repeated\\nso that we can compare it side by side\\nwith each city's average.\\nAnd this has been achieved by including a subquery inside\\nof our select statement as we're doing here.\\n\\nAnd this effectively responds to the latest inquiry\\nby WSDA Music Management,\\nwhich is how is each individual city performing\\nagainst the global average sale?\\nAnd here, we have the response to this inquiry.\\nWell done.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4400957\",\"duration\":195,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Non-aggregate subqueries\",\"fileName\":\"2501656_en_US_09_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to apply a subquery in scenarios that don't require an aggregate calculation.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5466298,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] A subquery does not always contain\\nan aggregate function as we've been showing previously.\\nIf we take a look at the following SQL statement,\\nwe will see that the query shows the transaction date\\nfor a specific transaction,\\nand that date is January 9th of 2012.\\nNow if we wanted to see\\nif there were any other invoices that were received\\nafter the invoice date reference here,\\nwe would build a subquery\\nwrapped in a pair of parenthesis\\nand then build an outer query around it.\\n\\nLet's go ahead and do that.\\nTo make this a subquery or the inner part of the query,\\nlet's go ahead and put some parentheses at the start\\nand all the way at the end of this.\\nNow let's go ahead and build out the rest of our query.\\nI'm going to start above here,\\nand let's go ahead and say from, and we'll say invoice,\\nand let's include what we want to select here.\\n\\nLet's select a few fields from the invoice table.\\nSo going above the from and putting our select,\\nlet's include the fields invoice date.\\nLet's also include billing address,\\nand let's include the billing city.\\nOkay, we have our three fields included.\\nNow, let's say what we want to filter down to.\\nAnd if we recall, we would like to get all of the invoices\\nthat occurred after this particular invoice date.\\n\\nSo now we want to say where, and we'll say the invoice date\\nis after meaning greater than,\\nso we put our greater than sign,\\nthis particular value, which we know is January 9th, 2012,\\nbut we've surrounded our subquery\\nwith open and closed parentheses.\\nAnd now let's go ahead and run our query.\\nBefore doing so, just taking a look at the full thing,\\nall we've done is wrapped our original query\\nwhich gave us our date of January 9th, 2012\\nin a pair of open and closed parenthesis.\\n\\nThen we built out our outer query\\nto include the where clause,\\nand we are saying where the invoice date\\nis greater than this particular value,\\nwhich we already know.\\nNow let's run our query.\\nNow, we've effectively responded to the question of,\\ngive us all the invoices that have occurred\\nafter this particular date,\\nand we've done so by including a subquery\\nin our where clause and pointing out that this subquery\\nis not aggregated in any way,\\nbut it has been effectively used\\nto respond to this particular request.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4414014\",\"duration\":229,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"IN clause subquery\",\"fileName\":\"2501656_en_US_09_05_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to apply the subquery as a solution for queries that return multiple values.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6484384,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Up to this point,\\nwe've only used subqueries to calculate a singular value,\\nwhich is then passed to the outer query.\\nIt is possible to use subqueries\\nthat return multiple records.\\nNow, let's say that our WSDA Music Management team\\nis interested in three particular invoices.\\nTo select these individual invoices,\\nlet's consider our present SQL statement here.\\n\\nWe've used a SQL statement\\nthat is going to employ the IN operator, or the IN clause,\\nto select three specific invoices,\\nand our result is showing the invoice date\\nthat is associated with these three invoices.\\nNow, let's say that we are asked\\nif any purchases were made on these particular days.\\nIf we want to select all invoices for those three days,\\nwe can either start a brand new query,\\nor we can just use our existing query here as a subquery.\\n\\nLet's go ahead and do that.\\nNow, to create a subquery, as we said,\\nlet's put our existing query in a pair of parenthesis.\\nOpen at the start, and close at the end.\\nWith this step taken,\\nlet's now start to build out our outer query.\\nLet's start with the FROM,\\nand we want to say from the invoice table,\\nand let's select a few fields.\\nLet's go above that FROM,\\nand let's select the invoice date.\\n\\nLet's also select the billing address.\\nAnd last but not least, let's select the billing city.\\nNow we need to just include a WHERE clause,\\nto say where the invoices\\nare matching the invoices that we've already selected\\nfrom our previous statement,\\nwhich we've now wrapped\\nin a pair of open and closed parentheses,\\nthereby putting it into the inner part of this subquery.\\n\\nNow, let's include our WHERE clause,\\nand we want to say WHERE, the invoice date,\\nand we want to now include, yet another IN clause.\\nSo we're going to say invoice date, IN.\\nAnd now, we're just simply\\ngoing to include our subquery here.\\nNow, we know that this particular subquery,\\nor now that it's the inner part of our subquery,\\nis producing three records.\\n\\nSo it is producing multiple rows of records.\\nBecause we are anticipating multiple rows\\nin our outer query,\\nwe've now equipped it with a WHERE clause,\\nas well as the IN operator,\\nwhich as we know, can handle multiple records.\\nSo let's now go ahead and run our query.\\nNow here, we have completely responded to the request,\\nand we're able to supply WSDA Music Management\\nwith the result of this query.\\n\\nAnd it now responds effectively to them,\\ntelling them if any other purchases were made\\non those three original dates.\\nNow, this technique of turning an existing query\\ninto a subquery,\\nis pretty useful when you're dialing in on your data.\\nThis method actually allows you to reuse an existing query,\\nand modify it further\\nto narrow down your search, as we have done here.\\nGreat job.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4414015\",\"duration\":389,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"DISTINCT clause subquery\",\"fileName\":\"2501656_en_US_09_06_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about the impact of using the DISTINCT keyword and how it can be used in conjunction with a subquery.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11171103,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] As we've seen from other examples\\nthat we've done so far in this chapter,\\nsub queries are very helpful, for scenarios\\nwhere you want to view or compare a query\\nby a condition that requires its own query to calculate.\\nAs we learned prior, there is usually\\none unique field in every table,\\nknown as the primary key, which contains\\na unique number for every record.\\nHowever, the other fields in a table\\nmay contain redundant information.\\n\\nTo work better with redundant information,\\nit's often convenient to filter this data,\\nso that it only displays unique or distinct values.\\nNow this is where the distinct keyword comes in.\\nTo demonstrate sub queries and the distinct keyword,\\nlet's take a look at the invoice line table.\\nThe invoice line table shows us which individual\\nWSDA music tracks were purchased on each invoice.\\n\\nNow, let's take a look at this query\\nin front of us here, which is selecting\\njust the track ids from our invoice line table.\\nWhen we run this query and we take a look\\nat some of our results, for instance track number two,\\nwe do see that this track is duplicated.\\nNow if we take a look a little lower down,\\nwe also see that this happens with other tracks,\\nlike track number eight.\\n\\nAnd if we take a close look here,\\nwe don't see any value for track number seven.\\nWe see it goes from six through eight,\\nso we can infer that no one in our record\\nhas purchased track number seven.\\nNow our WSDA Music Management team\\nis interested in discovering which tracks are not selling.\\nNow to do retake, to respond to this request,\\nwe would need to find a table that links\\nthe track id with the invoice id.\\n\\nNow we can use a sub query to list\\nall the tracks by composer and name,\\nthat don't appear in the invoice line table.\\nNow, let's include our distinct keyword\\nand modify our existing query here,\\nto see the impact that it has on our result.\\nNow I'm going to just include the distinct keyword\\nafter the select, and let's take a look at the result.\\n\\nBefore running this query, let's look at\\nour current record count, which is 2,240 records.\\nAnd again, as we recall, we did spot check\\na couple records, and saw that we do have\\nduplicate numbers appearing for them.\\nNow, let's run this query.\\nOur updated query with the distinct keyword\\nhas shrunk our record countdown to 1,984 rows.\\nAnd if we take a look at record number two\\nand record number eight,\\nthey're now both appearing just one time.\\n\\nWe don't see record number seven still.\\nOkay, so now we need a query that's going to list\\nall tracks from our track table,\\nthat are not in this particular list.\\nSo, let's go ahead and modify our query to do this.\\nWhat we are actually doing here\\nis needing to first create an inner query\\nfrom our existing query here, and let's do that\\nby wrapping it in open and closed parentheses.\\n\\nWith that done, we could now go ahead\\nand start building out our outer query.\\nSo let's say from on this time, we want the tracks table,\\nand let's include our where clause.\\nAnd we want to say where track id.\\nAnd this time, instead of saying in,\\nwe want the tracks that are not selling\\nmeaning the tracks that do not belong to this list.\\n\\nSo we're going to say not in the result of our inner query.\\nOkay, now let's complete the build out\\nof our outer query and include the select.\\nAnd we want to say, select the track id.\\nLet's also select the composer.\\nAnd last but not least, let's include the name.\\nOkay, now let's look at our full query here.\\n\\nAnd what we've done is, as we've done before,\\nbut this time we're now including\\nnot in this particular result.\\nAs we saw, we still have our result,\\nwhich is a distinct list of all of the tracks\\nthat we have in our database.\\nNow we're simply responding to WSDA music's\\nlatest managerial request, which is to say,\\nwhich tracks are not selling?\\nSo, by composing our query like this,\\nby saying not in this list, we'll get\\nthe lists of tracks that are not selling.\\n\\nLet's go ahead and run our query.\\nNow, our query has produced a list.\\nAnd, as we can see, we have gone to 1,519 rows.\\nAnd if we take a look at the first record\\nin our list, we do see that record number seven,\\ntrack ID seven that is appearing in this list,\\nas it was not in our previous list.\\nAnd this is a sign that confirms to us\\nthat we are indeed displaying\\na list of tracks that are not selling.\\n\\nThis is exactly the request\\nthat has come from WSDA Music Management.\\nAnd now we can supply this list to them,\\nwhereby they can now in turn make some\\ninformed, data-driven decisions.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:66478732498e2bc18d94999c\",\"duration\":600,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge:  Uncovering unpopular tracks\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1031876\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:3895206\",\"duration\":193,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Uncovering unpopular tracks\",\"fileName\":\"2501656_en_US_09_08_FY24Q4_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"updateType\":\"ADDED\",\"editingNotes\":\"#EDITING New movie - CP Solution\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":255,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":null,\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5343615,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(lively music)\\n- [Instructor] In this challenge,\\nyou are asked by the management\\nof Red30 Tech to help them identify tracks\\nthat have not been selling.\\nSo let's take a look at how we would respond\\nto this request.\\nFirst, starting with our comment block.\\nWithin our block, we've specified the query's author,\\nthe query's create date,\\nas well as a short description of what it's doing.\\n\\nWith this done, we can go down to our FROM clause\\nand here we've specified the track table,\\naliased with a t, joined to our genre table,\\naliased with a g via the ON keyword,\\nwhich is specifying the GenreId in both of these tables\\nas the common field between the two.\\nWith our FROM clause complete,\\nwe can go up to our SELECT clause\\nand include the fields from these tables that we want\\nto see in our results.\\n\\nAnd these fields are the TrackId field\\nfrom the track table,\\nthe name field as well from the track table.\\nAnd these fields are aliased as TrackiD\\nand TrackName respectively.\\nAgain, pointing out\\nthat both these aliases have spaces in them,\\nso we've surrounded them both with double quotes.\\nThe next field that we're interested in seeing\\nfrom the track table is the composer field,\\nand the last field in our result set is going\\nto be the name field from the genre table, aliased as genre.\\n\\nNow, let's go down to our WHERE clause\\nand in here, we have TrackId NOT IN\\nand we have a subquery within our brackets here\\nand that subquery states SELECT DISTINCT InvoiceLine.TrackId\\nFROM InvoiceLine.\\nNow, just referencing our question again, we're asked\\nto identify the tracks that are not selling.\\nWhere is our selling information or data held?\\nAnd that is in our InvoiceLine table.\\n\\nSo by us stating within our WHERE clause TrackId\\nNOT IN the InvoiceLine table,\\nthen we are saying we effectively want the tracks\\nthat are not selling.\\nThat is do not have a record\\nof sale in the InvoiceLine table.\\nLast but not least is our ORDER BY clause\\nwhere we've specified our Track Name field as the field\\nthat we want our results sorted by in ascending order.\\n\\nLet's now hit test my code\\nand take a look at our result.\\nUnder console output,\\nwe can see that we have our four fields specified\\nand we also are identifying the tracks\\nthat are not selling effectively,\\nhelping Red30 Tech's management to optimize their database.\\n\"}],\"name\":\"9. Nesting Queries\",\"size\":50255569,\"urn\":\"urn:li:learningContentChapter:4409082\"},{\"duration\":923,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4408115\",\"duration\":90,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"View introduction\",\"fileName\":\"2501656_en_US_10_01_MM30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how views can significantly enhance your efficiency as a DA. It's crucial to recognize the virtual tables known as views: queries that are saved and can be executed repeatedly as needed or used as subqueries in other SQL statements. They are instrumental in your ability to respond to questions that you are required to answer as data analysts.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4161116,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Imagine if WSDA Music Management\\nasked you to generate a report of all albums,\\ntheir associated tracks, and the artists who sang them.\\nA good approach would be to create a query\\nthat would join the three tables needed to get the data:\\nthe artist, album, and tracks tables.\\nThis is a great approach,\\nbut it's still a relatively complex query\\nthat involves joining three tables.\\nNow, imagine that you had to generate this report\\nevery couple of weeks,\\nwhich means rewriting this complex query\\nagain and again every time.\\n\\nThere's got to be a better way.\\nLuckily, there is, with something called Views in SQL.\\nA view is simply a query that's saved\\nand can be executed repeatedly\\nor referenced by other queries.\\nViews are helpful for a number of reasons,\\nbut convenience is definitely the main one.\\nIf you find yourself\\nhaving to repeatedly write the same query,\\nit may be a case where creating a view is the solution.\\n\\nIn our scenario, a view is the perfect solution\\nfor repeatedly generating a report of all albums,\\ntheir associated tracks, and the artist who sang them.\\nLet's examine some of the ways we can use views\\nas a powerful new addition to our existing SQL knowledge.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4414016\",\"duration\":280,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating a view\",\"fileName\":\"2501656_en_US_10_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to recognize the syntax and how to create a view.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8606441,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] As I just mentioned,\\nif you find yourself repeatedly constructing the same query,\\nparticularly if this query is complex or difficult to write,\\nthen it's worth your while to start looking at views.\\nNow, officially, a view is referred to as a virtual table.\\nAnd all of the SQL statements\\nthat we've been introduced to thus far in our course\\ncan all be saved and reused\\nwhen we create a view out of them.\\n\\nNow, let's take a look at our query in front of us,\\nwhich we visited a few times in the past.\\nWhat this query is doing\\nis just simply getting us the average total amount\\nfor our invoices.\\nWhen we run this query,\\nwe do see that we have a total of $8.06 as we visited prior.\\nWe can turn this particular SQL statement into a view\\nby means of some particular keywords.\\n\\nLet's go ahead and do this now.\\nTo start, let's go ahead and push our query down a little.\\nAnd above the SELECT,\\nwe're going to introduce a new keyword,\\nand that is CREATE.\\nAnd what we want to create is a view.\\nSo we put that keyword as well, CREATE VIEW.\\nNow, we have the two keywords, CREATE VIEW.\\nNext, we need to say what would we like to name this view?\\nNow, it's a usual best practice to name views,\\nstarting with the letter V,\\nthen we place an underscore next to this V.\\n\\nAnd next, we should create a name\\nthat is meaningful to the function of this view.\\nSo in our current example,\\nthis view is giving us the average total,\\nso let's name the view, the AverageTotal.\\nWe'll abbreviate average and say Avg, and Total.\\nNow we have CREATE VIEW, the view name,\\nwhich is written in a best practice manner,\\nV_, and a descriptive name.\\n\\nThen the last key word we want to include is AS.\\nNow with this, we have a complete syntax\\nthat is now going to take our regular query\\nand turn this into a view.\\nLet's go ahead and hit Run.\\nSomething is a little different\\nfrom previous queries we've run.\\nWe don't see a result.\\nBut what we do see,\\nand this is the thing we need to look out for\\nwhen we're creating views,\\nis the confirmation message\\nthat this was actually created as we would expect.\\n\\n\\\"Execution finished without errors.\\nQuery executed successfully.\\\"\\nNow, that indicates that our view was created well.\\nAnd how do we verify this view?\\nWell, if we head over to our Database Structure tab\\nand we take a look under the section titled Views,\\nwe now do see that we have an item there\\nand that is our newly created view, V_AvgTotal.\\nAnd just like any other object\\nin the Database Structure tab, we can expand this view\\nand have a look at the column inside of it.\\n\\nJust like tables, we can head over to the Browse Data tab\\nand have a look at our newly creative view.\\nNow, if I expand this table selection here,\\nwe do see that we have a view now available to us\\nthat we can select.\\nAnd selecting that view displays the contents of it.\\nAnd in this case,\\nwe're actually displaying the average invoice amount\\nthat our view is calculating.\\n\\nNow, one final thing to show about views here.\\nIf we head back over to our Database Structure tab\\nand we go and highlight our newly created view,\\nif we right click this item,\\nwe now see available to us a few different menu choices.\\nWe can browse, modify, and delete a view via this menu.\\nAll right, great.\\nUp next, let's take a look\\nat how we would go about modifying an existing view.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4410120\",\"duration\":239,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Editing a view\",\"fileName\":\"2501656_en_US_10_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to recognize the syntax and how to edit a view.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7514212,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that we have created our first view,\\nV-Average-Total, there are times\\nwhen we would want to make changes\\nto an existing view.\\nNow, this is a good time to point out\\nthat there are indeed some changes\\nbetween relational database management systems.\\nI did mention earlier\\nthat there is a 5% or so difference\\nfrom one RDBMS to another.\\nSo, with regard to the relational database management system\\nwe are using here, which is SQLite,\\nthere is a slight difference\\nto let's say, SQL Server,\\nwhich does have a little bit different way\\nof handling a view.\\n\\nNow, when it comes to the modification of views in SQLite,\\nthere isn't a syntax\\nthat actually modifies the existing view.\\nInstead, the existing view is actually deleted,\\nthen recreated with the modification\\nthat you intend to make.\\nTo do this modification,\\nwe can handle it in a number of different ways.\\nIn the browser here, as we have it,\\nwe can go ahead and start editing our view\\nbut the first thing we must do\\nis drop that view first.\\n\\nSo a good way I handle it,\\nat least in the SQLite environment\\nis to go back to our database structure tab\\nand locate the view that we want to change.\\nAnd if you recall, we did see\\nthat there is a right click functionality\\nwhich gives you a few menu options.\\nSo there is a modified view option.\\nWhen we click this option, we do see a a popup.\\nAnd this is to just give us a bit of a warning\\ntelling us that this action will open a new SQL tab\\nwith the following statement.\\n\\nAnd if you look at the statement\\nthe first thing that it starts\\noff with is dropping the view.\\nLet's take a look at this view\\nin the actual execute SQL tab by clicking okay.\\nThis has created a new tab\\nand let's take a look at the statement.\\nThe only difference with regard to creating a view\\nversus modifying it is this first statement here\\nand this first statement\\nwhat it does is drop the existing view.\\n\\nDrop is another way of saying deleting it.\\nSo the first thing you do is delete\\nand then you modify the existing view.\\nSo let's modify this view\\nand make it a little less accurate.\\nSo let's remove the nesting\\nand I'm going to remove the round function\\nand remove the closing bracket as well\\nas that two and the comma.\\nSo what we remain with is a plain average function\\nwhich as we know would give us many places\\nafter the decimal point.\\n\\nBut just for demonstration purposes to modify our view,\\nlet's go ahead and run this statement.\\nJust as we did before, we look in our messages pain,\\nand we do see a confirmation\\nthat this query has been executed\\nwithout error and it has been executed successfully.\\nWhen we go back to our database structure tab\\nwe take a look at our view.\\nIt looks the same as far as the name, but when\\nwe go over here to see the contents of it, we do see\\nthat that average total is just by itself,\\nnot being surrounded with the round function\\nas it did before.\\n\\nNow we can also go back to our browse data tab,\\ntake a look at our view.\\nAnd this time if we look at the value,\\nit is not rounded to the two decimal places,\\nbut it is now showing our modified value without it.\\nSo this is the way that we would go\\nabout modifying a view in SQLite.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4412035\",\"duration\":190,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Joining views\",\"fileName\":\"2501656_en_US_10_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to create and connect multiple views.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6085803,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Now, as I first mentioned when we first got into\\nthe introduction of views,\\nthe whole point of creating a view is to save us time,\\nparticularly when we are dealing with more complex queries.\\nNow, if you recall on our chapter on subqueries,\\nwe introduced the invoice line table and the tracks table\\ntogether to find out which songs\\nfrom our tracks table had never been ordered.\\n\\nNow, this is a perfect scenario\\nwhere it would've been helpful to have a join\\nthat linked these two tables together.\\nThen we could have simply created a view\\nout of this sort of complex query.\\nWhat I've done in front of us here\\nis created that join that is actually connecting\\nour invoice line to our track table,\\nand the result here we have a pretty useful view\\nof what tracks, what prices, and what invoices\\nall go together.\\n\\nPretty useful but slightly involved when you\\nconsider the join that has to go into creating this query.\\nNow this is a perfect candidate\\nto start creating a view out of.\\nNow let's go ahead and do that.\\nTo do this, we're just going to push our original query\\ndown a bit, and as we did before,\\nwe're going to do a create and then keyword view.\\nThen we want to create a useful name for this,\\nso this is actually joining the tracks to the invoices,\\nso we have a deeper view of what's going on with regard\\nto these two data sets, so let's appropriately call this,\\nstarting with our best practice capitalized v,\\nthen an underscore, then a descriptive name,\\nso tracks, and we'll call this invoice line.\\n\\nSo this name clues us into what's going on within the view,\\nand last but not least, we include the \\\"as\\\" keyword,\\nand by doing this, we're now ready to execute this query\\nand now create a view out of this\\nrelatively complex SQL statement.\\nLet's go ahead and do that.\\nNow we look for the message that we want to see,\\nwhich is that this query was executed\\nsuccessfully without errors.\\n\\nWe go over to our database structure to double check,\\nand there we have our newly created view:\\nV tracks invoice line.\\nWe could go over to our browse tab, as well,\\nand check out our newly created view, as well,\\nand here we have our V tracks invoice line,\\nwe have the actual contents of this view,\\nand we have successfully created a new and useful view,\\nparticularly one that is relatively complex,\\nthe perfect use case for a view.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4414017\",\"duration\":124,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Deleting views\",\"fileName\":\"2501656_en_US_10_05_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to identify the syntax and how to remove an existing view.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4267232,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] So we've looked at creating a view,\\nwe've looked at modifying a view.\\nNow, let's take a look at deleting it.\\nAs you may have guessed, an easy way to do this\\nis by going to our database structure\\nand when we right-click,\\nit was hard not to notice the last option here\\nwhich is to delete a view.\\nAnd this is one of the ways\\nthat we could actually simply delete a view.\\nNow, I could hit delete view.\\n\\nWe do get a confirmation message\\nand we could just simply confirm that.\\nI'm not going to do that just yet.\\nI'm going to hit no for now.\\nTake a note of our view, v average total.\\nI'm going to head over to the execute SQL tab,\\nand show you one more way\\nthat we could actually delete a view via SQL syntax.\\nAnd that is the keyword, drop.\\nAnd we want to specify\\nthat we are dropping a view, so we say, view.\\n\\nAnd next, we want to simply state the view name\\nthat we would like to drop.\\nAnd we want to drop our V_Avg\\nand there is IntelliSense saving us some typing.\\nAnd I just select v average total.\\nAnd now, if I simply run or execute this statement,\\nwe see that it has been executed without error.\\nWe head over to the database structure tab\\nand we notice that our view is now removed.\\n\\nThat v average total is no longer there.\\nSo these are the two ways that you could go\\nabout removing or deleting a view from within SQLite.\\nAnd just one reminder, when you do remove your view,\\nit is not affecting your tables.\\nSo the data that the view\\nactually references all still there,\\nthe view itself is simply being removed.\\n\"}],\"name\":\"10. Stored Queries\",\"size\":30634804,\"urn\":\"urn:li:learningContentChapter:4408119\"},{\"duration\":835,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4413018\",\"duration\":98,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Analysis and administration\",\"fileName\":\"2501656_en_US_11_01_MM30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn about DML and its role on a high level. This aspect of SQL is important because of its ability to manipulate data. This is a task that may be asked of a data analyst and an important skill to be aware of.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3955408,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Before we do anything else,\\nI want to pause and recognize the great strides\\nthat got us to this point.\\nYou deserve an official shout out for being determined,\\nsticking with it, pushing through, and taking this step\\nthat's bound to future-proof your career.\\nOkay.\\nWe've learned a lot of commands and clauses and statements\\nthat allow us to respond to data inquiries\\nand discover trends, insights, and opportunities.\\n\\nHowever, how would we respond\\nif we were tasked by WSDA music\\nwith adding a new employee to the database\\nor updating a customer's phone number\\nor deleting a song from the catalog?\\nEvery one of these actions, adding, updating, and deleting\\nchanges or manipulates the data in our database in some way.\\nIn SQL, there are a few specific set of commands\\nthat together are called data manipulation language\\nor DML for short\\nand these are SQL statements used to change or alter data\\nstored in the tables of a database.\\n\\nDML is more commonly used in roles,\\nsuch as database developer and database administrator\\nthat oversee the growth, improvement,\\nand management of the company's database.\\nLet's take a look at some of the various DML statements\\nthat can change or alter the data\\nin our WSDA music database.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4411049\",\"duration\":335,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Inserting data\",\"fileName\":\"2501656_en_US_11_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to navigate three DML statement types.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9614886,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now, up till this point,\\neverything that we've taken a look at, as far as SQL code\\nor syntax, did not modify any data in our database.\\nNow, as I just mentioned, the statements that are referred\\nto as DML or Data Manipulation Language\\nare the statements that actually do cause some sort\\nof alteration to our existing data.\\n\\nSo the official statements that are referred\\nto as DML are insert, update, and delete.\\nAnd as these names imply,\\nthese statements can be used to add, modify,\\nand remove data from the tables in your database.\\nNow, let's take a look at how we would handle a request\\nfrom WSDA Music management to add an additional artist\\nto our existing catalog.\\n\\nNow, to add data to our database,\\nthe INSERT statement is actually used to accomplish this.\\nNow, there are a couple ways that we can use\\nthe INSERT statement\\nand one way is to use the keywords INSERT INTO\\nand specify the actual field that we want to INSERT INTO.\\nSo let's say that our management at WSDA Music\\nis expanding their music selection and they want us\\nto add some additional artists to the Artist table.\\n\\nLet's take a look at the INSERT INTO statement\\nthat we would use to accomplish this.\\nSo, unlike our previous beginnings in our composing\\nof SQL statements, we're not going to start with the FROM\\nbecause there is no FROM clause when it comes to DML.\\nWe're going to start with the INSERT INTO keywords.\\nNext, we want to say the table we'd like to INSERT INTO.\\n\\nAnd as we mentioned, targeting the Artist table.\\nNow, we want to specify what column or field we would like\\nto INSERT INTO the Artist table.\\nNow we can remind ourselves about the structure\\nof the Artist table by going to the Database Structure tab.\\nAnd if we expand the Artist table, we can remind ourselves\\nthat these are indeed the columns that are available.\\n\\nAnd we could also take a look at the types.\\nSo integer is numeric, an NVARCHAR is text.\\nSo we have now reminded ourselves what columns\\nare within the Artist table.\\nSo let's head Back over to the Execute SQL tab\\nand let's complete our INSERT statement.\\nSo we've already placed INSERT INTO, we've stated a table.\\nNow, we want to say the column, name,\\nbecause we want to add a new artist.\\n\\nSo we first put a pair of parentheses open and close.\\nAnd between there, we state the name\\nof the column or the field\\nfrom the Artist table that we want to INSERT INTO.\\nAnd that is the name.\\nOkay, next, we need to say what we want to insert\\ninto this particular column of the Artist table.\\nSo to do this, we need another keyword which says VALUES.\\n\\nAnd next, we're going to put another pair\\nof open and closed parentheses.\\nAnd between these parentheses,\\nbecause we want to INSERT INTO the Artist Name field,\\nand that Name field, if you recall, is an NVARCHAR or text.\\nLet's jump over and check really quick.\\nIn this Name field,\\nit is an NVARCHAR type and that is text.\\nAnd when comes to putting text,\\nwe must put a pair of single quotes.\\n\\nAnd within this single quotes,\\nwe're going to state what is the artist\\nwe'd like to insert in here.\\nSo let's put in a popular artist.\\nHow about one of my favorite artists?\\nSo with this, we have a complete INSERT statement,\\na complete DML statement.\\nAnd now, let's run our statement and observe the result.\\nOkay, when it comes to DML statements,\\nwe will not see a result.\\n\\nWe do see in our messages pane\\nthat our query executed without error and it was successful.\\nAnd that's what we want to see.\\nOkay, so we can now go over to our Artist table.\\nIf we go to the Browse Data tab, select the Artist table.\\nAnd now, if we do a quick sort on this column here,\\nlet's take a look to see\\nif we see our newly inserted artist.\\n\\nAnd that would be down at the Bs.\\nAnd we do see in position 276\\nwhich is the last record, Mr. Bob Marley has been entered.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4413019\",\"duration\":217,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Updating data\",\"fileName\":\"2501656_en_US_11_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to update data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5795505,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Now the UPDATE statement is used\\nto modify existing data in a table.\\nNow the update is generally used\\nwith the WHERE clause.\\nThe WHERE clause is used to specify the particular role\\nof data that you want to update.\\nNow without the WHERE clause\\nthe update statement will actually update\\nevery single row in a table.\\nSo including the WHERE clause is pretty important\\nwhen it comes to updating your data.\\n\\nNow let's take a look\\nat how we would update the record\\nthat we just placed in our table,\\nwhich was to update the artist table\\nwith the artist Bob Marley.\\nThe management at WSDA music\\nhas just sent word\\nthat they actually made an error.\\nInstead of Bob Marley\\nthey actually wanted to add his son\\nDamien Marley.\\nSo let's actually see how we would update\\nour record and perform this update.\\n\\nNow the first step we would want\\nto do is go back to our browse data tab\\nand remind ourselves what actually was\\nthat artist ID\\nthat was added\\nwhen we added our new record.\\nAnd if we look down here\\nwe see that Bob Marley\\nis having an artist's ID of 276.\\nSo with this, let's go back\\nto our execute SQL tab\\nand construct our DML statement.\\n\\nThe first thing we'd need to do\\nis indicate the keyword UPDATE.\\nAnd now we want say\\nwe want update the artist table.\\nThe next keyword we need is SET.\\nS-E-T\\nAnd now we want to set the actual field name\\nof the artist table to the updated value.\\nAnd this we know is the artist's name\\nand we want to update this particular field\\nwith a text value.\\n\\nAnd again, when it comes to text\\nwe put that between a pair of single quotes\\nand as management indicated\\nwe want to put Damien Marley.\\nNow at this point\\nif we were to run this statement\\nit would actually update every single name\\nin our artist table with the name Damien Marley.\\nWhich why, again I'll stress\\nthat including the WHERE clause is very\\nimportant when it comes to updating data.\\n\\nSo here, let's include the WHERE.\\nAnd we want to say where the artist ID\\nis equal to\\n276.\\nLet's remind ourselves again\\nthat that number indeed is 276 for Bob Marley.\\nLet's jump back over to execute SQL\\nand run this statement.\\nAlright, once again, we don't get a result.\\nWe do see in our messages pane\\nthe message that we want to see\\nwhich is a successful completion of our query.\\n\\nNow we can verify again by going over\\nto our browse data\\njump back to record 276.\\nAnd now Bob has been updated to Damien Marley.\\nWell done.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4408116\",\"duration\":185,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Deleting data\",\"fileName\":\"2501656_en_US_11_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to delete data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4909653,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The final DML statement\\nwe're going to take a look at is the delete statement.\\nNow the delete statement is used to remove existing records\\nfrom a table, and just like the update, it is usually used\\nwith the where clause.\\nWithout the where clause,\\nusing a delete statement will delete every single record\\nfrom your table.\\nNow this is extremely important\\nand again very important to include the where clause\\nwhen using a delete statement.\\n\\nSo let's demonstrate the use of a delete statement\\nand remove our record that we have just inserted.\\nSo once again, we've gotten word from the management\\nat WSDA Music that once again\\nthere has been a little bit of a mix up.\\nThey have now decided to remove altogether\\nthe newly added then modified artist Damien Marley\\nfrom the database.\\n\\nLet's see how we would construct a DML statement\\nto remove this record.\\nNow we do need a few new keywords, starting with delete.\\nSo we specify the keyword delete.\\nNext we need from, so with these two keywords\\nthen we can now specify\\nthe table we would like to delete data from.\\nAnd in this case, we're targeting the artist table.\\nSo we specify the artist table.\\nNow at this point, again, word of caution.\\n\\nIf we were to run this statement,\\nwe would delete every single record\\nfrom the entire artist table.\\nAnd we do not want to do this,\\nso we must include a where clause at this point.\\nAnd I'm going to specify where,\\nand we want to say where that artist ID is equal to 276.\\nAnd if we weren't sure about this,\\nwe would go over to that browse data tab\\nwhich is always worth the time\\nto take a check to make sure\\nyou're deleting the right record\\nand make sure you have the right id\\nwhich is 276.\\n\\nI head over to execute.\\nI put 276 there.\\nAnd now I'm ready to run this statement,\\nand I check for the message I want to see,\\nwhich is that this query executed successfully\\nwithout error.\\nI verify the action of this delete\\nby going over to the browse data.\\nI can tell already that my total account is 275.\\nIt was just 276.\\nIf I go look for a record 276, it is no longer there.\\n\\nSo our delete statement has worked,\\nand we could report back to WSDA Music Management\\nthat we have now removed that newly added record.\\n\"}],\"name\":\"11. Adding, Modifying, and Deleting Data\",\"size\":24275452,\"urn\":\"urn:li:learningContentChapter:4409083\"},{\"duration\":521,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4513410\",\"duration\":199,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Preparing for a Non-Technical Interview\",\"fileName\":\"2501656_en_US_11a_01_2023Q3_XR30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this movie, learners will be able to take the skills reviewed in the mock interview scenario and apply them to real-world data analysis interview experiences.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5931297,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Great job so far,\\nyou've gained the fundamental knowledge and skills\\nnecessary to work with SQL databases effectively.\\nNow it's time to put your skills to the test\\nin a job application simulation.\\nIn this simulation, imagine yourself as a candidate\\nfor the position of SQL Data Analyst at Red 30 Tech.\\nAnd this role requires at least two years of experience\\nin developing and maintaining databases and data systems.\\n\\nAlong with a passion for creating data visualizations\\nand providing actionable insights.\\nYou'll be presented with a set of non-technical\\nand technical questions, simulating a real job interview.\\nThe non-technical questions\\nare designed to assess your communication skills\\nand ability to articulate your thoughts effectively.\\nFor these questions, you'll be prompted to pause the video,\\nthink about how you'd respond and record your responses\\nusing your mobile phone and review them later\\nfor self-evaluation and improvement.\\n\\nOn the other hand, the technical questions\\nwill be presented to you in the CoderPad environment\\nwhich is a special interface\\nbuilt into the LinkedIn learning platform.\\nYou can use this environment to compose and run SQL queries\\nand respond to the technical questions.\\nWe'll talk a little more\\nabout the CoderPad environment soon.\\nThe non-technical questions will focus on your experience,\\nyour approach to data analysis,\\nand your ability to collaborate with cross-functional teams.\\n\\nIt's important to demonstrate your communication skills\\nand provide clear and concise answers.\\nFor the technical questions,\\nYou'll be challenged to develop SQL queries\\nto extract data from large databases\\nand create reports and visualizations.\\nYou should leverage your SQL skills and experience\\nworking with large databases\\nto tackle these questions effectively.\\nRemember to approach each question thoughtfully,\\ndemonstrate your problem solving skills\\nand provide well structured and logical answers.\\n\\nTake the opportunity to showcase your ability\\nto analyze complex data sets, identify patterns and trends,\\nand interpret findings to provide actionable insights.\\nKeep in mind that this simulation\\nis designed to help you improve your interview skills\\nand technical proficiency.\\nUse it as an opportunity to learn and grow\\nand feel free to review and analyze your responses afterward\\nfor self-assessment and improvement.\\n\\nOne more thing, I'd like you to meet Charlotte Navarro.\\nCharlotte is a fictitious character\\ncompleting the non-technical interview questions with you,\\nso you're not alone.\\nCharlotte's responses are a great guide\\nyou can reference when you prepare and craft\\nyour own interview answers.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2707385\",\"duration\":322,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Non-Technical Interview Practice\",\"fileName\":\"2501656_en_US_12_02_FY24Q4_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"updateType\":\"UPDATED\",\"editingNotes\":\"#EDITING Reuse this movie but cut the final two sentences (We're now done with the non-technical portion of our interview. Up next, we're going to move into some more technical questions. You'll be doing those in the CoderPad platform.)\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this movie, learners will be able to reflect on the mock non-technical interview questions, while also answering practice questions themselves to commonly asked interview questions.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7835311,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Interviewer] Welcome to Red30 Tech.\\nThank you for coming in today.\\nCan you please introduce yourself\\nand tell us a little bit about your background\\nand experience in data analysis?\\n- [Instructor] Keep your answer brief.\\nYour response will set the tone for the interview.\\nThink of this as a 62nd pitch of your resume.\\n(upbeat music)\\nSo I'm curious, what do you think about Charlotte's response\\nand how about your own response?\\nDid you compare them a bit?\\nLet's take a look at some of the pointers\\nto pay attention to\\nin the response that Charlotte gave.\\n\\nShe began by introducing herself\\nand providing a concise overview of her background\\nand experience in data analytics.\\nAnd this approach is effective\\nbecause it allows the interviewer\\nto quickly understand Charlotte's qualifications\\nand sets the foundation\\nfor further discussion about her skills and expertise.\\nNow, by mentioning her degree in statistics\\nand experience working on data analysis projects\\nduring her studies,\\nCharlotte establishes her educational background\\nand practical experience\\nshowing how prepared she is for this role.\\n\\n- [Interviewer] That's great to hear.\\nOur company places a strong emphasis\\non sustainability and innovation.\\nCan you now tell us about a project\\nor experience you've had\\nthat demonstrates your commitment to these values?\\n- [Instructor] This is a great opportunity to draw\\non your own expertise\\nand find ways you've showcased any values\\nthat are important to the interviewer.\\nDon't be afraid to ask for a moment\\nto think about your response.\\n(upbeat music)\\nNow, let's walk through some of the pointers\\nthat we should pay attention to\\nin this response of Charlotte's.\\n\\nAnd again, I'm curious about your own response\\nand how you think you did when you compared Charlotte's\\nfor this response,\\nCharlotte shares a specific project experience\\nthat demonstrates her commitment to the values\\nof sustainability and innovation.\\nThis response goes beyond a generic statement\\nand provides tangible evidence of Charlotte's dedication\\nto these principles.\\nBy mentioning her analysis of energy consumption data\\nfor a large corporation, Charlotte showcases her ability\\nto apply data analysis techniques to drive positive change.\\n\\nThis example also highlights Charlotte's awareness\\nof the importance to use data to make informed decisions\\nthat contribute to sustainable practices.\\n- [Interviewer] That's an excellent example.\\nNow, can you tell us about a time when you had\\nto communicate technical information\\nto non-technical stakeholders?\\n- [Instructor] Use this question as a moment to be creative.\\nReference your own personal experience,\\neven if it's not related to IT.\\n\\nThis question is more about your ability to communicate.\\n(upbeat music)\\nNow how about this third question here.\\n\\nHow'd you fare? How'd you make out?\\nWell, let's once again take a look at Charlotte's response\\nand point out some of the items\\nthat are worth taking note of.\\nNow for this question, Charlotte describes a situation\\nwhere she had to present technical information\\nto a non-technical audience by emphasizing the use\\nof visualizations and reports\\nto make the information more accessible.\\nCharlotte demonstrates her ability to bridge the gap\\nbetween technical and non-technical stakeholders.\\n\\nThis approach showcases her skill in translating complex\\ndata and analysis into easily understandable insights.\\nEffective communication of technical information\\nis critical in a data-driven role\\nas it ensures that stakeholders can make informed decisions\\nbased on the presented findings.\\n\"}],\"name\":\"12. Scenario: Mock SQL Interview\",\"size\":13766608,\"urn\":\"urn:li:learningContentChapter:4411053\"},{\"duration\":119,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4512419\",\"duration\":119,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps and tips\",\"fileName\":\"2501656_en_US_14_01_2023Q3_MM30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this movie, learners will be able to navigate to additional learning opportunities in order to expand their SQL knowledge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7219361,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- And that brings us to the end of the course.\\nIn Trinidad and Tobago,\\nwe like to celebrate with Soca, Carnival, Steel Drums,\\nlimbo, okay, yep, I'm officially on a tangent.\\nThe point is you did great\\nbut we've only scratched the surface\\nof what SQL has to offer.\\nSo by no means is this the end.\\nI encourage you to keep moving forward\\nin your learning journey.\\n\\nYou can start by taking my live cohort-based course\\n\\\"SQL Zero to Hero Accelerator.\\\"\\nAlso, SQL is just one\\nof the data analyst tools that's used to perform analysis.\\nSome complimentary skills and tools\\nyou can start learning are Microsoft Excel,\\nused as a data analyst would and Microsoft Power BI,\\na data visualization tool used to take results of queries\\nlike the ones you learned in this course and visualize them\\nin charts and graphs for even deeper analysis.\\n\\nYou can also reinforce the materials you learned\\nby getting my best selling book, \\\"SQL QuickStart Guide\\\"\\nand nothing replaces just plain old practicing.\\nSo take a second try at any questions that were not clear\\nor you haven't answered in this course.\\nAnd if you haven't, do the final project\\nand ask questions in the course Q and A section.\\nAgain, my name is Walter and it has been both my pleasure\\nand honor to teach you the skill of SQL.\\n\\nThank you for watching, investing in yourself\\nand for allowing me\\nto be a small part of your learning journey.\\n\"}],\"name\":\"Conclusion\",\"size\":7219361,\"urn\":\"urn:li:learningContentChapter:4412038\"}],\"size\":547993427,\"duration\":16459,\"zeroBased\":false},{\"course_title\":\"Complete Your First Project in SQL\",\"course_admin_id\":3811063,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":3811063,\"Project ID\":null,\"Course Name\":\"Complete Your First Project in SQL\",\"Course Name EN\":\"Complete Your First Project in SQL\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"Grow your knowledge of SQL and gain a new perspective of how to maximize SQL\u00e2\u20ac\u201dand learn how to go beyond creating databases and pulling data from tables. This course provides a real-world application SQL programmers can utilize to enhance learning\u00e2\u20ac\u201dand a project you can add to your coding portfolios. Instructor Megan Silvey presents a project that focuses on performing an analysis on retail data by utilizing SQL. Analyze data that includes tables with product, sales, and customer information for an ecommerce retailer. Learn about updating information in the tables, analyzing sales data, evaluating customer data, and more.The objective of this course is to provide a life-like application for SQL programmers that they can use to enhance their learning along with providing them with a potential project to add to their portfolio. This project will focus on performing an analysis on retail data by utilizing SQL. The data will include tables with product, sales, and customer information for an e-commerce retailer such as Binaryville robots or H+ Sport from the LinkedIn Learning asset bank. The analysis will include updating information in the tables, analyzing sales data, evaluating customer data, and more.\",\"Course Short Description\":\"Elevate your SQL skills and gain a new perspective of how to maximize SQL.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":20822000,\"Instructor Name\":\"Megan Elizabeth Silvey\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Data Science Consultant\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2024-03-18T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"No\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/complete-your-first-project-in-sql\",\"Series\":\"Project\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Databases\",\"Primary Software\":\"SQL\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":4634.0,\"Visible Video Count\":28.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":490,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3863080\",\"duration\":37,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"SQL introduction\",\"fileName\":\"3811063_en_US_00_01_WL30\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"editingNotes\":\"Tetris LA SAN:\\n/Volumes/Video/Video In Progress/Complete Your First Project in SQL_1283446/2_Project/2_Footage/2_Live Action\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be familiar with the concept of using SQL to analyze retail data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1708097,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Employers are seeking professionals with SQL skills\\nand this course helps you grow your knowledge of SQL.\\nHi, I'm Megan Silvey, a data science consultant,\\nand I enjoy using data to solve real world problems.\\nYou can use SQL for more than creating databases\\nand pulling data from tables.\\nI'll show you how to use SQL\\nto analyze customer and sales data.\\nBy the time you've finished,\\nyou'll have elevated your SQL skills\\nand gain a new perspective on how to maximize SQL.\\n\\nAre you ready? Then let's get started.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3870071\",\"duration\":34,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"3811063_en_US_00_02_XR30\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":49,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will understand that you need to have a basic understanding of SQL and general analysis for this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":831691,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] Before you begin this course,\\nthere are a few themes you need to be aware of.\\nFirst, it is important to have a basic knowledge of SQL code\\nand table structure, such as what SQL tables are\\nand how to perform a basic select statement.\\nAnother thing to be familiar with\\nis basic data analysis techniques,\\nsuch as finding top values or gathering percentages.\\nFinally, you'll want to have a GitHub account\\nto access the SQL database and run the code in Codespaces.\\n\\nIn the next video,\\n\"},{\"urn\":\"urn:li:learningContentVideo:3861089\",\"duration\":419,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Codespaces\",\"fileName\":\"3811063_en_US_00_03_XR30\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"editingNotes\":\"Cut 4:39 to 6:30\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will understand how to use Codespaces for this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15703506,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this video,\\nI will show you how to use codespaces for this course.\\nGitHub Codespaces is a development environment\\nthat's hosted in the cloud.\\nYou can access codespaces in the course GitHub repo.\\nLet's take a moment to look at the GitHub repo\\nyou will be using for this course.\\nWhen you look at the GitHub repo,\\nyou'll notice there are three different chapter folders\\ncontaining the different coding files.\\nIf you click on the chapter one,\\nyou'll see the different coding files used\\nfor the different videos in this course.\\n\\nThe way these files are denoted is, for example,\\nyou'll see 0103, and that will be for the third video\\nin chapter one.\\nYou'll also notice that there is usually a B,\\nor an E at the end of most of these coding files.\\nThe B file is the one I suggest you start with,\\nand this will be the file you\\ntype your code in to follow along\\nwith the course video.\\nThe E file can be used for reference if you are stuck\\non your code to check and make sure\\nthat your code is matched to that one,\\nand that it's running appropriately.\\n\\nYou'll notice that some of the coding files have check\\nat the end of them.\\nThis is just a set of code that\\nis already ready to go for you\\nto use to do a quick check on something\\nthat you ran in the code.\\nThere are also three coding files that will have challenge,\\nand then also solution for each of these chapters\\nfor the different challenges you'll be completing.\\nSo again, the challenge file is the one you'll start with,\\nand the solution file is the one that you will end with.\\nSo, like the other files,\\nthe challenge file is the one you will type your code in,\\nand the solution file is the one you will use for reference.\\n\\nLet's navigate back out here.\\nIf you scroll down, you can also look at the readme\\nfor this particular GitHub repo.\\nAgain, I should explain the file structure\\nof those different coding files throughout.\\nThere are a few different ways\\nyou could download files directly from the GitHub repo.\\nLet's say you wanted to download the readme file.\\nYou could go over to the right here\\nand select download raw file.\\nClick that, and it should download to your computer.\\nAnother way you could do this if you don't want to download\\neach file individually is you could go over\\nto this coding button, select the local tab,\\nand select download Zip.\\n\\nThat should download all the files in the GitHub repo\\nto a zip folder on your computer.\\nJust make sure that you unzip it\\nfirst to then access the files\\nand place them wherever you wish on your computer.\\nNext, we will explore how to navigate to codespaces\\nin your GitHub repo.\\nSo going back to this coding tab,\\nyou'll make sure you're selected on the codespaces.\\nHere at the top, you'll have the option\\nto create a codespace on main,\\nor you can select one of the codespaces\\nyou already have active.\\nSo, I already have Zany Garbanzo,\\nso I can select that to open that current codespace\\nthat I'm using.\\n\\nTo navigate to the codespace this way\\nand create a new codespace on main,\\nyou click this plus button,\\nand it should open a new tab up to set up your codespace.\\nNote that the first time you do this,\\nit may take a few minutes\\nto build and then load your codespace.\\nAfter that, once you navigate to a codespace\\nthat you already have built,\\nit shouldn't take nearly as long to load.\\nAnother option to navigate to codespace\\nis by using the link\\nbelow this video of the course to get to this same spot.\\nOnce your codespace loads,\\nit should look something like this,\\nwhere it has the readme file here, it has the terminal,\\nand off to the left is the navigation bar,\\nwhere you can use to navigate to the different coding files\\nand the database.\\n\\nI highly recommend you use the same codespace\\nthroughout the course.\\nThat way, it saves the work for each\\nof the different coding files you are editing.\\nThis is because work you do in a codespace\\ndoes not transfer to other codespaces you may have.\\nLet's begin by navigating the left side menu.\\nSo, you'll notice here is the explorer option,\\nso that should show all the different files in the GitHub.\\nSo, you could click on chapter one,\\nand it should show all the different files\\nthat you have here.\\nSo, if you click on the 0102 check SQL file,\\nthis should show the SQL file for you to use.\\n\\nIf you click on it once, it'll show up,\\nbut if you navigate to other files,\\nit'll simply replace where that one is\\nwith these different files you're opening.\\nIf you want to open this file and keep it there,\\nyou simply click on it twice.\\nAnd so now, if you open other files again,\\nyou can easily navigate back to this one.\\nSince this course is going to be running on a SQL database,\\nyou will be running these different coding files\\non your active database connection.\\nSo, the way to do that is you'll click run\\non active connection.\\nAnd the first time you do this,\\neach time you go into your codespace or reload it,\\nyou will be selecting this H plus support option here,\\nand then it should show the results over here.\\n\\nIf you wish to push this down,\\nsince it is kind of in the way,\\nyou simply click it here and drag this down.\\nor you can even select the X over here to completely get rid\\nof that terminal.\\nIf you wish to have the terminal show back up,\\nsimply click on console, and it'll show up again.\\nAs you can see here, after you have run this\\non the database connection,\\nyou'll still see your code off here to the left.\\nBut then, off to the right,\\nyou will see the output for that particular code.\\nSo, this will be fairly typical for us throughout the course\\nto see where, again, you'll have the code on the left side\\nand then your output on the right-hand side.\\n\\nAnd you can use the scroll bar here to navigate along\\nthe output you are seeing.\\nAnd again, if you run this multiple times,\\nthen you'll just see multiple outputs.\\nAnd again, you can click on these to navigate\\nto the different outputs you see.\\nIf you wish to see the database directly,\\nyou can simply go down here to the cylinder\\nwhere it says SQL tools.\\nAnd then you can navigate amongst the database.\\nI will show in a later video how to navigate\\nthrough this database more thoroughly, but in the meantime,\\nthis is how you can access it.\\nOnce you are done with your coding session,\\nthere are a few different ways\\nyou could close the codespace.\\n\\nOne easy way is going down to the bottom left here,\\nselecting this, and select stop current codespace.\\nWhat this will do is it'll stop the current codespace\\nwhile saving all the edits\\nthat you made throughout your session.\\nOnce your codespace has stopped,\\nit should show something like this where it says codespace\\nhas stopped, and it gives you the option\\nto restart the codespace here.\\nSo, you could click this,\\nand then it will bring you back into your codingspace,\\nand it should show like it did when you recently left off.\\nSo again, it'll even show the different files\\nthat you had open and all of that.\\n\\nLike I said, this codespace now that it is created\\nshould take a lot less time to load\\nthan the first time around that it was created.\\nNow that it's loaded,\\nyou could see it has us in the connection portion\\nof the SQL database.\\nIt has your file up.\\nThe main thing I won't have up\\nis the different outputs since\\nthat is cleared each time you go out of your codespace.\\nIf you go to your GitHub repo\\nand look at the code section after you refresh your browser,\\nyou should be able to now see your new codespace.\\nMine is called Scaling Happiness.\\nThat is ready to go, and it's currently active\\nbecause it is currently open.\\n\\nAnd again, this is one of the ways you can navigate to it.\\nin between your different coding sessions.\\nNow that you know how to use GitHub Codespaces,\\nyou are ready to follow along with me as we code some SQL.\\n\"}],\"name\":\"Introduction\",\"size\":18243294,\"urn\":\"urn:li:learningContentChapter:3868076\"},{\"duration\":1584,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3863081\",\"duration\":188,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introducing H+ Sport\",\"fileName\":\"3811063_en_US_01_01_XR30\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":309,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be familiar with the retailer you are analyzing data from.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6297794,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's imagine you work as a data analyst\\nfor the active lifestyle store H+ Sport.\\nYour job is to upkeep their customer and sales data,\\nalong with determining how to improve their sales\\nfor their mineral water department.\\nA great way to accomplish both of these tasks\\nis by using these SQL coding language.\\nLet's learn more about H+ Sport, the company you work for.\\nH+ Sport is a fictitious company\\nthat sells nutritional products\\nand active lifestyle clothing.\\n\\nH+ Sport is dedicated to creating eco-friendly,\\nhigh quality, nutrient-rich nutritional products\\nthat enhance active lifestyles,\\nalong with clothing that is made to be functional,\\nstylish, comfortable, and durable.\\nYou can navigate to the H+ Sport website\\nby going to the URL hplussport.com.\\nOnce you navigate to the website,\\nyou should see the homepage of H+ Sport.\\nThe homepage shows a few menu options\\nsuch as the tab for home, the company,\\nsome products tabs,\\nshop, blog, and contact.\\n\\nIf you navigate to the company history page,\\nyou can learn more about how Henry Tool\\nfounded H+ Sport in 2006\\nto sell nutritional products such as supplements,\\nenergy bars, and rehydration solutions,\\nalong with activewear to meet the needs\\nof active lifestyle consumers.\\nLet's go ahead and take a look at the products\\nthat H+ Sport sells.\\nHere if you look at this tab,\\nyou could see there is H+ apparel for women,\\nH+ active apparel for men, H+ power supplements,\\nand H+ Sport mineral water.\\n\\nLet's take a closer look at the mineral water section\\nsince that is the department you are tasked to assist with.\\nOn this page, you can learn more\\nabout the H+ Sport mineral waters\\nthat they sell to their customers.\\nSo if you scroll down here,\\nyou could see the six current mineral waters that they sell.\\nYou can learn more about what is included\\nin the mineral waters,\\nalong with their hydration and nutritional information,\\nand finally, the six delicious flavors that they have,\\nincluding strawberry, raspberry, peach,\\norange, lemon lime, and blueberry.\\n\\nIf we go to the shop tab, we can click on the mineral waters\\nto then learn more about them.\\nLet's look at the blueberry mineral water.\\nOn this product page, you can find the name of the product,\\nwhich is Blueberry Mineral Water,\\nalong with a description\\nabout the mineral water they are selling.\\nThere's also a photo of the product\\nalong with how much the product is selling for.\\nPlease note the bottle sizes\\nand prices of the mineral water shown on the website\\nwill differ from what you will come across\\nin the dataset you will analyze.\\n\\nI suggest taking a minute\\nto explore H+ Sport's website on your own\\nto familiarize yourself with the company and their products.\\nOnce you familiarize yourself with the company,\\nyou can begin exploring the dataset\\nyou'll be analyzing in this course.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3868073\",\"duration\":247,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understanding retail data\",\"fileName\":\"3811063_en_US_01_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":449,\"solutionVideo\":false,\"editingNotes\":\"Cut 02:24 to the end of the movie. \\nInsert PU\\n\\n3811063_en_US_01_02_DataPU_VT\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":true,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be familiar with the retail data you will be analyzing.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7376203,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's begin by having you\\nfamiliarize yourself with the data in H+ Sports database.\\nIn Codespaces, you can navigate to H+ Sports database\\nby clicking the SQL tools button on the left-hand side\\nwhere it looks like a cylinder.\\nSo if you click here, you could see you have\\nthe H+ Sport database connection.\\nYou can click on the arrow to the left of it to navigate\\nto the H+ Sport database within this connection.\\nAnd here you could see there are folders\\nfor tables and views.\\n\\nIf you click on the views folder,\\nit'll say that there's nothing here.\\nSo that means there are currently no views in your database.\\nIf you click on the tables folder,\\nyou can see there are the different tables\\nincluding customer, order item, orders,\\nproduct, and salesperson.\\nIf you click on the customer table here,\\nyou can see the different columns inside that table\\nalong with their data types.\\nSo for example, you see customer ID as an INT\\nand first name is a VARCHAR which is essentially a strain.\\n\\nIf you right-click the customer table,\\nyou'll have the option to show table records\\nwhere now, you can see the records\\ninside the customer table, for example,\\nthe customer ID, their name, email,\\nphone number, and so on.\\nAnother way to do this is, you could go back to your files\\nand select your 0102 check SQL file.\\nAnd here you will have what is called a select statement.\\nA select statement allows you\\nto pull values from a data table.\\nHere you are selecting all the columns\\nand rows from the data table\\nby using this little star function here.\\n\\nNext, you have your FROM,\\nwhich is where you are gathering the data from.\\nAnd in this case, you're looking at the customer table.\\nIf you click run, your data will show up\\nlike it did before, where again,\\nyou have all your different customer data\\nfor the different columns.\\nLet's take a moment to understand these data tables.\\nI will provide a short explanation\\nalong with an example for each table.\\nLet's begin with the customer table.\\nThis table contains information for each of the customers\\nthat I purchased something from H+ Sport,\\nor have subscribed to their marketing campaigns.\\n\\nHere you could see the information\\nfor the customer, Carol Shaw,\\nand you have her CustomerID, FirstName, LastName,\\nEmail, Phone, Address, City, State, and Zipcode.\\nNext is the order item table, which contains how much\\nof each product was purchased in each order.\\nThis can be connected to the orders table with the OrderID.\\nIn this example, you'll see there is the OrderItemID,\\nOrderID, ProductID, and Quantity.\\n\\nAfter that is the orders table.\\nThis table contains the basic order information\\nalong with payment status to keep track of\\nwho still owes H+ Sports money for their purchases.\\nThere are many ID values that can be used\\nto connect this table to other data tables\\nsuch as OrderID, CustomerID, and SalespersonID.\\nHere you could see OrderID 1000 has OrderID,\\nCreationDate, TotalDue, Status, CustomerID,\\nand SalespersonID.\\n\\nNext, we have the product table which contains information\\nabout the different mineral water products H+ Sport has.\\nSo for this blueberry mineral water,\\nyou could see you have its product ID, product code,\\nproduct name, size, variety, price, and status.\\nFinally, there's the salesperson table\\nwhich contains personal information\\nof the different salespeople employed by H+ Sport.\\nSo here you'll see there's Jack Powell\\nand he has his SalespersonID,\\nFirstName, LastName, Email, Phone, Address,\\nCity, State, and Zipcode.\\n\\nThis should give you a general overview\\nof the data you'll be working with throughout the course.\\nI highly recommend you take time on your own to explore\\nand understand the data within the Codespaces database.\\nLet's get to analyzing that data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3863082\",\"duration\":237,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to add a new product\",\"fileName\":\"3811063_en_US_01_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":343,\"solutionVideo\":false,\"editingNotes\":\"Insert PU at 04:57 (don't cut anything just add PU please) then continue with 01_03 (slide with voiceover to the end of the movie).\\n\\n3811063_en_US_01_03_AddPU_VT\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":true,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to add a new product to the product data table.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8647208,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Most retail stores\\ncycle through the products they sell.\\nH+ Sport is no different\\nand continues to create new products\\nto meet the needs of their customers.\\nIn particular, they want to expand\\ntheir popular mineral water product line\\nto add a new flavor option\\nfor their customers to choose from.\\nH+ Sport plans to add a new great flavor option\\nin the smaller bottle size\\nto see if customers are interested in this new flavor.\\nAs H+ Sports data analysts,\\nthey need you to update their product database\\nto allow salespeople to select it\\nwhen creating orders for this new product.\\n\\nLet's head over to Codespaces to add this new product.\\nBefore you add the product though,\\nlet's take a moment to look at the product table.\\nHere you can view the current mineral water products\\navailable to customers.\\nYou can see there are a variety of mineral water flavors\\navailable, such as blueberry, mango, peach, and strawberry.\\nThere are also two size options with corresponding prices\\nand the status of the product.\\nIn order to add a new entry to the product table,\\nyou need to use an INSERT INTO statement.\\n\\nThis statement allows you to insert new values\\ninto a current data table.\\nIn your case, you will want to insert\\nthe new row of data into the product table.\\nNext, you will want to list out the columns\\nin the data table that you are inserting values into.\\nMake sure you enclose your list of columns in parentheses\\nand separate them by commas.\\nI highly suggest listing out the column names\\nin the same order\\nThe columns you need to list out are ProductID,\\nProductCode,\\nProductName,\\nSize,\\nVariety,\\nPrice,\\nand finally, Status.\\n\\nOnce you list out the column names,\\nyou'll want to type out the values\\nyou wish to insert into your data table.\\nYou can do this by using the VALUES keyword.\\nLike you did last time,\\nyou'll also want to enclose this by parentheses\\nand separate the values incited by commas.\\nMake sure when you list out the values,\\nthey are in the same order\\nas the column names you listed above\\nto ensure they get entered into the data properly.\\nThe values you need to insert into your data table are 17\\nfor the new product ID,\\nMwPRA20 for the new product code,\\nmineral water for the product name,\\n20 for the size,\\ngrape for the new variety,\\nor in this case flavor,\\n179 for the price,\\nand finally, active as the status.\\n\\nOnce you complete your SQL INSERT statement,\\nclick Run on the active connection at the top of the code,\\nwhich you can scroll up here\\nand click Run on active connection.\\nAnd you can click on the console option below\\nto see that one row were affected,\\nwhich means that you inserted one row of data\\ninto your product table.\\nIf you wish to double check\\nthat you entered the new product correctly,\\nyou can use the 01_03 check SQL file.\\nAnd here you'll see you have\\na SELECT * FROM Product statement.\\n\\nSo if you run this,\\nyou can now see you have your new mineral water\\nat the bottom with the product ID of 17.\\nAnd if you scroll to the right,\\nyou can see it is in fact the grape flavor.\\nPlease note that you will only want to run\\nyour INSERT INTO statement once because otherwise,\\nyou may get an error from SQL\\nsaying that the row of data is already in there.\\nNow you have updated the product database\\nto ensure it is ready to go\\nfor selling the new great flavored mineral water.\\nYou can use this process in the future to add new products\\nas the business continues to grow.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3873004\",\"duration\":107,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to order sales data\",\"fileName\":\"3811063_en_US_01_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":188,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to order the sales data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4327197,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] H Plus Sport has made many sales\\nand needs to keep track\\nof which orders have been paid or not.\\nThe way they could keep track\\nof this is using a SQL table for their orders.\\nYou can view the orders table\\nby using the SELECT star FROM Orders statement.\\nWhen you run this on your connection,\\nyou can view the orders that have been made,\\nhow much the order total was, the status of the order,\\nand if you scroll to the right,\\nyou can also see the Customer ID and Salesperson ID.\\n\\nUnfortunately, the data is ordered a bit randomly,\\nwhich is a common occurrence in SQL data tables.\\nLet's view the date in a sequential order by date\\nto clearly see what orders are made when.\\nAn easy way to do this is by using an ORDER BY clause.\\nThis clause allows you to sort the data in a table by one\\nor more columns specified.\\nIn your case, you will want to sort the data\\nby the CreationDate column.\\nSo if you click here\\nand run this, you could see the orders data table is now\\nordered by the creation date\\nof the orders starting way back in June, 2015\\nat the top.\\n\\nLet's say you want\\nto view the most recent orders instead of the oldest orders.\\nYou can easily accomplish this\\nwith one quick addition to your code.\\nAt the end of your SQL statement, you'll add in DESC\\nand that will order the data in descending order\\ninstead of ascending order.\\nSo if we run this again,\\nnow you could see the orders data table is ordered\\nby the creation date in descending order\\nwith the most recent order being from August of 2016.\\nNow that you know how to order data tables,\\nyou can easily order any data you view to make it easier\\nto interpret and share with others in reporting.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3861090\",\"duration\":174,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to find missing values\",\"fileName\":\"3811063_en_US_01_05_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":291,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to find null values in the data tables.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6538029,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructional] H plus four is planning to run a campaign\\nto share the news about their new product,\\nGreat Mineral Water.\\nThey want to share this news with their customers\\nvia an email and text marketing campaign.\\nIt is your job to provide them a list of names, emails,\\nand phone numbers of the current customers\\nin their database.\\nFor this specific campaign,\\nthey need to have the first name, last name, email,\\nand phone number for all customers they send the campaign to\\nwith no missing values.\\n\\nThis way, the company can ensure all marketing materials\\nare delivered appropriately and nothing gets returned.\\nThis means you need to first determine\\nif there are any null values in the customer database.\\nLet's head over to code spaces\\nto determine if there are any null values.\\nYou'll start off by doing a select star\\nfrom customer statement.\\nIf you run this, you can view all the customers\\ncurrently in the customer database.\\n\\nIf you look down to the bottom right,\\nyou could see here there is a null value\\nfor this particular email for Steven Mason.\\nIn order to narrow down which values are null\\nfor your campaign, you need to add a where clause\\nto your statement.\\nSo when you add a where clause,\\nthis allows you to filter your SQL statement.\\nFirst, you'll want to check\\nif any of the first name entries are null.\\nSo you'll call the first name column,\\nand then you will use the is null function.\\n\\nThis function is a way to check if any values\\nin the specified column are null or not.\\nSince we want to check if multiple columns are null,\\nwe will need to add the or operator.\\nThe or operator allows you to check\\nif any of the multiple conditions are met.\\nIf you want to check if all conditions are met,\\nthen you would need to use an and operator instead.\\nNext, you will check to see if any of the last name values\\nare null by using last name is null.\\n\\nYou'll add another or for our next line,\\nand next you will check to see\\nif any of the email entries are null.\\nSo you'll have email is null, and then finally,\\nyou'll check to see if any\\nof the phone number entries are null,\\nso you'll have phone and then use is null.\\nOnce you run this,\\nyou will see any null values in the customer database.\\nIf you look to the right,\\nyou will notice that there are six total customers\\nthat contain null values.\\nHere you will see there are three customers\\nthat have null values for their email.\\n\\nAnd if you scroll to the right,\\nyou'll see three of the customers\\nhave null phone number values.\\nIn the next video, I will show you how\\nto remove these values from your select statement.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3872008\",\"duration\":165,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to remove missing values\",\"fileName\":\"3811063_en_US_01_06_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":309,\"solutionVideo\":false,\"editingNotes\":\"Cut 02:44 to the end of the movie. Replace with PU\\n\\n1283446_en_US_01_06_RemovePU_VT.mov\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":true,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to get rid of null values in the customer data table.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7458479,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In the previous video,\\nyou discovered there are six null values\\nin the customer list you need to send over\\nfor the marketing campaign.\\nYou could see that list to the right for reference\\nwhere there were those three null emails\\nand three null phone numbers.\\nIn this particular case, you need\\nto remove the null values from the customer list\\nbefore you send it over.\\nYou can accomplish this by using a SELECT statement.\\nSo you'll start off by selecting the FirstName.\\nNext, you'll select the LastName.\\n\\nAfter that, you'll do the Email\\nand finally the phone number.\\nBy gathering these four columns, you'll provide the company\\nwith the exact information they need.\\nMake sure you pull this from the customer data table.\\nNext, you'll want to add a WHERE clause to specify\\nthat you only want to pull rows of data\\nthat do not contain null emails or phone numbers\\nsince you already know there are\\nno null first or last names.\\nYou could do this by checking where email is not null.\\n\\nThen you'll want to use the AND operator\\nto separate the two values you want\\nto check if they are null,\\nbecause you want to make sure neither value is null\\nin any row of the data.\\nSo after that, you'll go down here\\nand check to see where phone number is not null.\\nOnce you run this, you will see you have a list\\nof all customers in the database\\nthat don't have any missing contact information.\\n\\nSo here you'll see you have their first\\nand last names along with their email and phone number.\\nIf you do a quick scroll through,\\nyou could do a quick sanity check\\nand see there are no null values showing up.\\nIn order to share these results with the marketing manager,\\nit is best to give it to them in an easy to use format such\\nas a CSV.\\nIn Code Spaces,\\nyou could click on the export button\\non the bottom right here\\nto export the data as a CSV or a JSON.\\nIn this case, we want to save it as a CSV,\\nso we'll go up here, click save results as csv.\\n\\nLet's call it Data.csv.\\nClick okay, and then now you'll see the data\\nto the right in a CSV format.\\nYou'll also notice it here to the left notated as data.csv.\\nIf you wish to download this to your main computer,\\nsimply right click and select Download.\\nNow you could see the data in a CSV in Microsoft Excel\\nwith the first name, last name, email, and phone number.\\nNow all the values are in a CSV file making it easy\\nfor the marketing manager to use in the customer campaign.\\n\\nNow H Plus Four is ready\\nto market their new great mineral water to their customers.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3873005\",\"duration\":142,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to make a month column\",\"fileName\":\"3811063_en_US_01_07_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":183,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to make a month column from a date column.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6142508,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] H+ Sport needs you to retrieve\\ntheir order data along with the months\\nof when orders were made.\\nThis will make it easier to determine\\nwhen customers are ordering\\nversus having to look at each exact order date.\\nYou'll begin by adding a select statement,\\nso you'll have select star from and orders.\\nThis will gather the data from the orders table.\\nHere you could see the current view of the orders table,\\nbut as you'll notice,\\nwe do not have any month columns at the moment.\\n\\nWe just have this creation date column here.\\nNow you will create two different month columns\\nto provide H+ Sport multiple viewing options\\nfor their order data.\\nThe first column you will create in your select statement\\nwill be the number of the month.\\nFor example, the number six would correspond\\nto the month of June.\\nYou can do this by using the month function\\non the creation date column.\\nWhat this will do is it will return the number\\ncorresponding to each month.\\n\\nNext you'll want to specify\\nwhat you want to name this new column.\\nSo you will use the keyword as and specify your column name.\\nSo let's call it month number.\\nYou are welcome to name the column whatever you want,\\nas long as it is not a current column name\\nin your data table.\\nNow there will be a new column created\\nin your select statement for the month number.\\nAfter that, you will create one more column\\nwith the name of the month, such as October or June.\\nSo on this line, we will use the month name function\\non the creation date column to gather the name of the month\\nfor each entry.\\n\\nLike last time, you'll want to specify the name\\nof your new column.\\nSo here we can call it as,\\nand let's call this one month name.\\nBefore we end our statement, we will adjust our from orders\\nand enter a comma here\\nand insert this at the bottom of our statement.\\nOnce you run this query,\\nyou'll have all the order data like you had before,\\nbut if you scroll all the way to the right side,\\nyou will now have the corresponding month number\\nand month name for each date in your data set.\\n\\nNow the data is ready to go for their order analysis.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3870072\",\"duration\":114,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Add new customer\",\"fileName\":\"3811063_en_US_01_08_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":120,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to proceed with completing the challenge to add a new customer to the customer data table.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2685593,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(light electronic music)\\n- [Instructor] It's time for a challenge.\\nH+ Sport has gained a new customer name, Jane Patterson,\\nthat has purchased some of their mineral waters.\\nIt is your job to add her information\\ninto the customer database\\nto ensure the customer records stay updated.\\nFor this task, it is best to use an INSERT statement,\\nlike you did in the video where you learned\\nhow to insert a new product into a SQL data table.\\nYou will use the format displayed here\\nfor your INSERT Statement.\\n\\nThis includes specifying the columns\\nand associated values to insert into each column.\\nMake sure you insert data into the customer data table\\nfor each of these columns.\\nThe columns you will need to list out are:\\nCustomerID, FirstName,\\nLastName, Email, Phone,\\nAddress, City, State, and Zipcode\\nYou will notice here that these are notated exactly\\nhow they are in the SQL database, so again,\\nmake sure the way you are spelling and capitalizing\\nthese columns are the same.\\n\\nFor the values for each of these columns,\\nyou will enter in 1100, \\\"Jane,\\\"\\n\\\"Paterson,\\\" \\\"jane.paterson@gmail.com,\\\"\\n\\\"(912) 459-2910,\\\"\\n\\\"4029 Park Street,\\\"\\n\\\"Kansas City,\\\" \\\"MO,\\\" and \\\"64161.\\\"\\nAgain, these values are associated with each\\nof the column names you saw previously.\\nThis challenge should take you\\naround five to 10 minutes to complete.\\n\\nI highly recommend you use the coding files\\nfrom the previous videos for reference if you get stuck.\\nAlso, make sure to type out the CustomerID value\\nas an integer and the rest of the values as strings.\\nJoin me in the next video to review the solution\\nfor this challenge.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3869072\",\"duration\":210,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Add new customer\",\"fileName\":\"3811063_en_US_01_09_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":311,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will understand how to successfully complete the challenge to add a new customer to the customer data table.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6885623,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] Welcome back.\\nGreat job on adding Jane Patterson to the Customer database.\\nIf you feel overwhelmed with this challenge, don't worry,\\nI will explain step by step what should have been filled in\\nand what each line of code is doing.\\nLet's head over to Codespaces.\\nYou'll begin your code with an INSERT INTO statement,\\nand this will be for the Customer data table.\\nLet's begin by listing out these columns,\\nso we will have CustomerID,\\nthen we will have FirstName, then LastName,\\nEmail, Phone,\\nAddress, City,\\nState, and ZipCode.\\n\\nNow that your columns are listed out,\\nyou'll want to specify each value to insert for each column\\nwith Jane's information.\\nSo you'll begin by going outside of your parentheses\\nand specifying the VALUES keyword\\nto gather each value for each column.\\nLike last time, you'll want this inside parentheses\\nand separated by commas.\\nThe values you'll want to type out are 1100,\\nJane,\\nPatterson,\\njane.patterson@gmail.com,\\na phone number, which will be (912)459-2910.\\n\\nThen you'll have your address, which is 4029 Park Street.\\nThen you'll have your city with Kansas City.\\nYour state, MO.\\nAnd then, finally, your zip code.\\nRemember, you will use quotation marks\\nfor each of the values except the customer ID, 1100,\\nsince that is going to be inserted into the data\\nas an integer value.\\nOnce your SQL INSERT statement is complete,\\nyou can click Run at the very top of your code\\nto then insert this data into your Customer data table.\\n\\nJust note that if you run this more than once,\\nyou'll most likely get an error message stating\\nthat this row of data already exists.\\nIn order to check if you successfully inserted the data,\\nyou can use the code in the 01_09_check.sql code.\\nSo here you'll see we have SELECT * FROM Customer,\\nand here we have our WHERE clause stating\\nthat we want to only see data\\nwhere CustomerID is equal to 1100.\\nThe reason we do this\\nis so we only see Jane Patterson's information show up.\\n\\nSo if we run this,\\nyour data should show up for Jane Patterson.\\nSo here you'll see we have our customer ID,\\nher first and last name of Jane Patterson,\\nher email address, her phone number, her address,\\nthe city she resides in, the state she resides in,\\nand finally, her zip code.\\nNow, Jane is officially a part of the H+ Sport family\\nand is ready to purchase more products in the future.\\nI hope you enjoyed this challenge\\nand that it helped you reinforce what you learned\\nin this chapter.\\nIf you struggled or were not able to get the code\\nto run properly on your own, that is totally okay.\\n\\nProgramming takes practice to get better at,\\nso keep up the great work.\\n\"}],\"name\":\"1. Maintaining Data\",\"size\":56358634,\"urn\":\"urn:li:learningContentChapter:3871011\"},{\"duration\":1338,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3861091\",\"duration\":143,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to find how many products sold\",\"fileName\":\"3811063_en_US_02_01_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":194,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to find the total number of products and the total number of unique products sold.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4670351,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] H+ Sport wants to gain\\na better overall understanding of the sales they have made.\\nThis way they can inform their sales employees\\nif they're doing a good job overall\\nor need to work on finding ways to increase their sales\\nto meet their quotas.\\nThe company seeks your help with providing them\\na summary analysis on their sales data.\\nLet's head over to Codespaces to do so.\\nTo begin your analysis, you first figure out\\nhow many unique products have sold,\\nalong with how many products have sold overall.\\nYou can accomplish this by using a SELECT statement.\\n\\nThis SELECT statement will be pulling data\\nfrom the OrderItem table.\\nLet's first find the total number of unique products sold.\\nIn order to find this, you can use the COUNT function\\non the ProductID column in order to find\\nhow many times a certain product ID shows up in the dataset.\\nIn this case, you want to use ProductID\\nbecause this is a unique identifier\\nfor each product compared to some\\nof the other column options.\\nSince you want to find the distinct count\\nof how many unique products sold, you'll want\\nto add the keyword DISTINCT in front\\nof your ProductID column name.\\n\\nThis way, it'll count each product ID only once in the data\\ninstead of counting all the instances\\nwhere ProductID shows up.\\nYou can alias this new column using the as keyword\\nand let's call it TotalUniqueProducts.\\nNext, you will figure out\\nhow many products were sold overall.\\nYou can accomplish this\\nby using the SUM function on the Quantity column.\\nThis will sum up all the numerical values in this column\\nand gather that total quantity sold.\\n\\nYou can alias this column as TotalQuantity.\\nFinally, you want to finish out your statement\\nwith your FROM, and you want to look\\nat the OrderItem table to gather your data.\\nOnce you run this,\\nyou will see you have 16 total unique products,\\nso this means there are 16 different products being sold.\\nYou will also see the total quantity is 12,552.\\nThis is the total number of products sold overall.\\n\\nIt looks like the sales employees have been keeping\\nbusy selling all those mineral waters to their customers.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3863083\",\"duration\":76,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to find which items are discontinued\",\"fileName\":\"3811063_en_US_02_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":119,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to determine which products are discontinued.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2647437,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Next, you decide to investigate\\nwhich items have been discontinued.\\nThis way H Plus Four can investigate if it makes sense\\nto bring back any\\nYou can find this by using a SELECT statement.\\nHere, you can just use a star for ease when it comes\\nto selecting all the columns from this table.\\nFor this statement, you'll want\\nto gather data from the Product table.\\nAfter that, you'll want to add a WHERE clause\\nto filter your statement to show only products\\nthat were discontinued.\\n\\nWith this one,\\nyou can have the column status equal to discontinued.\\nWhen it comes to WHERE clauses,\\nyou can type this in all capital letters\\nor all lowercase letters\\nbecause where clauses are not case sensitive in SQL.\\nOnce you run this, you can see there are four total products\\nthat have been discontinued.\\nIf you scroll to the right,\\nyou can see this includes both sizes for the cranberry\\nand mango varieties of the mineral waters.\\n\\nIf you wish to further investigate this on your own,\\nyou can see how many products sold for each of these flavors\\nto see if low sales had anything to do\\nwith their discontinuation.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3871006\",\"duration\":345,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to find any sales people who made no sales\",\"fileName\":\"3811063_en_US_02_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":457,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to join the orders and salesperson tables to determine which sales people did not make any sales.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10667395,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] As you continue in your analysis,\\nyou decide you want\\nto evaluate the sales employees\\nto ensure they're all actually making sales.\\nThis way, the company can know if any\\nof their sales employees are struggling to make a sale\\nand need assistance working on their sales techniques.\\nLet's head over to Codespaces to do this.\\nThere are a few ways you could do this,\\nbut in this case, you will use a select statement\\nthat will join the Salesperson and Orders tables.\\nStart by selecting the salesperson ID,\\nthe first name and the last name.\\n\\nThese columns will be coming from the Salesperson table.\\nIn this particular query, you'll want\\nto add Salesperson in front of the SalespersonID column.\\nThis will specify exactly\\nwhich table this column is coming from.\\nIf you don't specify this,\\nSQL can get confused when there are multiple tables\\nwith the same column name, causing it to return an error.\\nIn this particular case, you don't need\\nto add a table name in front of the first name\\nand last name columns since those only come\\nfrom the Salesperson table.\\n\\nNext, you'll want to gather this information\\nfrom the Salesperson table.\\nOnce you do this, you'll then be doing a join\\non the Orders table.\\nLet's take a moment first, though,\\nto quickly review the different joins in SQL.\\nThere are four main types of joins in SQL,\\nincluding inner, left outer, right outer and full outer.\\nThe types of joins specify exactly\\nwhat data is gathered from each table.\\n\\nStarting with the inner join,\\nthis will only return data\\nthat has matching values in both tables.\\nThis means it will leave out any data from either table\\nthat does not match on the column\\nor columns being joined on.\\nAs you could see in the visual,\\nthe middle portion is the only portion shaded in\\nbecause this is where the data is returning,\\nwhere both tables have matching values.\\nNext, we have the left outer join,\\nwhich returns all records from the left table,\\nalso known as the table in the from portion\\nof this statement and only the matched records\\non the right table.\\n\\nSo in this one, you'll see the visual\\non the left side is completely shaded in,\\nbut the right side only has that overlap shaded in.\\nAfter that is the right outer join,\\nand this is essentially the opposite of a left outer join,\\nwhere it returns all records from the right table,\\nalso known as the table in the join part of the statement,\\nand only the matched records on the left side of the table.\\nSo if you look at the visual here,\\nyou'll see the right side is all shaded in,\\nbut again, that left side only the portion\\nthat overlaps is shaded in.\\n\\nI suggest you only use left or right joins\\nin your statements instead of mixing them.\\nIn this course,\\nyou'll focus on using left joins throughout for consistency.\\nFinally, a full outer join returns\\nall the records from both tables.\\nFor all the rows of data that don't match in a join,\\nSQL will fill in those empty spaces with null values.\\nThe same goes for the join table in left\\nand right outer joins.\\nLet's jump back over to Codespaces to begin your join.\\n\\nIn this case, you'll be doing\\na left outer join on your data.\\nThis time you'll be joining it on the Orders table,\\nand this will allow you to keep all the values\\nfrom the Salesperson table\\nwhen you connect it to the Orders table,\\nbut only keeping the values\\nthat are matched on that Orders table.\\nFor each join clause, you will need to specify which column\\nor columns you are going to be joining on.\\nUsually databases are structured with unique identifiers,\\nalso known as keys for this purpose.\\n\\nThese two tables have the SalespersonID column in common.\\nSo this is what you'll be joining them on.\\nFirst, you'll need to specify the keyword on,\\nand then you'll be gathering\\nthe different SalespersonID columns to join on.\\nSo first, you'll gather the Salesperson.SalespersonID column\\nto come from the Salesperson table,\\nand you'll then equal this to the orders version\\nof the SalespersonID column.\\n\\nGenerally, when using a join,\\nI suggest ordering the columns the same\\nas they're ordered in the from\\nand join clauses to avoid any confusion\\non which column belongs to which table.\\nSo for this one, since the Salesperson table is\\nin the front portion of the statement,\\nyou'll do the salesperson version\\nof the SalespersonID column first,\\nand since the Orders table is in the join portion,\\nyou will do the orders version\\nof the SalespersonID column after that.\\nAfter you have the data joined,\\nyou'll want to filter out any null values\\nin the Orders table for SalespersonID.\\n\\nSo you can do this by using a where clause\\nand looking at the orders version\\nof the SalespersonID column,\\nand here you can use the function is null to check\\nto see if any of the values are null.\\nThis way, you can find out if any employees have not made\\nany sales since they will not appear in that Orders table,\\nbut they will again appear in that Salesperson table.\\nOnce you run this, you could see only one employee shows up.\\n\\nIt looks like Robin Vasquez\\nwith salesperson ID 104 has not made\\nany sales according to the data.\\nNow you'll be able to pass his name along\\nto the sales manager, so they can investigate\\nwhy he has not made any sales thus far\\nand assist him with making that first critical sale.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3868074\",\"duration\":168,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to find the top product size sold\",\"fileName\":\"3811063_en_US_02_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":316,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to find the top size of products sold.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6080836,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructional] Let's dive a bit deeper into the details\\nof the mineral water products sold by H Plus Sport.\\nThe mineral waters come in two sizes,\\n20 ounces and 32 ounces.\\nIt'll be helpful for H Plus Sport\\nto know if one size is significantly\\nmore popular than the other.\\nThis way, they can determine if they should still sell\\nboth sizes or just focus on one if that is what the majority\\nof customers are purchasing.\\nLet's head over to Code Spaces to figure this out.\\n\\nIn order to do this, you will use another select statement.\\nYou'll be gathering the size column from the product table\\nalong with summing the quantity column\\nfrom the order item table.\\nYou can alias this new column as total quantity.\\nYou'll gather this data from the order item table.\\nWith this table, you will then want to perform\\na left outer join on the product table.\\nThis way, you'll keep all the orders made\\nin the order item table and potentially leave out\\nany sizes that didn't sell that come from the product table.\\n\\nYou will join these two tables on the product ID column,\\nso you'll have the order item version of product id,\\nand you will equal that to the product table version\\nof product id.\\nNext, you will want to group the data\\nusing a group by clause.\\nThe group by clause is used to group rows together\\nby one or more columns.\\nThey are commonly used when there are aggregate functions\\nin the select statement, such as the sum function\\nyou used on the quantity column.\\n\\nThis group by clause will help you better summarize\\nthe results returned.\\nFor this query, you will group by the size column.\\nThis will ensure you only have one row returned\\nfor each size listed with the total quantity\\nsummed up for that size.\\nFinally, you'll want to have the order by clause\\nto sort the data by the total quantity.\\nIn this case, you can sort it in descending order\\nto have the highest value at the top.\\n\\nFor this particular query, you don't have\\nto have an order by clause due to the small output,\\nbut it is good practice for future queries\\nthat will have many rows of data to organize.\\nOnce you run this, you could see there were sales made\\nfor both sizes of 20 and 32 ounces.\\nYou'll see the total amount sold\\nfor the 20 ounces was 6,346,\\nand the total amount sold for the 32 ounce is 6,206.\\nSince these values are fairly similar to each other,\\nit would be good to recommend the company keep producing\\nand marketing both sizes to their customers.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3871007\",\"duration\":169,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to find the top three items sold\",\"fileName\":\"3811063_en_US_02_05_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":259,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to find the top three products sold.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6093183,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The H+ Sport marketing team\\nwants to know what their top selling products are\\nin order to see if there's a certain flavor\\nand/or size that stands out among the rest.\\nYou'll gather the top three products\\nto send over to help the marketing team know\\nwhat to push in their upcoming marketing campaign.\\nIn Codespaces, you'll use a SELECT statement to do this\\nand you'll select Variety and Size\\nboth from the Product table.\\nThese two columns will help distinguish exactly\\nwhich product is being pulled and is clear to understand\\nthan the ProductID.\\n\\nAfter that, you'll get a SUM on the Quantity column\\nand you can alias this as TotalQuantity\\nlike you did last time.\\nAnd what this will do\\nis it'll show how many products were sold\\nfor each combination of Variety and Size.\\nFor this particular query, you don't need to specify\\nwhere any of these columns come from\\nsince they are all unique to their respective tables.\\nYou'll gather this data from the OrderItem table,\\nand then you'll do a LEFT OUTER JOIN\\non the Product table.\\n\\nThese two tables,\\nyou will join them on their ProductID columns,\\nso we will begin with the OrderItem version\\nof the ProductID column\\nand equal that to the Product version\\nof the ProductID column.\\nAfter that, you'll add a GROUP BY clause to your statement\\nand group it on the ProductID column.\\nFor this particular column,\\nit does not matter which table it comes from.\\nIt can come from OrderItem or Product.\\nIn this case, we will just use Product.\\nNext, you will add an ORDER BY clause to your statement,\\nand you'll order it by the TotalQuantity\\nin descending order.\\n\\nThis way, you will return the mineral waters\\nwith the highest quantity sold at the top of your data.\\nSince you only want to return the top three products sold\\ninstead of all the products sold,\\nyou'll need to add a LIMIT clause\\nat the end of your statement.\\nThe LIMIT clause is used to specify exactly\\nhow many rows of data you want to return.\\nThis clause is always added\\nat the very end of your statement.\\nIn your case, you'll have the LIMIT 3\\nbecause you only want to return\\nthe top three values from your data.\\n\\nOnce you run this,\\nyou could see the top three products sold by quantity.\\nIt looks like the orange variety in size of 20 ounces\\nhad the highest quantity sold by a decent margin\\nwith 1,312 sold.\\nAfter that is the raspberry variety in the 20 ounce size,\\nand then the lemon lime variety in the 32 ounce size.\\nYou can send this information over to the marketing team\\nso they can highlight these three products\\nin their upcoming marketing campaign.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3868075\",\"duration\":222,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to find sales by month and year\",\"fileName\":\"3811063_en_US_02_06_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":326,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to find the product sales broken down by month and year.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7975678,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that you've seen how many products\\nwere sold overall by size and by item, it is time\\nto analyze the sales by month.\\nSince the data extends over the span of 15 months,\\nyou will want to separate the data by month and year.\\nYou can do this with a select statement by gathering data\\nfrom the orders and order item tables.\\nYou will first want to gather the name of the order month\\nby using the month name function.\\nYou will use this on the creation date column\\nand you could alias it as month name.\\n\\nNext, you will gather the order year\\nby using the year function on the creation date column\\nagain, and then you'll use the as to alias it as order year.\\nFor this particular query, you'll gather\\nthe total orders made, total quantity sold,\\nand the total dollar amount purchased by customers\\nsince these are all valid measures for determining\\nwhat sales were made during this time period.\\nTo get the total number of orders made,\\nyou can use the count function on the orders table order ID.\\n\\nAgain, for this particular column, you need to alias it\\nfor the orders table since this column is not unique\\nfor this particular table.\\nYou can alias this as total orders,\\nand then next you will gather the total quantity sold.\\nYou could do this by using the sum function\\non the quantity column and alias this as total quantity.\\nFinally, you will use the sum function again,\\nbut this time you will use it on the total due column\\nand alias this as total amount\\nfor the total dollar amount sold.\\n\\nOnce you get all the columns selected,\\nyou'll gather the data from the orders table\\nand you'll do a left outer join on the order item table.\\nFor this particular query, you will be joining the data\\non the order ID column,\\nso you'll get the orders table version\\nof the order ID column and equal that\\nto the order item table version of the order ID column.\\nRemember, since this is a left outer join,\\nit will keep all the data values from the orders table\\nand only the matching values from the order item table.\\n\\nSince you are using aggregate functions, it is a great idea\\nto use a group by statement to group the data together.\\nSo here you will use group by, and in this case,\\nyou will want to group it on the month name\\nalong with the order year.\\nThis will ensure the results are displayed properly\\nwith the multiple years amongst the different months.\\nFinally, you'll add in an order by clause to order the data\\nfrom the oldest date to the newest date.\\nYou'll first want to order by the order year in order\\nto have the oldest year show up first.\\n\\nAfter that, you'll want to order it by month.\\nUnfortunately, month name would not make sense to use\\nbecause it will organize it alphabetically.\\nInstead of month name, you can use the month function\\nwithin your order by statement on the creation date column.\\nThis will organize it by the number of the month\\ninstead of the name of the month,\\nproviding the correct results.\\nOnce you run this query, you can evaluate the total orders,\\ntotal quantity, and total dollar amount sold for each month\\nand year combination in the data.\\n\\nSo for example, you can see for June 2015,\\nthere were 44 total orders made, 1052 products sold,\\nand approximately $8,916 sold.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3873006\",\"duration\":100,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Average daily sales\",\"fileName\":\"3811063_en_US_02_07_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":166,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to proceed with completing the challenge to determine the average daily sales.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2301320,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] It's time for another challenge.\\nYou just found the sales made by month and year,\\nbut it would also be good to know\\nwhat the average daily sales were.\\nYou can provide this measure to (indistinct)\\nas a benchmark to use for the future.\\nYou can determine this by using the techniques learned\\nthroughout the previous videos.\\nFor this task, it is best to use a SELECT statement\\non the Orders and OrderItem tables.\\nThis includes having SELECT\\nwhat the columns you want to return,\\nyou're FROM with which table\\nyou want to gather the data from,\\nand finally, the JOIN on your two tables specifying\\nwhich column to join them on.\\n\\nYou will use some arithmetic to get the average daily sales.\\nTo do this, you will divide the total quantity sold\\nby the total number of days in the dataset.\\nNote that this particular equation\\nwill only include days that a sale was made on.\\nI suggest using the CreationDate column\\nto gather the number of days.\\nThen you will gather this information from the Orders table.\\nThen you will want to do a LEFT OUTER JOIN\\non the OrderItem table,\\nsince you will want to keep all the dates\\nfrom the Orders table,\\nbut not add any items unnecessarily from the OrderItem table\\nin case there are any.\\n\\nThis challenge should take you\\naround 5 to 10 minutes to complete.\\nI highly recommend you use the coding files\\nfrom the previous videos for reference if you get stuck.\\nAlso note that for the CreationDate column,\\nyou don't need to convert the format\\nto remove the timestamp since they are already zeroed out.\\nJoin me in the next video\\nto review the solution for this challenge.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3861092\",\"duration\":116,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Average daily sales\",\"fileName\":\"3811063_en_US_02_08_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":154,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will understand how to successfully complete the challenge to determine the average daily sales.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3923144,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] Welcome back.\\nExcellent work on finding out the average daily sales.\\nIf this challenge felt overwhelming, don't worry.\\nI will walk you through the code step-by-step\\nto explain how to accomplish this task.\\nSo if we head over to Codespaces,\\nyou can begin your code by using a SELECT statement\\nand then jump right in to creating\\nyour average daily sales column.\\nFor this one, you'll want to use the SUM function\\non the quantity column\\nto gather the total number of items sold.\\n\\nThen you will be dividing this by the days.\\nFor this column, you'll want to use the COUNT function\\non the creation date column,\\nbut make sure you add in DISTINCT\\nto ensure you only count each date once.\\nThis will gather the distinct number of days\\nwhere at least one item was sold.\\nYou can alias this new column as AverageDailySales.\\nYou'll gather this data from the Orders table\\nand then do a LEFT OUTER JOIN on the OrderItem table.\\n\\nFor these two tables,\\nyou will join them on their OrderID column,\\nso we will begin with the Orders version\\nof the OrderID column\\nand equal that to the OrderItem version\\nof the OrderID column.\\nOnce you run this,\\nNow, you can notify H+ Sport\\nthat they sell almost 80 mineral waters\\neach day their employees make at least one sale.\\nThey can use this as a benchmark\\nto see what days in the future are performing well\\nand which ones are slow for business.\\n\\nI hope you like this challenge\\nand that it assisted you with reinforcing\\nwhat you learned in this chapter.\\nIf you struggled or were not able to get the code\\nto run properly on your own, that is totally okay.\\nProgramming takes practice to get better at,\\nso keep up the great work.\\n\"}],\"name\":\"2. Exploring Sales Data\",\"size\":45028078,\"urn\":\"urn:li:learningContentChapter:3868077\"},{\"duration\":1173,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3869073\",\"duration\":236,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to find top customers\",\"fileName\":\"3811063_en_US_03_01_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":312,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to find the top customers by the number of transactions and amount of product purchased.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8778229,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that you helped H+ Sport\\ngain a better overall understanding of their sales,\\nyou will dive deeper into the customers behind these sales.\\nThis way, you can help the sales and marketing teams\\nbetter understand H+ Sports' customers\\nand their purchasing patterns.\\nTo begin this analysis,\\nyou will help find who H+ Sports' top customers are.\\nYou could do this by gathering the data\\nby the total number of orders made,\\ntotal quantity of goods purchased,\\nand total dollar amount sold.\\n\\nAll three of these are useful measures\\nto determine how much customers are buying\\nand who is purchasing the most.\\nLet's head over to Codespaces\\nto determine these top customers.\\nTo find the top customers,\\nyou'll create a SELECT statement\\ngathering data from the orders,\\norder item, and customer tables.\\nYou will first gather the FirstName\\nand the LastName of each customer.\\nAfter that, you will begin by gathering\\nthe total number of orders made by each customer.\\nTo do this, you'll do a COUNT\\nand you will want a DISTINCT count of the OrderID\\nfrom the Orders table.\\n\\nFor this one, again, make sure you specify\\nit is from the Orders table\\nbecause this column name is not unique to this table.\\nYou can alias this new column as TotalOrders.\\nAfter that, you can get the total quantity purchased\\nby each customer.\\nSo to do this, you could do SUM on the Quantity column\\nand alias this as TotalQuantity.\\nFinally, you can then gather the total amount spent\\nby each customer in dollars,\\nso this will be the SUM on the TotalDue column,\\nand you could alias this one as TotalAmount.\\n\\nOnce your columns are selected,\\nyou will want to gather data from the Orders table,\\nso we will gather this from the Orders table,\\nand you'll be doing not one but two Left Outer Joins\\nbecause there are three tables total\\nthat are going to be connected.\\nLet's begin with the LEFT OUTER JOIN on the OrderItem table,\\nand this one we'll be joining on the OrdersID column.\\nSo first, you'll call the OrderID column\\nfrom the Orders table\\nand equal that to the OrderItem.OrderID column.\\n\\nLet's go onto our next join,\\nwhich will be a LEFT OUTER JOIN on the Customer table,\\nand this one will be joined on the CustomerID column\\nfrom each table.\\nSo again, start with the Orders one\\nand then go to the Customer version\\nof the CustomerID column.\\nNow that all three tables are joined,\\nyou will want to group the data by CustomerID,\\nso you'll add in your GROUPBY clause.\\nAnd this one you can specify it\\nto come from either the Customer or Orders table,\\nit doesn't matter.\\n\\nSo in this case, we'll just use\\nthe Customer version of the CustomerID column.\\nFinally, you'll want to order the data\\nby using an ORDERBY clause.\\nYou can order this by the total orders,\\ntotal quantity, or total amount.\\nLet's go with total amount for now,\\nand let's have it also be in descending order\\nto have the largest dollar amount at the top of the list.\\nWhen you run this,\\nyou'll see the list of H+ Sports customers\\nwith the total number of orders made,\\ntotal quantity of goods sold,\\nand the total dollar amount purchase for all the orders,\\nwith the top one being at the top of the list,\\nit looks like Virginia Porter and Deborah Kennedy\\nhave purchased the highest dollar amount.\\n\\nIf you scroll to the right,\\nyou can see they both purchased over $3,000 worth\\nwith their one order each.\\nTake a moment to edit this query on your own\\nto see who the top customers are by the total orders\\nand total quantity.\\nThis way, you can get an overall understanding\\non who is purchasing what.\\nNow that you know who the top H+ Sports customers are,\\nyou can notify the sales team\\nto reach out to these customers\\nto express the company's appreciation for their business.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3872009\",\"duration\":155,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to find infrequent customers\",\"fileName\":\"3811063_en_US_03_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":215,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to determine which customers have only made one purchase.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6119094,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] One way to help H+ Sport\\nincrease its sales\\nis to determine which customers have only made one purchase.\\nIt is generally easier to make someone a repeat customer\\nthan to find a brand new customer.\\nLet's use the SELECT statement\\nto compile a list of these customers.\\nFor this SELECT statement,\\nfrom the Orders and Customer tables.\\nFirst, you'll be gathering the CustomerID,\\nbut for this one,\\nyou'll need to specify whether it comes from the Orders\\nor the Customer table.\\nIn this case, it doesn't matter.\\nYou can use either one.\\n\\nSo we will go ahead and use the Customer version\\nof that column.\\nNext, you'll gather the FirstName\\nand the LastName of the customer.\\nAfter that, you'll be gathering the total number of orders,\\nso you'll do a COUNT and a DISTINCT COUNT\\non the OrderID column,\\nand you can alias this as TotalOrders.\\nThis column is not necessary for this query,\\nbut it's a good sanity check to ensure\\nthat each of the customers\\nreturned have only made one order with H+ Sport.\\n\\nNext, you'll gather this data FROM the orders table\\nand do a LEFT OUTER JOIN on the Customer table.\\nFor this one, you'll be joining them\\non the CustomerID column.\\nSo again, you want to do the Orders version\\nof the CustomerID column\\nand equal that to the Customer version\\nof the CustomerID column.\\nAfter that, you'll be adding a GROUP BY clause\\nto group the data.\\nYou'll be using the CustomerID to group the data on.\\nAnd again, you need to specify\\nwhether it comes from the Orders or the Customer table,\\nlike last time, doesn't matter.\\n\\nYou could choose either one.\\nFinally, you'll include what is called a HAVING clause.\\nA HAVING clause is similar to a WHERE clause\\nwhere you can specify a condition\\nthat needs to be met for the data to be returned.\\nThe HAVING clause is necessary\\nwhen you want to use an aggregate function\\nwithin your condition\\nsince WHERE clauses can't contain\\naggregate functions within themselves.\\nIn your case, you'll be doing the DISTINCT COUNT\\nof the OrderID column,\\nand you'll have this equal to one.\\n\\nWhat this will do\\nis it will ensure SQL returns only customers\\nthat have made exactly one purchase from H+ Sport.\\nWhen you run this,\\nyou'll gather a list of all the infrequent customers\\nwho have only made one purchase from H+ Sport.\\nIf you look at the row count on the bottom right here,\\nyou'll see there are 158 customers who meet this condition.\\nThis customer list will be very useful to provide\\nto the sales and marketing teams\\nto help them reach out to these customers\\nto see if they would be willing to make another purchase.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3861093\",\"duration\":145,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to determine which state purchases the most products\",\"fileName\":\"3811063_en_US_03_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":199,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to find which state that customers reside in purchases the most products.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5282216,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Something else that may be helpful\\nfor H+ Sports Marketing team is to know\\nwhich state's customers have purchased\\nthe most amount of products.\\nThis way, the marketing team can create targeted ads\\nfor potentially new customers in that area.\\nLet's use a SELECT statement\\nto determine which state is at the top.\\nFor this SELECT statement,\\nyou will want to gather data from the Orders,\\nOrderItem, and Customer tables.\\nYou'll begin by gathering the State column\\nthat'll come from the Customer table.\\nYou can find the top state\\nby the total number of orders made,\\ntotal quantity of goods sold,\\nor the total dollar amount sold.\\n\\nFor now, let's gather the sum of the Quantity column\\nto get the total number of goods sold.\\nYou can alias this as TotalQuantity.\\nYou'll gather this data from the Orders table\\nsince this is the table that is connecting\\nthe two other tables you are actually selecting data from.\\nLet's begin by doing a LEFT OUTER JOIN\\non the OrderItem table.\\nAnd for this one,\\nyou'll be joining it on\\nthe Orders version of the OrderID column\\nequal to the OrderItem version of the OrderID column.\\n\\nNext, you'll do another LEFT OUTER JOIN\\non the Customer table.\\nFor this one, you'll do it on the Orders table version\\nof the column CustomerID,\\nand then you'll equal that to\\nthe Customer version of the CustomerID.\\nAfter that, you'll want to group the data by the State.\\nThat way the State only shows up once\\nregardless of how many orders were made.\\nNext, you want to order the data using the ORDER BY clause\\non the TotalQuantity column,\\nand you could do this in descending order\\nto ensure you have the highest quantity\\nat the top of your data.\\n\\nSince you only need the top state,\\nyou can do a LIMIT 1 clause at the very end,\\nand what this will do is ensure\\nthat it returns only the very top state and no other states.\\nWhen you run this,\\nyou will see the top state by total quantity sold is Texas\\nnotated as TX,\\nand they sold 1,460 products.\\nNow you can notify the marketing team\\nto look into potentially creating\\ntargeted marketing campaigns\\nfor those in the state of Texas.\\nI also suggest taking a moment\\nto edit this query on your own,\\nto see which states are the top ones\\nby the total orders made and the total dollar amount sold.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3871008\",\"duration\":188,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to find what products are sold together\",\"fileName\":\"3811063_en_US_03_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":263,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will learn how to find what products are often sold together.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7086435,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Another way H+ Sport can increase their sales\\nis learning which products often sell together.\\nThis way they can market these products together\\nduring future sales interactions.\\nLet's use a select statement on the order item table\\njoined to itself to accomplish this.\\nBegin by selecting the product ID column\\nfrom the A version of this table.\\nYou can alias the table whatever you want,\\nI'm just using A for brevity,\\nbut as long as you're consistent throughout the query,\\nyou're welcome to call it something else.\\nLet's alias this new column as Product ID one.\\n\\nYou'll do the same thing as last time,\\nbut you'll do the B version of the product ID column.\\nYou can alias this one as product ID two.\\nAfter that, you'll be doing count on star,\\nso you can count all the columns\\nand see how many times these items were purchased together.\\nYou can alias this as times purchased.\\nYou'll be gathering this data from the order item table,\\nand again, since you're joining it to itself,\\nyou'll be aliasing this one as A.\\n\\nSince you are doing an inner join\\non the order item table to itself,\\nthis is what we call a self join.\\nA self join is when you join a table to itself.\\nThis version of the table, you'll alias it as B.\\nThese tables are going to be joined\\non two different columns.\\nThe first is the order ID,\\nso you'll join the order ID version for the A table\\nand then the B version of the order ID column.\\nSince you're joining it on two columns,\\nyou'll need to use the and operator\\nfor the next column you wish to join on.\\n\\nNext column you're joining on are the product ID columns.\\nOne way you can join these two columns\\nis by using the not equal sign,\\nwhich is an exclamation point and an equal sign,\\nbut this will cause duplicate values in your data.\\nFor example, it will return the value three\\nfor the product ID one column\\nand the value four for the product id two column.\\nAlong with that, it will also return the value of four\\nfor the product ID one column\\nand the value of three for the product ID two column.\\n\\nThese are the same combination just in different orders,\\nso in order to avoid this,\\nyou will want to use a greater than\\nor less than sign to only have one combination\\nof each product Id show up.\\nSo let's go ahead and use a less than sign.\\nAnd again, you want to do this on the B version\\nof the product ID column.\\nAfter that, you want to group the data by both product IDs,\\nso you'll have it grouped on the A version of the product ID\\nand the B version of the product ID.\\nFinally, you'll want to order the data\\nusing an order by clause\\nand have it be ordered by the times purchased\\nin descending order to have the combination\\nof times purchased with the highest amount\\nat the top of your data.\\n\\nOnce you run this, you will see the combination of products\\nthat are most frequently purchased together.\\nIt looks like products nine and 14, nine and 10\\nand nine and 11 are most frequently purchased together.\\nIt is interesting that the product nine shows up\\nin all top three values.\\nNow you can notify the marketing and sales team\\nof this trend so they can market these products together\\nin the future.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3871009\",\"duration\":219,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Repeat customer rate\",\"fileName\":\"3811063_en_US_03_05_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":531,\"solutionVideo\":false,\"editingNotes\":\"Cut 4:44 to 7:23\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will learn how to determine what the repeat customer rate is.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7513521,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] A common measure businesses use\\nto track their customers is repeat customer rate.\\nThis is the proportion of customers\\nwho have made at least two purchases\\nduring a certain time period.\\nThis way businesses can see how many of their customers\\ncome back to buy more from them,\\nwhich can indicate how good or not\\ntheir customer retention rate is.\\nThe way you calculate this is by dividing the number\\nof repeat customers by the total number of customers.\\nRemember, repeat customers are those\\nwho have made at least two purchases from H+ Sport.\\n\\nFor this query, you will need to use\\nwhat is called a common table expression,\\nalso known as a CTE.\\nA CTE acts as a virtual table created\\nand used within a query\\nand then deleted after the query executes.\\nCTEs are used by adding a WITH clause\\nbefore the SELECT, INSERT, UPDATE, DELETE,\\nor MERGE statements.\\nLet's head over to Codespaces\\nto calculate the repeat customer rate for H+ Sport.\\nYou'll begin your query by creating a CTE\\nusing the WITH clause, and you'll call it Repeat_Customers.\\n\\nYou will use the AS keyword\\nand parentheses to contain your new expression.\\nWithin the CTE, you'll gather all the customer IDs\\nthat correspond with customers\\nwho have made at least two purchases from H+ Sport.\\nYou'll begin by having your SELECT statement\\nand selecting the CustomerID,\\nand you'll alias this as Repeat_Cus\\nin order to avoid confusing it later on\\nwith the other customer ID column.\\nYou'll gather this from the Orders table\\nand group it by the CustomerID.\\n\\nFinally, you will use a HAVING clause\\nin order to count the OrderID\\nin with this count.\\nYou want to ensure\\nthat the count of this OrderID is greater than one.\\nThis will ensure the return customers are those\\nwho have made at least two purchases from H+ Sport.\\nNow that your CTE is complete, you can continue on\\nwith your select statement.\\nLet's gather that customer repeat rate.\\nTo do this, you will first gather the distinct count\\non the Repeat_Cus column that you created in your CTE.\\n\\nYou will divide this by the distinct count\\nof the CustomerID column.\\nIn order to show this as a percentage,\\nyou can multiply this by 100 and then alias the column\\nas CustomerRepeatRate.\\nYou'll be gathering data from the orders table\\nand performing a left outer join on your CTE\\nRepeat_Customers.\\nRemember, your CTE acts like any other SQL table\\nwithin this query, so you will join it the same way\\nyou would any other SQL table.\\n\\nFor these tables, you will be joining them on\\nthe orders version of the CustomerID column\\nand equal that to the Repeat_Customer's CTEs column\\nRepeat_Cus.\\nOnce you run this, you can see the repeat customer rate\\nis 11.73%.\\nThis means for every 100 customers H+ Sport has\\nalmost 12 of them make at least one more purchase\\nfrom their first one.\\nThis rate may be good or bad, depending on the business,\\nso it is good to share this with management\\nand continue to track it over time\\nas a key performance indicator to see how it changes.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3870073\",\"duration\":71,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: How to find new customers\",\"fileName\":\"3811063_en_US_03_06_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":99,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to proceed with completing the challenge to determine how many new customers have not made a purchase yet.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1827432,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(gentle bright music)\\n- [Instructor] Let's do one last challenge.\\nYou've evaluated the sales patterns of H+ Sport's customers,\\nbut there's a whole list of new customers\\nin the customer database who have yet to make a purchase.\\nYou can find this list of customers\\nwho have yet to make a purchase\\nby using the techniques learned\\nthroughout the previous videos.\\nYou can accomplish this task by using a SELECT statement\\nwith data from the Orders and Customer tables.\\n\\nYou will gather the first name\\nand last name for each customer.\\nThen you'll gather the total number of orders\\nto check that they're all zero when your list returns.\\nYou'll be gathering data from the Customer table\\nand do a left outer join with the Orders table.\\nYou'll want to group this data by each customer.\\nFinally, you'll use a HAVING clause\\nto gather data from customers\\nwho have made exactly zero orders.\\nThis challenge should take you\\naround 5 to 10 minutes to complete.\\n\\nI highly recommend you use the coding files\\nfrom the previous videos for reference if you get stuck.\\nJoin me in the next video\\nto review the solution for this challenge.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3861094\",\"duration\":159,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: How to find new customers\",\"fileName\":\"3811063_en_US_03_07_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":237,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will understand how to successfully complete the challenge to determine how many new customers have not made a purchase yet.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5637171,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Narrator] Welcome back!\\nGreat work on discovering the customers\\nwho have yet to make their first purchase.\\nIf this challenge felt overwhelming, don't worry.\\nI will walk you through the code step-by-step\\nto explain how to accomplish this task.\\nLet's make our way over to Codespaces.\\nYou'll begin your code with a SELECT statement\\nwith data from the orders and customer tables.\\nYou'll gather the FirstName\\nand the LastName for each of these customers.\\n\\nThen you'll do a COUNT on the OrderID\\nand alias this as TotalOrders.\\nWith this column, it should show zero\\nfor all the options to use to check to make sure\\nthat all the customers returned have yet to make a purchase.\\nBecause all the values should be zero,\\nyou don't need to do a DISTINCT COUNT\\nof OrderID in this case.\\nYou'll be gathering this data\\nFROM the Customer table and doing a LEFT OUTER JOIN\\non the Orders table.\\nIn this case, you want to make sure\\nyou are doing the LEFT OUTER JOIN\\non the customer table to ensure\\nthat all the customers are returned\\nbecause only customers\\nwho have made at least one purchase will show\\non the Orders table.\\n\\nThis way, we don't have any customers\\nfall through the cracks.\\nYou can join these two tables ON the CustomerID,\\nso you'll do the CustomerID for the Customer table\\nand equal that to the Orders version of the CustomerID.\\nNext, you'll want to have a GROUP BY statement to allow you\\nto have that HAVING clause,\\nso you'll group that on the CustomerID\\nAnd then finally, you'll create your HAVING clause.\\nWhat this HAVING clause will do is it will gather data only\\nfrom customers who have made exactly zero orders.\\n\\nThe way you can do this is getting a COUNT on the OrderID\\nand equaling this to zero.\\nAgain, this time you don't need\\nto do a DISTINCT COUNT on the OrderID\\nsince zero should stay the same regardless.\\nOnce you run this,\\nyou'll have a long list of customers who have yet\\nto make their first purchase with H+ Sport.\\nIf you look at the bottom right for the row count,\\nyou can see there are 822 customers on this list.\\nIt'll be very helpful for the marketing\\nand sales teams to use this list\\nto continue pursuing these customers\\nto see if they can get them to make that first purchase.\\n\\nI hope you liked this challenge\\nand that it assisted you with reinforcing\\nwhat you learned in this chapter.\\nIf you struggled or were not able to get the code\\nto run properly on your own, that is completely okay.\\nYou're doing a great job by continuing to persevere\\nthrough the trial and error process of coding.\\n\"}],\"name\":\"3. Exploring Customer Data\",\"size\":42244098,\"urn\":\"urn:li:learningContentChapter:3868078\"},{\"duration\":49,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3871010\",\"duration\":49,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"3811063_en_US_04_01_MM30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"editingNotes\":\"Tetris LA SAN:\\n/Volumes/Video/Video In Progress/Complete Your First Project in SQL_1283446/2_Project/2_Footage/2_Live Action\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will understand how to continue to use SQL to analyze retail and other data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2196976,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- That brings us to the end of the course,\\nbut you have only scratched the surface of what SQL can do.\\nThis isn't the end, and I encourage you\\nto take the initiative\\nto continue improving your SQL skills.\\nYou can continue to analyze other data sets\\nalong with refining your SQL skills in other areas\\nsuch as creating and managing databases.\\nYou can post the project you completed in this course\\non your GitHub or LinkedIn to showcase your SQL skills\\nto potential employers.\\n\\nI would love to connect with you on LinkedIn\\nand learn more about your experience with this course.\\nYou can also check out my other LinkedIn learning courses,\\nwhich can be found on my LinkedIn profile.\\nAgain, I'm Megan Sylvey, and I thank you for joining me\\nand learning more about how you can expand your horizons\\nwith SQL.\\n\"}],\"name\":\"Conclusion\",\"size\":2196976,\"urn\":\"urn:li:learningContentChapter:3868079\"}],\"size\":164071080,\"duration\":4634,\"zeroBased\":false},{\"course_title\":\"Intermediate SQL for Data Scientists\",\"course_admin_id\":5925685,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":5925685,\"Project ID\":null,\"Course Name\":\"Intermediate SQL for Data Scientists\",\"Course Name EN\":\"Intermediate SQL for Data Scientists\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"&lt;p&gt;Organizations are increasingly adopting digital transformation strategies that lead to greater data generation and, as a result, a greater need for data analysis. This course is designed for analysts and data scientists who work with SQL databases. Instructor Dan Sullivan outlines how to perform common data science tasks, including finding, exploring, and extracting data within relational databases. Explore the basics of using aggregates, statistical functions, window operations, regular expressions, and subqueries. Along the way, learn how to use JSON, a semistructured data type, and perform join operations to enable more complex queries.&lt;/p&gt;&lt;p&gt;This course is integrated with GitHub Codespaces, an instant cloud developer environment that offers all the functionality of your favorite IDE without the need for any local machine setup. With GitHub Codespaces, you can get hands-on practice from any machine, at any time\u00e2\u20ac\u201dall while using a tool that you\u00e2\u20ac\u2122ll likely encounter in the workplace. Check out \u00e2\u20ac\u0153Using GitHub Codespaces with this course to learn how to get started.&lt;/p&gt;\",\"Course Short Description\":\"Get hands-on experience using SQL for common data science tasks, including exploratory data analysis, data transformation, complex querying, and aggregations.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":7382682,\"Instructor Name\":\"Dan Sullivan\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Data Architect, Author, and Instructor\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2025-03-14T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"Yes\",\"LIL URL\":\"https://www.linkedin.com/learning/intermediate-sql-for-data-scientists-25322592,https://www.linkedin.com/learning/intermediate-sql-for-data-scientists-revision-fy25q2\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Data Science\",\"Primary Software\":\"SQL\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":14679.0,\"Visible Video Count\":57.0,\"Learning Objectives\":\"Perform statistical functions and string, numeric, and regular expression functions in SQL.,Apply advanced filtering and aggregation techniques.,Perform various types of join operations.,Work with ordered datasets by windowing functions.,Use JSON data structures and common table expressions.\",\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":49,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5241087\",\"duration\":24,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"The need for SQL in data science\",\"fileName\":\"5925685_en_US_00_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":27,\"solutionVideo\":false,\"editingNotes\":\"Storyboarded\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1261780,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Dan] If you work with data,\\nyou will likely work with SQL at some point.\\nIn this course, we build the kinds of SQL queries\\nthat data analysts and data scientists often use,\\nincluding specialized functions\\nand operations that will save you time\\nand help improve the quality\\nand depth of your analysis.\\nMy name is Dan Sullivan,\\nand I'm a data architect specializing in data science.\\nStart this course today\\nto learn how to perform analysis tasks faster\\nand more efficiently with SQL.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5236203\",\"duration\":25,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"5925685_en_US_00_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":33,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":628125,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now I do make some assumptions\\nabout what you already know.\\nNow, this is an intermediate SQL course,\\nso I assume you have some familiarity\\nwith relational databases and SQL.\\nAnd I also assume that you're comfortable working with\\nan editor or an integrated development environment, or IDE.\\nSo for example, we'll be working in Visual Studio Code\\nin the Codespaces environment within GitHub.\\n\"}],\"name\":\"Introduction\",\"size\":1889905,\"urn\":\"urn:li:learningContentChapter:5232359\"},{\"duration\":1516,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5232358\",\"duration\":423,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview of data science operations\",\"fileName\":\"5925685_en_US_01_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"editingNotes\":\"Repurpose: 2875265_01_01_XR30_data_science_overview with edits - this video has graphics and will need to be edited\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how data science fits in a larger organizational perspective.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12906800,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Data is an integral part\\nof how many organizations work,\\nand that doesn't matter if those organizations\\nare businesses, governments, research institutes,\\nor other types of organizations.\\nNow, sometimes when we think about data,\\nwe think about small amounts of data,\\nsay for example, the data that's needed for me\\nto complete an online transaction so I can, for example,\\npurchase a book online.\\nBut other times, we're looking at large amounts of data\\nbecause our interest isn't so much as, say,\\nconducting a business transaction,\\nbut analyzing, say, thousands of transactions\\nand trying to gain insights\\ninto our customers' behaviors and interests.\\n\\nWhat we're focused on in this course is that second category\\nof sort of data problem.\\nWe want to help our colleagues gather insights from data\\nto help with the operations of our particular organizations.\\nSo to do that, we will set up, say,\\ndata science operations within our organizations.\\nWell, what kinds of things do we need to do\\nfrom a data perspective\\nif we want to have a data science operation?\\nWell, first of all, we have to be able to link data\\nfrom different sources, because no one system,\\nwhether it's a transaction processing system\\nor an HR system that tracks employees\\nor a payroll system,\\nno one of them can tell the complete picture\\nof the business,\\nto get a bigger, more comprehensive view\\nof our organization.\\n\\nNow, oftentimes, we're pulling a lot of data together,\\nbut we don't always need to work with all of the data,\\nso we have to be able to filter out\\nand focus on particular subsets of data.\\nWe also need to reformat data\\nso that we have a consistent representation for data\\nwithin our data science operations.\\nNow, a key thing that we typically do\\nin data science operations is look at aggregate data.\\nSo we want to be able to understand the big picture.\\nSo rather than look at a single transaction,\\nwe want to look at patterns over the course\\nof thousands of transactions.\\n\\nAnd we want to be able to answer specific questions\\nabout business operations,\\nso for example, we might want to know\\nwhat are the top 10% most profitable stores\\nin a particular chain of retail stores?\\nOr we might want to know which stores are not as profitable\\nthis quarter as they were last quarter.\\nThose are very specific questions\\nthat business people may ask, and we,\\nfrom a data science and analytics perspective,\\nare able to answer those questions with the techniques\\nthat we're going to talk about in this course.\\n\\nNow, of course, to do data science,\\nwe have to start with data,\\nwhich means we have to go to various sources.\\nOftentimes, we'll find the data that we need\\nor the data we're interested in is already in a database,\\nand it could be a relational database\\nor it could be a NoSQL database.\\nNow, relational databases are often used\\nfor transaction processing systems and data warehouses.\\nNoSQL databases are typically used in a couple of cases.\\nOne is when you have very large volumes of data\\nand very high velocity,\\nso a large amount of data coming in short periods of time\\ntypically use NoSQL databases.\\n\\nAlso, when the data is semi-structured,\\nit's not really as well structured or as consistent\\nas we typically find in relational databases,\\nthat's another time we might use a NoSQL database.\\nNow, we may also get data straight from applications\\nthat are running in our data centers or in the cloud.\\nWe may also get data streaming in from mobile devices.\\nNow, if you're in an organization that has, say,\\na fleet of vehicles, those vehicles might be instrumented\\nwith IOT, or internet of thing sensors\\nwhich collect data about the state of the vehicle\\nand send that information for analysis.\\n\\nAnd we also may be looking at, say, web logs.\\nSo applications typically write out information\\nabout the state of the application at any point in time,\\nand sometimes, we need to be able to analyze that.\\nThose are examples of the kinds of data sources\\nthat might come from application-related data.\\nAnd then finally, there's a third category\\nI want to talk about, and this is one that's easily missed,\\nand that's manually managed data.\\nWe may work with colleagues who collect a lot of data,\\nwork with data, and then essentially track their own data\\nin spreadsheets or in a local database,\\nand they derive that data and they work with that data,\\nand that data is only available in the spreadsheet\\nor database that they manage.\\n\\nNow, sometimes we need to pull those\\ninto our data analysis as well.\\nHow do we get data from these various types of sources\\nbasically into a single source that we can work with?\\nWell, the process is known\\nas extraction, transformation, and load.\\nAnd here, the idea is basically that we extract\\nor we read the data from its source,\\nif that's in, say, a database or a spreadsheet,\\nor we extract it from applications or IOT data streams.\\n\\nAnd then we do things to transform the data,\\nand basically, we can think of each data source\\nas a puzzle piece, and the transformation is the operations\\nthat help us reshape those puzzle pieces\\nso that they all fit together in a logical way.\\nAnd then finally, once we have the data transformed,\\nthen we can load it into a relational database,\\nat which point we can then start using SQL to analyze it.\\nNow, transformations is a very broad topic,\\nso let's look at just some simple examples.\\n\\nWhen we talk about transformations,\\nit could be something as simple as, say,\\nmaking sure text values don't have any extra white spaces\\nor extra spaces or tabs at the beginning or end of the text.\\nWe also often have to reformat dates.\\nDifferent systems use different representation for dates,\\nso we want to make sure we have a consistent way\\nof looking at dates.\\nAlso, two different systems to talk about the same thing,\\nsay, for example, the departments in your organization,\\nthey might use different codes to refer to them,\\nso one application might use two letter codes\\nto refer to departments,\\nand another application might spell out the whole name\\nof the department.\\n\\nWhen we're transforming the data,\\nwe'll want to make sure we pick some standard way\\nof categorizing or some standard set of codes\\nand make sure that we consistently use those.\\nAnd then we may have other kinds of transformations,\\nsomething as simple as making sure data values\\nare properly cased, or we might need to, say,\\nreformat numeric values into currency values, or vice versa.\\nSo those are just some examples of simple transformations.\\nAnd transformations can get quite complex,\\nespecially if there's additional business logic\\nthat's involved.\\n\\nAnd then finally, after the transformation are performed,\\nthe data is loaded, then we get to what many of us consider\\nthe most interesting part, which is the analysis phase.\\nAnd here we're going to work with various tools,\\nwhether it's SQL or Python or Spark,\\nto do different kinds of analysis.\\nAnd the goal here is to basically derive insights\\nfrom the data so we can provide those insights\\nto our colleagues who make decisions\\nabout our organization's operations\\nand strategies and tactics.\\n\\nSo next, we're going to take a look\\nat some specific SQL commands\\nthat we will be using throughout this course.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5232357\",\"duration\":479,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data manipulation commands\",\"fileName\":\"5925685_en_US_01_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":568,\"solutionVideo\":false,\"editingNotes\":\"03:14 - 03:30 - CUT \\\"The basic components are\u2026.we want to update\\\"\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Review basic structured query language (SQL) data manipulation, including insert, update, delete, and select.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10890328,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] This is an intermediate SQL course,\\nso I assume that you have some familiarity with SQL,\\nbut I do want to make sure\\nthat we all have some common understanding\\nand a little bit of a refresher\\njust in case it's been a little bit of time\\nsince you've seen some of the more common SQL commands\\nthat we'll be using in this course.\\nSo for starters, I just want to point out\\nthat there are two types of SQL commands\\nthat we're going to focus on.\\nOne type is called data manipulation commands,\\nand the others are data definition commands.\\nIn this video,\\nwe're going to focus on data manipulation commands.\\n\\nNow, the first data manipulation command\\nI want to start with\\nis called insert.\\nAnd here's an example of an insert statement.\\nAnd I'm going to walk through this one section\\nor one clause of the insert statement at a time.\\nBut I want to show you the entire command,\\nwhich is used to insert one row of data\\ninto a table called company regions.\\nSo we start with the first part of the insert statement,\\nthe first clause is the insert into,\\nand then following those two key terms\\nis the name of a table.\\n\\nNow, in our case,\\nthis assumes that we have a table\\nthat we've already created called company regions.\\nAnd in company regions,\\nwe have at least three columns,\\nregion id, region name and country.\\nSo we're specifying these specific column names\\nthat we want the data to go into.\\nNow, the way we specify the data\\nis we use the values clause.\\nSo after saying insert into,\\nand then a company name,\\nand then listing the columns\\nthat we want to insert the data into,\\nwe have a values clause.\\n\\nAnd in the values clause,\\nwe have a list that's the same length\\nas the list of the columns.\\nAs you can probably guess,\\nthere's a one-to-one correspondence\\nbetween an item in the values list\\nand an item in the column list.\\nSo in this example,\\nthe value one is written to the region ID column,\\nNortheast is written to the region name column,\\nand USA is written to the country column.\\nSo after executing this,\\nwe would have a row in the table\\nwith a region ID of one,\\na region name of Northeast\\nand a country of USA.\\n\\nNow, if we do multiple insert statements,\\nsuch as we're showing here,\\nwe end up with a table\\nthat will have multiple rows.\\nSo in this case,\\nwe have a table with three rows,\\none row corresponding to each of the insert statements.\\nAnd in each case,\\nan ID, a region and a country\\nis set for each of those rows.\\nNow, insert is really useful\\nwhen we want to put just fresh data,\\nbrand-new data into a table.\\n\\nNow, if we have data that's already in the table,\\nbut we want to manipulate it,\\nwe want to change it somehow,\\nwe can use the update command.\\nNow, here's an example of an update command,\\nand again, we'll break this down and go clause by clause.\\nSo the first part of the update command\\nis the update clause where we specify update,\\nand then the name of the table we're updating,\\nfollowed by the set clause.\\nNow, here we just have a single column\\nthat we want to update,\\nand the country column\\nis the one we're going to update.\\nAnd so we're specifying both the column to update\\nand the value we're going to write to that column.\\n\\nIn this case, we're going to set the country\\nto the words United States.\\nNow, optionally, you can use a where clause.\\nSo in this case,\\nwe're only going to update rows in the table\\nwhere the country is equal to USA.\\nSo if it's not equal to USA,\\nwe're not going to update it at all.\\nSo that's the basics of an update statement.\\nNow, a delete statement, as you can imagine,\\nis useful for removing data from a table.\\nNow, the basics of the delete statement\\nare, first, there is a delete from term\\nand part of that clause\\nis the delete from\\nand then the name of the table.\\n\\nSo again, we're going to continue working with company regions.\\nNow, actually you could have a where clause here.\\nAnd again, like with updating,\\nwhen we use a where clause,\\nwe are limiting the number of rows\\nor the specific rows that are being operated on.\\nSo in our case,\\nthe operation that we're talking about is deleting.\\nAnd what we're saying here\\nis we're only going to delete rows\\nfrom the company region's table\\nwhere the country is equal to Canada.\\nSo for example, after we inserted our three rows,\\nwe have a table that looks like this.\\n\\nAfter running the delete command,\\nwe're essentially going to remove the third row,\\nwhere the ID is three,\\nthe region is Quebec\\nand the country is Canada.\\nBut that's the only row\\nthat gets removed by this delete statement.\\nNow, a select statement\\nis one of the most interesting commands to work with in SQL,\\nand it's one we will spend a lot of time with\\nbecause this is what we use\\nto execute most of our queries.\\nNow, a select statement has a much richer syntax.\\nSo I just want to cover out some of the highlights of that\\nand just talk a little bit about that\\nbefore we actually take a look\\nat an example select statement.\\n\\nSo with a select,\\nalong with a list of columns to retrieve.\\nNow, sometimes when we're working with more than one table,\\nwhen we do that, we often use something called a join.\\nA join is like a way of linking tables\\nor connecting related information in different tables,\\nlinking them together\\nso that we can actually build a row in a result set\\nor an output from a select statement\\nthat includes information from both tables.\\nNow, like the update and delete statements,\\nwe can use a where clause\\nto limit what we're interested in.\\n\\nIn case of a select statement,\\nwhen we use a where clause,\\nwe limit what rows are returned.\\nWe can also apply what are known as aggregate functions.\\nThose are really useful for doing work,\\nlike, oh, adding up all the values of say, you know,\\na particular column,\\nlike if you're working with sales data,\\nadd up the total sales price of all of the rows\\nthat are returned by the select cause,\\nor find the median or average value\\nfor some particular column.\\nSo aggregate functions are really useful\\nand we use them quite a bit in data science work.\\n\\nThere's also sorting and grouping commands.\\nSo we might want to have a specific order.\\nWe might want to say the top 10 sales regions\\nin our company database,\\nor we might want to group say\\nwhat are the marginal revenues grouped by say product line.\\nSo select statements are really useful\\nin that they have different commands\\nor different clauses actually\\nfor specifying how to order the output,\\nso how to sort the output,\\nand then also how to group\\nor collect subsets of rows\\nso that you can do things like apply aggregate functions\\nto those subsets.\\n\\nSo one of the simplest select statements\\nis known as the select star.\\nSo here's an example where we're saying select star\\nfrom our table country regions.\\nNow, select star is basically a shorthand way\\nof saying select all of the columns in the table,\\nin this case, country regions.\\nSo that query would return all of the data in the table.\\nSo we're not limiting the columns that we're selecting\\nand we're not using a where clause,\\nso we're not selecting a subset of rows.\\nSo select star from a table\\nis a way to return all of the data in that table.\\n\\nNow, we can also restrict or constrain\\nwhich rows we return.\\nSo for example, we can say select star,\\nwhich will still give us all of the columns,\\nfrom country regions.\\nAnd now we use a where clause,\\nwe say, well, only return rows where the ID column\\nhas a value that's in this particular list.\\nAnd the list we're talking about is one and two.\\nNow, when we do that,\\nwe select just the rows\\nthat have the ID one or two.\\nSo we're just selecting these first two rows.\\n\\nSo that's an example of how to work with select statements,\\nas well as delete, update and insert.\\nThose are the main data manipulation tools\\nthat we use when we work with SQL.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5234308\",\"duration\":472,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data definition commands\",\"fileName\":\"5925685_en_US_01_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":542,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Review basic SQL data definitions, such as creating tables, views, and schemas.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11099553,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now we're going to look at data definition\\ncommands, the second type of SQL command.\\nData definition commands allow us\\nto manipulate data structures within a relational database.\\nNow there are many different types of data structures\\nthat we have built into relational databases.\\nWe're going to focus on four\\ntables, indexes, views, and schemas.\\nNow, tables are probably the most familiar data structure\\nto those of us who've worked with SQL databases.\\nHere's an example of a data manipulation command\\nfor creating a table,\\nand we'll walk through each of the main components.\\n\\nBut as you can see, it allows us\\nto create a particular table that has certain columns\\nand certain data types.\\nSo the first part of a create table statement\\nis the set of key terms or keywords create table.\\nNow we follow that by the name of the table.\\nIn this case, we're creating a table called staff.\\nNow, after the create table clause, we have a list,\\nand those are a list of columns.\\nNow each column is made up of a name like ID, or last name,\\nor department name, followed by a data type.\\n\\nAnd the data types are typically things like integer or text\\nor dates, but there are additional data types as well.\\nBut those are the ones we see quite frequently.\\nNow our columns can take on these various data types.\\nNow you'll notice here that we have data types of\\ntext and date.\\nNow text is sometimes referred to as character\\nor variable character data type.\\nAnd in some relational databases\\nand actually older versions of Postgres,\\nyou can also specify a specific length to a text.\\n\\nSo for example, you could say last name has a maximum size\\nof 30 characters, and it's fine to specify a text,\\nbut it's generally considered a good practice\\nto not constrain yourself unnecessarily with regards\\nto the length of strings or texts that we work with.\\nSo oftentimes what you'll see is that when we work\\nwith strings, we use the text data type\\nand don't specify a particular length.\\nNow another thing you'll notice in the create table\\nstatement is we have these additional clauses\\nwhich can provide additional information about the table.\\n\\nAnd one really important additional clause\\nis the primary key clause.\\nAnd that specifies which of the columns\\nthat we have listed is the one that we should treat\\nas the primary key.\\nNow, because ID in this case is the primary key,\\nit can't be null and it has to be unique per row.\\nSo we can't have two rows\\nwith the same value in the ID column.\\nSo by specifying the primary key clause\\nand specifying a column\\nor set of columns that uniquely identify a row,\\nwe tell the database to essentially add some constraints\\nor some mechanisms under the covers to enforce the idea\\nthat the primary key has to be unique\\nand it has to be not null.\\n\\nIndexes are another important data structure\\nin relational databases.\\nNow here's an example of a complete create index command,\\nand it basically starts with the key term create index,\\nfollowed by an index name,\\nand then we specify tables and columns.\\nBut let's look at each of those components.\\nSo first we have our key term\\nor our command, which is create index,\\nfollowed by the name of the index.\\nSo we're creating an index\\nso we can make up any name we want.\\nNow oftentimes we use naming conventions\\nwhen we name tables and indexes and other data structures.\\n\\nSo one that I've seen quite a bit is\\nto use the prefix IDX_ to start the name\\nof an index followed by the name\\nor an abbreviation for the table.\\nSo in this case, IDX_staff.\\nAnd then followed by some indication of the columns\\nthat are used in the index.\\nSo for example, IDX staff, last name is pretty easy\\nto read if you're scanning through a long list of indexes\\nto be able to understand pretty quickly.\\nAh, this is on the staff table\\nand it's an index for the last name.\\n\\nThen we have the on clause.\\nThe on basically just specifies\\nwhat table this index is applied to.\\nIn this case it's the staff table.\\nAnd then the using clause precedes a list of the columns\\nthat are included in the index.\\nNow in this case, we're only using the last name column,\\nbut this again, it's pretty common.\\nIf you're working with some kind of database\\nwhere we have people's names\\nand you often look up by last name,\\nthen you would want to have an index on that last name column\\nto improve the query performance\\nwhen looking up by name.\\n\\nViews are another widely used data structure\\nin relational databases.\\nAnd here's an example of a view,\\nand it's fairly complicated\\nmostly because of the select statement.\\nBut what we can see here is\\nthat we have a command called create view,\\nand we're giving it a name.\\nIn this case it's staff_div or short for staff_division.\\nAnd then we specify, we're creating this table as,\\nand then we give a select statement.\\nNow that select statement is going to query\\none or more tables.\\nNow in this case, we are selecting an ID\\nand a last name from the staff table.\\n\\nAnd then we're also working with a new table\\nwe haven't seen before called Company Divisions.\\nAnd we are aliasing that or kind of referring to\\nthat using the shorthand notation CD.\\nAnd so CD stands for company divisions.\\nAnd in the company divisions table,\\nwe want to get the company division column.\\nNow we specify in the from clause\\nwhich tables we're working with.\\nSo we're working with staff obviously,\\nbut then we're not just working with staff,\\nbut we're also joining the rows from the staff table\\nto rows in the company divisions table.\\n\\nAnd we specify a left join.\\nAgain, if you're familiar with SQL,\\nyou have probably heard about things like left join,\\nouter joins and so on.\\nAnd the left join indicates that we're going to use all\\nof the rows from the table on the left.\\nIf you think of staff, left join, company division,\\nstaff is on the left, followed by left join,\\nfollowed by company on the right,\\ncompany divisions on the right.\\nSo this is a left join.\\nSo we have all the rows from staff,\\nand anytime we find a matching\\ncompany divisions based on department,\\nthen we're going to include that.\\n\\nSo the on statement\\nafter left join with the additional table name.\\nWe have the on statement.\\nAnd the on basically says, well,\\nwhat columns do I use in each of these two tables to match\\nor to link or to join?\\nAnd in both the staff table\\nand the company divisions table,\\nwe have a column called department.\\nAnd so we join these rows from these two tables,\\nor we link them together\\nwhen they have the same department value.\\nSo that's an example of a view.\\nNow the last data structure we're going to look at\\nin this video is called schemas.\\n\\nNow a schema definition is very straightforward.\\nHere's an example of a create schema statement,\\nwhich specifies the phrase\\nor the key terms create schema\\nfollowed by the name of a schema.\\nIn this case, data sci short for data science.\\nNow we can think of a schema as an organizing structure.\\nAnd within a schema in the database,\\nwe have things that we work with together that are related.\\nSo for example, we might have a set of tables,\\nand with each table there may be one or more indexes\\nor zero or more indexes associated with that table,\\nand those tables might also be used in views.\\n\\nSo you can see a schema is a way of packaging or organizing\\nand keeping logical separation\\naround different data structures\\nlike tables, indexes, and views that we use.\\nSo when we think about data manipulation language,\\nbasically we're talking about how we create and delete\\nand maintain things like\\ntables, indexes, views, and schemas.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5234307\",\"duration\":142,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"SQL standards\",\"fileName\":\"5925685_en_US_01_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":184,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn about the potential for different SQL syntax across database management systems.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3390803,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Speaker] Now as data scientists working\\nwith relational databases, we need to be aware\\nof SQL standards.\\nNow, this is because\\nthere is no single SQL language standard.\\nIn fact, the standard has changed over time.\\nAnd also, although we have a common standard,\\nthere are variations across databases.\\nSo one database like Postgres might implement\\na certain subset of a broader standard,\\nwhereas another database like Oracle or DB2\\nor SQL Server might implement different subsets\\nof the standards.\\n\\nSo we just want to be aware\\nthat different databases may have different commands.\\nThis is especially important if you're trying\\nto follow along this course\\nand you're using a database other than Postgres.\\nNow, when we talk about SQL standards,\\nwe are typically talking about ANSI standards SQL .\\nAnd ANSI stands\\nfor the American National Standards Institute.\\nAnd the original SQL standard from ANSI\\ncame out in 1986, but there have been 11 versions\\nthat have come out since then.\\n\\nAnd basically, each version adds new features,\\nnew capabilities.\\nSo for example, some of the latest features include support\\nfor JSON data structures, as well\\nas multidimensional arrays, and also advanced triggers.\\nNow, what does this mean for us?\\nWell, what it means is your database is SQL implementation\\nmay be different from what you see here.\\nSo, for example, I've talked about\\nhow text is a variation on variable character\\nor VARCHAR data types,\\nand that some databases might use VARCHAR, others use text.\\n\\nSo you may see differences in\\nhow we specify things like the data types.\\nSo you want to check your database documentation, especially\\nfor data types.\\nThere may be additional data types\\nor extra functions that might be available in your database\\nthat aren't in the standard.\\nAnd also, functions may be different\\nacross different database implementations.\\nNow, this may mean\\nthat there's a slight variation in the name of the function\\nor the function in your database may take a different set\\nof parameters in that same function call\\nin a different database.\\n\\nSo again, you just want to check your database documentation\\nwhen working with those.\\n\"}],\"name\":\"1. Foundations of SQL for Data Science\",\"size\":38287484,\"urn\":\"urn:li:learningContentChapter:5231407\"},{\"duration\":2707,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5232356\",\"duration\":469,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Getting started with GitHub Codespaces\",\"fileName\":\"5925685_en_US_02_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":578,\"solutionVideo\":false,\"editingNotes\":\"01:15 - Overlay - https://github.com/LinkedInLearning/intermediate-sql-for-data-scientists-5925685\\nCut 6:30-7:09\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learnhow to create a schema and load data into PostgreSQL.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13922337,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, we are almost ready\\nto get started actually working with SQL.\\nBut before we do that,\\nI want to spend a little time\\ntalking about the development environment\\nor the learning environment\\nthat we're going to be working with.\\nWe are going to use a service called Codespaces,\\nwhich is a virtualized environment\\nthat's provided by GitHub.\\nAnd, GitHub, if you're not familiar with it,\\nis a service which provides repositories\\nfor version-controlled code,\\nand it makes it very easy\\nto share and collaborate on code development.\\n\\nWell, that's GitHub.\\nGitHub plus Codespaces goes a step further\\nby providing an environment\\nin which we can execute our code.\\nSo we are actually going to be using Codespaces\\nfor installing and running a Postgres server\\nand actually interacting with Postgres\\nusing our SQL commands\\nthat we're going to be learning about in this course.\\nSo the first thing we need to do\\nis to navigate to the GitHub repository for this course.\\nNow, I've highlighted the URL here.\\n\\nAnd the URL is github.com/linkedinlearning\\n/intermediate-sql-for-data-scientists-5925685.\\nSo that's the URL where you'll find this.\\nNow, when you first go to github.com,\\nif you aren't logged in,\\nyou may be prompted to log in.\\nBut once you are into GitHub and you see this page,\\nyou will be able to see a couple of things.\\nFirst of all,\\nthis is a standard GitHub repository landing page.\\n\\nAnd so, for example,\\nwe have a list of different files and folders in here\\nas well as a README\\nwhich provides some basic information\\nabout how files are organized.\\nWe are primarily concerned with exercise files,\\nand I'll just show you the structure of that folder.\\nIn here, we have folders associated with each chapter\\nthat has exercise files,\\nand within each chapter or each folder,\\nthere's another set of files.\\nNow, most of these follow the naming convention\\nof the number of the chapter\\nplus the number of the video within that chapter,\\nand that's the name of the SQL exercise file.\\n\\nSo, for example, 02_02 is Chapter 2, second video.\\nThis is the exercise file associated with that.\\nWe do have one exception in this folder\\ncalled Exercise Data,\\nwhich is actually the file that we will use\\nto create our schema and load some initial data.\\nSo we'll be talking about that shortly.\\nBut that is basically an overview of the exercise files.\\nAnd now what we need to do\\nwhen we're ready to start working with Postgres\\nis to go to this button labeled Code.\\n\\nThis will bring up some Codespaces.\\nNow, if you have some existing Codespaces,\\nthey may be listed here,\\nor you may just see this one option of Codespaces.\\nSo I'm going to create a new Codespace by clicking the plus.\\nAnd now we are going to go through a series of steps\\nwhich Codespace is executing for us\\nto create a virtualized environment for us.\\nNow, this virtualized environment\\nwill include an editor or a IDE that we can work in\\nbased on Visual Studio,\\nand it will also include an extension called SQLTools.\\n\\nAnd SQLTools is where we have Postgres installed.\\nAnd we will be able to interact using both our IDE\\nand a command line for executing SQL commands\\nwith the Postgres that's installed in Codespaces.\\nNow, this operation is going to take a few minutes,\\nso we may edit some of this out,\\nbut we'll be back as soon as the environment\\nhas completed setting up.\\nOkay, the first part of the setup has completed now.\\n\\nWe have our basic Codespace created\\nand we have our files loaded.\\nAnd the next operation will be installing Postgres.\\nOkay, it looks like we are done with the installation,\\nand I just want to briefly describe\\nthe environment within the Codespaces.\\nWhat we see here in the main panel is the README\\nthat we saw in the GitHub repository.\\nSo this is a good indication\\nthat we have loaded our repository.\\nAnd we also see, over to the left,\\nwe see a hierarchical environment where we can work,\\nand we see that under our workspace within Codespace,\\nwe have some of the folders that we saw in our repository,\\nlike the dev container.\\n\\nBut we also have exercise files.\\nSo we see, for example, we have the files from Chapter 2.\\nSo all of this looks like, you know,\\nexactly what I would expect once we have our repo loaded.\\nNow, I'm working under the explorer,\\nand we're looking at the contents of the GitHub repository\\nthat have been brought into our container.\\nNow, another tool that we're going to use\\nis one called SQLTools,\\nand it has this little barrel icon down here.\\nIf you click on that,\\nyou'll see that we have a new prompt here\\nor a new working area called Connections.\\n\\nWell, one of the things we want to do\\nis click Add New Connection,\\nand what we're going to do is create a connection\\nor our ability to communicate\\nbetween our container and our Postgres database.\\nSo I'm going to select,\\nthe kind of connection I want to use\\nis a PostgreSQL connection.\\nSo I'll click on that.\\nAnd we can give this a connection name.\\nSo I'm going to call it data_sci.\\nI'm not creating any connection groups.\\nMy connect string is correct.\\nThe server address is correct.\\nPort 5432 is correct.\\nNow, we do have to add\\ndatabase, username, and user password.\\n\\nThe database name is postgres.\\nThe username is also postgres.\\nAnd for the password,\\nwe're simply going to select\\nSave as Plain Text in Settings,\\nand we're going to enter the password.\\nAnd the password here is postgres.\\nNow, this is not standard security practice,\\nbut we are in a learning environment,\\nso that's why we are doing things like\\nusing the default password for the Postgres database\\nand saving it in plain text.\\n\\nNothing we are working with\\nrequires any more security than this.\\nSo I'm just going to then scroll down\\nthrough the other options.\\nWe don't need to change any of the other default options.\\nAnd I'm just going to go over to the bottom.\\nThere's Save Connection.\\nI will save that connection.\\nAnd let's connect now.\\nAnd if we look over here,\\nback to the left in the Connections work area,\\nwe'll see that we have an icon with the Postgres icon\\nand a green dot and data_sci.\\n\\nI'm going to click on the arrow over on the left\\nto expand hierarchically on this.\\nAnd what we're interested in is the schemas.\\nAnd what we see here is we have four standard schemas\\nthat are created by Postgres.\\nOne is called public,\\nand then also there are two related to tiger\\nand one to topology.\\nThese have to do with extensions\\nlike geo-encoding extensions related to Postgres.\\nWe won't be using them.\\nYou can delete them if you like.\\nI'm just going to leave them,\\nbut I just want to explain why\\nyou might see these three schemas\\nthat you might not be familiar with.\\n\\nThey're fairly new additions\\nto the default Postgres configuration,\\nbut they're definitely not going to be in our way.\\nSo what we have now at this point\\nis we have a connection to the Postgres database.\\nIf we look in the public schema\\nand drill down to Tables,\\nagain, there's some default tables that are created.\\nSo basically what this means is,\\nwe have successfully navigated\\nto the course GitHub repository,\\nand we have successfully launched Codespaces,\\nincluding a Postgres server that we will be working with.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5240087\",\"duration\":249,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating tables and loading data\",\"fileName\":\"5925685_en_US_02_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":281,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH02 > 02_02.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to create tables and load data that will be used in this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9989075,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] With Codespaces now set up and running,\\nwe can move on to our next step,\\nwhich is to create a schema, a few tables,\\nand then load data into those tables.\\nAnd once we have that, then we can start working\\nwith our SQL queries.\\nSo I'm in Codespaces here\\nand what I want to do is I want\\nto just make sure my connection is working.\\nSo I navigate down to the SQL Tools icon\\nand I see yes, my data-side connection is active,\\nand I see the four schemas that are created by default.\\n\\nSo that is good.\\nAnd the next thing I want to do\\nis I should have a connection open.\\nSo, for example, I have my data_sci session here.\\nAnd what I want to do next is actually open the file\\nthat has the code that we want to execute.\\nNow, that file is located under Exercise Files,\\nunder chapter two, and it's called exercise_data.sql.\\nSo let's open that file.\\nNow, what I'd like to do is just walk through briefly\\nwhat are the main components of this file,\\nand what are the main things that we will be executing.\\n\\nSo, for example, the first thing we will do\\nis we will create a schema called data_sci.\\nAnd if one already exists, it won't error out.\\nWe'll set the search path to data_sci.\\nSo by default, tables that are created\\nor data that's inserted will be using the data_sci schema.\\nAnd then we're going to create a table.\\nAnd in this case, we're going to create a table\\ncalled company_departments.\\nAnd this will have an id as an integer, a department_name,\\na division_name, an id will be the primary key.\\nThen there's a series of insert statements for that table.\\n\\nAnd if we scroll down, we see we have another set\\nof create table commands.\\nAnd the set of commands is basically,\\nit drops the table if it already exists\\nand then recreates it.\\nSo this is just a good practice in SQL\\n'cause if you try to create a table that already exists,\\nyou'll get an error and the file won't finish executing.\\nSo here's another create table.\\nThis one's for company_regions,\\nand we have several insert statements for different regions.\\nWe also have a table for employees.\\nAnd employees have things like an id,\\nwhich we use for primary key,\\nbut also things you might expect,\\nlike last_name, email, salary, job_title, and so on.\\n\\nNow, this table also has some indexes,\\nso we'll create indexes,\\nand then we have a series of insert statements.\\nSo that's basically the pattern of this file.\\nYou're welcome to look at it in more detail,\\nbut I want to run this.\\nNow, what I like to do is I like to copy the contents\\nof the files.\\nSo I'm just going to highlight everything and copy that\\nand go back to my session.\\nThis is where I will run all of my commands\\nor most of my commands, and I'm going to paste in the commands.\\nAnd looks like everything was pasted in correctly.\\n\\nThat all looks great.\\nAnd I'm going to select this option\\nto run on active connection.\\nSo I'm going to go ahead and run that.\\nAnd we'll notice down here in the bottom part,\\nwe're seeing some commands\\nor some informational messages printed on the SQL console.\\nAnd I'm just going to move some windows around.\\nAnd if we navigate back over to the left\\nand click on SQL Tools,\\nand now our schemas are expanded.\\n\\nSo you'll notice we have a new schema called data_sci.\\nSo I'm going to click on data_sci\\nand I'm going to click on tables.\\nAnd I see we have tables for employees,\\ncompany_regions, and departments.\\nSo let's just navigate and drill down into employees.\\nAnd we see the employees have columns id, last_name, email,\\nstart_date, and so on.\\nSo what we have done here\\nis we have successfully created a schema called data_sci,\\nand we've created three tables,\\none called employees, one called company_regions,\\nand one called departments.\\n\\nAnd so at this point, we should be ready to go\\nwith actually executing SQL commands,\\nand we'll start that in the next video.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5233341\",\"duration\":489,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Basic aggregate functions\",\"fileName\":\"5925685_en_US_02_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":538,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH02 > 02_03.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn the basic aggregate functions in SQL. Aggregate functions are widely used in data science and SQL developers should know them in detail.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16508077,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now it's time to start working with\\nselect statements to query the data.\\nI'm going to start with some very basic select statements\\nprobably which are very familiar to you,\\nbut I just want to make sure we're all starting\\nfrom a common understanding.\\nSo you can think of this first couple of of videos\\nwhere we're executing select statements\\nas kind of a refresher for some of the basics,\\njust again, so that we're all\\nstarting at the same common understanding.\\nNow, I'm just going to take a quick look over here\\nunder the connections work area.\\n\\nAnd I notice here that we are under schemas.\\nWe have the data size schema, which we expect,\\nand under tables we have three tables, company departments,\\ncompany regions, and employees.\\nSo those are the three tables we'll work with.\\nSo let's start by working with employees\\nand we'll start with probably the simplest\\nSQL statement there is,\\nwhich is simply select, star, from,\\nand now I'm going to specify a table\\nand I'm going to select from data_sci.employees.\\n\\nAnd I'm going to run on active connection.\\nAnd what we'll see here is over on the right\\nwe have our output.\\nSo let's see what we have here.\\nWe have several columns.\\nWe have an ID column, which has integers,\\nand they look like they range from one\\nto a little over a thousand.\\nOkay, A little bit more over,\\noh, they go quite high, so that's interesting.\\nAnd then we have last name, some email addresses,\\nstart dates, salary, job titles, a region ID, department ID.\\n\\nSo some very basic information about employees.\\nAnd now what we can do is, this is quite a bit of data.\\nOftentimes when I'm first working with tables\\nand I want to kind of get a feel for the data,\\nI don't want to see necessarily all of the rows in the table.\\nWell, in that case, I can use the limit command\\nor the limit clause.\\nSo for example, I can limit myself to 10.\\nSo I'm going to run that on the active connection.\\nAnd now we see we're just getting 10 rows back.\\n\\nSo this is the limit clause is particularly useful\\nwhen you're just trying to get a feel for data\\nthat's maybe in a new table you haven't worked with.\\nSo we see now we are still using select star,\\nso we're getting all of the columns,\\nbut we're just limiting the number of rows\\nthat are being returned.\\nNow when we're looking at the data, we see that there are,\\nwe can think of terms of subsets, like, oh by region\\nor by department ID.\\nSo if we want to look at just employees,\\nmaybe from region two, we can add a where clause.\\n\\nSo from there, typically the where clause\\nis positioned after the from clause.\\nSo we'll say where,\\nand I want to look at region ID,\\nand I want to look only at region ID.\\nAnd let's limit to 10 rows as well.\\nAnd okay, so now we have 10 rows returned.\\nLet's just verify, yep.\\nThe region ID here is all for region two.\\nSo we can see with the where clause,\\nwe can limit what subsets of rows that we actually look at.\\n\\nWell, okay, so now we feel like we've got a sense\\nof what the data looks like, what kind of data to expect.\\nOne of the things I often want to know is,\\nwell, how many rows are in a table?\\nWell, to do that, I'm going to remove\\nthe where clause in the limit,\\nand I'm going to replace a select star\\nwith a select count star.\\nAnd this says basically count all the rows in the table.\\nSo let's run that on the active connection,\\nand we'll see there are 1046 rows in this table.\\nSo at this point, I feel like I've got a good understanding\\nof the overall shape of the data,\\nlike how many rows are in the table\\nand what are the particular columns that are available\\nand just a rough idea of what the data\\nin each of the columns look like.\\n\\nNow, one of the columns we have is something called salary.\\nAnd I'm just going to go back over to the explorer\\nand on the left side of the screen,\\nand I'm just going to look at the column titles\\nand just verify, yep, salary is the correct one.\\nWell, let's say I'm interested in salary.\\nSo in addition to count, I might want to know,\\nwell what's the highest salary and the lowest salary.\\nSo for that, I can use the min aggregate function.\\nSo I can say, gimme the min salary, the minimum salary,\\nas well as the maximum salary or max salary.\\n\\nSo now when I run this,\\nI'll get the count of the number of employees,\\nplus the minimum salary and the maximum salary.\\nAnd we'll just run on the connection.\\nAnd we see, okay, our minimum salary is about 40,000,\\nlittle over 40,000.\\nOur maximum salary is almost 150,000.\\nSo now we're starting to get more of a feel\\nfor the distribution of the data for one particular column,\\nin this case salary,\\nwhich is oftentimes something we want to know\\nwhen we are working in a data science environment,\\none of the things we wanted to understand\\nis what are the distributions of different columns?\\nSo that's when we're working with salary.\\n\\nI do want to talk a little bit about the ID column.\\n'Cause the ID column is a primary key.\\nNow, the ID column in this dataset is a meaningless key.\\nThe number, whether it's 1 or 27 or 1,018,\\nit doesn't really mean anything.\\nHowever, it is tempting to make assumptions\\nabout meaningless keys.\\nFor example, you might think, oh, well a meaningless key\\nthat has a small value is probably\\nan older row or an older record than a row\\nthat has a much larger ID number.\\n\\nThat could be the case, but it's not always the case.\\nMeaningless keys are generated in different ways,\\nand there may be different circumstances\\nor the history of the data that we're working with\\nmight be such that maybe multiple tables\\nwere combined or rearranged,\\nor new IDs were assigned to new rows at some point\\nfor some reason that had to do with\\nmaybe there was a data anomaly\\nand something had to be corrected.\\nSo we have to be very careful about assumptions we make.\\n\\nSo for example, if we look at the minimum ID\\nand the maximum ID, and so now if we look at the count\\nmin and max ID and we run this,\\nwe see that we have a total of 1,046,\\nbut we have a max that's much higher, 1944.\\nSo there's almost 900 more IDs\\nthat are in a sense missing.\\nLike where are those other 900 IDs?\\nThose could be people that no longer work at the company.\\n\\nThere could be gaps in the way we assigned IDs.\\nWe might have skipped over large blocks.\\nSo I just want to point this out\\nthat when you are working meaningless keys,\\nyou have to be careful about the assumptions you make.\\nSo meaningless keys like an ID are really fundamentally\\ndifferent in their characteristics than say\\nthe natural attributes of the entities\\nthat we're working with.\\nSo for example, a natural attribute\\nof an employee is a salary.\\nNow those salaries mean something\\nand so we can make certain assumptions about them.\\n\\nSo we just want to be careful when we're doing\\ndata science work or data analytics work,\\nwe understand the type of data that we're working with.\\nAnd if it's an attribute that naturally goes with an entity,\\nlike a salary that's associated with an employee\\nor a longitude and latitude\\nthat are associated with a geographic location,\\nit's perfectly fine to make certain assumptions about that\\nand look at things like distributions of data.\\nWhen we're working with meaningless keys,\\nit's really a completely different ball game.\\n\\nAnd any assumptions we might be tempted to make,\\nwe really should verify before we go too far with those.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5231406\",\"duration\":583,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Statistical aggregate functions\",\"fileName\":\"5925685_en_US_02_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":642,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH02 > 02_04.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn more advanced descriptive statistics in SQL. These descriptive statistics are used to understand the overall structure of normally distributed data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":20193473,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] Now let's look at some\\nadditional aggregate functions.\\nAnd in particular, I'd like to look at aggregate functions\\nthat help us with what are known as descriptive statistics.\\nSo I'd like to understand what's the distribution\\nor the shape of the data in particular columns.\\nWhat does that look like?\\nWell, let's start with, again,\\nwith a real basic command, like SELECT *\\nFROM employees,\\nand that'll be from the data_sci schema.\\n\\nAnd if we just run that on the active connection,\\nwe'll see all of our rows, as we expect.\\nAnd let's just quickly limit to 10\\nand run that.\\nYep, okay, so we're seeing a subset of our data.\\nNow so one of the things I'm particularly interested\\nin this case is salary.\\nAnd I can see there's a fair rate difference\\nlike from the largest to smallest that I see,\\njust a quick look at just 10 employees.\\n\\nBut let's see, for example,\\nwhat are we spending in total across all of our employees?\\nSo if we take the sum of the salary column\\nfor all of the employees, we'll get a single value.\\nSo here's our value that we have.\\nSo this is essentially what we're spending on salary.\\nNow I might want to know, okay,\\nhow does this break down by, say, by department?\\nSo rather than look at just the sum of the salary\\nfor the whole company, if I want to look\\nat each individual department\\nand see what I'm spending in each department,\\nI can use the group BY clause\\nand I can group BY the department_id.\\n\\nNow if I run it just like that,\\nI'll go ahead and just run it like this.\\nWhat we'll see is we'll see a list of salaries\\nor sums of salaries,\\nbut I don't know which departments these apply to.\\nSo simply grouping by the department\\ndoesn't include the department ID in the output.\\nSo I need to add that.\\nSo I'm going to put that first in my select list\\nand I'm going to say department_id and now run that again.\\nAnd now this is much better.\\nNow I have two columns.\\nSo I can see the sum of salary\\nand I know which department this sum applies to.\\n\\nNow so this is useful information.\\nI can see there is some spread,\\nbut it's not too big of a spread across departments,\\nwhich makes sense.\\nI used a random number generator with a normal distribution.\\nSo I fully don't expect to see wide variance in the salary.\\nSo this is making sense.\\nAnd let's see, in addition to the sum\\nor the total amount of salary,\\nlet's see what our average salary is.\\n\\nAnd this is where I really expect to see\\nnot a lot of variation.\\nAnd so the first thing we notice here is with the average,\\nthe average is printing out a number\\nwith a large number of decimal points.\\nAnd that's just because we have fractional values.\\nWe're doing division.\\nAnd so what I'd like to do\\nis make this a little bit more readable.\\nSo I'm going to use the round function\\nand I'm simply going to take the average\\nand I'm going to take the results of the average function\\nand apply the round function.\\n\\nAnd with round, I put in a number that I want to round.\\nIn this case it's the result of averaging,\\nand then specify the number of decimal places I'd like.\\nSo I'd like two in this case, so let's run that.\\nAnd what we see here, this is much easier to read.\\nSo here we have an average salary\\naround 98, 99,000, 103, 92, 103.\\nSo again, oh, 87, so again,\\nwe're not seeing a large amount of variance here,\\nbut rather than just depend on, you know,\\nkind of a rough look, you know,\\nwhat does my eyeballing this tell me?\\nWhy don't we actually use some statistical functions\\nlike variants to understand\\nhow distributed the salary really is?\\nWell, there is a function in Postgres called var_pop,\\nwhich stands for variance of the population.\\n\\nAnd we can specify salary here\\nand we can get a variance of this population\\nor the entire of all of the salaries we look at.\\nAnd so here we see fairly large numbers\\nand that has to do with the nature\\nof how variance is measured.\\nSo the first thing I want to do\\nis I want to apply the round function.\\nSo I kind of trim this down\\nto get just two decimal points and let's run that.\\nAnd what we see here is we're really working\\nwith actually much larger numbers than salaries themselves\\nbecause again, that has to do with the nature\\nof the variance calculation.\\n\\nIt's actually working with like the squares,\\nlike the order of magnitude of variance\\nis the square of what our salaries are.\\nNow a couple things I want to point out.\\nOur column names here are department_id, sum,\\nround, and round (1).\\nThat's because I haven't specified\\nany alias for this or column name.\\nSo I am going to specify an alias\\nby saying round, and I'm going to give it,\\nthis is avg_salary.\\n\\nSo this is an alias that I'm going to use.\\nAnd here I'm going to put V-A-R for variance\\nof salary and let's run that.\\nNow what we see here is we're getting the same numbers back,\\nbut our column names are different.\\nIn this case, we have average salary and variance of salary.\\nSo variance is a statistical function\\nthat's available to us.\\nTypically, I don't use it a lot\\nbecause it doesn't work with my intuition as well\\nas another statistics function called standard deviation.\\n\\nAnd so let's look at standard deviation.\\nThe function for that is\\nstddev_pop for population.\\nAnd if we look at the standard deviation of salary,\\nand let's give this an alias of stddev of salary\\nand let's go ahead and apply the round\\n'cause we will almost certainly need that.\\nAnd let's apply a parameter of two to specify\\ntwo decimal places and let's run this.\\n\\nAnd now what we'll notice here is now we're seeing things\\nwhere the standard deviation is like 34,000,\\n35,000, 32,000, 30,000 and so on.\\nSo it's at the same scale as salaries.\\nThat's because standard deviation is basically\\nthe square root of the variance.\\nSo typically when we're trying to understand\\nwhat the distribution of data is\\nin a normally-distributed kind of attribute, like a salary,\\nwe oftentimes use standard deviation.\\n\\nNow there are some really nice features\\nabout standard deviation that make it, you know,\\nit was one of the reasons that we use it so much.\\nOne is that the values are at the same scale\\nas the column that we're analyzing.\\nIn this case, we're analyzing salary,\\nso we're seeing scales at like tens of thousands,\\nmaybe a hundred of thousand of dollars at the most\\nas opposed to the square of that which we saw with variance.\\nThe other thing that's really useful\\nis that with standard deviation,\\nwhat we can do is we can take the mean or the average\\nand we can subtract the standard deviation\\nand then add the standard deviation.\\n\\nSo for example, if we have an average salary\\nin one department is 100,000\\nand our standard deviation is 30,000,\\nwe can subtract 30,000 from 100,000 and add 30,000.\\nAnd that gives us a range of 70,000 to 130,000.\\nWell, about 65%, 66%, 68%,\\nsomewhere in that range of all\\nof the salaries in the department will fall\\nwithin that range, within one,\\nwhat we term as one standard deviation of the mean.\\n\\nNow if you double the standard deviation, say,\\nso instead of 30,000, it's 60,000\\nand you subtract that from 100,000\\nand you add it to 100,000,\\nthat gives us a range of 40,000 to 160,000.\\nWell, in that case, we would typically find about 95%\\nof all of the salaries fall within two standard deviations.\\nAnd if you go out to three standard deviations,\\nyou pretty much find almost all\\nof the salaries within three standard deviations.\\nSo when we're working with data\\nthat has a normal distribution,\\nthat is, a bell-shaped kind of curve,\\nthen working with statistical functions\\nlike means and the variance\\nand the standard deviation really work really well\\nand they help us understand, give us a sense\\nof what the shape of that bell curve looks like.\\n\\nSo that's why we sometimes refer to these statistics\\nas descriptive statistics.\\nThey describe the distribution of the data.\\nAnd again, we use them really quite often in data science.\\nSo working with functions like the average to get the mean\\nor working with standard deviation of a population\\nare functions we'll often use when we're working\\nwith data science problems\\nand using SQL to interrogate the data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5233340\",\"duration\":444,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Grouping and filtering data\",\"fileName\":\"5925685_en_US_02_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":514,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH02 > 02_05.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to group subsets of data in preparation to apply aggregate functions. Groupings apply aggregate functions to subsets of data returned by a query.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13523752,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] I want to continue our work\\nwith grouping and filtering.\\nSo I'm going to build a select statement\\nand I'm going to select from employees\\nand let's select a few things.\\nLet's, first of all, we'll go over to the navigator\\nand just list the columns over there.\\nSo let's get last name, department ID,\\nand salary from employees.\\nThat looks correct.\\nSo let's run that\\nand just make sure, oh, you know what?\\nI forgot to specify the schema name.\\n\\nNow let's try this again. There we go.\\nSo what we have here is our list of employees\\nwith their department and their salaries.\\nNow, I might want to filter it\\nand look for someone with just a particular name.\\nSo let's search the date of site employees table\\nwhere the last name is equal to.\\nLet's go with Boyd, okay, and let's run that.\\nAnd what we see here is, oh, we actually have four people\\nwith that last name.\\n\\nSo this is one way we can search for a specific name.\\nWe could simply say, let's search for last name equal to\\nwhatever the last name is.\\nNow, what if I wanted to search for everybody\\nthat maybe whose last name began with the letter B?\\nWhat if I wanted to do some kind of matching?\\nWell, the way we can do that kind of simple\\nkind of pattern matching is to use a percent sign,\\nwhich says match any number of characters.\\n\\nSo what this particular criteria says\\nis look for any last names that begin with B,\\nand then have any number of characters after that.\\nNow I'm going to run this\\nand now I'm going to explain why we got no data.\\nThe reason we got no data is\\nbecause this is saying, please go ahead\\nand look at the last name column\\nand list any rows where the last name is B percent sign.\\nBut we don't want to do an exact match.\\n\\nWhat we want to do is we want\\nto tell the Postgres query plan builder,\\nthe thing that actually builds the steps\\nto execute this statement.\\nWe want it to, instead of doing a literal match for a B,\\nfollowed by a percent sign, we want it to do a pattern match\\nwhere it looks for a B\\nand then the pattern that we associate\\nwith an ampersand is basically any number of characters.\\nWell, to indicate we want to use pattern matching\\ninstead of literal matching,\\nwe need to use the like operator instead of the equal sign.\\n\\nSo let's run this and see what happens.\\nThis is much better, this is more what I was expecting.\\nFor example, we have Boyd at the top, Boyd, Burns, Butler,\\nanother Boyd, more Burns, Butler, and so on.\\nSo these are all B names.\\nSo this is one way that we can search, for example,\\nlooking for names to begin with the letter B.\\nNow, let's suppose we want to also add\\nas a further constraint\\nthe name has to end with the letter D, so it begins with B,\\nthere's any number of characters in between,\\nand then there's a letter D.\\n\\nNow let's run this.\\nAnd what we see here is, okay, so only Boyd fits\\nthat pattern, but this indicates\\nthat we can use a combination of literal strings\\nlike the B and the D and pattern matching indicators,\\nlike the percent sign.\\nNow let's do a little bit more with this,\\nwith our grouping by.\\nSo let's say, let's maybe get rid of the...\\nSay we don't want to match last letter D,\\nbut we want to match on the first two letters being B and O.\\n\\nSo we can run that and we see\\nnow in addition to Boyd, we also have Bowman.\\nSo what we're building here are these strings,\\nwhich are the series of characters\\nthat we exactly want to match.\\nAnd then these sort of more freeform\\npattern matching characters like the percent sign.\\nNow we can also add other constraints.\\nSo we could add another condition.\\nWe could say, and let's say we want to match\\nwhere the last name is, like B-O-Y-D.\\nAnd salary is greater than 100,000.\\n\\nAnd let's run that.\\nOkay, so now we have two,\\nwe have, excuse me, we have three Bowman and then two Boyds.\\nNow, I don't always want to have all\\nof the conditions satisfied when we use\\nand in a aware clause, both of the conditions\\naround that and need to be satisfied.\\nIf I want to list the last name, department ID,\\nand salary of any employee whose last name starts with BO,\\nor whose salary is greater than 100,000,\\nthen I'm likely to get a mix where I'll have some salaries\\nbelow a hundred thousand,\\nbut only if the last name starts with BO,\\nand some I will have salaries greater\\nthan a hundred thousand\\nand the last name could be anything,\\ndoesn't necessarily start with BO.\\n\\nSo let's run that.\\nAnd that's exactly what we have.\\nSo we see one row here, Boyd,\\nwhere the salary is less than a hundred thousand.\\nBut when we see other names in here like Gonzalez and Owens\\nand Webb, these are all over 100,000.\\nSo we can use a combination of Boolean conditions\\nand we can either make them, you know, all need to apply\\nand when we want them all to apply,\\nwe use the and operator,\\nwhen we want any one of them to apply,\\nwe can use the or operator.\\n\\nNow we can also combine these different pattern matching\\nand different filtering with group buys.\\nSo for example, let's say,\\nI want to know what's the sum of salaries,\\nsum of salaries\\nfor all departments\\nwhere the salary is greater than 100,000.\\nNow if I ran this, let's go ahead and run that.\\n\\nIf I ran this, this is going to sum up all\\nof the salaries greater than a hundred thousand\\nfor the entire company.\\nSo that's the answer.\\nThat's if we look at just our\\nhundred thousand plus salary employees\\nand we add up their salaries,\\nthis is the value that we get over here.\\nNow if I want to see, well, what is that sum\\nfor each department?\\nI can group by department ID.\\nNow I want to know\\nwhat the department is when I list each of these salaries.\\n\\nSo I'm going to go ahead and put the department ID\\nin the list of output columns as well.\\nAnd let's run that.\\nAnd now what we see is we have our department IDs\\nand we have the sum of all\\nof the salaries greater than 100,000.\\nSo this doesn't count any salaries\\nthat are maybe 90,000 or 80,000.\\nSo basically what we're saying is gimme the sum\\nof the salaries for all of our top earners\\nand group them by department.\\nSo that's what we're showing with this example query.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5239153\",\"duration\":646,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Joining and filtering data\",\"fileName\":\"5925685_en_US_02_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":720,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to use the SQL features described in this chapter to derive descriptive statistics for a data set. This provides additional practice using descriptive statistics.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":25056094,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] One of the things\\nwe find pretty quickly when we're working\\nwith relational databases is that we often have\\nto combine information that's stored in different tables.\\nAnd we do that from a data modeling perspective\\nquite intentionally.\\nWe split up data into different tables to help avoid\\nwhat are known as data anomalies\\nor problems that can creep into the data\\nsuch as inconsistencies and how we report things.\\nWell, the fact that we split up related information\\ninto different tables means there will be times\\nwhen we're querying that we want to combine\\nthat related data back into say, a single query.\\n\\nAnd we're going to look at that in this video.\\nWe're going to look at joining tables\\nand also how we can filter on join tables.\\nSo let's start with our go-to employee table.\\nSo I'm going to SELECT, star, from\\nlet's from data_sci is the schema\\nand it's the employee's table.\\nAnd again, I like to just run this regularly\\njust to make sure I'm building this correctly\\nas I incrementally build it.\\n\\nNow what we see here, when we look at the data\\nthat comes out of the employee table,\\nwe see we have the employee_id,\\nthen we have these things called\\nSo we don't have any other information\\nabout the region_id in this table.\\nSo we don't know what country it's in,\\nwhat the name of the region is.\\nSimilarly, we don't know what the department name is,\\nbut there will be times when we're generating queries\\nthat we're going to want to include\\nthat information in the results.\\nWhile the way we get that information\\nis we perform what's known as a join.\\n\\nSo a join operation is something\\nthat we specify in the from clause\\nand we use the keyword JOIN\\nand then we specify what additional table we want to join to.\\nSo we have employees, let's say we want to work\\nwith company_regions.\\nI'm just going to jump over real quick\\ninto the SQL connections area and just drop down\\nor expand the company_regions table explanation\\nwhere we can see the columns that are available.\\nAnd there are two columns available with descriptive text.\\n\\nOne is called region_name, one is called country_name.\\nSo we're going to join on data.sci.employees to data.sci.\\nI need to specify the schema again.\\nAnd then the name of the table.\\nIn this case, it's company_regions\\nand I'll just take that.\\nNow when we specify a join, we need to tell Postgres\\nor SQL what columns you want to join on.\\n'Cause a join basically says link up two rows\\nor two or more rows, but link up rows that are related.\\n\\nWell, how do we relate things?\\nIn relational data modeling,\\nwe relate them based on column values\\nand both of our employee tables\\nand the company_region table have\\nIDs for region_names.\\nSo we can link up or join on\\nand we actually use the keyword on\\nand then specify the names\\nof the columns we want to join on.\\nSo in the employee's table, we have region_id\\nand in the company_regions table,\\nwe have an id.\\n\\nNow the id has the same values.\\nThat would be the corresponding value\\nthat would be stored in the employee's region table.\\nSo let's just run this real quick.\\nAnd what we see is we are doing a select star,\\nso we're selecting all the columns,\\nbut now we select all the columns from both tables.\\nSo we have columns from the employee table.\\nSo for example, we have last name, email, start date,\\nand so on region_id and department_id.\\nAnd then we have id with a parenthetical one after it.\\n\\nSo we have two different tables that we're joining to\\nand they each have a column name with the same name.\\nIn this case, it's id.\\nSo we've got a different column alias put in here,\\nbut then we also see we have these other text fields,\\nregion_name and country_name.\\nSo using a join\\nis how we can get additional pieces of information.\\nSo now what I'd like to do is I'd like\\nto make this a little more streamlined.\\nSo one of the things I'd like to do\\nis I don't like spelling out the full table names\\nin the on statement.\\n\\nSo what I'm going to do is I'm going\\nto alias the tables in the join clause.\\nSo I'm going to alias data.sci.employees as e,\\nand we're going to alias data.sci.company_regions as cr.\\nAnd I'm going to use the aliases\\nwhen I reference those tables.\\nNow another thing I like to do\\nis to explicitly list the columns that I'm working with.\\nSo I'm going to go ahead and do that here.\\nI'm going to say that I want to,\\nlet's say we want all the columns from the employee table,\\nbut we only want the region_name\\nand country_name from the company_regions table.\\n\\nSo we can say e*.\\nSo for the table employees, we're going to get\\nall of those columns.\\nAnd then for cr, we're going to get region_name.\\nAnd from cr, we're going to get country_name.\\nNow let's run this.\\nNow what we see is we see that we have all of the columns\\nfrom the employee table, including the id,\\nlast name, start date, job title, region_id, department_id.\\nThose are from the employee table.\\n\\nAnd then we only have region_name and country_name.\\nSo we've dropped that id\\nwith the parenthetical one afterwards\\nbecause we are just pulling region_name\\nand country_name from the company_regions table.\\nWell, in addition to doing joins\\nand doing it efficiently in terms of aliases\\nand explicitly stating columns,\\nwe can also do some filtering.\\nSo let's say I am interested in listing all the employees\\nwho are in Canada.\\n\\nI can add a WHERE clause here.\\nNow, before, if I was working just with the employee table,\\nI would have to like know what department_ids\\nor what region_ids probably would be the best ones\\nthat are associated with Canada\\n'cause it is the country_name is a function of the region.\\nSo I would have to know all the region_ids in Canada\\nif I'm only working with the employee table.\\nBut since I've joined related information\\nfrom the company_regions table, I can use the other columns\\nor the other attributes that are available in that table.\\n\\nAnd in particular, I can use the country_name\\nthat's available and I'm going to put cr in front.\\nNow the only table that I'm referring to\\nthat has country_name is the company_regions table.\\nSo I could be okay by just saying country_name,\\nbut I really like to spell out in\\nas much detail as possible.\\nYou know what I'm working with.\\nI like to make it explicit.\\nIt's not that I would necessarily forget\\nwhile I'm working today about this, but if I come back\\nto this code in three months\\nor six months from now, I want to make it easy on myself.\\n\\nSo that's why I try and spell things out\\nas much as possible anytime I'm working\\nwith code that I think I will be using in the future.\\nSo in this case I'll use cr.country_name\\nand I'm going to use the equality operator\\nand I want to look for Canada as the country_name.\\nSo I believe I've got my syntax right there.\\nLet's give it a try.\\nYep. Okay.\\nSo now we have rows that we returned.\\nAnd it looks like, if you can see down here,\\nit looks like we are looking at the first 50 of 421 rows.\\n\\nAnd if we scroll over, we'll see we have\\nall of our employee information and then we have region_name\\nand then country_name.\\nAnd sure enough, country_name is all Canada.\\nSo this is how we can use joins to add additional columns\\nthat we can use both for providing more information\\non our results set,\\nbut also for making our filtering a little more explicit\\nor a little more obvious as to why we're doing it.\\nNow, I could have done something\\nlike list the region numbers.\\nLike it looks like regions five, six, and seven\\nare in Canada.\\n\\nI could say have a WHERE clause\\nwhere I say WHERE region_id in and then list 5, 6, 7.\\nBut that's not nearly as obvious to me\\nas seeing country_name equals Canada.\\nSo again, I want to be sort of kind to my future self\\nwho might be looking at this code several months from now.\\nAnd so definitely doing a join, it allows, again,\\nmore explicit data to make the data more obvious to somebody\\nwho might be reading it.\\nBecause you may download\\nand export this data set to an Excel spreadsheet\\nthat you pass off to someone else.\\n\\nWell, think about them as well.\\nHow can you make their lives easier\\nand make it faster for them to kind of assess\\nwhat your data is trying to tell you.\\nSo you want to think about that.\\nAnd then you also just want to make it easier on yourself\\nas someone who is coding this and may come back\\nand revise the code later.\\nSo in that vein, just to really push this example,\\nI would also change the e*\\n'cause I don't need to list everything\\nthat's in the employee table.\\nFor example. I really don't need id, but I do want last_name\\nand email, start_date,\\nsalary, and let's add job title too.\\n\\nAnd get a comma in there.\\nYeah, so now, this SQL statement is very explicit.\\nI'm identifying exactly which columns I want,\\nwhat tables they're coming from.\\nAnd then also, I have the WHERE clause,\\nwhich is fairly obvious, right?\\nWe're not working with just some obfuscated id.\\nSo let's run this.\\nAnd what we see here is, okay, so now we don't have any\\nof that sort of database specific things like primary keys\\nthat are meaningless keys.\\n\\nThis is much more useful information\\nfor someone who's maybe particularly interested\\nin the business domain.\\nSo maybe somebody who works in HR\\nand wants to identify all of the employees in Canada\\nbecause they need to do maybe comply\\nwith some government regulation in Canada.\\nAnd so they need a list of all the employees in Canada.\\nWell this is the kind of information\\nthat would be useful in a case like that.\\nSo this just shows an example of how we can use joins\\nto get additional data to improve the quality\\nof the result sets that we're doing\\nto make things more obvious\\nand also allow us to more easily filter that data\\nthat we're working with.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5231405\",\"duration\":15,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Test an attribute for normal distribution\",\"fileName\":\"5925685_en_US_02_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":31,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Some descriptive statistics are useful for normally distributed data. This challenge tests your ability to determine if data is normally distributed.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":449224,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Narrator] Okay.\\nIn this challenge, I want you to write a SELECT query\\nto return the last name, email, department name\\nfor employees with salaries greater than $120,000.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5233339\",\"duration\":61,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Test an attribute for normal distribution\",\"fileName\":\"5925685_en_US_02_08_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":78,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Get a solution to the challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2701546,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] Here is the solution to the challenge.\\nWe have a SELECT statement and we have three columns.\\nWe have the employee last name and employee email,\\nwhich is coming from the employees table.\\nAnd we've alias that in the FROM clause using e.\\nWe also have the department name\\ncoming from a table with an alias of cd.\\nWell, the cd table is listed in the JOIN clause,\\nand that's a join on the data_sci.company_departments table.\\n\\nNow, because this is a join,\\nwe have to specify which columns we're going to join on.\\nWe're going to use the department ID\\nfrom the employees table and join that to the ID column\\nof the company_departments table.\\nAnd then, finally, we have a WHERE clause\\nwhere the salary is greater than 120,000.\\nAnd I've executed that.\\nAnd so we can see in our result pane over to the right,\\nwe have a list of employees,\\ntheir last name, email, and department name.\\n\"}],\"name\":\"2. Basic Statistics with SQL\",\"size\":92354503,\"urn\":\"urn:li:learningContentChapter:5231408\"},{\"duration\":2591,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5239152\",\"duration\":531,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Reformatting character data\",\"fileName\":\"5925685_en_US_03_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":811,\"solutionVideo\":false,\"editingNotes\":\"cut 10:26 to end of video\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH03 > 03_01.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn string manipulation functions, such as trimming, changing case, padding, etc.  Manipulating strings is a common task in data engineering and data science.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14411828,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In data science and data analysis,\\nwe often work with numbers, but we also often work\\nwith text and short strings.\\nSo it's important to think about\\nhow we can use different functions in SQL\\nfor manipulating text, such as reformatting text.\\nSo in this video, we're going to work\\nwith the company_departments table\\nbecause that has a couple of text columns in there.\\nSo let's start by just selecting all of the columns\\nfrom the data side, company_departments table.\\n\\nAnd let's just run that,\\nmake sure I got everything typed correctly.\\nOkay.\\nYes, and what we see here is we see all of the data\\nin the company_departments table,\\nand we'll notice, it's all lowercase.\\nSo if we want to list all of the department names,\\nwe can list just department_name and run that\\nand get just one column back.\\nBut the format of the text that we're looking at\\nis however it is stored in the row.\\nNow this may be fine for, you know,\\nwe may want all lowercase in the database for some reason,\\nbut for some reporting requirement, we really want\\nall of these department names to be uppercase.\\n\\nWell, in that case, we can use the upper function\\nand simply pass in the name of the column we want\\nor the text that we want to uppercase.\\nAnd when we use the upper function,\\nwe'll again, get back a single column,\\nbut now everything is uppercase.\\nNow, sometimes, you may just want the first letter\\nor the initial letter capitalized,\\nand you want everything else however it normally is.\\nWell, in that case, we can use the initcap function\\nand pass in department_name.\\n\\nAnd now if we run that,\\nwe'll see only the first name is capitalized.\\nNow one of the nice things about the way SQL works\\nis that you can take the results of a function\\nand pass that in as a parameter\\nand argument to another function.\\nSo for example, here, we're taking department_name\\nand then whatever else it is, but we are going\\nto make sure the first letter\\nin the department_name is capitalized.\\nWell, what if we want to then lowercase?\\nSo basically undo our upper initial capitalization.\\n\\nWe can do that.\\nAnd so what we've done is we take the department_name,\\nwe turn the first or initial letter into a capital,\\nand then we lower it again.\\nSo we're getting back to right where we started from.\\nSo it's not terribly useful,\\nbut I just wanted to demonstrate\\nhow we could have a whole series of functions\\nthat take as their input, the results of other functions.\\nBecause that can be quite handy when you want to do\\na whole series of operations on a string.\\nNow sometimes, when we work with strings,\\nwe can run into some subtle problems with white spaces.\\n\\nThis happens more than I would like\\nat least in my work where I might be working\\nwith a new data set.\\nIt might come out of maybe an older source data system\\nthat doesn't have quite the, you know, data quality checks\\nthat we might be used to.\\nSo for example, let's say we're working\\nwith somebody's name.\\nAnd we want to work with, say the name Kelly.\\nWe can for example, type SELECT Kelly.\\nAnd if we run this, this is simply going to\\nreturn the word Kelly.\\n\\nNow, if I put a space in front of the word Kelly\\nand run this, I get a result back,\\nwhich looks surprisingly similar.\\nThere's a space before the K in this case,\\nbut it looks very similar.\\nLike I couldn't tell if I was just looking at the result\\nwhether or not there might be an extra space there.\\nWell, let's test\\nand make sure I'm actually looking at something\\nother than capital K-E-L-L-Y.\\nSo let's run this.\\nSo I'm basically doing a Boolean test.\\nI am asserting that this string is the same as this string.\\n\\nSo character for character,\\nare these two the same character string?\\nAnd the answer is false.\\nNo, they're not.\\nThat's because we've got an extra space here.\\nSo that's throwing us off.\\nIf we remove the extra space and then run this check,\\nof course, we're going to get true again.\\nSo sometimes, when we're working with strings,\\nwe want to use functions which help us trim off white space\\nfrom the beginning and the end of a column.\\nSo for example, if we use the function called ltrim\\nor left trim, and pass in the word Kelly,\\nand we just apply ltrim, we're going to get just Kelly back.\\n\\nSo let's test that and see if this is equal to K-E-L-L-Y.\\nAnd it is.\\nNow, if we add a space at the beginning\\nbefore Kelly and run this\\nand do an ltrim, we get a true back.\\nThat's because ltrim will remove the extra white space.\\nSo let's put a couple more in there.\\nWhoops, don't want a period for sure.\\nLet's put a couple more in there and run this.\\nAnd what we see here is ltrim will trim up all\\nof the leading white spaces, whether it's one space\\nor two or three spaces, it will remove those.\\n\\nSo that is one way of trimming down.\\nNow let's imagine we have an extra space\\nafter the word Kelly.\\nSo this is on the right side of the string.\\nSo let's run that.\\nAnd now, it turns out the ltrim does not\\ntake care of the extra white space on the right side.\\nHowever, we can do what we were talking about before,\\nwhich is pass the results\\nof this function into another function.\\nAnd in this case, I'm going to use rtrim or right trim,\\nand I want to right trim the results\\nthat I get back from my ltrim over this string that Kelly\\nwith spaces on both sides.\\n\\nSo now we're trimming the left side,\\nand then we're taking the result of that\\nand trimming the right side.\\nSo let's see if we get a true back.\\nWe do.\\nSo we can do this kind of trimming both on the left\\nand on the right to help us clean up potentially unseen\\nor non-printing characters.\\nNow in addition to working with things like uppercase\\nand lowercase or initializing capitalization\\nand trimming off leading white spaces from the beginning\\nand ends of strings, sometimes,\\nwhat we want to do is we want to construct larger strings\\nfrom component, text columns, or text strings that we have.\\n\\nAnd to do that, we can use the concatenate operator.\\nSo let's look at an example of how we can concatenate\\nor join together two strings.\\nLet's look at the employee's table.\\nAnd from the employee's table, I want to get the job title.\\nAnd I want to concatenate it to the last name.\\nAnd this is going to be from data.sci.employees.\\n\\nSo the double pipe is the concatenation operator,\\nso it just runs two strings together.\\nSo let's run this and see what we got.\\nAnd sure enough, what we have is we have a job title,\\nlike in this case, in the first result,\\nwe have structural engineer immediately followed\\nby the last name, in this case, Kelly.\\nNow what we're doing here is we're missing\\nsome kind of separator or delimiter\\nthat indicates we have, you know,\\nthe end of the job title and the beginning of the last name.\\nWell, one way we can fix that is to add\\na delimiting character, like a dash sign,\\nand we can concatenate that to the job title\\nand then we can concatenate the results\\nof that concatenation operation to last name.\\n\\nSo we'll take job title, we'll append or concatenate a dash,\\nand then we'll append or concatenate last name.\\nSo let's see how that looks.\\nOkay, so this is better.\\nThis is a little easier to read.\\nSo now I can see easily that's structural engineer Kelly\\nand recruiting manager Carr\\nand marketing assistant Alexander.\\nWhat we can do is use the basically\\nthe double pipe operator.\\nNow when we work with the double pipe operator\\nand with column names, we want to think about\\nwhat happens when a column has a null value.\\n\\nSo let's say there might be someone,\\nfor some reason, their last name is missing\\nand there's a null in for their last name.\\nWhat happens when we concatenate a null\\nto the string using the pipe operator?\\nWell, let's see.\\nWhat we get back,\\nthe entire result of the concatenation operator is null.\\nIt's not simply the job title, dash, and then nothing else.\\nYou might expect that,\\nbut that's not how the concat operator works.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5240086\",\"duration\":513,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Extracting strings from character data\",\"fileName\":\"5925685_en_US_03_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":562,\"solutionVideo\":false,\"editingNotes\":\"cut 6:58 to end of video and replace with pickup1\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH03 > 03_02.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to find substrings within strings, test strings for substrings, and locate the position of substrings within strings. Data scientists sometimes need to create new attributes by extracting substrings of other attributes.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16536804,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Sometimes when we work with text,\\nwe want to extract a section of a piece of text or a string.\\nSo we might want to maybe break off or take off\\nthe beginning of a piece of text\\nor extract the end of a piece of text.\\nWell, SQL gives us a number of different ways to do that.\\nSo let's start with a really simple example,\\nand let's start with a select and then a series of letters.\\nSo we'll start out with letters from the alphabet here\\nat the start of the alphabet,\\nand we'll just run this and make sure.\\nYep, okay, so when we get back,\\nwhen we do a select from A through K,\\nwe get a string with all the letters A through K,\\nas we might expect.\\n\\nNow sometimes we might decide, it's like, oh,\\nlet's take a subsection of that.\\nWell, first thing I want to do is I'm going to alias this\\nas test string.\\nSo before I take a subsection, let me just run this.\\nOkay, yeah, so there we have our alias here.\\nNow let's say, for our test string,\\nI want to take a substring.\\nSo we'll use the substring function over this\\nand we'll specify what part of that string\\nwe want to extract.\\n\\nIn this case, we can specify where we start.\\nSo the first character is character one,\\nand we want to go for three characters.\\nSo let's run this,\\nand this should return A, B, C, which makes sense.\\nA, B, C are the first three characters.\\nSo this is the position where we start,\\nand that's the length of the string we want to return.\\nSo that makes sense if we understand the purpose\\nof each number in each of these positions.\\nNow sometimes, if you're not quite as familiar with SQL,\\nor you're going to be sharing code with people\\nwho might not be as familiar with SQL as you are,\\nyou might want to use the alternative syntax\\nwhere we specify from and then the starting position\\nand then however long we want to go for.\\n\\nAnd that would be for,\\nin our case we're going for three characters.\\nSo let's go ahead and run that.\\nOops, excuse me, I need to remove the comma.\\nThere we go.\\nSo this is a little more explicit.\\nSo we're taking the substring\\nand it's given this string, we want to go from position one\\nand we want to go for three characters.\\nSo you just want to remember to remove the comma\\nthat you might have in front of the from\\nif you did editing, like the one three like I did.\\n\\nSo that is an alternative way of specifying\\nthe text we want to extract.\\nNow, if we don't specify a for clause\\nand we just say something like from five,\\nthat means extract all of the string\\nstarting from position five and going on,\\nbasically for the length of the string.\\nSo start at position five\\nand then go all the way to the end.\\nSo that's how we can extract from the fifth character on.\\nSo substring is really useful.\\nThere are a couple of different ways of specifying\\nthe parameters.\\n\\nYou can either specify the starting position\\nand the number of characters just as a list of numbers,\\nlike 1, 3, or you can be more explicit\\nand say like from whatever the starting position is\\nfor the number of characters\\nthat we're interested in extracting.\\nOkay, so that's one example.\\nLet's work with the employees table,\\nbecause we have some strings in there.\\nSo let's just start with select star\\nfrom data_sci.employees\\nand yep, working right,\\nand what we want to do is, let's look at job titles.\\n\\nAnd where are our job titles.\\nHere we go.\\nSo we have somebody here who's a marketing assistant,\\nthere's an office assistant three.\\nOkay, so let's say we want to understand more about people\\nwho have assistant in their title.\\nWell, one thing we could do is we could create\\na where clause and match where job title is like assistant.\\n\\nNow we don't want to necessarily start\\nwith the word assistant.\\nWe want to match anytime the word assistant falls anywhere\\nwithin a title.\\nSo I'm going to put a percent at the end and at the beginning.\\nSo this says, this Boolean expression,\\njob title like \\\"%assistant%\\\"\\nthat's a Boolean expression, and it'll be true\\nwhen the word assistant is somewhere in the title,\\nregardless of where it is.\\nSo let's go ahead and run this\\nand let's just check our job titles\\nand what we have are marketing assistant, assistant manager,\\nmarketing assistant, office assistant three,\\nadministrative assistant, human resource assistant,\\nI believe that will be assistant.\\n\\nLet's see.\\nYep, oh wait, assistant two.\\nSo we see we seem to be matching.\\nThis is working as we expect.\\nSo this is useful.\\nThis kind of pattern matching is really useful\\nwhen we want to look inside of a string of text\\nand be able to match a pattern within it.\\nNow I will offer this one warning.\\nIf you are working with a very large table\\nand you're trying to do pattern matching like this,\\nyou may run into some performance issues.\\n\\nBecause when we do this kind of matching\\nwhere we have a percent at the beginning of our string,\\nor we're matching a string and we have a percent sign\\nat the beginning of the pattern we're looking at,\\nwhen that's the case, we can't use indexes on that column.\\nSo we might have an index on the job title column,\\nwhich makes it, in general, very fast to look up,\\nsay all the assistant managers\\nwhen we specify just quote assistant manager.\\nWhen we put a wild card, like the percent sign,\\nthat basically indicates to the query plan builder,\\noh, we can't use an index for this,\\n'cause we don't know where the word assistant will fall\\nwithin the larger string.\\n\\nSo what you end up doing is,\\nrather than doing an index lookup,\\nwhere kind of like when you use an index\\nin the back of the book,\\nyou see which page you'd want to go to,\\nInstead of doing that, we end up doing what you might do\\nif you had to read every word in the book\\nto see if the word assistant shows up.\\nAnd so we start at the beginning of the table\\nand look at basically all of the values of job titles\\nuntil we get to the end.\\nThat's called a sequence scan.\\nSequence scans can be relatively fast on small tables,\\non very large tables they can be much slower.\\n\\nSo just something to be aware of.\\nThis kind of pattern matching does come with some costs\\nwhen you were talking about working with very large tables.\\nNow another thing we can do\\nwith this kind of pattern matching is that we can derive\\nsome additional columns that might be of use\\nfor someone maybe looking at a data set.\\nSo for example, we could select a job title,\\ntitle, and we could look at this same condition\\nwe have down here in the where clause.\\n\\nWe can copy this and put it up here in our select statement\\nin the list of columns or values that we're returning.\\nAnd we can treat this as a Boolean expression.\\nWell, it is a Boolean expression,\\nand we can give it a name and that can be is_assistant,\\nand this column, is_assistant,\\nwill return either true or false.\\nSo let's run that and see.\\nAnd we're getting, okay, so all of these are assistants.\\n\\nNow that's what we would expect because of the where clause,\\nbut if we remove the where clause, we should see a mix\\nof some assistants are true and some are false.\\nAnd we're saying, yep, some are false, some are true.\\nSo when it's structural engineer or recruiting manager,\\nit's false.\\nElectrical engineer is false,\\nbut marketing assistant is true.\\nSo we can use this kind of pattern matching\\nto also build up other columns,\\nwhich can sometimes be useful.\\nAgain, this might not be as useful to you\\nif you're doing ad hoc queries\\nand you know exactly what you're looking at.\\n\\nBut if you're building data sets\\nfor somebody else to work with,\\nwho's more oriented toward the business domain,\\nyou might want to use these kind of expressions like this\\nto build up columns that are more closely aligned\\nwith the terminology that is used in the business domain,\\nor indicate certain attributes\\nthat might not be explicit in the table,\\nbut are useful to say the end consumer of the data\\nif you do make them explicit.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5236202\",\"duration\":623,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filtering with regular expressions\",\"fileName\":\"5925685_en_US_03_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":696,\"solutionVideo\":false,\"editingNotes\":\"00:00 - cut very beginned where the code gets deleted to clear the screen before the instructor starts.\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to use regular expressions to find and filter on complex patterns. Regular expressions can reduce the complexity of filter conditions applied to character data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17254567,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] As our work\\nwith text strings becomes more complicated\\nand the patterns we're working\\nwith are really capturing more complex business logic,\\nit's sometimes useful\\nto start using regular expression pattern matching,\\nrather than using the like operator.\\nSo let's work with the employees table again\\nand let's work with job titles.\\nSo let's select,\\nand we're going to be selecting from,\\non the data sci employees table.\\n\\nAnd what we will select, first of all, is the job title.\\nSo let's make sure we have that working as expected. Okay.\\nYep.\\nNow let's say, as we're looking through this,\\nI see we have, here's a VP of product manager,\\nI think there's some other VPs.\\nHere's VP of sales. Yeah, let's take a look.\\nOh, VP of product management, I think.\\nLet's take a look and see if we can find all\\nof the employees who have VP in their title.\\n\\nWell, to do that, we'll use a where clause\\nand we'll say where job title is like,\\nand we can use VP followed by a percent sign,\\nso to match anything afterwards.\\nSo let's see what that gives us.\\nWe should get, okay, we get quite a few.\\nOh, but see some of these,\\nlike we have VP accounting, accounting, accounting.\\nSo there are many rows\\nthat have a job title that match this.\\nWhat I'm really interested, though,\\nin is just seeing the individual job title.\\nSo I don't need to see VP of accounting more than once.\\n\\nWell, for that, I can use the select distinct operator.\\nAnd when we run that, now we get each title just once.\\nAnd that's really what I want.\\nI want to work with just a particular job title here.\\nSo now that we have that,\\nnow we have the distinct job titles.\\nNow we also noticed there were some other job titles.\\nI believe there were some that were started with web.\\nSo for example, if we want to look at job titles\\nwhere the title is VP\\nor the job title is like web developer,\\nlet's put that in there.\\n\\nSo that's where, or job title.\\nLike, and now let's bring in web developers\\nand anything else that has a job title that begins with web.\\nAnd we'll run that and we see, yeah, okay,\\nso we have some web designers, one and three\\nand web developers, different web developers.\\nYeah, so we have a mix of web developers and VPs.\\nNow if the logic that you're trying\\nto match gets more complex,\\nyou can imagine adding additional or clauses,\\nor and clauses and things like that.\\n\\nWell, at some point, it might be easier\\nto switch from using the like operators in a whole series\\nof Boolean conditions like this to using what's known\\nas the similar to operator.\\nSo let's replace like with similar to.\\nAnd when we're working with similar to, we can combine\\nand basically search for VPs and web in one line.\\nAnd the way we do that is,\\nwe can say we want to match VP and web.\\n\\nNow we're interested in starting in titles\\nthat start with web or start with VP.\\nSo we can put the percent sign to indicate that.\\nNow I want to basically match either this VP pattern\\nor this web pattern.\\nWell, to do that, I just simply put these into a list\\nthat are bounded by parentheses\\nand I put a single pipe in between the two patterns.\\n\\nAnd what this says is exactly the same logic\\nthat I had before with the or clause.\\nThis says, where job title similar to VP percentage,\\nor job title similar to web percentage.\\nBut what we've done is we've reduced it to a single line.\\nSo the logic's a little simpler.\\nAgain, we have a list and we're going to be choosing,\\nbasically it's an or operation across each\\nof the options in this list.\\nAnd the different options are demarcated by the single pipe.\\nSo we have VP percent or web percent.\\n\\nSo let's run that and see what we get,\\n'cause we should get the similar list.\\nOh, you know what?\\nI have forgotten a critical thing,\\nwith anytime we're working with strings,\\nand that is my quotation marks.\\nAnd let me put in the quotation marks. And that goes here.\\nSo now the similar to operator is going to operate,\\nusing this specification,\\nlooking for rows that have the job title\\nthat match the specification.\\n\\nSo now let's run.\\nAnd now we get the same list that we saw before.\\nSo with the quotation marks,\\nand this is really critical, we have to have that\\nbecause the similar two expects a string at that point.\\nAnd so that's what we have specified here is a string\\nthat searches for VP or for web,\\nfollowed by any number of characters.\\nNow usually when I'm writing select statements,\\nI like to use a lot of white space.\\nSo typically I might, you know, put a space there,\\nspace there, 'cause it's easier to read.\\n\\nNow what happens when we do that?\\nWe run into a problem. We get no data back.\\nAnd the reason is, these spaces\\nthat I added are now part of the pattern.\\nSo this part of the specification says, match any row\\nwhere the job title starts with VP, has any number\\nof characters, and then a space at the end, okay?\\nNone of our VP titles have a space at the end.\\nThis part of the specification says, match any row\\nwhere the job title begins with a space, followed by W-E-B,\\nand then any number of characters.\\n\\nWell, none of our job titles begin with space W-E-B,\\nand that's why we're not getting any data.\\nSo we have to be very careful about things like white space\\nwhen we're constructing these specifications that we use\\nwith similar to, so that we have exactly what we want,\\nwhich in this case is titles without spaces in there.\\nOkay, let's look for just VPs again.\\nLet's kind of simplify this thing again,\\nand let's switch over to searching\\nfor quote VP percent and run that.\\n\\nThat looks good. Okay, so we're getting just the VP titles.\\nNow let's say I want to find, say,\\njust like VP of accounting.\\nYeah, just something that like begins with A.\\nI could, for example, try VP percent A. Now let's run this.\\nAnd what we're finding here is\\nthat we're not getting anything.\\nWell, what's going on is, our pattern is match VP,\\nfollowed by any number of characters followed\\nwhere the character string and job title ends in an A.\\n\\nWell, none of our job titles actually match that.\\nWhat I really want to do is I'm just trying to find, you know,\\nall of the rows where I'm going to find like,\\nVP of accounting or VP of auditing, something like that.\\nAh, so maybe if I just add a percent at the end.\\nSo I'm looking for a VP with an A afterwards.\\nWell, let's run that. Okay.\\nNow what we realize is, oh, right,\\nall of our VP titles also have the letter A in them.\\nSo that's not helpful.\\n\\nAll right, so what I really want is I just want the VP,\\nfollowed by a space, followed by an A,\\nand then any number of characters after that.\\nLet's see how that works. Okay, that's finally it.\\nThat's what I was shooting for.\\nI wanted to find something like that.\\nAnd so what we have here now is we are able to see\\nthat we can match a VP,\\nfollowed by a space, followed by an A.\\nWell, what if there's, you know, some other character\\nthat might be in there like, I don't know,\\nmaybe there's a tab or some other character\\nthat we might want to match on,\\nor not so much a tab, maybe just some other character.\\n\\nWell, I could specify an underscore\\nand that'll match on any single character.\\nSo if you want just one character to match,\\nwe use an underscore.\\nIf we want any particular character match,\\nwe use the ampersand.\\nNow what if we want to match on, say, VP followed\\nby VP of something that starts with an A,\\nor VP of something that starts with an M?\\nWell, for that, we can try VP, followed by,\\nand then we're going to build a list.\\n\\nAnd in that list, we're going to say either A or M,\\nfollowed by, let's get rid\\nof the percent and see what happens.\\nJust to say, yep, yeah, we don't match on anything.\\nSo VP with a space followed by an A\\nor an M, there's definitely things that match there.\\nBut again, they don't end with that.\\nSo there's additional characters afterwards.\\nSo now let's run.\\nSo now what we've done is we've matched VP of accounting\\nand VP of marketing.\\n\\nSo here we are.\\nWhat we're doing now is it's similar\\nto working with a like operator,\\nbut we're using more of a regular expression syntax,\\nwhich allows us to do like lists where we can choose\\namong the different options in a list,\\nlike starts with A, or starts with M in the second word.\\nAnd anytime you want to build more complex logic\\naround pattern matching,\\nI would suggest looking at the similar two operator.\\nNow there are more things you can do\\nwith regular expressions, like specifying, oh,\\nI want to match on digits or a certain number of characters\\nor, you know, a number of other kinds\\nof very specific patterns.\\n\\nSo I would suggest looking at the Postgres documentation\\nfor similar to, to see\\nwhat other ways you can specify patterns, using this.\\nBut it is really a really useful\\nand expressive syntax for describing text patterns\\nand doing regular expression matching.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5231404\",\"duration\":222,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Reformatting numeric data\",\"fileName\":\"5925685_en_US_03_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":247,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH03 > 03_04.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to use FORMAT, CEILING, FLOOR, and other SQL functions to format numeric data. The functions are often used when transforming numeric data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5682590,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now sometimes we need\\nto reformat numeric data.\\nSo let's take a look at salaries again.\\nSo we're going to work with select\\nand let's select salary from\\nthe data_sci.employees table.\\nAnd actually what we want to do is let's work\\nwith the average salary\\n'cause we want to see different ways that we can work with\\nvalues that have a fairly large number\\nof digits in the decimal places.\\n\\nSo for example here, we have the average salary\\nof around $97,433 and 54 to 55 cents,\\nbut there's a lot of digits in this calculation,\\nwhich is really helpful.\\nIt's great that we can use Postgres\\nto do some fairly high precision calculations like this,\\nbut really oftentimes we don't need to have\\nthat level of precision.\\nAnd we have seen in previous videos\\nhow we can use the round operation to round this value.\\nSo this should round up to 97,434\\nif I read this correctly and it does.\\n\\nSo we can use the round operation.\\nAnd remember, we can also specify a number\\nof digits we want to round to.\\nSo for example, we can round to two decimal places\\nor to four decimal places.\\nAnd so we can determine what's the level\\nof decimal point precision we'd like to have\\nwith our numeric data.\\nWell, in addition to round, we can also use other functions.\\nSo let's just go back to the average for now\\nand see what our value is.\\n\\nAnd we're on that.\\nSo it's 97.33, about 54.55.\\nSo there is a function called trunc, short for truncate.\\nAnd what this does is it basically drops decimal points.\\nIt doesn't round, it simply just eliminates them\\nfrom the numbers.\\nThere we go, so it completely just drops off\\nthe number of digits.\\nNow, we could also specify with trunc some number of digits\\nto truncate two.\\n\\nAnd here you'll notice that it's 97433.54.\\nSo we're truncating everything after the two decimal digits.\\nWe're not rounding, we're just truncating.\\nNow, typically I just use round,\\nbut there may be reasons,\\nlike there might be some business logic rule\\nthat requires you to use truncate instead.\\nThere's also the ability to truncate farther out.\\nSo for example, if you wanted to truncate\\nto four decimal places, you could do that as well.\\n\\nNow there is also a function called ceiling or ceil,\\nwhich basically goes up to the next largest whole number.\\nAnd so for example, this is going to 97434.\\nSo when you round\\nand the decimal value of a number is greater than 0.5,\\nthen that's the equivalent of using the ceiling function.\\nIf the decimal value is less than 0.5, that's the equivalent\\nof using the trunc value.\\nSo the functions\\nthat we typically use\\nwhen we're reformatting numeric data are rounding.\\n\\nFor rounding the number, trunc,\\nfor just truncating off some decimal digits,\\nand the ceiling for going up\\nto the next largest number.\\nThose are typically the functions we use\\nfor that kind of numeric reformatting.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5239151\",\"duration\":644,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Fuzzy string matching\",\"fileName\":\"5925685_en_US_03_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":739,\"solutionVideo\":false,\"editingNotes\":\"cut 4:24-5:09\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH03 > 03_05.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to use SOUNDEX to compare strings that not identical but are spelled similarly. SOUNDEX is useful when working with data that may have misspellings.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16397885,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] When we're working with texts,\\nsometimes we want to be tolerant\\nof small variations in a text.\\nFor example, if we're using single words\\nand we're trying to match on those, maybe there's a typo\\nand maybe there's an extra S for example,\\nlike at the end of the word Postgres,\\nwell, if somebody has two S's at the end of Postgres\\nversus one, do you want to be able to match on that column?\\nWell, sometimes the answer's going to be no, absolutely not.\\nThose are two distinct values.\\nBut then there are other use cases\\nthat have different requirements,\\nand you might want to be tolerant.\\nBut when you want to be tolerant\\nabout how you match on texts,\\nit's helpful to use some functions that are available\\nin a Postgres extension known as fuzzystrmatch.\\n\\nAnd to use this, we need to enable the extension.\\nNow, extensions in Postgres are like libraries of code\\nthat are available with the Postgres code,\\nbut not necessarily installed by default.\\nAnd so what we do is when we want to use\\nan optional extension\\nis we use the create extension command,\\nand we specify the name of the extension we want to create,\\nand in our case we want to use the fuzzystring,\\nor fuzzystrmatch extension.\\n\\nSo now I'm going to run this,\\nand what we'll see here is if the fuzzystrmatch\\nis already installed,\\nand it's already installed in my database, we get an error.\\nNow we can use an optional clause,\\ncreate extension if not exists\\nfuzzystrmatch, and we run that,\\nit simply gives us a notice down here in the SQL console,\\nnotice that the fuzzystrmatch extension already exists.\\nSo if I had other operations, other commands in this file,\\nI wouldn't have errored out completely.\\n\\nSo we have the fuzzystrmatch extension installed,\\nand what that means is we now have three functions\\navailable to us that are, actually more than that,\\nbut ones that we're going to talk about today\\nare three functions we're going to work with,\\nand one is called Soundex.\\nLet's take a look at what Soundex does.\\nSo let's SELECT the Soundex of a word like Postgres,\\nand let's run this.\\nAnd what we see is that the Soundex function\\nconverts a string like Postgres\\ninto a four-character code, in this case P232.\\n\\nSo that is the Soundex value.\\nThe idea behind Soundex is that we map a string into a code,\\nand the mapping from a string to the code is such\\nthat two words that sound very similar\\nare going to have matching Soundexes.\\nSo for example, let's create another Soundex\\nand this time let's make this Soundex on Postgres,\\nbut with three S's at the end.\\n\\nSo if we run this,\\nwe see both Soundex, oh and actually Soundex 1.\\nLet me just put some alias columns in here.\\nLet's do p1 for Postgres 1,\\np2, there, and run that.\\nAnd so our two Soundexes are the same.\\nNow, let's see, if we were to evaluate a Boolean,\\nand we evaluate it,\\nis this string equal\\nto this string?\\nWell, we run that, as we would expect, we get a false.\\n\\nHowever, if we copy this same thing\\nand then, and we'll make this\\nBoolean 1, this same,\\nbut now instead of matching the string,\\nwe're matching on the Soundex of these two terms.\\nThere we go.\\nAnd now if we run, what we see is we have the two Soundexs\\nare the same, the two strings are different,\\nbut our Soundex Boolean in this case,\\nI forgot to put an alias here, but our Boolean,\\nour second Boolean is true.\\n\\nSo this is how Soundex works.\\nIt basically, it looks at the beginning characters,\\nlike the first four characters and determines how similar\\nor what a value is for a particular string.\\nSo this is useful if we want to know, can we map, you know,\\na term or a string into a Soundex value?\\nAnd if we do that for a couple\\nof different terms, are they equal?\\nWell, sometimes we might want to know,\\nwell what's the difference?\\nLike maybe, you know,\\nhow big is the difference if a sound isn't exact?\\nWhat if it's close?\\nWell, if that's the kind of way\\nwe want to think about a problem,\\nwe can use a different function from Soundex.\\n\\nWe'll use the difference function.\\nAnd this is part of the extension.\\nAnd if we take the difference between Postgres\\nand Postgres with three S's at the end,\\nand we run that,\\nwe see that we have a difference of 4.\\nAnd the value of the difference function\\nranges from 0 to 4.\\n0 means there's no matches in the Soundex\\nof the two values, of the two terms,\\nA 4 means all four values in a Soundex are the same.\\n\\nSo let's experiment a little bit here.\\nAnd now let's do another Soundex of Postgres,\\nthat's our baseline.\\nAnd now let's change the first character\\nand do a Soundex of, let's change the P to a K,\\nlet's run those values.\\nOh, I forgot my comma.\\nAnd what we see here is our Soundex value\\nof Postgres is P232.\\n\\nOur Soundex for Postgres with the P replaced by K\\nis K232.\\nAnd now what we notice here is the difference,\\noh, I forgot to change this.\\nSo we know that there's no difference between those.\\nBut if we use Kostgres there and now run that,\\nwhat we see here is that the difference is now 3.\\nThat's because there is three values\\nthat are the same in the two Soundexes,\\nthere's 232 is the same,\\nbut the P is different in the second Soundex\\nwhere we have a K.\\n\\nNow what this tells me is that Postgres\\nand Postgres with a K, they don't sound the same,\\nbut they're fairly similar.\\nAnd the difference, there's only a difference of 1\\nin this particular metric that we're using.\\nSo if you are willing to tolerate some level of difference\\nbetween the Soundex even, then a difference,\\nso for example, you might say,\\nas long as difference is 3 or 4\\nI'm going to accept that as a match.\\n\\nOr maybe if you're really tolerant, you might say, oh,\\nif it's within 2, 2, 3, or 4 is the difference value,\\nthen I'll accept that as a match.\\nSo you can experiment with your data\\nand see what works for your particular use case.\\nSo difference again works with Soundex.\\nSo, and it just tells you\\nhow different are two Soundexes, and that's useful.\\nIf you're moderately tolerant\\nand you want the Soundex to be exact,\\nyou can just use Soundex.\\nIf you're willing to let a little bit of variation\\nin the Soundex value kind of creep into your data\\nand you're fine with that,\\nthen using Soundex with difference is useful.\\n\\nNow another function which can be useful,\\nespecially when you're working with like really longer\\npieces of text and you're more tolerant,\\nand you want to say, I want to kind of specify, like,\\nhow many different changes I need to apply\\nto one word like Postgres.\\nIf I do two or fewer changes,\\ncan I get whatever the second value is?\\nI might be tolerant.\\nOr if I have to make 10 changes,\\nor 20 changes to some really long string,\\nI might be tolerant of that.\\nWell, for that we can use the levenshtein function,\\nwhich is L-E-V-E-N\\nand S-H-T-E-I-N.\\n\\nI want to make sure I spell that correctly.\\nSo let's look at the results of the levenshtein function\\nwhen we apply that to Postgres with one S,\\nand well, we'll start, we'll just make an exact match.\\nAnd if we run that we see 0.\\nSo there are zero changes that we need to apply to this term\\nto get this term.\\nNow a change could be an addition, a deletion,\\nthere's different kinds of operations,\\nbut basically I don't have to do any operations\\non one term to get to the other term.\\n\\nSo that means there's an exact match.\\nWell, let's see if we change this P to a K,\\nand run that.\\nWell, now we see we have to make one change.\\nSo if we make one change, we change the K to a P,\\nwe've got an exact match.\\nNow maybe we've got to make a lot of changes.\\nSo maybe we're typing in something like MySQL\\nand we run that.\\nWe see that we actually have to make eight changes.\\nWe have to eliminate all six of the letters\\nMy in MySQL and add two more.\\n\\nSo that's an example of how we can use levenshtein\\nto help us understand, you know, how close different pieces\\nof text are in terms of matching character for character,\\nand how different they are.\\nAnd we can use those measures of difference to allow us\\nto kind of set our own tolerances\\nfor how much we want to match.\\nNow before we go,\\nI do want to mention there are some limitations to Soundex,\\nespecially if you're using multi-byting coating like UTF-8.\\n\\nAlso, sometimes with non-English languages,\\nSoundex may not work as well.\\nWell, the fuzzystrmatch extension also has\\nan additional Soundex function\\ncalled the Daitch-Mokotoff Soundex or DM Soundex,\\nand that works well with other languages like multi-coding,\\nmulti-encoded character strings like UTF-8,\\nor if you're working with some non-English language text,\\nyou might want to look into using that function as well.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5236201\",\"duration\":25,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Prepare a data set for analysis\",\"fileName\":\"5925685_en_US_03_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":30,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"This challenge tests your ability to apply string and numeric transformations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":752532,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat instrumental music)\\n- [Instructor] In this challenge,\\nI'd like you to answer a few questions.\\nFirst, what function could you use\\nto reformat a number from having eight decimal places\\nto having two decimal places?\\nAlso, what operator would you use\\nto filter using regular expressions?\\nAnd then finally, what function would you use\\nto measure the difference in characters between two strings?\\n\"},{\"urn\":\"urn:li:learningContentVideo:5232355\",\"duration\":33,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Prepare a data set for analysis\",\"fileName\":\"5925685_en_US_03_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":36,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"This video offers a solution to the challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":941921,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(bright music)\\n- [Instructor] Okay, the solution with regards to the\\nquestion of what function could you use\\nto reformat a number from having eight decimal places\\nto two decimal places?\\nWe could use either the ROUND\\nor the TRUNC specifying two as a second parameter.\\nWith regards to what operator we can use\\nto filter using regular expression,\\nit's the similar to operator.\\nAnd then finally, the Levenshtein function is a function we\\nwould use to measure the difference in characters\\nbetween two strings.\\n\\n\"}],\"name\":\"3. Data Munging with SQL\",\"size\":71978127,\"urn\":\"urn:li:learningContentChapter:5240088\"},{\"duration\":2608,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5239150\",\"duration\":448,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Using the HAVING clause to find subgroups\",\"fileName\":\"5925685_en_US_04_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":520,\"solutionVideo\":false,\"editingNotes\":\"01:20 - Dog barking. The instructor pauses but does not rephrase\\ncut 5:14-7:44\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH04 > 04_01.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to select groups with specific properties using the HAVING clause. HAVING clauses are used to filter based on subgroups instead of rows.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13194328,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now we're going to turn our attention\\nto filtering and aggregation.\\nNow, sometimes when we're working with data\\nwe want to work with subgroups and we want to make decisions\\nabout what to include in our result set\\nbased on say values or aggregates about those subgroups.\\nSo, we're going to work on some examples of that\\nand see how we can do that.\\nSo, first of all, how about if we work\\nwith departments and employees?\\nAnd in particular, let's select the number\\nof employees in each department.\\n\\nSo, I have a SELECT statement.\\nAnd I know I'm going to want to look up department names,\\nso for that I'm going to use the company_departments table.\\nAnd I know I'll alias that as cd.\\nAnd I'm going to want to list by department name.\\nI'm going to want to get the count of rows,\\nand I'm going to want to pull this data\\nfrom two different tables in the data_sci schema.\\n\\nAnd they are data_sci employees,\\nand that's going to be aliased as e.\\nAnd I'm going to be joining to another table in data_sci\\ncalled company_departments,\\nand that's going to be aliased as cd.\\nAnd now of course I need to specify the ON clause.\\nAnd so, we're going to specify the column\\nwe're going to look for in the employees table\\nis department_id, and the column we're going to look for\\nin the company_departments is simply id.\\n\\nNow, what I need to do is I want to have a GROUP BY\\n'cause I have an aggregate, so I'm going to GROUP BY,\\nand I'm going to group by the department name.\\nSo, that's cd.department_name.\\nAnd that looks correct, so I'm going to go ahead and run this.\\nAnd what we have is a list of different departments,\\nand it's just in a fairly random order.\\nAnd we have the count, which is the number\\nof employees in each department.\\nWell, okay, so that worked well so far.\\n\\nWhy don't we add an ORDER BY clause so that we can see this\\nlist in alphabetical order by department name.\\nAnd so, for that I'm going to add an ORDER BY,\\nand I want to order by cd.department_name\\nand we'll run this.\\nAnd so, we see it's basically in alphabetical order,\\nA through T.\\nNow, I could also order this by count,\\nso I can pick one of the other columns that we're outputting\\nand I can order by that.\\n\\nAnd so, let's order by that and see what happens.\\nSo, now we see ah, music is at the top,\\nthat has the lowest number of employees with 36.\\nClothing and other are at the top with 53 and 88.\\nSo, what I'm seeing here is that I am able to order by\\nthis ascending order by the number of employees.\\nBut let's suppose I want to see the top departments\\nin terms of count of employees.\\nWell, I can order by count and specify DESC for descending.\\n\\nAnd when I do that and I run\\nI see now the order is flipped.\\nSo, now the departments with the most employees\\nare at the top of the list and the departments\\nwith the fewest employees are at the bottom.\\nSo, that's how the descending works.\\nNow, you can also, if you really want to be explicit,\\nyou can say ASC for ascending.\\nAnd if you run that you get the default behavior.\\nSo, depending on what you want you can have ASC,\\nor you can just leave it out.\\nASC is the default.\\nNow, another thing you can do with ORDER BY is you can use\\na reference to the position in the list of selected columns.\\n\\nSo, for example, if you want to order by department\\nyou could say ORDER BY one and run that.\\nAnd that will list in alphabetical,\\nascending order by department name.\\nYou could also specify descending department name,\\nand now we should have toys at the top.\\nYep, toys and tools, sports and so on.\\nNow, if you want to reference count,\\nwhich is the second column in the list of selected columns,\\nwe can just change that to two.\\nAnd this will be in descending order.\\nAnd we can also make that ascending\\nby just running just two.\\n\\nSo, you can use this short notation.\\nThat's convenient, again, if you're doing like ad hoc stuff.\\nI really don't like to do that when it's code\\nthat I'm saving because somebody,\\nespecially if it's a complex query,\\nmight not notice an ORDER BY is relative to column order.\\nAnd they might add a column somewhere\\nin the SELECT statement up at the top.\\nSo, I typically prefer to be very explicit\\nabout the ORDER BY column.\\nSo, let's change this back to the department_name,\\nand let's make sure that runs correctly.\\n\\nYep, okay, good.\\nNow, so we've seen how we can reference a column in the row,\\nlike department_name, in the ORDER BY clause.\\nWe've also seen how we can use an aggregate like count\\nin the ORDER BY.\\nWell, now let's see how we can filter.\\nWell, let's say we know we can put department_name\\nin a WHERE clause.\\nSo, for example, we could have a WHERE clause here\\nand filter on some particular department name.\\nSo, we could filter where department_name,\\nWHERE cd.department_name is equal to,\\nand then we could put something like baby,\\nthere's a baby department.\\n\\nSo, let's run that.\\nAnd we see we're getting just the baby.\\nOkay, so that works, that's very similar to the ORDER BY.\\nBut now let's put a WHERE count(*) is greater than 50.\\nNow, I'll tell you now it's not going to work,\\nand I want to show you what kind of error messages we get.\\nWell, this doesn't work because we get an error,\\naggregate functions are not allowed in the WHERE clause.\\nWell, and that makes sense because let's think back\\nto what we use the WHERE clause for.\\nThe WHERE clause allows us to tell Postgres\\nwhich rows we want to include in our result set,\\nor whatever operation we're doing.\\n\\nSo, before we even have a count we have to decide\\nwhich rows are going to be included in the count.\\nSo, aggregates just logically don't make sense\\nin a WHERE clause.\\nBut that's okay because there's another clause\\nthat we have available to us called HAVING.\\nAnd HAVING is basically a clause which allows us\\nto specify an aggregate like the count\\nand specify some condition on that, like greater than 50.\\n\\nSo, now let's run this.\\nAnd what we see here is these are the departments\\nthat have greater than 50 employees.\\nAnd so, we use the HAVING clause anytime\\nwe want to basically filter a result set.\\nAnd in cases where that result set use is aggregating\\nand we're producing aggregates in the output,\\nand we want to use the results of the aggregation\\nas part of our criteria for deciding\\nwhat to include in our output.\\n\\nSo, those are some different ways\\nthat we can filter using aggregates.\\nSo, really the HAVING clause is really quite powerful\\nwhen you want to be able to filter\\nbased on the results of aggregation functions.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5241086\",\"duration\":463,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Subqueries for column values\",\"fileName\":\"5925685_en_US_04_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":525,\"solutionVideo\":false,\"editingNotes\":\"Cut 5:24-end of video\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH04 > 04_02.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to use correlated subqueries to compute column values.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13191773,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] One of the nice things\\nabout the way SQL is designed\\nis that it allows you\\nto have SELECT statements within SELECT statements,\\nwhich can be really useful\\nfor creating a kind of modularized logic\\nand allowing us to break down business logic\\ninto components which are easy to understand\\nand relatively easy to implement.\\nNow, in SQL, there are a few different places\\nin a SELECT statement\\nwhere you can use another SELECT statement.\\n\\nYou can use that in the list of columns\\nthat we're selecting,\\nwe can use it in a FROM clause,\\nand we can use sub-statements or subqueries\\nin where clauses as well.\\nSo we're going to take a look at\\nhow we can use a subquery\\nwithin our list of columns that we want to select.\\nSo for this, what I'd like to do\\nis I want to be able to list out employees\\nin each department, their salary,\\nand then also the average salary\\nof people in their department.\\n\\nSo within a single department, what is the average?\\nSo let's see how we might go about doing that.\\nSo the first thing I'm going to do is SELECT.\\nI know I'm going to be pulling from the employee table,\\nso I'm going to pull employee, e.\\nAnd let's go with last name.\\nAnd let's look for the salary, of course.\\nWe'll want that.\\nAnd let's also get the department ID.\\nAnd now what I want,\\nand I'll select this from data_sci.employees.\\n\\nAnd what I want now is basically I want to create something\\nthat's going to give me the department average.\\nAnd so for that, I'm going to create another SELECT statement,\\nand I'm going to select the average salary of something.\\nI'm building up the logic here incrementally.\\nSo I know I'm going to be working with the employees table.\\nSo there's going to be a from data_sci.employees.\\n\\nOkay, but wait a minute,\\ncan we reference the same table twice\\nwithin a single SELECT statement?\\nAnd the answer is yes.\\nBut we need to be careful about our aliases\\nbecause we need to be able to distinguish\\nwhich reference to employee table we're talking to.\\nAre we talking when we're doing something\\nlike averaging the salary here?\\nAre we using the employee table\\nthat we're referencing here in the subquery\\nor the outer query?\\nTo be able to distinguish between those,\\nwe need to have different aliases.\\nSo instead of using just the letter e\\nfor the employee table,\\nI'm going to use e1 for the outer table\\nand e2 for the inner table.\\n\\nAnd you can use any naming convention you like.\\nThis is one I typically use.\\nI just suggest you be consistent about it\\nbecause, again, it just helps\\nwith making code easier to read.\\nSo now, right now,\\nthis SELECT statement will return the average salary\\nfor employees across the entire company.\\nI want to limit where we're looking in this where clause\\nto just the employees in the same department\\nas the employee whose last name and salary and department ID\\nwe're currently looking at.\\n\\nSo this is basically,\\nthis set of data is going to change as we go row by row.\\nAnd the last name will change, the salary will change.\\nHowever, this SELECT statement,\\neach time we have a new row,\\nit will be doing a new calculation.\\nAnd it will be based on taking into account\\nthis employee's department ID.\\nSo one of the things I'll need to do\\nis change the alias here from e to e1.\\nSo this is always, I'm referring to,\\nwhen I'm talking about\\nlast name, salary, department ID here,\\nI'm referring to this table.\\n\\nWhen I'm calculating an average,\\nI want to take it from the data_sci.employee.\\nAnd alias to that is e2.\\nBut I only want to look at one department's worth of data.\\nSo I'm going to have a where clause,\\nand I'm going to have\\nwhere a department ID is equal to, what?\\nWell, I want this e2, this second table.\\nSo as I'm calculating the average,\\nI only want to look where the department ID\\nis equal to the department ID\\nof the person referenced in the e1 instance.\\n\\nSo e1.department_id.\\nSo let's take a look at what we're doing here.\\nWe are querying the employee table\\nfor last name, salary, department ID.\\nReally straightforward.\\nWe're calling or aliasing that employees table, e1.\\nSo each time we're walking through\\nthe list of each employee,\\nwe're doing that with a reference to the e1 table.\\nWe're also running a subquery.\\n\\nSo for each row that we encounter,\\nwe're going to run this subquery,\\nand we're going to calculate the average salary\\nfrom what happens to be the same table.\\nIt's the data_sci.employees table.\\nSo we're working with the table in two different ways,\\nhowever, we're synchronizing them\\nor we're coordinating them or correlating them\\nby making sure the department ID\\nof all the rows that we're averaging\\nis the same as the department ID\\nof the person we're collecting information about\\nin our outer query.\\n\\nSo let's go ahead and run this\\nand see what kind of results we get.\\nOh, I forgot to put round in for the average of salary.\\nSo let me just put that in real quick.\\nAnd we'll go to two decimal places.\\nAnd I'm going to rerun that.\\nAnd so what we're seeing here is,\\nso here's the department ID.\\nThere's Kelley, Carr...\\nKelley, Carr, Alexander.\\nThe salary is different,\\nbut Kelley is in a different department\\nfrom Carr and Alexander.\\n\\nCarr's, for example,\\nCarr, Alexander are in Department 1.\\nThey both have the same average salary.\\nKelley is in a different department.\\nKelley is in Department 6.\\nThey have a different value.\\nSo depending on which department you're in,\\nyou're going to have a different average.\\nSo let's just see.\\nHere, we're splitting,\\nHere is someone in Department 2.\\nLooks like they actually have the same salary.\\n\\nThat's just a coincidence because here's Department 3.\\nDepartment 4 has 103.\\nSo we can see different departments\\nhave different averages in most cases.\\nAnd, again, what we are doing here is we're using a subquery\\nto help us calculate a new column.\\nSo this is one way we can use subqueries\\nto build fairly complex logic.\\nNow, I will say this isn't the only way to calculate this,\\nbut using a subquery is often a good first way to do this,\\nand oftentimes it will be quite performant.\\n\\nThere may be times when there are\\nmore efficient ways to do things,\\nbut that's really more of an advanced topic.\\nHere we're focused on,\\nhow do we capture the logic\\nof a particular business problem,\\nin this case, comparing the salary of an individual\\nwith the average for that person's department?\\n\"},{\"urn\":\"urn:li:learningContentVideo:5240085\",\"duration\":177,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Subqueries in FROM clauses\",\"fileName\":\"5925685_en_US_04_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":229,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH04 > 04_03.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to use correlated subqueries in FROM clauses.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4831068,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We can use subqueries and FROM clauses,\\nand that's really useful when we have some complex logic\\nthat we want to build up\\nand apply to generate basically a set of rows\\nthat we then want to do another SELECT statement over.\\nSo let's look at a simple example,\\nlooking at the average salary of employees.\\nSo we'll start with SELECT,\\nand we're going to select the avg(salary).\\nAnd before I forget, I'm going to add the round function\\nand round this to two decimal places.\\n\\nAnd I'm going to select this from data_sci.employees.\\nLet's just run that to make sure I have that correct.\\nYes, okay, so we have the average salary.\\nWell, now let's assume I only want to get the average salary\\nof employees who earn over $100,000.\\nWell, instead of selecting\\nfrom the data_sci.employees table directly,\\nI'm going to build a subquery here\\nwhere I select all of the columns\\nfrom the data_sci.employees table,\\nand I'm going to specify a WHERE clause.\\n\\nAnd so I'm going to say WHERE\\nsalary > 100000.\\nNow, if I try to run this, let's go ahead and run this\\nand we'll see what happens,\\nwe see that we get a different value here.\\nSo, for example,\\nrather than averaging over all of the employees,\\nwe're averaging only over those\\nwho earn over $100,000.\\n\\nSo we're getting a different average here.\\nNow, you might think, \\\"Well, why do we have to do this\\nbecause couldn't we just use a WHERE clause?\\\"\\nAnd we could, this could easily be written as the following.\\nActually, I'll just copy this and then make a new version.\\nAnd we could select from data _sci\\nwhere salary > than 100000.\\n\\nSo these two queries are logically equivalent.\\nSo this is typically the way we would normally do it,\\nbut I wanted to just show this example\\nwhere we do have a subquery here\\nbecause you may have a lot more complex business logic\\nthat you want to apply as you build up\\nto generate your final result set.\\nSo sometimes when we have that complicated logic,\\nit makes sense and makes for a more readable query\\nif we isolate some of that logic into a subquery\\nin the FROM clause.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5231403\",\"duration\":162,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Subqueries in WHERE clauses\",\"fileName\":\"5925685_en_US_04_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":201,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH04 > 04_04.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to use subqueries in WHERE clauses.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4195789,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Using a subquery in a where clause\\ncan be helpful when we want our filtering conditions\\nin the where clause to be based on some property\\nthat we can extract or identify using a select statement.\\nSo let's take an example\\nwhere we want to find the department that has the employee\\nwho has the maximum salary.\\nNow it's possible there may be multiple employees\\nwith the max salary.\\nSo let's have a query that selects the department,\\nthe department ID from data_sci.employees.\\n\\nNow I want to select where\\nsomehow the maximum salary.\\nI want to find the maximum salary of everybody in the company.\\nWell, to do that I would use a select statement,\\nand I would write a select max salary\\nfrom data_sci.employees.\\nAnd that looks pretty straightforward.\\nYeah, that'll find the max salary.\\nNow the problem is we have, again, two references\\nto the employees table.\\nSo this is an inner query,\\nso I'm going to use the convention E followed by two for that.\\n\\nAnd I will alias the outer query as E one.\\nAnd what I want it to do here is,\\nnow what I'm doing is I'm in a where clause,\\nwell, where clauses work with Boolean conditions.\\nSo something has to evaluate to true or false.\\nAnd so I want this to evaluate to a true or false\\nbased on whether or not this is equal to\\nthe employee that we're looking at in the outer query.\\nWhat's their salary?\\nWell, if their salary is equal to the max, whatever that is,\\nI don't necessarily know exactly what the maximum salary is,\\nbut this select statement will tell me what it is.\\n\\nAnd if the current employee's salary is equal to that,\\nthen we're going to return that department ID.\\nSo let's go ahead and run this and see how this works.\\nAnd what we see is, ah, there's only one department,\\nso department 10 has at least one employee\\nwith a maximum salary equal to the maximum salary\\nof anyone in the company.\\nNow we could also do joins to look up things\\nlike the department name and other things,\\nbut this is just a, kind of a typical example\\nof how we use subqueries and where clauses.\\n\\nAnd the thing to remember about using a subquery\\nin the where clause is we always want it to be used\\nin a way that evaluates to a Boolean\\nbecause we need that kind of Boolean condition\\nto be a predicate for the where clause.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5239149\",\"duration\":447,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Using ROLLUP to create subtotals\",\"fileName\":\"5925685_en_US_04_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":499,\"solutionVideo\":false,\"editingNotes\":\"Cut 5:20 to end of video\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH04 > 04_05.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to use the ROLLUP aggregate to create subtotals. ROLLUP can combine individual row data and aggregate data in the same query result.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12647149,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] When we work with aggregates,\\nwe can use the aggregates, say, to get the totals,\\nmaybe the average or some sums\\nor some counts across an entire table.\\nOr we have seen, we can also use the group by expression\\nor the group by clause in the select statement\\nto apply those aggregates to subgroups within, say, a table.\\nSo for example, we could maybe group by a department name\\nto see the count of the number of employees\\nwithin each department.\\n\\nWell, SQL and Postgres actually has another operator\\nwe can use called roll up, which gives us a third way\\nof working with aggregates.\\nAnd roll up allows us to do sort of like sub aggregates\\nin a hierarchical fashion.\\nSo for example, in our data model,\\nwe have company regions,\\nand these regions are part of countries.\\nSo we could, for example, with roll up,\\nget our total number of employees by region\\nand then also get a total count by country\\nand then also a total count by the whole table of employees.\\n\\nSo that's what the roll up operator does.\\nSo let's take a look at that\\nand we'll start by looking at a way we've typically used,\\ngroup by with a single subgroup.\\nSo let's say I want to know the count\\nof employees by region.\\nSo I'm going to select, I wanted to work with regions,\\nso I know I'm going to work with the country regions table.\\nSo I'll use CR 'cause that's how I alias company regions.\\nAnd I want to know the country name.\\nAnd I also want to know the region name.\\n\\nAnd I'm interested, the metric I'm interested in,\\nor the aggregate I'm interested,\\nis the count of the number of employees.\\nWell, I know I'm going to have to work\\nwith the employees tables,\\nand I typically alias that with an E.\\nSo I'll count the number of rows in the employee table,\\nand I'm going to select this from\\nthe employees table in data sci.\\nSo again, data sci is the schema, employees is the table.\\nAnd I'm going to alias that as E.\\n\\nAnd I want to join this to the company regions table,\\nwhich is also in the data sci schema.\\nAnd that will be aliased as CR.\\nAnd then of course, I need to specify my on clause.\\nAnd the columns we're going to join on\\nare the employee's region ID\\nand the company region's ID.\\n\\nNow also, I want to group by the country name\\nand the region name.\\nAnd that looks correct.\\nLet's run that and see what we have here. Yep, okay.\\nSo what we see here is we have a total of seven rows,\\nand we have three rows are for regions in Canada,\\nQuebec, British Columbia, and Nova Scotia.\\nAnd then we have four regions in the United States,\\nnortheast, southwest, northwest, southeast.\\n\\nAnd we have a subtotal for each region.\\nNow, what I'd also like to know is, well, what if I want\\nto know, well, what are the totals in Canada\\nand the United States?\\nNow could write another query for this,\\nor I could use the roll up operator.\\nAnd we apply the roll up operator in the group by clause.\\nAnd it's really simple to use.\\nWe simply go into our group by clause,\\nadd the term roll up,\\nand then provide a list\\nof the columns we want to roll up by.\\n\\nAnd so what this specifies is basically a hierarchy.\\nThe top of the hierarchy is going to be country name\\nand then followed by country region.\\nSo we're going to group by country region\\nand do a account of employees.\\nThen we're going to group by country name\\nand do a count by country name.\\nNow let's run this.\\nThis is not going to be exactly what we want,\\nbut let's just run it and see.\\nAnd what we see here is we have some new\\nrows that were inserted.\\nFor example, here is a subtotal for USA,\\nhere is a subtotal for Canada.\\n\\nHere is a total for the entire set.\\nBut obviously this is not in the right order,\\nor at least it's in an order.\\nIt's not the order I would like.\\nSo I'm going to add an order by clause where we order by\\nthe country name and the region name,\\nand let's run that again.\\nLet's get things in an order that's a little more fit\\nfor what I'm looking for.\\nThere we go.\\nSo what we see now is, what the roll-up does,\\nis it does the standard kind of group by operation\\nthat we expect, but it also inserts additional rows.\\n\\nSo now we have a row with a subtotal\\nfor the number of employees in Canada.\\nSo we roll up from region into country.\\nWe do the same thing with USA.\\nWe roll up from the regions in the United States\\ninto a total for the United States.\\nAnd then we have a roll up for the entire data set,\\nfor the entire result set, which is the total\\nof all employees across all regions,\\nwhich means across all countries or both countries.\\nSo this is a really useful feature when you're generating\\ndata, say for a final report, maybe you're producing it\\nor you're exporting this data to a spreadsheet.\\n\\nA roll up can be really useful.\\nI will say a roll up is what's known\\nas a non-relational feature of SQL.\\nAnd what that means is it produces an output\\nthat doesn't necessarily fit well\\nin other relational operators.\\nSo for example, sometimes we use subqueries.\\nAnd subqueries, as long as they don't have a roll up\\nor other non-relational operator in there,\\nwill return a set of rows or a set of pupils\\nin relational database terminology.\\n\\nIt'll return a set of pupils\\nor a set of rows that we can then use as the source\\nfor other operations, like we could do selects over it.\\nThat's not the case when we're using a roll up,\\nor at least we could run into unexpected behaviors\\nbecause roll up is injecting things\\nthat we don't necessarily see\\nwhen we're doing relational operators.\\nSo again, roll ups are kind of a non-standard operator\\nin SQL in that it's injecting things\\nthat we don't normally have.\\n\\nSo typically we save the roll up\\nor other non-relational features\\nto the last process we're working on.\\nSo we might have a whole bunch of subqueries\\nand other operations we do.\\nBut the last thing we do, that can include a roll up,\\nassuming we're just generating a data set\\nthat we're going to then either use in a report or export.\\nBut you just want to be careful using roll ups\\nto make sure you don't use them, say inside a subquery,\\nbecause you probably get something unexpected in terms\\nof the result, your final results.\\n\\nBut that said, roll ups are super useful for cases like this\\nwhere we actually want to have these embedded subtotals.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5231402\",\"duration\":640,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Using CUBE to total across dimensions\",\"fileName\":\"5925685_en_US_04_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":735,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH04 > 04_06.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to use CUBE to create subtotals across dimensions. CUBE can be used for aggregates that would be difficult to implement with other SQL statements.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":22597578,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] When you're working\\nwith data sets like we have been,\\nwe see that there are a lot of different ways\\nof grouping data.\\nSo in our example dataset, we could group by country name\\nor region and department name.\\nAnd there are many different combinations\\nthat we can look at.\\nNow, there is another non-relational operator called Cube,\\nwhich is really useful for data exploration when we want\\nto just get a feel for,\\nwhat are the different possible combinations\\nof different dimensions.\\n\\nDimensions is another term for a column that we use\\nfor grouping data by and then applying aggregates.\\nSo let's think, for example, if we want\\nto see different combinations of country name, region name,\\ndepartment name, and count employees by that,\\nhow we might be able to do that\\nand how we could use this cube operator\\nto help us with that.\\nSo the first thing we're going to do is,\\nwrite a pretty sizable select statement\\nthat joins across three different tables, employees,\\ncompany regions, and company departments.\\n\\nSo I'm going to hold up on what columns I'm going to select\\nthat I'm going to select from,\\nand now I'm going to build my from clause.\\nSo I want to select from, of course, data sci,\\nemployees, and we're going to join.\\nSo we'll first join two company regions\\nand we'll join on employee region ID,\\nand we'll join that to company regions ID.\\n\\nAnd I see I forgot to alias that.\\nSo let's take care of that.\\nNow the results of that,\\nso we have employees table being joined\\nto the company regions.\\nThat's going to produce a set of rows.\\nAnd those rows, we can join\\nto the company department's table,\\nand let's alias that as CD.\\n\\nAnd we'll join on the employee's department ID\\nand join that to the company department's ID.\\nSo that gives us our join. And now let's put our columns in.\\nSo we're going to be pulling from all three of these columns,\\nand we want to pull in,\\nlet's start with the country, country name.\\nSo that'll come from company regions.\\nSo we'll pull in country name,\\nand then we're going to pull in the region name.\\n\\nAnd I would like the department name.\\nAnd then finally, we're going to do a count,\\nand we're going to count all the rows.\\nAnd we could use count star\\nor E dot star, makes it a little more explicit\\nthan I'm particularly interested in counting employees,\\nbut the results are the same.\\nSo we'll just go with count for E star.\\nAnd oh, we could go without a group by,\\nand we would see totals for the entire data set.\\n\\nBut instead, I want to group by.\\nAnd now I'm going to group by all of the columns\\nthat are not aggregates in our select statement.\\nSo I want to group by country name,\\nregion name, and department name.\\nSo I'm just going to copy and paste that in.\\nNow, I also want an order by statement,\\nso I can have this in the order that I'm interested in.\\nAnd the order by is going to be the same sequence\\nas the group by.\\n\\nSo this is a fairly long query.\\nSo let's take a look at this.\\nSo we're going to select country name, region name,\\ndepartment name, and we're also going to select the count\\nof employees in each.\\nSo for each country in each region,\\nand then within each department in each region,\\nwe're going to get the count of employees.\\nAnd we're getting that\\nbecause we're joining the employees table\\nto the company region's table on the region ID\\nfrom the employee table.\\nAnd we're taking the results of that\\nand doing another join to company departments.\\n\\nAnd we're using department ID from employees for that.\\nSo that makes sense.\\nAnd then we're simply grouping by\\nand ordering by country name, region name, department name.\\nSo, all right, so everything looks good.\\nEverything has an alias, and I think my commas are correct.\\nSo let's give it a try and see. Nope, have a syntax error.\\nI forgot a comma. There we go.\\nNow we'll run it.\\nAnd what we see here is we see\\nthat we're ordering by country names.\\nSo we have Canada first and then USA,\\nand then within Canada, we have a list of region names.\\n\\nAnd so for example, we have British Columbia,\\nfollowed by Nova Scotia and Quebec and Canada,\\nand then Northeast, Northwest in the US,\\nfollowed by Southeast and Southwest.\\nAnd then for each of those regions, we have each department.\\nSo we start with automotive, baby, beauty,\\nclothing, et cetera.\\nAnd then around here,\\nwhen we're shifting from British Columbia to Nova Scotia,\\nwe're starting again with our list\\nof department names, like automotive, baby, beauty.\\nAnd if we scroll down past Nova Scotia to Quebec,\\nwe see we restart again.\\n\\nSo what we have here is a grouping where we're grouping\\nby a country name, region name, and department name,\\nand we're getting a count for each.\\nWell, what if we wanted to look at different combinations?\\nLike for example, we might want to know,\\nhow many employees are there in Canada, in British Columbia,\\nacross all departments, or in British Columbia,\\nall across regions without regard to country?\\nWell, in this case, region name,\\nthere's a one-to-one mapping\\nfrom country name to region name.\\n\\nBut there might be cases where we're grouping\\nby things that are independent.\\nSo if we wanted to look at, say, all possible combinations\\nof these three dimensions or country name,\\nregion name, department name,\\nwe could use an operator called Cube.\\nAnd the Cube is an operator we apply\\nin the group by clause,\\nand it's very similar to Rollup\\nin that we simply ask Postgres\\nto apply the Cube operator to this grouping structure.\\n\\nSo I'm just going to run this.\\nI'm going to run this new command,\\nand let's see what we get.\\nWell, at first, it looks very similar\\nto what we already have seen.\\nWe see Canada, British Columbia,\\nand a list of department names starting with automotive,\\nbaby, beauty, et cetera.\\nWell, let's scroll down a little bit.\\nWe come down here to the end of British Columbia.\\nSo at the end, toys is the last department.\\nWe have injected here a new row, which is Canada,\\nBritish Columbia, and then a Rollup\\nfor all of the departments.\\n\\nSo again, it's kind of like Rollup.\\nAnd we see if we keep scrolling down\\nand we are looking at the grouping for Nova Scotia\\nand all the departments,\\nwe see also there's a Rollup for Nova Scotia.\\nOkay, so this all makes sense.\\nFeels very similar to Rollup.\\nSimilarly for Quebec, we have a Rollup for that.\\nAnd then all of a sudden,\\nwe've got a bunch of nulls in the region name.\\nAnd what we're doing here is,\\nwe're basically looking at a combination of for country name\\nand department name, what's the Rollup.\\n\\nSo this is giving us a combination of country name\\nand department name without regard to regions.\\nSo we're rolling up across regions,\\nbut it's as if we're grouping\\nby country name and department name.\\nSo we're getting a total of all the automotive employees\\nin Canada, for example, in this\\nthat I have highlighted here,\\nwe're getting a list of all of the employees\\nin Canada that work in clothing departments here.\\nSo what we see here is,\\nso that's an example of what Cube does.\\n\\nIt's looking at different possible combinations\\nof our three dimension columns.\\nSo let's keep scrolling down to see what else we have.\\nWell, we also have a total for Canada without regard\\nfor region name or department name,\\nor just in a sense,\\nlooking at just the combination of just country name.\\nSo this is like with Rollup.\\nSo we have a total of employees across Canada without regard\\nto region or department name.\\nAnd then we transition to the United States\\nwhere we see Northeast,\\nand we see each individual department.\\n\\nAnd if we keep scrolling down,\\neventually we'll see a total for Northeast,\\nand then we switch to Northwest,\\nand then we have a total for Northwest\\nafter each individual department.\\nAnd so we're seeing the same pattern again.\\nHere, we have it with Southeast,\\nand then we see the total for Southwest.\\nAnd now we see for US, USA,\\nall of the employees in the automotive department in the US,\\nthe number of employees in the baby department\\nin the US and so on.\\n\\nAnd then we have another set of combination.\\nThis is a set of region names and department names.\\nSo what we're looking at is,\\nall possible combinations of our dimensions.\\nAnd what we see here is, again, we have total,\\nhere's the total for just British Columbia.\\nSo all the employees in British Columbia here is a total\\nfor all the employees in Northeast\\nand all the employees in Northwest.\\nAnd if we keep scrolling down,\\nwe're going to keep seeing this pattern.\\nAnd at the end, we're going to see a total\\nfor all the employees in automotive only.\\n\\nSo we're getting totals for all\\nof the different ways of breaking this down.\\nAnd finally, at the end, we'll get a total for the number\\nof employees across all of the countries,\\nregions, and departments.\\nSo Cube is really useful in data exploration if you want\\nto see these different possible combinations.\\nNow, like Rollup, Cube is not a relational operator.\\nSo if you start applying, you know, relational operators\\nto the results of a select statement that uses Cube,\\nyou're almost certainly going to get unexpected\\nor surprising results.\\n\\nSo Cube, like Rollup, is really good\\nfor things like data exploration,\\nor if you're generating reports\\nand you want to produce all these possible combinations\\nand people to analyze them, you know, in some other way\\nor share the data.\\nCube is a great way\\nto help you reduce the number of select statements you have\\nto write to get the same amount of data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5234306\",\"duration\":198,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Using Top-N queries to find top results\",\"fileName\":\"5925685_en_US_04_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":231,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH04 > 04_07.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to use the Top-N clause to find top results in a data set. Top-N queries are often used to identify the most important entities in a query result.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6129554,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now if we want to find,\\nsay the top 10 employees by salary\\nor some other, you know, top by count of employees,\\nthere are a couple, actually a few different ways\\nof doing it within Postgres.\\nAnd I just want to show you one of the ways.\\nOne is, so let's say we're interested in looking at\\nthe top employees by salary.\\nSo we can write a select statement.\\nSelect, let's say we'll select all the columns\\nfrom employees.\\n\\nWe want to know everything about these employees\\nfrom data_sci.employees.\\nNow let's just alias that just because it's a good habit.\\nAnd then I'm going to,\\nnow one of the things I can do is simply order by the salary\\nand I can add the descending keyword.\\nSo this will return all the employees in the order,\\nand I can just simply count one, two, three, four.\\n\\nThere, and I have my top 10 employees.\\nWell, that's one way to do it.\\nNow sometimes if it's like a lot of data like this,\\nI really don't want to see the other data.\\nI can use an operator in Postgres\\nthat is in many different relational database systems,\\nbut is not part of standard SQL.\\nIt's called the Limit Operator.\\nAnd I can say Limit 10,\\nand that will run the query that I have above here.\\nSo that will run all of this.\\nIt'll execute this entire thing,\\nget the entire results sets back,\\nand then trim the results that it returns to the UI\\nor to the user interface down to the first 10 rows.\\n\\nSo let's run that.\\nSo that's kind of a quick way to get the top 10.\\nNow limit is not a standard SQL operator,\\nbut there is an operator called Fetch First.\\nFetch first 10 rows only,\\nthat you probably don't see this too much,\\nbut in case you're working with a version of SQL\\nthat doesn't support the limit operator,\\nyou can write this as well.\\n\\nNow I will say one thing to keep in mind\\nwhen using an order by and a Limit\\nis that typically the query plan builder,\\nwhich is the part of the database\\nthat kind of builds the plan or the procedural steps\\nthat go about executing the query,\\nthat query Plan Builder will build the steps\\nto bring back the entire data set without regards\\nto whatever the limit number is.\\nLike limit 10, limit 100.\\nSo if you have a very large table, you might, for example,\\nbring back 1,000 rows or 10,000 rows or 100,000 rows.\\n\\nSo the database is going to do all the work\\nto get say 10,000 rows, and then take that result set\\nand chop off everything but the first 10 rows.\\nSo there could potentially be a lot of work done\\nthat's not necessary.\\nSo there are some alternative ways\\nthat we're going to discuss later,\\nbut this is just a quick way.\\nIf you're just, again, doing data exploration,\\nyou want a quick look at something, then order by\\nfollowed by limit or fetch first is a really useful way\\nto kind of get a quick look at a top N or a top K results\\nin a data set.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5240084\",\"duration\":21,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Filter and aggregate a data set\",\"fileName\":\"5925685_en_US_04_08_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":22,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Test your ability to filter subgroups of data. This validates your understanding of filtering and aggregation.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":633792,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] For this challenge,\\nI would like you to write a query\\nthat returns the count of employees in departments\\nwhere the total salary paid in that department\\nis greater than $5 million.\\nNow, the results should be ordered\\nfrom highest to lowest total salary.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5241085\",\"duration\":52,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Filter and aggregate a data set\",\"fileName\":\"5925685_en_US_04_09_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":56,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH04 > 04_09.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1600276,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat synth music)\\n- Okay, here's the solution to our challenge.\\nTo return the departments\\nin which the total salary is over $5 million,\\nwe select a department ID\\nand we sum the salary\\nfrom the employees table in the data_sci schema.\\nWe group by the department ID,\\nand then we filter using a having clause\\n'cause we want to filter based on a property of something\\nthat we're aggregating,\\nin this case the sum of salary.\\n\\nSo we can have it in the where clause.\\nWe need to use a having clause.\\nand we want this in from greatest value down to lower value.\\nSo we're going to order by the sum of salary,\\nand we're going to use the descending keyword\\nto ensure that we filter it with the highest value first,\\nand then go in descending order from there.\\n\"}],\"name\":\"4. Filtering and Aggregation\",\"size\":79021307,\"urn\":\"urn:li:learningContentChapter:5232360\"},{\"duration\":1426,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5240083\",\"duration\":290,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introducing window functions\",\"fileName\":\"5925685_en_US_05_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"editingNotes\":\"cut from 5:32 to end\",\"solutionVideo\":false,\"rawDurationSeconds\":460,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH05 > 05_01.sql\",\"challengeVideo\":false,\"graphicsIncluded\":false,\"hasSlides\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn basic window functions. These are commonly used functions when ordering subsets of data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8815421,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now, we have seen\\nthat we can derive information about groups of rows\\nfor using subqueries.\\nAnd one of the ways that we use subqueries is, for example,\\nto find out information from a group of rows,\\nsuch as maybe all the rows for employees\\nin a particular department.\\nAnd let's say we want to find, well,\\nwhat's the minimum salary in that department?\\nWell, we could use a subquery for that.\\nWell, SQL also provides another mechanism\\nfor doing that kind of calculation,\\nand it's something known as window functions.\\n\\nAnd the term window you can think of as a metaphor\\nfor like a window in a house.\\nAnd if you look out the window,\\nyou can see a certain amount of landscape.\\nNow, if you move the window a little bit to the left,\\na little bit to the right, you can see a different set.\\nIf you move the window maybe all the way to the other end\\nof the house, you'll see something completely different.\\nWell, similarly with window functions, you can think\\nof a window function as a way of grouping a set of rows,\\ndoing an operation on that row,\\nand then returning some bit of information.\\n\\nAnd so oftentimes I feel like the best way\\nto learn about window functions\\nand really grok what they do is to actually try one.\\nSo let's take a look at the employees table.\\nSo let's write a SELECT statement.\\nAnd we're going to be selecting from\\nthe data_sci.employees table.\\nAnd now I want to get some information based on departments.\\nSo the first thing I want to include is a department_id.\\nAnd I also want to look at an individual level.\\n\\nSo I want the last_name of our employees\\nand the salary.\\nOkay, this looks pretty straightforward\\nand I'm just going to run it\\nYep. Okay, great.\\nNow, what I really want to know\\nis what is the minimum salary in each department?\\nAnd I want to include that with the information\\nthat I provide when I'm returning information\\nabout employees in different departments.\\nWell, for that, I can use a function called first_value.\\n\\nAnd first_value is a kind of window function.\\nAnd we're going to think of it as a window\\nover the salary column.\\nAnd specifically, we want the first_value of the salary,\\nand we want to work over a grouping.\\nAnd the group that we want to work on\\nis specified by PARTITION BY,\\nand that's the term that SQL uses for specifying the group\\nthat we want to operate on.\\nAnd what I want to operate on is a group\\nthat is defined by the department_id.\\n\\nLet's run this query, which is not complete yet.\\nIt's a building block towards something.\\nSo what we see here is we have a list of employees\\nthat's grouped by departments.\\nSo in the first column, we have all of department ones\\nfor now, oh, and there's some department twos,\\nand I'm going to scroll down\\nand there's some department threes.\\nSo we're organizing these by department_id.\\nAnd we see\\nthat we have the salary in the third column,\\nand that changes.\\nAnd then we see we have a first_value column.\\n\\nWell, the first_value column in the first row is 101768.\\nSo that's the first_value of salary\\nthat we encounter in this department.\\nNow, we'll see that the next person down\\nhas a salary of 144.\\nSo this first_value is not the maximum salary\\nbecause we have some salaries with a higher salary.\\nAnd it's not the minimum salary\\nbecause we see that there are some\\nthat have a lower salary.\\nBut what it is it's the first_value\\nthat we encounter in the salary column\\nin this grouping by department.\\n\\nSo let's jump down to say, oh, here we are.\\nWe're at some group three. We see the value here.\\nAnd first_value was 85227,\\nand group four,\\nthe first_value is 83144.\\nSo what we have here is the first_value\\nthat we encounter in this list of employees by department.\\nWell, if we don't specify in an ORDER BY clause,\\nwhen we return a set of rows in SQL,\\nit is returned in some arbitrary order.\\n\\nWe don't really know what it is.\\nIf we want to know what the maximum value\\nor the minimum value of the salary is,\\nwe want to be sure to include an ORDER BY clause.\\nAnd in this case, we want to ORDER BY salary.\\nAnd actually, what I'm going to do\\nis I'm going to shift this a little bit\\nand move this down to the next row\\nso we have a little more space\\nto look at our results sets here.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5232354\",\"duration\":339,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"NTH_VALUE and NTH_TILE\",\"fileName\":\"5925685_en_US_05_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":554,\"solutionVideo\":false,\"editingNotes\":\"cut 3:26-3:37.  cut 3:52-4:06  cut 6:14-end.  Insert pk1 at 6:14.\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH05 > 05_02.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to select and group data with a windowed subset of data. These functions provide the means to select rows within a window.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10186044,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now when we're using windowing operations,\\nwe can use some of the aggregate functions\\nthat we've seen before, like sum or average.\\nAnd then we can also use ones\\nthat are really more specialized toward\\nthe windowing operation itself.\\nAnd two that we'll look at are called ntile and nth value.\\nBut before we look at those specialized ones,\\nlet's see how we can use our more commonly used\\naggregate functions like average with windowing operations.\\nSo to do that, I want to work with the employees table\\nand look at salaries by department.\\n\\nSo I'm going to create a select statement,\\nand I'm going to select department ID and how about salary,\\nand I also want to know the average salary.\\nAnd now the average salary, I want this average.\\nI don't want the average of everyone\\nin the company or anything like that.\\nI want the average over a particular window.\\nAnd that window that I'm interested in is defined\\nby the expression partitioned by\\nand then department ID.\\n\\nSo I want to basically organize this window\\naround department ID\\nand I want to find the average salary in each department.\\nAnd then after that, I can simply finish building my\\nstandard select statement from data_sci.employees.\\nAnd I think that looks correct, let's give that a run.\\nOkay, and so what we see here\\nis that we have our department ID, our salary,\\nand then we have an average.\\n\\nNow you'll notice here,\\nour average has a lot of decimal places.\\nSo I forgot to put the round in, so let's put round.\\nNow you might think, oh yeah, we'll just do round\\nof average salary, two.\\nWell, this won't work.\\nBut let's see what happens when we do try and run this.\\nAnd what we get is an error and we get an error\\nbecause over is specified,\\nand round is not a window function or an aggregate function.\\nSo basically what we're saying is we want to apply round,\\nwhich basically says, take the results of this operation,\\nand then in this case, round the value.\\n\\nBut we're not rounding the average value\\nof each individual salary.\\nInstead, what we're doing is we want to round\\nthe average salary that we calculate\\nover this specification over this window.\\nSo we need to include the entire clause.\\nThis is the first argument to the round function.\\nThe second argument is the number two.\\nSo now if we run this, we're going to get what I expect,\\nwhich is to have the result, the round value rounded\\nto two decimal places.\\n\\nSo that's an example of how we can use an average aggregate.\\nNow, another aggregate that we can use\\nthat's more specialized is called ntile.\\nAnd ntile is a function which allows us\\nto break our group, like our group,\\nsay department one or department two,\\nall of the rows that fit in to say a particular department.\\nWe can chunk them into equal sizes.\\nSo for example, if we're interested in working\\nwith four groups or quartiles,\\nwe could specify an ntile of four.\\n\\nIf we're interested in working with deciles\\nor 10 groups, 10 subgroups, we could specify 10.\\nSo let's work with four subgroups\\nover a particular window.\\nAnd that window is defined by the partition\\nover the department ID.\\nNow remember, if I just specify this,\\nthen I will get rose returned in an arbitrary order\\n'cause I don't have any kind of order by specification.\\n\\nSo I want to make sure I order by\\nwhat I'm particularly interested in.\\nIn this case, I'm interested in salary,\\nand I want my first quartile to have the biggest salaries.\\nSo I'm going to make this in descending order,\\nand I'm going to give this an alias.\\nI'm going to call this quartile.\\nSo it's obvious what that is.\\nNow I'm going to run it,\\nand let's see what I did wrong, we have...\\n\\nMistyped the partition by phrase\\nso let's try that one more time, there we go.\\nNow what we see here is that we have department one,\\nwhich goes from here.\\nIt scrolls down well down past the first page down to here.\\nAnd actually there's the last one.\\nAnd what we see here is that we have department one\\nand now we have a salary.\\nNow this salary 146,167 is in the first quartile,\\nand so is the next one, 144,724.\\n\\nAnd in fact,\\nthe smallest salary in the first quartile is 127,521.\\nNow, we'll notice after that we start the second quartile.\\nAnd the second quartile goes all the way down\\nto 101,768, after which we're into the third quartile,\\nwhich is bounded by 101,006, and that goes to 71,367.\\nAnd after that, we're into the fourth quartile\\nand the fourth quartile goes down to 42,602.\\n\\nAnd after that, we're switching departments.\\nNow we're in our second department.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5238124\",\"duration\":439,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"RANK, LEAD, and LAG\",\"fileName\":\"5925685_en_US_05_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":505,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH05 > 05_03.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to select data based on offset of the current row. These functions provide additional ways to select rows in a window and to add an integer attribute based on the rows position in a window.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14057726,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Window functions\\ncombined with order by statements\\nreally enable some interesting kind\\nof additional work that we can do with sets of rows.\\nSo for example, if we are able to say\\ngroup employees within a department,\\nwe can start doing things like ranking them\\nand actually looking maybe ahead or behind\\na particular employee relative to their position in a list.\\nSo let's look at three functions\\nthat will help us do those kinds of things.\\nThere's the rank function, the lead and the lag function.\\n\\nSo we're going to work with salaries again.\\nSo I'm going to be selecting,\\nlet's pull the last name.\\nActually, let's start with department_id,\\nlet's make sure we have that.\\nAnd last_name\\nand salary.\\nAnd now we're going to pull this\\nfrom data_sci.employees.\\nAnd I'm going to want to put a windowing function in here.\\n\\nAnd the function I want to use is basically,\\nI want to know what's the order like from one to a hundred\\nor whatever the number of employees we have?\\nI want to rank them from one\\nto whatever the number of employees we have.\\nWell, the function to do that is called rank.\\nAnd that assigns a rank,\\nand we can assign that over some window\\nthat we want to look at.\\nNow if we want to do the entire list\\nof all of the employees,\\nthen we can simply do this over and then specify.\\n\\nWe don't have to specify a partition by,\\nwe can specify order by salary\\ndescending if we want to go from top to bottom.\\nSo let's take a look at that.\\nSo this is a little bit different\\n'cause we're not breaking it up into subgroups.\\nI just want to assign a rank\\nfor every employee in the company.\\nAnd what we see here is that we have a rank\\nthat is assigned in the first row.\\nSomebody in department 12 named Stanley\\nhas the highest salary at 149,929\\nand then it goes down from there.\\nAnd you'll notice the rank is just a count.\\n\\nWe're going from count from one to two,\\nand it continues all the way up\\nuntil we get to the lowest value\\nor the lowest valued salary,\\nwhich would have the highest valued rank.\\nSo that's what the rank function does.\\nNow we can also rank based on salary\\nand group by a department.\\nAnd to do that we would simply add PARTITION BY\\nand we would specify PARTITION BY\\ndepartment_id.\\nAnd let me just move that to a separate line.\\n\\nSo PARTITION BY department_id and then order by salary.\\nNow let's run that.\\nAnd what we see here is we have a ranking-\\nHere is the highest paid person\\nis Sanchez in department one.\\nAnd that goes up to,\\nhow many people do we have in this department?\\n46, we have 46 people, and then we switch over\\nand there's department two,\\nwhich seems to be mirroring department one,\\nso let's go down to department three.\\nHere we go, here's three starting here.\\n\\nDepartment three goes from one to\\n45, and then finally department four.\\nIf we look there, that goes to a rank of up to 53.\\nSo we can use the rank function\\nto assign an ordinal value,\\na value based on the order\\nin a particular list that we're generating\\nor a particular partition that we're working with.\\nNow, another function that's useful\\nwhen we're working with groups\\nis the lead then the lag functions.\\n\\nSo let's switch this up\\nand let's change rank to\\nlead and we'll take the lead of a column.\\nIn this case, we'll look at the lead of salary.\\nLead basically looks ahead\\nand allows us to look at rows\\nrelative to the one we're currently working with.\\nSo for example, when we partition\\nthese employees by department\\nand we order by sales and descending,\\nwe see that the first person in department one\\nthat's listed is Sanchez and has a salary of 146,167.\\n\\nWell, when we look at the lead,\\nbasically lead says go one row ahead\\nand look up that salary value, in this case 144,724,\\nand copy that value back or print that value\\nor return that value as part of the row before it.\\nSo when we're looking at a row,\\nwe see the salary in that row\\nplus the salary in the row ahead.\\nSo if you wanted to do something\\nlike calculate the difference between an employee\\nand the next highest paid employee in the department,\\nyou could use the lead to go basically\\npeek ahead at the next row, grab that value,\\nand then do the calculation.\\n\\nNow lead by default looks one ahead,\\nbut we could also look, for example,\\nat two ahead if we wanted to do that.\\nSo let's run a lead with two.\\nAnd what we see now is we still have Sanchez at 146,167,\\nbut now the value in the lead column is 141,501,\\nwhich is where we get from row three.\\nSo we look two ahead to get this value and bring it back.\\nSo you can look ahead using lead\\nat some particular column.\\n\\nNow there's also a lag function,\\nwhich is a compliment which looks behind.\\nSo let's run that, and what we see is,\\nwell, we don't have two behind.\\nWe're at the first one or the first second.\\nWe can't look back two, we don't have like negative two,\\nbut as soon as we hit the third value,\\nthen we can go back and look at\\nwhat that lagging second row is.\\nIn this case, the value in the salary is 146,167.\\nSo we're copying that down and we're bringing that\\nand making that available in this row, in the third row.\\n\\nSo lag and lead can both take off sets,\\nbut they default to an offset of one.\\nSo let's just verify that.\\nAnd so what we see here, yeah,\\nso now when we're looking at the second employee\\nand we look at the lag,\\nwe don't specify a specific value.\\nThe lag defaults to one,\\nand so we're lagging or looking back one row.\\nSo what we can do, again,\\nif we want to assign a rank value,\\nan ordinal value across an entire partition\\nand then restart at the next partition,\\nsay we're grouping by departments,\\nwe can use the rank function.\\n\\nLead and lag are useful for looking ahead\\nand looking behind within that sort of\\nordered windowing of our subgroup.\\nIn this case, within our ordered\\ngrouping within a department.\\nSo lead and lag can help us look up\\na value either one row ahead, two rows behind,\\ndepending on what our criteria is.\\nSo rank, lead, and lag\\nare some additional windowing specific functions\\nthat can be really useful\\nwhen we're working with partitioning by different types\\nof ordering columns like department_id.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5234305\",\"duration\":588,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Width_buckets and CUME_DIST\",\"fileName\":\"5925685_en_US_05_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":787,\"solutionVideo\":false,\"editingNotes\":\"Cut after 10:31\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH05 > 05_04.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":18665784,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] As we're wrapping up our discussion\\nof windowing functions,\\nI want to point out two more functions that may be useful.\\nOne is called the width_buckets.\\nAny other is called cumulative distribution\\nor cume_dist for short.\\nThe first one, the width_bucket,\\nis kind of similar to ntile\\nin that it allows us to break up say,\\na group or a window into subsections.\\nBut rather than specifying a particular number,\\nlike within tile, specifying four if we want quartiles\\nor 10 if we want deciles, with a window bucket function,\\nwe specify a range of like what's the maximum value\\nwe want to work with and the minimum value,\\nand then how many different buckets\\nwe want to put those values in.\\n\\nSo let's take a look in an example of the width_bucket\\nand we're going to select from our employee table\\nand we will SELECT, let's start with department name,\\nexcuse me, not department name, department_id,\\nand last_name.\\nThat's where the name comes in, as well as salary.\\nAnd now I want to use the width_bucket function\\nand I want to apply this width_bucket to the salary column.\\n\\nAnd I expect the salaries\\nto be in the range of zero to 150,000.\\nAnd I would like to have\\nthat broken into 10 different buckets.\\nAnd I'm going to alias this with the term bucket.\\nAnd this is going to be from\\nthe data_sci.employees table.\\nAnd quick look, I think that looks right.\\n\\nSo let's see what the results look like\\n'cause that helps us understand\\nwhat this width_bucket is actually calculating.\\nSo what we're seeing here is that we have,\\nof course, the department_id a last_name, a salary.\\nWe don't have any order by, so this is arbitrary order.\\nAnd what we see here is this person\\nwith salary 67,470 falls into bucket five.\\nSomeone with 144,724. So a higher end of the salary range,\\nthey end up in bucket 10.\\n\\nSo the higher salaries end up in higher buckets.\\nAnd let's see if we have some closer or lower value buckets.\\nHere's one, a person who has a salary\\nof 42,602 falls in bucket three.\\nSo what we see here is basically, an ordinal value\\nthat's assigned and we can design what the range,\\nhow many different buckets\\nwe want to push these values into.\\nAgain, it probably feels like ntile.\\nAnd it is similar to ntile.\\nThere are some differences though.\\n\\nWith width_bucket, we have to specify a min and a max\\nas well as the number of buckets we want\\nthrough the number of tiles we want.\\nAnother difference is that width_bucket,\\nif there are numbers outside of those range,\\nso if we had like a negative salary\\nor maybe a salary that was greater than 150,000,\\nthose would fall into a couple of outlier buckets.\\nOne for less than the min and one for greater than the min.\\nSo that's a kind of a catchall.\\nAnd that's useful if you expect things\\nto be in a particular range.\\nFor example, if you're doing data exploration\\nand you're looking for outliers,\\ndoing width_bucket is a great way to kind of get a sense\\nof the distribution of the data,\\nlike how many values fall into different buckets.\\n\\nBut also quickly identify real distinctive outliers\\nif they fall outside your min or max range.\\nSo that's width_bucket.\\nThat's kind of handy for some use cases.\\nNow another thing that can be useful\\nis the cumulative distribution or the cume_dist function.\\nAnd let's look at that.\\nWe're going to use department and last_name and salary\\nand now we're going to change this specification.\\nSo what we're going to use is the cume_dist function.\\n\\nAnd that is like rank, it is parameter list.\\nWe don't specify parameter.\\nThis is simply going to give,\\nin some ways it's like a ranking,\\nbut it's not an integer ranking like one through N,\\nwhere N is the number of values in a list.\\nThe number of values in a list that we're working with.\\nCumulative dist is about a percentage.\\nSo if we took some value like the salary\\nand we added it all up,\\nand we add a total value, we can think\\nof cumulative distribution going from zero to one\\nwhere the first salary contributes some amount,\\nit might give you us maybe 0.001\\nof the total cumulative value.\\n\\nThen if we see two employees or three employees,\\nwe're going to see our cumulative distribution increase\\nbecause each of those employees is contributing their salary\\nor adding their salary to the total\\nof the cumulative distribution.\\nSo if you want to have a running total and know what percent\\nof the total we already have completed,\\ncumulative dist is a good function to work with.\\nNow with cumulative dist,\\nobviously, we're almost certainly going to want to use an over\\nand then order by something.\\nSo if we want to order by, let's say we want to work\\nwith all the salaries across the company.\\n\\nSo we don't need a partition by, we just need an order by.\\nOrder by and specify salary.\\nI want to order by salary and let's go with descending.\\nAnd with that, let's just see\\nwhat this returns at this point.\\nSo what we see here\\nis that we have this cumulative distribution\\nand we see a very small value here .00095.\\nSo that's the contribution of this first salary, the 149.\\n\\nAnd then we have another salary.\\nWe sort of come close to doubling\\nour cumulative distribution\\nand then getting close to three times\\n'cause all of these are very similar.\\nSo we're increasing the cumulative distribution.\\nAnd what you'll notice here\\nis the cumulative distribution keeps increasing.\\nAnd if we go toward the bottom of the list,\\nif we go down here, now you see we're in the 98%, 99.\\nFinally, after the last person salary\\nis counted, we get a one.\\nSo that's an example\\nof how we can use cumulative distribution.\\n\\nNow this is also an opportunity\\nto talk about something else as well.\\nAnd this is going to come up if we...\\nI'm going to close this actually.\\nIf we try and use the round function.\\nSo typically, you know,\\nwhenever we have like those large precision, I like to round\\nto two values just to make it a little more readable.\\nNow in our case, we might want to round to like four or five\\nbecause those small values early on.\\nSo let's use the round function.\\nAnd now we've learned from previous work with using round\\nand window functions, that we want the round operator\\nto apply to the full clause that we use\\nto specify the window and the operation on that window.\\n\\nSo let's say we went around this to five decimal places.\\nNow let's run this and see what happens.\\nWhat we get here is an error message.\\nAnd let's see.\\nIt says the function round double precision\\ninteger does not exist.\\nWell, there's a big clue in here about what's going on.\\nSo we know, so the function round, so there's round\\nand then it's expecting this first argument\\nto be a double precision.\\nAnd then the second argument to be an integer.\\nWell, five is definitely an integer.\\n\\nSo this must mean that dist, cume_dist\\nis not a double precision.\\nSo now at this point,\\nI typically would jump to the documentation.\\nI have an understanding of what's going on,\\nbut I want to verify.\\nSo I've happened to just go out\\nand look up Postgres documentation for the round function.\\nAnd if we scroll down\\nand we see here, ah, there are two signatures\\nor two sort of patterns\\nor two sets of arguments we can pass into round.\\n\\nWe can have a single value, which is either a dp,\\nthat stands for double precision, or a numeric.\\nOr we can pass in one value, which is a numeric type,\\nand then one which is an integer.\\nSo this is the value the round that we're using.\\nWe're trying to round to S decimal places.\\nYeah, we went a round to five decimal.\\nThis is exactly what we're trying to do.\\nSo what this tells me is the fact that we got the error,\\nit's telling me the cume_dist is not returning\\na numeric value.\\n\\nWell, in fact, if we look up cumulative dist,\\nit returns a double precision.\\nSo what we want to do is convert\\nthat double precision into a numeric value.\\nNow, the way we do that is we use an operation in Postgres\\nknown as casting.\\nAnd what I want to do is I want to demarcate\\nthe clause that we're interested.\\n\\nSo this whole cumulative distribution\\nover order by salary descending, I want to convert that\\nfrom a double precision into a numeric value.\\nNow, one way we can do that in Postgres\\nis to use the double colon notification.\\nSo that's tells Postgres, \\\"Hey,\\nwe're going to compute this value.\\nAnd then before we do anything else with it,\\nwe're going to convert it to, in this case, a numeric value.\\\"\\nAnd now let's run this and see what we got.\\n\\nAnd now what we see is, ah,\\nnow we don't get that error message.\\nSo we can see that we are getting\\nthe cumulative distribution\\nand the round function is working correctly.\\nAnd that's because we figured out using the documentation\\nthat we needed to convert this.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5232353\",\"duration\":19,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Segment a data set using window functions\",\"fileName\":\"5925685_en_US_05_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":19,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to work with subsets of data in a query result set. Ordering and grouping data is commonly done in SQL reporting.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":538027,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] In this challenge,\\nI would like you to write a query\\nto return department_id, last_name, salary,\\nand the sum of all salaries in a department.\\nDo not use a subquery in the solution to this challenge.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5241084\",\"duration\":41,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Segment a data set using window functions\",\"fileName\":\"5925685_en_US_05_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":42,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1381676,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] The solution to this challenge is as follows.\\nWe write a SELECT statement\\nwhere we include the department_id,\\nthe last_name, and the salary.\\nAnd then we specify a window function\\nusing the sum aggregate,\\nand we're going to sum salary.\\nAnd then we sum over the partitioning by department_id.\\nAnd of course, we select this\\nfrom the data_sci.employees table.\\nAnd we see over here on the right,\\nthe results are as we would expect,\\nwhere we are seeing the sum over partition\\nalong with the information on each employee\\nin the particular department.\\n\\n\"}],\"name\":\"5. Window Functions and Ordered Data\",\"size\":44829257,\"urn\":\"urn:li:learningContentChapter:5240089\"},{\"duration\":1171,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5231401\",\"duration\":201,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introducing common table expressions (CTEs)\",\"fileName\":\"5925685_en_US_06_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":218,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn about the purpose and structure of common table expressions. Common table expressions help minimize the complexity of multi-table queries.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4811465,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Sequel has a feature\\nknown as common table expressions, or CTEs.\\nNow, these are basically auxiliary statements\\nthat we use when we're working with larger,\\nmore complex queries.\\nAnd what they allow us to do is essentially\\nmodularize our query, much like you use modular approaches\\nin programming and software development to break down,\\nsay, complex tasks into smaller units,\\nand each unit is its own module or function.\\n\\nWell, you can think of a common table expression\\nas a module within a query that allows you\\nto break down and isolate some of the complexity,\\nand so we're able to essentially create\\nwhat are like temporary tables that are used within a query.\\nNow, we do this for the same reason\\nwe use modularity in programming.\\nIt helps us to simplify complexity.\\nSo in this case, common table expressions allow us\\nto often simplify complex queries\\nthat would otherwise require a lot of subqueries,\\nand maybe subqueries within subqueries,\\nand those can be effective,\\nbut they can also be hard to understand.\\n\\nOne of the things I particularly like\\nabout common table expressions is that they are\\nmuch easier to read and quickly grasped\\nthan, say, a very large, complex, monolithic query\\nwith a lot of subqueries.\\nNow, in addition to common table expressions,\\nthere is sort of a subtype or a specialized type\\nof common table expression known as a recursive CTE,\\nand recursive CTEs enable queries over hierarchical data.\\nSo if you need, for example, to encode\\nmaybe like a management hierarchy,\\nlike in an org chart in a relational data structure,\\nyou might want to use recursive CTEs\\nto basically query that structure.\\n\\nNow, we define common table expressions\\nusing a WITH statement.\\nThe structure is fairly straightforward.\\nIt's basically the keyword WITH\\nfollowed by a name for your common table expression,\\nAS is another keyword, and then a select statement.\\nSo whatever you put in that select statement\\nbasically defines the projection or the relation\\nthat we then reference later on as the CTE name.\\nAnd we can have multiple CTEs defined in a list.\\nIt's just a matter of saying WITH,\\nthen a CTE name, AS, select statement,\\nfollowed by another CTE name, AS, select statement,\\nand so on, and then when we're done listing\\nall of our CTE names, we have our main SELECT statement.\\n\\nAnd in that SELECT statement, we reference the CTE names\\nthat we have created in the WITH statement.\\nSo that's how we can basically build\\nthese sort of temporary tables\\nor temporary projections or relations\\nthat we can then reference in the SELECT statement.\\nSo you can imagine if you didn't have the ability\\nto do this, you might have to do a lot of subqueries\\nwithin the SELECT statement.\\nAgain, one of the nice things I really like\\nabout common table expressions, or CTEs,\\nis that they are much easier to read\\nand easier to understand.\\n\\nSo if you have the option of working with CTEs,\\nat least for me, I find it quite easier\\nthan working especially with nested subqueries.\\nSubqueries are great when you have just basic,\\nsimple kind of subquery, but anything other\\nthan a really basic subquery that's pretty obvious,\\nI tend to opt to use a CTE over a subquery\\nwhen I have that option.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5240081\",\"duration\":393,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Multiple table common table expressions\",\"fileName\":\"5925685_en_US_06_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":435,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH06 > 06_02.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you'll know how to use multiple common table expressions in a single query. Practice implementing CTEs to make a query more readable.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11686618,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, I want to do some analysis\\non salaries across regions.\\nNow in particular, I want to know\\nhow much do we pay in salaries in each region?\\nAnd actually what I really want to know is\\nwhich of the regions are above the average\\nfor what we pay in a region for salary?\\nSo this is kind of a multi-step analysis I want to do.\\nSo I'm going to start with kind of a high level approach,\\nand I'm just going to kind of think this one through.\\nI want to select all of the attributes\\nassociated with a relation\\nthat I'm going to call region salaries.\\n\\nNow, I haven't defined that yet and that's okay.\\nI'm just kind of thinking at a high level here\\nwhat my logic is.\\nAnd I want to pull from this list of region salaries,\\nonly those regions where the region ID\\nis in the list are in a set of regions\\nwhere they're above average.\\nSo I want to be able to create a list of regions\\nthat have above average salaries.\\n\\nWell, I'll figure out how to do that later.\\nAnd so for now, I'm just going to call it top region salaries.\\nNow if you're familiar with programming\\nin programming languages,\\nyou'll probably feel like a familiarity\\nlike what I'm doing is like writing a top level function\\nwhere I'm mapping out my basic high level logic\\nand I'm assuming that I will have access\\nto additional functions that will help me implement this.\\nNow with SQL, instead of implementing functions,\\nwe implement relations\\nor queries that return sets of data that we work with.\\n\\nSo what I'm saying here\\nis I've got to create two additional sets of data.\\nOne is called region salaries\\nand one is called top region salaries.\\nSo those are additional relations we will create\\nby using common table expressions.\\nSo let's start with region salaries.\\nSince I'm working with a CTE, I'm going to use a with clause\\nand I'm going to say with,\\nand then define region salaries as.\\nAnd now I simply specify a select statement\\nand that select statement, what I want to get is a region ID.\\n\\nAnd now I want to get the total of the salary for that region\\nand I'm going to alias this as region salary.\\nSo that's how I'll refer to this sum\\nor total for each region.\\nSo that's the attribute we want to get back.\\nThat's really one of the key thing that we're going\\nto be looking at.\\nAnd I'm going to pull this from\\nthe data_sci.employee's table.\\n\\nSo that's our first common table expression.\\nNow I have a second one I need to define as well.\\nThe second one is called top region salaries.\\nAnd I'm going to find that one as another select statement.\\nNow this select statement is a little bit different.\\nAll I want from top region salary is a region ID\\n'cause the only place I use top region salaries is\\nis in the subquery.\\nSo I'm simply going to select region ID.\\nAnd now I'm going to select that from.\\n\\nNow the question is where do I select this from?\\nWell, I want to get this from the region salaries relation\\nthat I just defined.\\nSo this is the CTE we defined here,\\nand now we're going to use it in our second CTE.\\nSo it's immediately available to us.\\nAnd what I want to do is from that list\\nof region salaries, I want to pick out\\nonly the regions where we have\\nabove average total salary payment.\\nSo the column I want to refer to\\nis region salaries.\\n\\nAnd I want only the ones that are above average.\\nSo I'm going to select where the region salaries\\nare greater than.\\nAnd now I've got to calculate the average salary\\nacross all of the regions.\\nWell, to do that, I can use a subquery.\\nAnd here, I'll use a select and I'm going to average.\\nAnd the thing I'm going to average is the region salary\\nthat I defined in the CTE above.\\nAnd this is going to be selected from region salaries\\nbecause that's the CTE that has that information.\\n\\nSo let's just walk through our logic again.\\nSo this first CTE, called region salaries,\\nselects a region ID\\nand then sums up the salary for each region.\\nThen we have another CTE, which defines the list of\\nabove average salaries.\\nAnd that's basically just returns a list of region IDs\\nfor all of the regions that have above average salaries.\\nSo that's perfect because what we really want to do is select\\nall of the attributes in region salaries,\\nwhich in this case is just region ID\\nand the total salary we paid out.\\n\\nAnd then we use our second CTE top region salaries\\nto help us filter down to only those regions\\nwith above average salaries.\\nSo let's go ahead and run this.\\nOh dear, I forgot the group by function here\\n'cause we don't want the sum of salary\\nfor the entire company.\\nWe only want it\\nby region ID.\\n\\nNow we'll run this\\nand it's not region salaries, it's region salary.\\nYou'll notice here the error messages operator\\ndoes not exist record greater than numeric.\\nWell region salaries, which is a CTE\\nor a relation also sometimes referred to as a record.\\nSo we can't do a numeric comparison\\nagainst the whole record.\\nSo instead we're doing it against the region salary,\\nwhich is a column within region salaries.\\n\\nSo now if we run, we're going to get what we want.\\nSo what we see here is we have three regions,\\nseven, four, and two,\\nthat have an above average total salary paid out\\nfor each of those regions.\\nSo this is an example\\nof how we can use common table expressions\\nto build incrementally some fairly complex query logic.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5231400\",\"duration\":265,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Hierarchical tables\",\"fileName\":\"5925685_en_US_06_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":297,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH06 > 06_03.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to model hierarchical data in a relational table. Sometimes data scientists need to work with trees and other hierarchical data within SQL.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7171327,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now when we work\\nwith hierarchical data structures,\\nlike on organization structures,\\nit's often helpful to use recursive CTEs.\\nSo let's take a look at\\nhow we might organize hierarchical data\\nin a relational database.\\nSo here's an example\\nof a very simple organizational structure\\nwhere we have three levels of management hierarchy.\\nWe have at the root\\nor the top of the organizational hierarchy\\nis the CEO office,\\nand then below that, we have two different departments.\\n\\nWe have VP of sales and we have VP of operations.\\nWithin each of those, we have two subsections.\\nSo under VP of sales,\\nwe have Southeast sales and Northwest sales,\\nand under VP operations,\\nwe have infrastructure operations and management operations.\\nSo this is an example\\nof some very basic hierarchical structure.\\nWell, if we model this in a relational table,\\nwe're probably going to want to use a recursive CTE\\nto query that data.\\nSo let's take a look at the structure of the CTE\\nand what the syntax looks like.\\n\\nWell, of course it's a CTE,\\nso we start with a WITH clause or a WITH term,\\nand then we have a new term called RECURSIVE.\\nSo in addition to having the WITH,\\nwe want to say WITH RECURSIVE to indicate to Postgres\\nthat we're now working with a recursive CTE.\\nWe follow that with a CTE name or a table name\\nor a relation name,\\nand then a list of columns\\nthat we're going to reference within our CTE.\\nAnd then we have the term AS,\\nand then we have two select statements.\\n\\nOne is known as a terminal select statement,\\n'cause we're working with recursion,\\nso if you're familiar with working with recursion,\\nyou have the recursive clause and the terminal clause.\\nThe terminal clause is basically where we bottom out.\\nSo that would be an indication,\\nthat would be a select statement,\\nabout say how we identify\\nwhen we're at the lowest position in the hierarchy\\nwe're interested in going to.\\nAnd we're going to UNION or UNION ALL that result\\nto the results that we get\\nfrom the recursive select statement.\\nNow, the difference between UNION and UNION ALL\\nis that UNION removes duplicates.\\n\\nUNION ALL does not.\\nYou can have dupes with UNION ALL.\\nSo we're just going to use UNION,\\nbut I just wanted to point out, UNION ALL is an option.\\nAnd then after we specify the terminal select statement\\nand the recursive select statement,\\nwe have our sort of\\nthe primary driving select statement after that.\\nNow, let's take a look at some example data\\nthat we're going to work with.\\nSo I've just pasted in a set of statements here,\\ndata definition language statements,\\nwhich are available in the exercise files,\\nand basically what I'm going to do is I'm going to create a table\\nin the data_sci schema, and I'm going to call it org_structure,\\nand this org_structure will have three attributes.\\n\\nIt's going to have an ID, it'll have a department_name,\\nand then a parent_department_id,\\nand that parent_department_id\\nis going to refer to other IDs in the table,\\nand we can see how that works here, in the insert statement.\\nSo what I'm going to do is insert a value,\\none for each of the different departments or levels\\nwithin our org structure that we were just looking at.\\nNow, the CEO Office is the root,\\nso it does not have a parent.\\n\\nSo when we insert a row for that,\\nwe have an ID, we have a department_name,\\nbut the parent_department_id is null,\\nso that's why you see a null at this point.\\nBut in all of the other cases, we actually do have a parent.\\nSo for example, rows with ID 2 and 3,\\nthose are the second level of the hierarchy,\\nand they constitute the VP of Sales and VP of Operation,\\nand both of those have the CEO Office as the parent,\\nso their parent ID is 1.\\n\\nAnd then as we move down,\\nwe see Northeast Sales and Northwest Sales.\\nWell, their parent ID is number 2,\\nso they're under the VP of Sales.\\nAnd then finally, the last two rows\\nare for Infrastructure Operations and Management Operations.\\nWell, their parent is VP of Operations,\\nso their parent ID is 3.\\nSo I'm just going to go ahead\\nand run this to execute everything.\\nAnd so what that did was created this table\\nand it inserted the data that we just described,\\nand in the next video,\\nwe're going to actually build a recursive CTE\\nto see how we can query this data.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5232351\",\"duration\":215,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Recursive common table expressions\",\"fileName\":\"5925685_en_US_06_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":229,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH06 > 06_04.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to use recursive CTEs with hierarchical data. Recursion is often required to process hierarchical data in SQL.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6591310,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's take a look at a recursive CTE.\\nSo what we have here is a query that begins using the term\\nwith recursive, so we're working with a recursive CTE.\\nWe name this report structure\\nbecause we're basically what we're doing is reporting\\non the structure of an organization.\\nAnd here we specify\\nafter the name of the CTE a list of the columns\\nthat we're going to be referencing.\\nNow, in our org structure table\\nthat we just created in the previous video,\\nwe have three columns, ID, department name,\\nand then the ID of the parent department.\\n\\nSo we're going to be referencing all of those.\\nAnd then within the recursive CTE,\\nwe specify two select statements 'cause remember,\\none is the terminal statement\\nand one is the recursive statement.\\nWell, the terminal statement is basically the one\\nthat indicates when have we bottomed out in our hierarchy.\\nSo for example, if we're traversing a path\\nthrough the hierarchy,\\nwe might hit the bottom when we'd say hit one\\nof the lowest level identifiers.\\nSo here I've just picked ID seven, which I know\\nis one of the lowest level parts of the organization.\\n\\nAnd what I'm doing in this terminal select clause\\nis basically selecting the ID, the department ID,\\nand the parent department ID.\\nSo I'm picking everything I know about that department\\nand I'm terminating or stopping this recursion\\nwhenever I hit a row where the ID is seven.\\nAnd then I'm going to return for the row with ID seven,\\nI'm going to return that ID number plus the department name\\nand the parent name.\\nAnd then I'm going to take that row and I'm going to union it\\nor combine it with the results of\\nthe recursive select clause.\\n\\nNow the recursive select clause, we are going to query\\nfrom the org structure, which is the table we created.\\nSo this is similar to what we did in the terminal clause.\\nAnd we're going to select the same columns.\\nWe're going to select the ID, the department name,\\nand the parent department name.\\nHowever, because this is the recursive clause,\\nwe also need to have a join in here\\nwhere we're rejoining on the report structure.\\nWell report structure is this CTE,\\nthat's the name of this CTE that we are defining.\\n\\nAnd this is where we are implementing the recursion.\\nSo this is the self-referential part of recursion.\\nWe're joining to itself\\nand we're joining using the parent ID\\nof the row that we're working with\\nand comparing that with the other ID, the OS ID.\\nSo that defines the recursive clause and union results,\\nso we're going to keep recursively iterating\\nthrough the structure until we hit an ID,\\nwhich equals number seven\\nand then we're simply going to return\\neverything that we have pulled back.\\n\\nIn this case, ID department name and parent department ID.\\nSo let's run that and see what happens.\\nSo we see with ID seven,\\nthat's the management operations office,\\nand that's in part of VP operations\\nand it's part of the CEO's office.\\nSo that fits what we would expect.\\nNow, I can also change this and run this say with ID four.\\nAnd then if we run that, we see that here we're working\\nwith the Northeast sales office that has ID four,\\nand we see that just above that is VP sales\\nand just above that is the CEO office.\\n\\nSo this is an example of how we can use recursive CTEs\\nto work with hierarchical data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5240080\",\"duration\":64,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Rewriting a complex query to use CTEs\",\"fileName\":\"5925685_en_US_06_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":78,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH06 > 06_05.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to map a complex, multi-join query to a CTE. This tests your understanding of how to use CTEs.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1844865,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat electronic music)\\n- [Instructor] In this challenge,\\nwe're going to be given a query\\nwith a subquery in the where clause,\\nand I'd like you to write an equivalent query\\nthat uses a common table expression instead of a subquery.\\nLet's take a look at the query we're going to start with.\\nSo in this case, we have a select statement\\nwhere we're working with salaries,\\nand the data_sci.employees table,\\nand we're also working the company_regions table.\\n\\nSo the query that we have is to select the sum of the salary\\nand then round the average of the salary\\nto two decimal places, query that\\nfrom the data_sci.employees table,\\nwhere the region_id is in a result set\\nwhere we select the id from the data_sci.company_regions,\\nwhere the region_name is like east.\\nIt has somewhere in that in the name, the pattern E-A-S-T.\\nSo the challenge is to re-write that\\nas a query that uses common table expressions\\ninstead of a subquery.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5238122\",\"duration\":33,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Rewriting a complex query to use CTEs\",\"fileName\":\"5925685_en_US_06_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":38,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":999752,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(bright music)\\n- [Instructor] And here's the solution to that challenge.\\nSo the challenge was to basically move the subquery\\ninto a CTE.\\nSo what we've done here is we have defined a CTE called the\\neast regions, and we define that\\nas selecting the ID from the data side company regions\\ntable, where the region name is like percent east percent.\\nAnd then we have our primary query\\nwhere we're selecting the sum of the salary\\nand then rounding the average of the salary.\\n\\n\"}],\"name\":\"6. Common Table Expressions\",\"size\":33105337,\"urn\":\"urn:li:learningContentChapter:5231409\"},{\"duration\":669,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5232350\",\"duration\":182,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview of types of joins\",\"fileName\":\"5925685_en_US_07_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":202,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn about the variety of joins in SQL and when to use them.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4259542,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now, I'd like to take a closer look at joins.\\nJoins are operations that we use in SQL\\nthat allow us to combine data from multiple tables.\\nNow, we do that combination\\nbased on related columns in each table.\\nTypically, a foreign key column in one table\\nhas a value that corresponds to the primary key column\\nin another table.\\nNow, there are different ways to join\\nbased on how we want to behave when we have matches\\nor don't have matches across the two tables.\\n\\nWe have several types of joins,\\nand I want to just briefly describe them.\\nThere are the INNER joins,\\nthe RIGHT and LEFT OUTER joins,\\nthe FULL OUTER join,\\nand then two rarely used joins, CROSS joins and SELF joins.\\nNow, let's take a look\\nat each of these a little more closely.\\nThe INNER join is what we use\\nwhen we want to return rows that have matching values.\\nSo, for example, if we have two tables, A and B,\\nwe want to use an INNER join when we want to return rows\\nthat have values in both tables.\\n\\nNow, if we want to return all of the rows in one table,\\nand then any values we look up in the other table,\\nthen we can use a LEFT or a RIGHT join.\\nNow, the LEFT and RIGHT\\nrefer to the position of the table name\\nwhen you type out the join statement.\\nSo for example, you might have a LEFT OUTER join on\\na.columnname = b.columnname.\\nWell, the LEFT in that case is the A,\\nand the B is the right.\\n\\nSo, if you want to return to all the columns from A,\\nyou would use a LEFT OUTER join.\\nAnd if you want to return all the columns from B,\\nyou would use a RIGHT OUTER join.\\nNow, what happens, however,\\nis when there is a row, say, in the LEFT table\\nand it doesn't have a corresponding row in the right table.\\nThen, all of the values that we would expect\\nto find in the B columns are simply null\\nin the results of the join.\\nNow, there are times\\nwhere we want to return all the rows from both tables\\nregardless of whether there's a match\\nin the alternate table.\\n\\nAnd to do that, we use what's known as a FULL join.\\nSo, we use a FULL join to return all rows from both tables.\\nNow, CROSS joins are hardly ever used. They're very rare.\\nThat produces what is known as a Cartesian product.\\nSo for each row in, say, Table A, you create an output row\\nthat includes all of the rows in column B.\\nSo, you're basically taking the number of rows in column A\\nand multiplying by the number of rows in column B,\\nand that will get you the size of the result set\\nof a CROSS join.\\n\\nSo, CROSS join or a Cartesian product join are rarely used.\\nNow, a SELF join is also rarely used,\\nbut that can be used when querying hierarchical data.\\nSo, it's an alternative to using recursive CTEs.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5241083\",\"duration\":195,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Inner joins\",\"fileName\":\"5925685_en_US_07_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":229,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH07 > 07_02.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to apply inner joins.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6475242,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, right now we're going to\\ntake a look at an inner join.\\nSo first thing I'm going to do is just\\nstart a select statement here.\\nAnd what I'd like to do is select all\\nof the columns from the employee table.\\nSo I'm just going to use E period star.\\nSo that E will be the alias for the employee table.\\nAnd the star indicates I want all of the columns.\\nAnd similarly, I want to join the employee table\\nto the company regions table.\\nSo I'm going to specify CR as an alias for company regions\\nand we'll get all the columns there as well.\\n\\nAnd so this will be from,\\nand we'll pull this from data _sci employees\\nand we'll alias that with an E and we are going\\nto do an inner join.\\nNow if I said just join, which I sometimes do,\\nthat's the join is a shorthand or defaults to inner join.\\nBut I'll spell out inner join in this case.\\n\\nAnd then I will specify the other table that we want\\nto join too, which in this case\\nis data_sci.company_regions.\\nAnd then of course we need to specify our on clause\\nand we want to specify from the employee table,\\nwe want to use a foreign key.\\nNow the foreign key is called region_id, so we'll use that.\\nAnd we want to join to the company regions table.\\nAnd here in this case, we want to reference the primary key,\\nwhich in company regions is id.\\nSo let's review again.\\n\\nWe have a select statement where we're pulling all\\nof the columns from E or Employees table and from the CR\\nor company_regions table.\\nAnd we're joining the employees table\\nto the company_regions table,\\nusing a foreign key in the employee_regions table,\\nwhich is called region_id.\\nAnd we're joining to the primary key in company regions,\\nwhich is simply called id.\\nSo let's run that\\nand see, I forgot to put in an alias there.\\nI'll run that again.\\n\\nAnd what we have here is we see that we have it very small,\\nbut we have 1046 rows returned.\\nWell, 1046 happens to be the number of employees we have.\\nSo what we've done is we've taken all\\nof the rows from the employee's table\\nthat have a corresponding row in the company region's table\\nand returned them.\\nNow the way our data is set up, all of the employees\\nhave an associated company region right now.\\nSo anytime we go\\nand we look up a value\\nfor a company region based on an employee, we will find one.\\n\\nSimilarly, all of the company regions\\nhave at least one employee associated with it.\\nSo all of the rows in the employee tables are returned\\nand all of the rows in the company regions table are\\nreturned in this case with this inner joint.\\nHowever, I do want to point out, there could be times\\nwhere maybe there's a company region\\nwhere there are no employees associated with it,\\nin which case we would never see that company region\\nin the result set like this.\\nBut in our case, since all\\nof the employees do have a company region,\\nand all of the company regions have at least one employee,\\nall employees and all regions are showing up\\nin the result set here.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5232352\",\"duration\":289,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Right outer joins\",\"fileName\":\"5925685_en_US_07_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":333,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH07 > 07_03.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to use right outer joins.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9948295,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now we're going to take a look at\\nright outer joins.\\nNow with the right outer join,\\nthe basic idea is that we want to return\\nall of the rows from the table in the right side\\nof the joint specification\\nand only those rows in the left side\\nthat have corresponding values.\\nWell, what I want to do is show\\nhow this behaves when there is a row on the right side\\nthat does not have a corresponding value\\non the left side of the joint statement.\\nSo to do that, I'm going to insert a new company region\\nand this company region, we're going to call it London,\\nand it's based in the UK.\\n\\nSo we're going to insert into the data side\\ncompany regions table.\\nBasically we're going to use a simple insert statement,\\nwe'll give it the primary key of eight\\nand we'll just say, this is London and the UK is the region.\\nSo I have a command for that here. I'm just going to run it.\\nAnd it looks like we now have\\nthe output is correct.\\nLet's just see. Yes, insert successfully.\\nSo I can see that down below in the SQL console.\\nSo I'm just going to clear this out now\\nthat insert statement.\\n\\nAnd now we're going to do a right outer join.\\nNow I'm going to use the same statement I've used previously,\\nwhich is select E for employee and all of the columns.\\nAnd then CR for company region, and all of the columns.\\nThis is going to be from data sci\\nemployees with the alias of E.\\nAnd we're going to then apply a right outer join.\\nAnd we can also use right join,\\nbut I'll spell it all the way out, right outer join\\non data sci, company regions,\\nand that has the alias of cr.\\n\\nAnd now we specify the on statement,\\nwhich is E.\\nAnd the foreign key in the E\\nor employees table is region ID.\\nAnd we want to match that to the primary key\\nin company regions or ID.\\nNow, before I run this, I just want to explain,\\nI tend to break up my select statements,\\nespecially like the join clauses into multiple lines.\\nAnd so it's not always obvious\\nlike why you would refer to this as right.\\nAnd so actually if we do a little switch here\\nand we actually change this to run it,\\nput it all on one line, it may make more sense.\\n\\nSo what we're saying here is this is a right outer join.\\nSo this is the table on the right,\\nthe specification on the right.\\nSo we're going to return all of the rows from company regions\\nand then the corresponding\\nor matching rows from on the left.\\nSo that's where this idea of on the right basically means\\nto the right of the right outer join clause.\\nAnd with that, I'm actually going to put it back\\nso we have a little more room to view our output.\\n\\nAnd so I'll grab that and open that up a little more.\\nNow let's run this and see, now it's hard to see,\\nbut we now have a total of 1047 rows returned.\\nNow when we did our inner join, we had 1046 rows returned.\\nSo we have one more row.\\nSo let's see if we can find that one.\\nI believe it's going to be at the bottom.\\nSo let's go all the way to the bottom.\\nAnd it is, and what we have here,\\nso basically we have all of the rows\\nfrom the employee's table\\nbecause all employees have a company region\\nthat has a value in the company region's table.\\n\\nBut we have a company region, one that's in London,\\nwhich doesn't have any employees associated with it.\\nBut with the right outer join, we're basically asking SQL\\nto return all of the rows in the table, in the right table,\\nwhich in this case is company regions.\\nSo let's scroll over\\nand we see that when we're looking at the columns\\nfrom the company region table,\\nwe have an ID8, a London, and a UK.\\nSo we have the values from company region.\\nSo that's what the write outer join does.\\n\\nIt makes sure we see all of the columns from\\nall of the company regions,\\neven ones that don't have employees.\\nAnd then in the employee columns\\nwe're just seeing nulls for everything.\\nSo that's the other thing that a right join\\nor a left join will do is\\nif you do have a non-matching row,\\nso for example, London, UK,\\nwe just return nulls for all of the values\\nof the other table.\\nSo in this case, we don't have a matching employee,\\nso all the employee columns are null.\\n\\nBut what we do have is one row corresponding to\\nthat London, UK company region,\\neven though it doesn't have any employees.\\nSo if that's the kind of behavior we want,\\nthat's when we use a right outer join.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5231399\",\"duration\":110,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Left outer joins\",\"fileName\":\"5925685_en_US_07_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":130,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH07 > 07_04.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to use left outer joins.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3768529,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this video, we're going to take a look\\nat the left outer join.\\nSo I have pasted in a query here.\\nA select statement that selects all of the columns\\nfrom the employees table and all of the columns\\nfrom the company regions table.\\nAnd in the from clause you'll notice we're saying\\nfrom data_sci employees e so that specifies\\nthat we're going to be working with the employees table\\nand aliasing it as e.\\nThe join clause is left outer join.\\nThat means the table on the left, in this case\\nthe data_sci employees, is going to have all rows returned.\\n\\nAnd then we're left outer joining the employees\\nto the company regions table.\\nAnd as with other types of joins that we've seen\\nin our lessons we're joining on the region id\\nand the employee and we're matching\\nthat to the company regions primary key which is id.\\nSo let's go ahead and run this.\\nAnd see the results and see if they make sense.\\nSo what we have here is we have 1046 rows returned.\\nAnd that actually does make sense 'cause if you remember\\nfrom the right outer join when we were joining\\nall of the rows from the right table\\nwhen we joined all of the rows from the right table\\nwe actually had more rows because the company regions table\\nhas an entry for London in the UK.\\n\\nNow the London, UK region does not have any employees\\nassociated with it.\\nNow in this case, we're doing a left outer join\\nso we're getting all of the rows from the employees table,\\nbut only the corresponding rows\\nin the company regions table.\\nSince there's no employee with a corresponding region\\nthat's tied to London, UK\\nwe're not going to be retrieving the London, UK row\\nfrom company regions and that's why we end up\\nwith 1046 rows in this example.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5241082\",\"duration\":134,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Full outer joins\",\"fileName\":\"5925685_en_US_07_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":139,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH07 > 07_05.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Discover what a full join is and when to use it.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4782873,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] I'd like to look at one more type of join,\\nand that's the full outer join.\\nNow in this select statement, we are querying all\\nof the columns in the employees table\\nand in the company regions table.\\nWe're querying from data_sci.employees\\nand we've aliased that as e.\\nWe're also querying from the data_sci.company_regions,\\nwhich we've aliased as cr,\\nand we're combining those two tables\\nusing a full outer join.\\nNow what that means is we're going to take all\\nof the rows from both tables.\\nWe're going to combine the rows\\nthat have corresponding values in the foreign key\\nof one table in the primary key of the other.\\n\\nSo for example, if there is an employee\\nwith a particular region ID\\nand that region ID has a corresponding row\\nin the company region table,\\nwe're going to combine those two sets\\nof columns in the single row.\\nNow this is a full outer join,\\nso we're going to return all\\nof the rows from the employees table\\nand all of the rows from the company regions table,\\neven if they don't have a corresponding row\\nin the alternate table.\\nSo let's run this and see the results.\\nNow remember we are working with company regions table,\\nwhich has an additional column\\nthat we added in a previous video for the London UK region.\\n\\nAnd I did that to show you\\nhow when we did a right join on the company regions table\\nusing the company regions table as the right table,\\nwe got essentially an extra row.\\nNow we'll notice again we have 1,047 rows\\nin the results set.\\nSo let's scroll to the end.\\nAnd what we see here\\nis that we have nulls, again,\\nthis is similar to what we saw with the right outer join,\\nand that is we have all of the employee rows returned,\\nbut we also have all of the company region rows return,\\nincluding the row for London, UK,\\nwhich has no corresponding employees.\\n\\nSo for the employee columns,\\nwe simply specify null as the value\\nfor all of the employee columns associated\\nwith the row where we have the London, UK company region.\\nSo with a full outer join,\\nwe get all of the rows from both tables.\\nAnd so if that's the behavior you want,\\nthen full outer join is the type of join you want to be using.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5234301\",\"duration\":27,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Choose the correct type of join\",\"fileName\":\"5925685_en_US_07_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":30,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":775161,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] Okay, here's our challenge related to joins.\\nLet's imagine that you need to create a query\\nthat enjoins the employees\\nand the company departments table.\\nNow, it's important that your query returns\\nall of the rows from both tables.\\nNow, this is even the case\\nif there's no corresponding row in the other table.\\nWhich type of join would you use?\\n\"},{\"urn\":\"urn:li:learningContentVideo:5239148\",\"duration\":21,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Choose the correct type of join\",\"fileName\":\"5925685_en_US_07_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":28,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":519884,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Narrator] The solution to this challenge\\nis to use a full outer join.\\nAnd the reason we're using a full outer join\\nis because we want to make sure that we return\\nall of the rows from both of the tables\\ninvolved in the join.\\nAnd when that's the case, we use full outer joins.\\n\"}],\"name\":\"7. Types of Joins\",\"size\":20581231,\"urn\":\"urn:li:learningContentChapter:5234309\"},{\"duration\":995,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5234303\",\"duration\":315,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"JSON in relational databases\",\"fileName\":\"5925685_en_US_08_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":366,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn about the importance of using JSON in relational databases.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7030721,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now in data science,\\nwe're often asked to work with JSON data structures.\\nJSON is a very flexible\\nand adaptable data schema that we can use\\nfor working with a lot of different types of systems,\\nwhether they're databases or applications,\\nor maybe third party services.\\nSo JSON is quite popular,\\nso it's not surprising\\nthat we're seeing JSON data structures\\nshow up in our data sets that we work with in data science.\\n\\nNow, one of the nice things that has happened over the years\\nis that there is increasing support for JSON data structures\\nin relational database management systems.\\nSo many relational database systems,\\nwhether it's Postgres, or MySQL, or Oracle,\\nhave some support for JSON.\\nAnd this is really valuable\\nbecause it enables us to store both types of data,\\nboth structured data, like we traditionally use\\nwith relational database management systems,\\nas well as semi-structured data,\\nwhich is what we typically use JSON for.\\n\\nNow, this provides us a lot of flexibility\\nin what data structure we use to capture, you know,\\na certain set of data\\nwhile still providing some of those core features\\nthat are so valuable to us in relational databases,\\nlike the ability to support atomic transactions\\nand enforcement of data integrity rules.\\nNow, having JSON in relational databases\\nhave some particular benefits.\\nFirst of all, JSON is an adaptable schema.\\nThis is really valuable.\\nSo what this means is we can capture a JSON structure\\ntoday that has a certain format.\\n\\nWell, tomorrow that format might change a little bit.\\nLike maybe there's a new attribute that has been added\\nto the JSON structure.\\nWell, we can ingest that without a problem.\\nWe don't have to explicitly model the fact\\nthat the JSON structure now has an additional attribute.\\nIn addition, we can support variable attributes.\\nSo what this means is we could have a JSON structure\\nthat describes, say, a product that might be an appliance,\\nand an appliance might have attributes\\nlike weight, and length, and width, and height.\\n\\nBut we might have other products,\\nlike digital music, that we are selling.\\nWell that doesn't necessarily have a weight\\nor a height or a depth.\\nNow, instead, a piece of digital music might have an artist,\\nit might have a studio that produced it,\\nit might have a duration\\nor the length of the time that the music plays.\\nBut those are different types of attributes,\\nthose others variable attributes.\\nJSON is well suited to supporting use cases\\nwhere we have a definite need for variable attributes,\\nand catalogs is one example of that.\\n\\nAnother example is API responses.\\nOftentimes now we're building systems\\nbased on a service oriented architecture\\nwhere we're making calls to different services via APIs.\\nTypically like rest APIs\\nor in high performance environment,\\nwe might use something like GRPC protocol.\\nBut in either case,\\nwe're still getting API responses\\nand those, ideally, we should be able to support\\nflexible structures because there could be\\ncomplex kind of information requirements in a JSON response\\nso we want to be able to capture all of that.\\n\\nNow, another example of a useful place\\nwhere we might want variable structures\\nis if we want to capture like preferences\\nand configurations for different environments\\nor maybe different user preferences.\\nJSON structures work really well for that as well.\\nNow, there are some best practices for using JSON\\nand relational database management systems.\\nTypically, we reserve the use of JSON columns\\nfor variable schema data.\\nSo if you have a service that returns a JSON structure\\nand that structure might change over time,\\nthat's a great candidate for using JSON.\\n\\nWhen we work with hierarchical data, we can, as we've seen,\\nwork with hierarchical data in relational databases,\\nbut it can be a little bit tricky.\\nFor example, working with hierarchical CTEs\\nisn't necessarily intuitive,\\nbut when we work with JSON,\\nJSON really lends itself to hierarchical data structures.\\nSo definitely a good use case for JSON\\nand relational databases is working with hierarchical data.\\nAnd that, again, external API responses\\nare a good candidate as well.\\nNow we still have plenty of need for traditional columns.\\n\\nSo whenever we have like fixed data structures\\nor data structures requiring relationships\\nwhere we want to be able to say, enforce, you know,\\ndata integrity or constraints that depend on, say,\\na value existing in another table,\\nthen that's the kind of time when we want to use\\nthe traditional columns.\\nSo again, JSON in relational databases\\nkind of extend the breadth of different types of data\\nthat we can capture\\nand the use cases that we can easily accommodate.\\n\\nJSON's very useful when we need some flexibility.\\nAnd the traditional columns\\nthat we use in relational databases\\nwork really well when we still have, kind of,\\nthose traditional requirements\\nthat relational databases were designed for,\\nlike fixed data structures\\nand enforcing integrity constraints.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5234304\",\"duration\":158,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"JSON data types\",\"fileName\":\"5925685_en_US_08_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":202,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After watching this video, you'll be able to distinguish JSON and JSONB data types and when to use them.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3921851,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now, in Postgres,\\nwe have the option of two different data types\\nthat we can use with JSON.\\nOne is simply called JSON,\\nand that was the original data type that was used\\nto support JSON.\\nAnd it's basically a text-based version of JSON.\\nSo it stores an exact copy of the input text.\\nSo you can think of it as a type of text\\nor a variable character string.\\nAnd one of the advantages of using JSON\\nis that it preserves whitespace.\\nSo it's literally taking the string representation\\nof the JSON.\\n\\nNow, a significant disadvantage of this\\nis that because we're essentially treating this JSON\\nas a string of characters,\\nit requires re-parsing that data structure\\neach time we do an operation.\\nLike if we want to go find a path or look up a value\\nof a particular attribute, we need to re-parse that string.\\nNow, the JSON data type is really useful\\nwhen we need to maintain an exact copy of the data.\\nSo for example, if we're using JSON\\nto store logs or to maintain auditing information.\\n\\nWe want to make sure anytime we have auditing information,\\nwe're doing minimal kind of processing on the data\\nto manipulate it.\\nSo JSON is really a good option\\nwhen you want an exact copy of what was returned to you\\nor provided to you for that value.\\nSo for example, an API response.\\nIf you're trying to troubleshoot something\\nwith regards to the API and you're collaborating\\nwith those developers, you want to make sure you have exactly\\nwhat was returned to you without any possible manipulation\\nby preprocessing or the way we store the data.\\n\\nSo the JSON data type is really useful\\nwhen you want the exact string of characters,\\nincluding white spaces.\\nNow, the JSONB data type is what's generally considered\\na better version, and for many cases, it's a binary format.\\nIt's more efficient for processing and formatting\\nbecause we basically process it once.\\nNow, it does mean that we have faster querying,\\nfaster read operations, but writes can be slower\\nbecause we're basically,\\nwe're going through the parsing work\\nand storing the results of the parsing.\\n\\nSo when we're ingesting data or writing the data,\\nit's a little bit slower than if we use the JSON data type.\\nAnother advantage of JSONB is it supports GIN indexes,\\nwhich makes lookups or queries much faster\\nthan they would be if we are using values\\nwithin the JSON structure in our where clause.\\nSo definitely consider using JSONB\\nany time you want to be doing any kind of filtering\\nbased on the values within the JSON structure.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5233338\",\"duration\":191,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Inserting JSON data\",\"fileName\":\"5925685_en_US_08_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":212,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH08 > 08_03.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to insert JSON data into a relational table.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5875944,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, so we talked about JSON\\nand the advantages of using JSON\\ninside of relational databases.\\nAnd we briefly discussed some differences\\nbetween the JSON data type and JSONB data type.\\nSo next let's just get a feel for\\nwhat it's like to work with JSON.\\nSo I've opened up code spaces\\nand what I want to do here is\\nI'm going to create a really just dead simple table\\nin the data site schema called API responses.\\nAnd it's going to have a primary key called the ID,\\nwhich will just be an integer.\\nAnd then we're going to have a column called response.\\n\\nAnd response is going to be a data type JSON.\\nNow remember, JSON is the data type we use when we want\\nlike exactly the literal string that we get,\\nwe want to store with minimal processing.\\nSo that's what we're going to use cuz now we're using\\nan example of an API response.\\nSo the most important thing is that we kind of\\ncapture exactly what the API response is,\\nso we're going to use JSON.\\nSo let's go ahead and run this create table command.\\nAnd it looks like we have created the table.\\n\\nAnd then the next thing I want to do is I'm just going to\\ncut this out and I'm going to paste in\\nanother piece of code for inserting.\\nAnd I'm just going to paste in an insert statement.\\nAnd so basically we're going to insert into the data site\\na pay response table.\\nWe're going to assign a value,\\njust pick an arbitrary integer for the primary key,\\nbut most importantly we're going to put in\\na JSON data structure and it's going in as a string,\\nyou'll see the single quotes,\\nand we have a basic JSON structure with a status,\\nand then an HCTP code, in this case 200.\\n\\nThen we have a little bit of data like the user ID.\\nSo for example, we have a user here, Jane Doe,\\nwho has ID one, two, three,\\nand their email address is jd@example.com.\\nNow, the metadata, we have a timestamp\\nand just some other source.\\nSo this is a very basic JSON structure.\\nSo if we run this, we will insert that.\\nAnd now if we do a select star\\nfrom data sci,\\nI'll just clean this up.\\n\\nAnd if we run\\nselect star\\nfrom data sci responses,\\nwe see here in our result window\\nthat we have one row,\\nand the response is a JSON structure.\\nWe can see here we have status, we have code, data,\\nand it has exactly the same format as the insert statement.\\nSo this is how we can go about inserting data\\ninto a JSON data structure.\\n\\nNow the insert operation would be similar\\nif we had used JSONB,\\nit's just that the right would've taken a little bit longer,\\nnothing that we would've noticed,\\nbut it would've taken a little longer\\nand it would've basically broken the JSON structure down\\ninto its constituent parts\\nand put it into a data structure\\nthat is more amenable to things like\\nindexing using gin indexes.\\nSo that's just an example of how we can\\ninsert JSON data into a relational table\\nthat has either a JSON\\nor a JSONB data type column defined in it.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5240082\",\"duration\":139,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Querying JSON data\",\"fileName\":\"5925685_en_US_08_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":161,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH08 > 08_04.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to query JSON data in a relational table.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3864203,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, so now let's query our JSON data.\\nSo what I'm going to do is I'm going to select the ID\\nand the response from API responses in the data side schema\\nwhere the ID equals one.\\nSo let's run that.\\nAnd as we expect, we get only ID one,\\nwhich is the only column in the table,\\nand then we have the JSON structure here.\\nNow, let's say we want to get the person's name.\\nWell, the name is within data and which is within user\\nand then we have name.\\nWell, we can do this\\nby essentially building up a path\\nthat we use to walk through the JSON structure\\nto get to the values that we're looking for.\\n\\nWhile we do that, using a notation where we have response\\nand then a single dash and an arrow sign,\\nand then as a string, the name\\nof the embedded JSON structure, the substructure,\\nin this case it's data.\\nSo let's see what happens when we return response data.\\nLet's run that.\\nNow this is a little bit different.\\nWe don't have like the success status or the 200 HTTP code.\\nWe just have the JSON structure\\nthat is the value of the data attribute.\\n\\nSo what we're doing is we're essentially walking\\ninto the structure.\\nWhat we can follow this pattern again and put a dash arrow\\nand then specify user,\\nand let's see what happens when we do that.\\nNow what we have is just the values associated\\nwith the user.\\nSo, okay, let's continue this pattern.\\nLet's go in and let's grab name.\\nAnd run that.\\nAnd now what we see is we have the user\\nor the column name Jane Doe.\\n\\nNow, because we want Jane Doe as a string, what we see is\\nthat automatically this converted it,\\nthere is sometimes you might see a convention\\nwhere there is a double arrowhead,\\nand if you run that, you get the same result.\\nBut that double arrow head, if you ever see that,\\nis just an explicit statement that says,\\nif this is the only value,\\nthen don't return a JSON structure,\\njust return the string value.\\nSo that is an example of\\nhow you can query the JSON's data structure\\nusing this kind of a path notation with the dash arrow\\nor dash greater than.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5234300\",\"duration\":126,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Indexing JSON data\",\"fileName\":\"5925685_en_US_08_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":167,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Exercise Files > CH08 > 08_05.sql\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to index JSON data in a a relational table.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3489580,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now let's take a look at working with indexes\\nand JSON data structures.\\nSo the first thing I want to do is I want to delete\\nthe API responses table and I want to recreate it,\\nbut this time I want to use the data type JSONB\\nfor the response column, which of course,\\nis where we're going to be storing our JSON structure.\\nSo I'll go ahead and run that.\\nAnd so now we have dropped that table,\\nso I'm going to erase that.\\nAnd now I'm just going to paste in an insert statement\\nand we'll go ahead and insert that.\\n\\nAnd so this is the same insert statement I had used before,\\nand we see we have successfully inserted that.\\nSo that's basically all the same up to this point.\\nNow what I want to do is I want to show you\\nhow to create an index.\\nAnd I'll just, again, paste in\\nthe create index statement here.\\nNow, what this statement does,\\nit follows this typical pattern for create index.\\nWe specify an index name\\nand then the on term followed by the name of the table.\\nIn this case, it's data side API responses.\\n\\nAnd then we specify a path to the column,\\nor excuse me, it's not a column, it's an attribute.\\nWe want to specify a path\\nto the attribute within the JSON structure\\nthat we actually want to index.\\nSo in this case, we want to actually index on name.\\nWell, we're going to use the path structure\\nthat we have seen before when we do querying.\\nWell, we use that same path structure\\nto specify a path to an attribute\\nthat we want to use when we index.\\nSo I'm going to go ahead and run that.\\nAnd we get a create successfully executed.\\n\\nSo now we have an index on username\\nthat is the username within the JSON structure.\\nSo that's how you can go about indexing\\nor creating an index on an attribute\\nwithin a JSON structure.\\nYou just have to specify the path.\\nAnd the path is the same path that we use when we query.\\nSo it's really simple to work with JSON\\nonce we get an intuitive understanding\\nof this path structure that we have\\nto specify both when we are querying\\nand when we're building indexes.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5239147\",\"duration\":20,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Query a JSON column\",\"fileName\":\"5925685_en_US_08_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":18,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to create a query of JSON data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":558885,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat instrumental music)\\n- [Instructor] Okay, here's a challenge\\nfor working with JSON Data structures.\\nCreate a query\\nto return the source attribute from an api_response\\nJSON data structure.\\nSo, this is the same api_response data structure\\nthat we have been using through this lesson.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5238123\",\"duration\":46,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Query a JSON column\",\"fileName\":\"5925685_en_US_08_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":48,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1300707,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat instrumental music)\\n- [Instructor] Okay, here's the solution.\\nI'm creating a select statement,\\nand I'm returning the primary key as well.\\nBut the JSON structure is in a column called response.\\nSo, I want a particular attribute\\nwithin that JSON structure.\\nSo, I need to specify a path to it,\\nwhich in this case is data.\\nAnd then within the data substructure,\\nI want to get the metadata structure within there.\\nAnd then from there, I want to get source,\\nand I want an alias that results as source.\\n\\nSo, if I just run that, we'll see that I get the source.\\nAnd the source is user_api.\\nSo, that's an example of how we can query\\nthe source attribute\\nfrom a table using JSON.\\n\"}],\"name\":\"8. Working with JSON\",\"size\":26041891,\"urn\":\"urn:li:learningContentChapter:5236204\"},{\"duration\":119,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5234302\",\"duration\":119,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"5925685_en_US_09_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":129,\"solutionVideo\":false,\"editingNotes\":\"02:02 - LinkedIn Overlay - dansullivanpdx\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2991788,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] Congratulations on completing this course\\non Intermediate SQL for data scientists.\\nIf you're interested in continuing your studies\\nand learning how you can use SQL\\nto solve problems in data analytics\\nand data science, then I suggest you view the course\\nAdvanced SQL for Data Scientists,\\nwhere I go into even additional further techniques\\nthat will help you with your work in data science.\\nIf you're also interested in application development\\nor you're interested in working with large volumes\\nof data in your data science work,\\nthen I would strongly suggest the Advanced SQL\\nfor Query Tuning and Performance Optimization.\\n\\nSQL is very easy to use in some ways,\\nbut it's also very easy\\nto develop poorly performing queries.\\nAnd this course on Advanced SQL for Query Tuning\\nand Performance Optimization helps you understand the best\\nways to craft queries to meet your goals from a data science\\nor data analytics perspective while doing it\\nin a highly performant way.\\nNow, if you work with time series data, for example,\\nif you work with IOT sensor data\\nor observability data around logging\\nor monitoring complex systems, then you're probably working\\nwith time series data.\\n\\nWell, there are certain techniques that work well\\nwith analyzing time series data,\\nand we cover those in the Advanced SQL\\nfor Data Science Time Series course.\\nAnd then finally, it always helps\\nto understand data modeling anytime you're working\\nwith relational databases.\\nSo I'd strongly urge you to consult the course catalog\\nand look for courses on data modeling\\nto help you understand the principles behind\\nhow we design relational databases.\\nMy name is Dan Sullivan,\\nand I'm always looking forward to talking to other people\\nwho are interested in SQL and data science.\\n\\nSo please feel free to connect with me on LinkedIn.\\nMy idea is Dan Sullivan PDX,\\nand I wish you the best going ahead\\nand working with SQL for data science.\\n\"}],\"name\":\"Conclusion\",\"size\":2991788,\"urn\":\"urn:li:learningContentChapter:5240090\"}],\"size\":411080830,\"duration\":13851,\"zeroBased\":false},{\"course_title\":\"Advanced SQL for Data Scientists\",\"course_admin_id\":2874221,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":2874221,\"Project ID\":null,\"Course Name\":\"Advanced SQL for Data Scientists\",\"Course Name EN\":\"Advanced SQL for Data Scientists\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"Many data scientists know how to work with SQL\u00e2\u20ac\u201dthe industry-standard language for data analysis. But as data sizes grow, you need to know how to do more than simply read and write from a database. This course provides a more sophisticated approach to designing data models and optimizing queries in SQL. Instructor Dan Sullivan begins with the logical and physical design of tables\u00e2\u20ac\u201dwith particular focus on very large databases\u00e2\u20ac\u201dand then presents a deep dive review of indexes, including specialized indexes and when to use them. The next section introduces query optimization and shows how to optimize basic, multi-join, and more complex queries. The course also covers SQL extensions, including user-defined functions and specialized data types. The techniques taught here enable more efficient analysis of large data sets using SQL, statistics, and custom business logic.\",\"Course Short Description\":\"Learn advanced techniques for analyzing large data sets with SQL. Find out how to build sophisticated data models, optimize queries, extend SQL with user-defined functions, and more.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":7382682,\"Instructor Name\":\"Dan Sullivan\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Data Architect, Author, and Instructor\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2021-05-27T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"Yes\",\"LIL URL\":\"https://www.linkedin.com/learning/advanced-sql-for-data-scientists-13972889,https://www.linkedin.com/learning/advanced-sql-for-data-scientists-2021-revision\",\"Series\":\"Persona\",\"Limited Series\":null,\"Manager Level\":\"General\",\"LI Level\":\"Advanced\",\"LI Level EN\":\"Advanced\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Data Science\",\"Primary Software\":\"SQL\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":9038.0,\"Visible Video Count\":38.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":95,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2422413\",\"duration\":51,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Advanced SQL techniques for data science\",\"fileName\":\"2874221_00_01_WX30_welcome\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2609341,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Dan] Welcome to this course  \\n on Advanced SQL for Data Scientists.  \\n In this course,  \\n we'll go beyond writing complex select statements  \\n to learning how to design performant data models  \\n using denormalization and read replicas.  \\n We'll review how and when to use various types of indexes,  \\n including GiST and GIN indexes,  \\n which we use for composite data types.  \\n Next, we'll move on to see how to optimize queries  \\n by analyzing query execution plans.  \\n SQL is extensible,  \\n so we'll spend time learning  \\n how to create user defined functions  \\n that can streamline working with SQL for data science.  \\n We'll also see how to take advantage  \\n of specialized features for working with JSON,  \\n as well as specialized data types  \\n for tree structures that can provide  \\n an order of magnitude speed up  \\n over conventional hierarchical queries  \\n based on recursive common table expressions.  \\n So let's get started  \\n and learn some Advanced SQL for Data Science.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2424379\",\"duration\":44,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"2874221_00_02_XR30_wysk\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1307361,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] This is an advanced course,  \\n so I make some assumptions about what you already know  \\n or at least familiar with,  \\n and that starts with assuming that you're fairly comfortable  \\n with writing complex SQL statements.  \\n So that includes working with different types of joins,  \\n applying different filtering operations,  \\n such as where and having clauses,  \\n applying windowing functions,  \\n and working with common table expressions.  \\n I also assume you're capable  \\n of installing Postgres or PostgreSQL,  \\n and as well as any kind of graphical base query tools  \\n you would like to use,  \\n and also, I assume you have some familiarity  \\n with data structures.  \\n So when we're talking about things like balanced trees  \\n and hashes in the index topics that you will be comfortable  \\n and be able to follow along.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":3916702,\"urn\":\"urn:li:learningContentChapter:2423380\"},{\"duration\":2252,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2425338\",\"duration\":379,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Rules of normalization\",\"fileName\":\"2874221_01_01_XR30_normalization\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"In this video, explore the first three rules of normalization and learn when they should be used. These are foundational design patterns data scientists use regularly.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11188963,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] When we talk about relational databases  \\n in SQL and data modeling,  \\n we're almost always going to be talking  \\n about normalization to some degree.  \\n Now, normalization is a practice in which  \\n we follow a set of rules for designing database tables  \\n which help us minimize the risk of data anomalies.  \\n Now, normalization is an important practice  \\n and it's widely adopted, but it's not always essential  \\n and there are cases where we actually don't  \\n want to normalize.  \\n So let's take a look at normalization  \\n and then we'll take a look  \\n at when we actually break the rules of normalization.  \\n So data anomalies are basically errors  \\n or inconsistencies in data that we really want to avoid.  \\n And there's three types we'll talk about here.  \\n There is an update anomaly.  \\n And an update anomaly occurs when you have redundant data  \\n and you only partially update that data.  \\n So for example, if we had a database of books  \\n and in that we tracked books and authors,  \\n and for authors we track things like their address.  \\n Well, if every time an author had a book entry,  \\n we captured the author's address along with the book,  \\n then we might have multiple addresses,  \\n say an author has 10 books published,  \\n there would be 10 copies of the address.  \\n Now, if for some reason we updated that address  \\n but only updated five of them,  \\n then we'd have five with old data and five with new.  \\n That's an example of an update anomaly.  \\n An insertion anomaly basically results  \\n when we're not able to add data to the database  \\n due to the absence of other data.  \\n So for example, if to add a new book  \\n I also have to be able to enter the author's address  \\n and I don't have the author's address,  \\n then I'm not able to insert that data.  \\n That's an example of an insertion anomaly.  \\n And then a deletion anomaly is when  \\n we unintentionally lose data  \\n because we've deleted other data.  \\n So for example, we might lose an author's address  \\n because we deleted the author's only book  \\n that we had in the database.  \\n Now we may have wanted to delete the book  \\n but actually keep the author and author information  \\n in the database for future use.  \\n So that's an example of a deletion anomaly.  \\n So the way we avoid anomalies like that  \\n is we follow normalization rules.  \\n Now there are many normalization rules  \\n but really the most important and the most widely used  \\n are the first three.  \\n The first normalization rule  \\n which is called first normal form,  \\n basically states that each value in a column  \\n is an atomic unit.  \\n So it's going to be a particular number or a unit string  \\n or a Boolean or some scalar values, some unit like that  \\n that can't be broken down further.  \\n The second normal form or second rule of normalization  \\n states that any attributes or any column we have in a table  \\n is dependent on the key.  \\n So it's a function of that key.  \\n So there's nothing in the table that's not related  \\n to that particular key.  \\n That gets us to second normal form.  \\n And then third normal form says we don't have  \\n any transitive dependencies  \\n or there's some pieces that are dependent  \\n on something in the table, but not the key.  \\n And that something else might be,  \\n for example, an author's address is dependent on the author  \\n and the author depends on, for example, the book,  \\n if we had that kind of relationship,  \\n that would violate third normal form.  \\n So we want to avoid that kind of transitive dependency.  \\n Now, oftentimes when we visualize normalized databases,  \\n we have diagrams which show rectangles representing tables  \\n and then we have lines connecting the tables  \\n which represent relationships,  \\n and then we have some kind of indicator  \\n for what kind of relationship it is.  \\n So for example, in many cases we have one table  \\n that's like a primary table,  \\n for example, like an order table.  \\n And then we have like a secondary cable  \\n that has a lot of detail, like an order's item.  \\n So if you have an order and there's maybe 10 different books  \\n on that order, we'd have one order in the order's table  \\n and 10 rows or 10 order items in the order items table  \\n that would be a one to many relationship or one to N.  \\n And sometimes the relationship can be one to one  \\n or one to zero or one or one to zero or many.  \\n And those all fit,  \\n these are all allowed under the rules of normalization.  \\n And we often see this kind of modeling  \\n when we're working with OLTP  \\n or online transaction processing systems.  \\n Now OLTP systems, for example,  \\n typically have many reads and writes  \\n so they're constantly being updated.  \\n Data is written once but then it may be updated  \\n again frequently by many different processes.  \\n So you can imagine like an e-commerce application  \\n with users updating many orders at the same time.  \\n Now, oftentimes these kind of OLTB systems  \\n are normalized to third normal form.  \\n Sometimes per performance they're de-normalized slightly  \\n but typically they're still considered normalized.  \\n Now contrast that with analytical databases,  \\n these are used typically for data analysis.  \\n And here we have many reads by many processes,  \\n but typically with analytic databases,  \\n we're not updating say a single customer's address  \\n or a single customer's order,  \\n we might be reading a single order  \\n or a single customer record,  \\n but in an analytical database we might look  \\n at thousands of orders or thousands of customers.  \\n So our reads tend to span many rows, but have fewer columns  \\n that we actually include in our query.  \\n We have many writes and with batch processing,  \\n many of those writes are done all at once  \\n like bulk updates where a job will run  \\n and it'll start from end to finish  \\n until the large number of rows are loaded in.  \\n We may also have streaming data  \\n where a process is ingesting data in near real time  \\n and writing it to a database.  \\n That's different from this idea of having maybe thousands  \\n of different users doing small updates or small writes,  \\n here we're doing fairly large amounts of writes  \\n when we have batch processes  \\n or we're consistently doing writes  \\n if we have a streaming process.  \\n These analytical databases are often de-normalized.  \\n And we'll take a look in the next video  \\n as to why that's the case.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2423375\",\"duration\":430,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Denormalization\",\"fileName\":\"2874221_01_02_XR30_denormalization\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover when to denormalize a data model to improve query performance. Normalized data models are not always as performant as denormalized data models so denormalization is often used to improve performance.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12796851,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Presenter] Now sometimes we choose  \\n to denormalize our data because denormalization  \\n can give us significantly better performance.  \\n So let's take a look at some of the characteristics  \\n of denormalized tables.  \\n Well, data is often redundant.  \\n So for example here in this example,  \\n looking at a book's table,  \\n we have author information that's duplicated  \\n as well as publisher information.  \\n We may have non-atomic values.  \\n So for example, some analytical databases allow  \\n and actually encourage the use of strokes or structures  \\n to have more complex values within a column  \\n than simple atomic values  \\n and then we also tolerate transitive dependencies.  \\n So if we have say a value that's dependent solely  \\n on a non-key attribute in the table  \\n and that non-key attribute is itself dependent  \\n on the primary key.  \\n That's totally fine.  \\n Again, it violates the third normal form  \\n but we're willing to tolerate that  \\n because in exchange for not being normalized,  \\n we're going to get significantly better performance.  \\n And the really the critical thing is  \\n in analytical databases in analytical applications  \\n we are typically at a reduced risk level  \\n with regards to anomalies.  \\n And that's because of how we work with the data.  \\n We have relatively few updates  \\n so while customers might constantly  \\n be updating their orders and their baskets  \\n and things like that.  \\n In an analytical database like a data warehouse  \\n or a data mart or say just a data science database,  \\n we typically load the table once  \\n and then maybe update it or add data  \\n but rarely do we go back and correct data  \\n that say for example we loaded a week ago.  \\n Now, if there are mistakes in the data,  \\n we may correct the batch or delete a batch  \\n or reload with corrected versions but outside of errors  \\n in data loading like that, we typically do few updates.  \\n Really we're more likely to be writing new data  \\n and reading data.  \\n Now, oftentimes we do batch inserts  \\n and those batch inserts go through  \\n an extraction load transform  \\n or extraction transform load process.  \\n So the data transformation is already kind of cleaning  \\n up the data massaging it and getting it into  \\n sort of our preferred form.  \\n So in that case, again,  \\n there's less risk of say an update anomaly.  \\n Again, unless say our transformation process broke  \\n but that would be a significant event  \\n and we would hopefully catch that  \\n with our monitoring system.  \\n Now sometimes we also have streaming inserts  \\n and those are relatively simple data structures.  \\n So you might think of like an IoT sensor  \\n that's measuring temperature and humidity  \\n and maybe some other environmental measures  \\n and then transmitting those say every minute.  \\n And so it might be a relatively simple structure  \\n that has a sensor ID, a timestamp  \\n and then whatever the measures are.  \\n So again, it's fairly simple  \\n with IoT data and streaming data.  \\n Again, you typically don't update it.  \\n And another thing about like IoT data  \\n which is typically used in the aggregate  \\n even if you did have a mistake even if there was an error  \\n in one of the readings and one of the minutes,  \\n if you're getting 60 readings per hour  \\n and you're aggregating those,  \\n it almost doesn't matter in terms of the aggregate value.  \\n So if you're looking at statistical measures  \\n across these data structures, very small errors  \\n just kind of get washed out and it actually  \\n doesn't make a material difference the way we report on it.  \\n So again, a data error may occur  \\n but it doesn't have as much of an impact  \\n as it would in an OLTP system.  \\n And another thing that happens,  \\n we eliminate the need for really complex joins.  \\n So that's particularly useful with regards  \\n to simplifying and speeding up performance.  \\n Now there's a couple of different ways  \\n that a denormalized database may look.  \\n A common way of building denormalized databases  \\n that's been popular for over a couple of decades  \\n is the star schema.  \\n And here are the idea is we have this fact table  \\n with a bunch of measures.  \\n For example, they might be all the measures  \\n that an IoT sensor is taking.  \\n And then we have different dimension tables  \\n and the dimension tables may be things like,  \\n oh, the sensor information and geography information  \\n and a time information.  \\n So we can keep track of like hours within days,  \\n within weeks, within years  \\n and be able to easily roll up data.  \\n So that's one way of denormalizing.  \\n And this is commonly done in databases  \\n where you have a row level orientation  \\n and row level orientation means  \\n when you go to read a data block,  \\n you read an entire row at a time.  \\n And that makes perfect sense when you're dealing  \\n with OLTP systems because if you're going to update something  \\n in a customer record you're likely  \\n maybe want to update multiple things.  \\n So it's helpful to have  \\n the entire customer data block available to you.  \\n Now in analytics systems when we're reading,  \\n we typically reading a few columns across many rows  \\n so a more performant way to work with it  \\n is using columnar orientation or using a columnar format.  \\n And in that way, when we go and we read data blocks  \\n we're actually reading many different attribute values  \\n from different rows but all from the same column.  \\n So if you want to do an average  \\n say of the temperature measure  \\n of a particular sensor over a 24-hour period,  \\n it makes a lot of sense just to grab the temperature data,  \\n grab the temperature column and not bring  \\n in the other data that you're not interested in  \\n and that's what columnar data stores do for us.  \\n Now, another common denormalization technique  \\n is to have what's called a white column table.  \\n And here you can imagine what some data modelers  \\n in the relational world might think of  \\n as their worst nightmare which is that we denormalize,  \\n we put everything into a single table  \\n and we make it very wide.  \\n Now this would be incredibly problematic for an OLTP system.  \\n It's actually quite advantageous  \\n for many analytical systems and the really high volume,  \\n large scale data warehouse systems.  \\n The kinds that scale to petabytes scale  \\n like Google big query or Google big table  \\n we'll use a wide column model.  \\n Now big table is a NoSQL database  \\n but big query is an analytical database,  \\n and it uses SQL for querying  \\n and it uses this kind of wide column model as well.  \\n Now the advantages of denormalizing  \\n that can be easier to query again,  \\n we're getting rid of complex joins.  \\n So we don't have the join table, A to B to C to D to E  \\n and introduce the risk of making mistakes  \\n in our complex queries.  \\n Queries can also be more efficient.  \\n Again, there are a lot of reasons for that,  \\n we discuss columnar versus row that's just one example  \\n and it also simplifies load procedures.  \\n Oftentimes with ETL processes, we can do joins sort of  \\n in stream and then write to a single table.  \\n It also makes it easier if you're dealing  \\n with something called slowly changing dimensions  \\n and you want to keep track of history.  \\n Well, by actually tracking the data  \\n in a road by denormalizing, you don't have to worry  \\n about tracking multiple records and say a dimension table  \\n and then pointing to the correct version of the data.  \\n So again, denormalizing  \\n can somewhat simplify load procedures.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2423376\",\"duration\":488,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Partitioning data\",\"fileName\":\"2874221_01_03_XR30_partitioning\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover when to partition data using range, list, and hash partitioning. Partitioning is an important strategy for improving the query performance of large data sets.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15040418,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] One of the most effective ways  \\n to deal with large data sets is to use partitioning.  \\n Now, what can happen  \\n when we're dealing with really large datasets  \\n is that large tables can be difficult to query effectively  \\n because they have so much data,  \\n and especially if you're scanning  \\n or you have to maintain very large indexes.  \\n So what partitioning does is it splits tables  \\n by either rows or columns into these subsections,  \\n which we call partitions.  \\n Now, horizontal partitioning  \\n is a way of limiting the amount of data we have to scan  \\n to a subset of a set of columns.  \\n We can have local indexes for each different partition,  \\n so in this example, we have a large table  \\n which is broken down into three different partitions,  \\n we could have three distinct sets of local indexes,  \\n and so our indexes would be smaller,  \\n scanning the indexes would be smaller,  \\n or if we needed to scan the entire partition,  \\n the amount of data would be smaller.  \\n And also partitioning makes adding  \\n and deleting data efficient,  \\n especially if you have like a time-based partition.  \\n It's very easy to say drop off the oldest partition,  \\n just drop that partition  \\n once that data set ages beyond whatever period of time  \\n you think it's useful for.  \\n Now, vertical partitioning  \\n is a little bit different from horizontal partitioning.  \\n With vertical partitioning we're separating out columns,  \\n and this is, in a way, what columnar storage does,  \\n it's sort of extreme vertical partitioning  \\n in that we increase the number of rows  \\n that can fit in a data block  \\n because we have fewer columns in each row.  \\n So when we're using a row oriented storage system,  \\n this is one way to start to get some of those advantages  \\n of columnar storage.  \\n Now we can have global indexes for each partition,  \\n so we're still covering the entire table  \\n like all global indexes,  \\n but we can still reduce I/O because when we fetch data,  \\n we're going to fetch a single block,  \\n or actually, you know, some number of data blocks  \\n but those data blocks will have the equivalent of more rows  \\n than if we had additional columns in the data block,  \\n and again, columnar storage has similar benefits.  \\n Now range partitioning is a type of horizontal partitioning,  \\n and basically what it allows us to do  \\n is to partition on non-overlapping keys.  \\n So we'll identify a column or set of columns  \\n that is a partition key,  \\n and those will be distinct,  \\n and we will be able to sort of designate which partition  \\n a particular row of data goes into based on that key.  \\n Now working with date is quite common,  \\n also numeric ranges are often used for range partitioning,  \\n and we could also use alphabetic ranges.  \\n This partitioning is a little bit different,  \\n we basically have a set of particular attributes.  \\n So for example, if we wanted to work with tables  \\n with data from global sensors,  \\n and we wanted to just partition them by continent,  \\n we could do that.  \\n Again, it's not overlapping keys,  \\n but there's some fixed list of values  \\n that we're going to be worked with,  \\n so that's the thing that's distinctive  \\n about list partitioning.  \\n Now another option is hash partitioning.  \\n In here, basically what we do,  \\n we still identify a partition key,  \\n but then we apply a hash to that,  \\n and then we take the modulus of that hash to identify,  \\n for example, which partition to go to.  \\n So with hash partitioning, we might decide,  \\n oh we want 10 or 20 or 30 partitions,  \\n and then we'll pick a partition key,  \\n that partition key will be used  \\n when it's time to determine  \\n where a particular row should go,  \\n and we'll just do the mod operator  \\n on the hash of that partition,  \\n and then based on whether the mod result is zero through 19,  \\n depending on where that lands,  \\n that'll determine which partition it should go into.  \\n Now, hash partitioning is useful  \\n when you don't have like a set of values  \\n that would make sense for list partitioning,  \\n or other numeric or alphanumeric,  \\n doesn't really make a lot of sense for range partitioning.  \\n It's also good if you want to make sure your data  \\n is evenly distributed,  \\n hash partitioning will keep it balanced.  \\n Okay, so now I've switched over to PG Admin  \\n which is a GUI tool we use for working with Postgres,  \\n and what I'd like to do now is to just do a quick example  \\n of creating a table with a range partition.  \\n I want to point out I'm using a Postgres database  \\n and I'm in a database named postgres,  \\n and right now I'm using a schema called iot,  \\n or internet of things,  \\n and so what I'm going to do is create a table,  \\n and I'm going to create it in the iot schema,  \\n and I'm going to call it sensor measurement,  \\n or msmt for short,  \\n and let's see, we will have a sensor ID  \\n which will be an integer, and it should be not null,  \\n and we'll have a measurement date,  \\n which will be a date and not null,  \\n and let's measure temperature,  \\n and for simplicity, we're going to keep this as an int,  \\n we will assume there's no decimal places on temperature,  \\n similarly with humidity, we'll call that an int,  \\n and that wraps up our list of attributes,  \\n but now I want to specify that I want to partition by,  \\n and I want to use range partitioning,  \\n so I'm going to petition by range over my measurement date,  \\n and I'll just execute that to create that table.  \\n And so basically I've created that table,  \\n and now what I need to do is actually create the partitions,  \\n and I do that by using actually the create table statement  \\n with the partition of option,  \\n so let me show what that looks like.  \\n So here we're going to to create a partition,  \\n one for each month.  \\n And so I'm going to create a table  \\n and I'm going to call it iot sensor measurement,  \\n and I'm going to call it year 2021, month 01,  \\n and I'm going to specify that this table  \\n is a partition of my iot sensor measurement table,  \\n and now I need to specify what values,  \\n so I'm going to say for values from,  \\n and I'll specify a date, I'll say 2021-01-01 to 2021-01-31,  \\n click on there, and then I will do the same,  \\n so let's say I want to do it for February as well.  \\n I won't do this for all 12 months,  \\n but I'll just do a couple partitions here,  \\n so you can just see, get a sense  \\n of what the syntax looks like.  \\n So let's see, we'll want to change that to 20,  \\n oh, and I made a mistake here,  \\n this should be 2021,  \\n this should be 2021,  \\n this should be 2021,  \\n 2021, and this should be actually 02,  \\n we'll change to February to 28,  \\n and this to 02.  \\n So we have our main table, which is iot sensor measurement,  \\n and then we specify that we're going to use a date column,  \\n in this case, it's called measurement date,  \\n and now we're going to be creating  \\n the partitions themselves.  \\n Now, if we wanted to have  \\n a whole year's worth of partitions,  \\n we'd have 12 create table partition of statements like this,  \\n but instead of doing that let's just run this,  \\n and it returns, and basically now we have a partition table.  \\n So that's the syntax, and the syntax,  \\n again, is similar for like list partitioning,  \\n hash partitioning,  \\n but again, the core idea here is  \\n you create like a a main primary table  \\n which specifies what the structure looks like,  \\n and then you create whatever partitions you need for that.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2428253\",\"duration\":496,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Materialized views\",\"fileName\":\"2874221_01_04_XR30_materialized_view\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore the benefits of materialized views and requirements to use them. Materialized views are a convenient and easy to maintain way to denormalize data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15707805,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now, another technique  \\n in our data modeling toolbox  \\n that we can use to officially work  \\n with really large datasets  \\n is something called materialized views.  \\n Now, a materialized view  \\n basically persists the results of a query.  \\n So rather than executing a query, say, multiple times  \\n and actually going through all the steps  \\n of the particular execution plan  \\n and then getting our results back,  \\n we can actually save the results  \\n after executing the query once  \\n and then we can go back  \\n and read that saved result sets many times.  \\n The value here is that reading from a single table  \\n can be much faster  \\n than executing a complex query over and over again  \\n to get the same data.  \\n So we can think of materialized views as a form of caching.  \\n It's also a way of getting some of the value  \\n of WideTable denormalization  \\n while still having on the side a more normalized model.  \\n And, ultimately, we're trading space for time,  \\n so you want to consider that in way the benefits  \\n of saving on time while adding more to your storage costs.  \\n Now, there are some kind of general rules of thumb  \\n about when is a good time to use materialized views.  \\n If you have a long-running query,  \\n that's a great candidate right there.  \\n Now, complex queries,  \\n especially when there are a lot of joins,  \\n or maybe you have subqueries  \\n or some common table expressions  \\n or things that might be complicated  \\n for people who aren't necessarily well-versed in SQL,  \\n it would make their life a lot easier  \\n if they could simply query a single table.  \\n So that's another good indicator  \\n that you might have a good candidate for materialized views.  \\n If you're doing a lot of aggregate computation  \\n and other derived data,  \\n that can be a form of long-running query,  \\n but that, again, is a good candidate for materialized view.  \\n Also, when there's really a distinction  \\n in the terms of class  \\n of like read operations and write operations,  \\n which we have, we have batch uploads or streaming processes,  \\n and then we have, you know, users  \\n building these queries and querying data,  \\n when you have that kind of clear partitioning  \\n between reading and writing,  \\n that's another sort of feature  \\n that lends itself to using materialized views.  \\n Now, there are some other considerations  \\n you want to keep in mind  \\n about when not to use materialized view.  \\n Materialized views use a form of consistency  \\n known as eventual consistency.  \\n And, basically, what can happen  \\n is the materialized view can get out of sync  \\n with the underlying tables  \\n that were used to build the query.  \\n So, for example, with our IoT data example,  \\n we might build a materialized view every hour.  \\n Now, since it's an IoT system,  \\n we're probably streaming the data in continually.  \\n So 15 minutes after we build a materialized view,  \\n we're going to have even more data in the database.  \\n Now, that 15 minutes of data won't be incorporated  \\n into the materialized view until the next hour,  \\n assuming we have a one-hour refresh.  \\n So if you can live with the fact  \\n that when you query, say, the materialized view,  \\n you might not have the latest data,  \\n and it may be up to maybe like 59 minutes of missing data,  \\n if that's not a problem, then that's fine.  \\n If it is a problem, then you need to either reconsider  \\n either refreshing more frequently  \\n to get it to something that's really in your tolerance range  \\n or not using materialized views  \\n if you really can't tolerate eventual consistency,  \\n if you need a stronger form of consistency.  \\n You also want to think of the cost of the the update process.  \\n So you're going to be executing an example,  \\n in our IoT example,  \\n going to be updating the materialized view every hour.  \\n Now, is it worth to run a very expensive query every hour?  \\n That's something you want to take into account.  \\n Also, you want to understand  \\n if you can concurrently read a materialized view  \\n while it's being updated.  \\n Now, in Postgres, that's the default,  \\n but it may not be in other databases.  \\n So if you're using  \\n other relational database management systems,  \\n you want to check into that.  \\n And, again, the size versus time trade-off,  \\n you want to consider the size of the materialized view data.  \\n And also, as we mentioned  \\n with regards to eventual consistency,  \\n you want to think about the refresh frequency.  \\n You know, what's the right balance,  \\n and can you strike that balance between eventual consistency  \\n and the benefits of having materialized view?  \\n Now, let's take a look at an example  \\n of creating a materialized view.  \\n I've jumped to a different schema here.  \\n I'm in the landon schema.  \\n And this is information about hotels,  \\n so we have a table about customers,  \\n and we have a table of expenses and locations,  \\n and there are some other data as well.  \\n But I would like to create a materialized view  \\n that will help me understand the expenses  \\n for each of my hotels.  \\n So what I'm going to do is start with a SELECT query.  \\n And so let's say I want to join  \\n my locations and expense tables.  \\n Let's fill the SELECT statement for that.  \\n And let's say I want to select from the locations table,  \\n which I'm going to alias with an l, so I'll use that here.  \\n I want to select hotel id  \\n and, let's see, the city, state or province,  \\n and let's get the country as well,  \\n and I think that's enough from location.  \\n From expenses, we want to get the year,  \\n let's get annual payroll and health insurance costs  \\n and finally the cost of supplies.  \\n And we're going to join or query from landon schema  \\n using the locations table, aliased with an l,  \\n and I'm going to LEFT JOIN,  \\n again, another table from the landon schema,  \\n this time it will be expenses, which we'll alias with an e,  \\n and we're going to join on the locations hotel_id,  \\n where that is equal to the expenses hotel_id, okay?  \\n So I'm just going to execute that query.  \\n And so, as we could see, we have some information  \\n from the locations table and from the expenses table.  \\n Now, something this simple,  \\n you probably wouldn't put into a materialized view,  \\n but I'm just keeping it simple  \\n purely for demonstration purposes.  \\n So now the next step now that I have my SELECT statement,  \\n to create a materialized view,  \\n I can specify CREATE MATERIALIZED VIEW,  \\n and I want to create this in the landon schema.  \\n I'm going to call this mv for materialized views,  \\n and then locations_expenses AS,  \\n and then I'm just going to wrap  \\n the SELECT statement in parentheses  \\n and execute.  \\n So now I have created a materialized view.  \\n Now, one thing I can do  \\n now that I have this materialized view  \\n is, of course, I can actually select from it.  \\n So if I do a SELECT * FROM landon_mv_locations_expenses,  \\n and I execute just that statement,  \\n I'm going to get the results,  \\n the same results I got from the query, of course.  \\n So that's how you create  \\n and basically work with materialized views.  \\n It's no different in a sense in terms of querying  \\n from working with other views.  \\n Now, if you want to refresh the materialized view,  \\n that's pretty straightforward too.  \\n The command is REFRESH MATERIALIZED VIEW,  \\n and then we specify the name,  \\n in this case it will be in the landon schema  \\n mv_locations_expenses.  \\n Now, when we execute this statement, what's going to happen  \\n is it will re-execute that SELECT statement  \\n that we specified in the CREATE MATERIALIZED VIEW statement.  \\n And so, it runs really quickly, of course,  \\n 'cause this is a small table.  \\n But that's basically the steps  \\n of working with a materialized view.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2427262\",\"duration\":225,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Read replicas\",\"fileName\":\"2874221_01_05_XR30_read_replicas\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover when to use read replicas to improve query performance. Read replicas can help improve the performance of extraction, transformation, and load processes.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6316664,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now in some cases, data scientists work  \\n with systems that may also be used  \\n by other people for different reasons.  \\n For example, you might want to use  \\n an online transaction processing system as a source of data,  \\n and you want to grab data continually.  \\n Well, you don't want to put too much stress  \\n on the OLTP system,  \\n which is being used by interactive users.  \\n One way to deal with a situation like that is  \\n for data scientists to work with database administrators  \\n to create something called a read replica.  \\n Now, when we think about database servers,  \\n it makes sense oftentimes we use just a single server.  \\n So for example, if we're in the cloud,  \\n we can decide how much memory we need,  \\n how much storage we need, and pick a virtual machine,  \\n or use a hosted database service  \\n and size the instance that we're interested in.  \\n And let's think of that as our primary.  \\n Now in that scenario, all the reads and all the writes  \\n go to the primary database operation.  \\n So we send data in, we also query from it.  \\n So the primary server is responsible for handling  \\n both read and write operations at the same time.  \\n An alternative option is to create a read replica.  \\n And here the idea is any time  \\n data is written to the primary,  \\n it's also written to another instance of a database server,  \\n full-blown Postgres instance that's on the network  \\n and can receive data from the primary  \\n and essentially keep a copy of all the data.  \\n When we have a read replica,  \\n then we can direct queries to the read replica.  \\n The data's kept up to date pretty quickly  \\n using the write ahead log in the primary.  \\n And so what happens is writes go to the primary,  \\n the primary sends data or replicates data  \\n to the read replica and read operations  \\n are routed to the read replica.  \\n So this allows the primary to do most of the work  \\n with regarding writing, but also offloading  \\n a lot of the read work.  \\n So the reason we use replicas is again  \\n so the primary can really focus on the writes.  \\n And we could even have multiple replicas if we needed that.  \\n So if we had so much read load  \\n that a single replica wouldn't satisfy our needs,  \\n we could add multiple replicas.  \\n And again, this is especially useful  \\n when you have really heavy read loads.  \\n Now, like with materialized views,  \\n we have to think about eventual consistency.  \\n Now, however, unlike materialized views,  \\n when we're working with a read replica,  \\n it's the update operations  \\n on the read replica are much faster.  \\n In Postgres, there's a parameter you can use  \\n to determine the level of consistency before a transaction.  \\n For example, a write operation is considered complete.  \\n The fastest way, but also the least reliable,  \\n is basically what's called the fire and forget method,  \\n which is the primary would just send the data  \\n to the read replica and just assume that it's written there.  \\n Now, there's a potential for the data to be lost  \\n on the read replica because of some error,  \\n but the primary's not waiting around for an acknowledgment.  \\n The slowest method enables strong consistency  \\n and basically the primary waits  \\n until it's guaranteed to have saved the data  \\n to the primary data storage,  \\n as well as the replica storage  \\n and get an acknowledgement  \\n that it has been stored like that.  \\n Now, that's the safest in terms of data integrity,  \\n but it's also the slowest  \\n in terms of when a transaction completes.  \\n So that's a factor you need to consider  \\n when using read replicas.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2425339\",\"duration\":86,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Design a data model for analytics\",\"fileName\":\"2874221_01_06_XR30_CH30_challenge\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Given a set of data analysis requirements, create a data model to support the requirements. A data scientist must know how to translate business requirements into data models that support analysis.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2734033,\"solution\":false,\"welcomeContent\":null,\"challenge\":true,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (bright music)  \\n - [Instructor] Here is a data modeling challenge.  \\n Let's assume you're a consultant  \\n and you're working with an internet of things,  \\n or an IoT company,  \\n that is collecting streaming data from thousands of sensors  \\n and they collect this data every minute.  \\n For them, low write latency is essential.  \\n So as soon as the data comes in,  \\n it needs to be written because we don't want to  \\n basically have a backlog of data or anything like that,  \\n and it's also important that the write latency  \\n be fairly consistent so we don't want,  \\n you know, some periods where it's very bursty  \\n and we have very low latency,  \\n and then other times it's more prolonged, it's more delayed.  \\n Now, at the same time,  \\n this IoT company has a team of data scientists  \\n and they're going to be performing different kinds  \\n of time series analysis  \\n including roll-ups of aggregate data.  \\n So, say for example, by each sensor, what's the,  \\n what are some aggregate measures over, you know,  \\n hours and over days?  \\n And with regards to that analysis,  \\n those data scientists need access to any data  \\n that's older than one hour.  \\n Anything newer than that isn't necessarily required  \\n for this aggregate analysis.  \\n So the challenge is to design a model  \\n that would support these requirements.  \\n Now this is a high-level model,  \\n just outline what kind of structures or design patterns  \\n would you use to address  \\n the business requirements identified here?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2427263\",\"duration\":148,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Design a data model for analytics\",\"fileName\":\"2874221_01_07_XR30_SO30_solution\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore the solution to the data modeling challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4749178,\"solution\":true,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Here is a solution  \\n to the data model challenge.  \\n So, first of all, the sensor data should be written  \\n to a table that models the data sent from the sensors.  \\n So, for example, if a sensor is sending a sensor ID,  \\n a timestamp, and then measure one, measure two,  \\n measure three, we should have a table  \\n with those five attributes: sensor ID,  \\n timestamp, and then the three measures.  \\n Now, likely the table will be partitioned by time.  \\n Now, we didn't explicitly state in the requirements  \\n the need for partitioning and we didn't discuss,  \\n for example, how long we wanted data  \\n to be kept in this table and if data past  \\n a certain age should be rolled off.  \\n Now, this is something you need to keep  \\n in mind when you're doing data science,  \\n and you're working with data modeling at the same time.  \\n Not all of the requirements are going  \\n to be outlined for you, the people that are,  \\n say, domain experts may not be aware of the kinds  \\n of factors that we think about when there's designing.  \\n So, any time you're presented with sort of a challenge,  \\n like we have here, it is important  \\n to not just take what's given, but also interrogate  \\n and probe other questions so, for example,  \\n how long should data be persisted?  \\n Now, this is a good use case for materialized views  \\n because materialized views can be used  \\n to generate a persistent view, or a materialized view  \\n of the aggregated data and whatever that aggregation is.  \\n For example, you might have one materialized view  \\n that is used for hourly aggregates,  \\n and another materialized view  \\n that's used for daily aggregates.  \\n And, of course, you want to refresh  \\n these, at least, once per hour.  \\n Now, with regards to the daily materialized views,  \\n we may not need to refresh at all  \\n because once the day has passed,  \\n and we build a materialized view with that day's roll-ups  \\n we shouldn't need to change those.  \\n Will data scientists be using that daily materialized views  \\n for partial results throughout the day?  \\n If that's the case, then you want  \\n at least that materialized view refreshed more frequently.  \\n Now, also, another factor we didn't delve down  \\n into too deeply with regards to the requirements  \\n is do the data scientists need access  \\n to the low level detail as well as the aggregates?  \\n Because if they need access to low level data  \\n like the raw data that comes from the IoT sensor,  \\n well, because of the low right latency requirement  \\n we don't want to bog down the primary server  \\n with a lot of ad hoc query, that would be  \\n a good use case for a read replica.  \\n \\n\\n\"}],\"name\":\"1. Data Modeling: Tables\",\"size\":68533912,\"urn\":\"urn:li:learningContentChapter:2422419\"},{\"duration\":981,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2424380\",\"duration\":232,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"B-tree indexes\",\"fileName\":\"2874221_02_01_XR30_b_tree_indexes\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Learn how b-tree indexes work and when to use them. B-trees are widely used and can significantly improve read performance.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6716368,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In addition to designing tables,  \\n part of data modeling is developing an indexing strategy.  \\n So let's take a look at indexes,  \\n different types of indexes,  \\n and how they apply when we're working  \\n with analytical queries.  \\n We'll start with B-Tree indexes.  \\n So, indexing is used primarily  \\n to reduce the amount of work we need to do  \\n when we have to go and fetch data.  \\n Now, typically this means we don't want to be  \\n scanning a lot of data blocks,  \\n so indexes are used to help us reduce that.  \\n Now, there is a cost associated with this  \\n because we have to maintain these indexes.  \\n So what that means is we're going to take up  \\n additional space, but we're also going to be doing  \\n additional rights, so anytime we're loading data  \\n or deleting data, we're going to need to update the index.  \\n Now, some things to keep in mind  \\n is that when we index a column, the higher the cardinality  \\n which means, you know, just the number  \\n of distinct values, really influences how well  \\n the index helps improve query performance.  \\n So for example, if you had a table of codes  \\n and the codes were numbers, one through 10,  \\n the codes are evenly distributed  \\n you can expect about 10% of the table to be returned  \\n if you look up a particular code.  \\n So 10% is not bad but it doesn't reduce a lot  \\n say compared to if the cardinality was say a 1000,  \\n and if you looked up a particular code  \\n you could reduce the amount of work that you need to do  \\n to maybe 1000th of the size of the table.  \\n Now, we're going to be talking about indexing,  \\n but again I want to point out in terms of things  \\n that are different in analytical databases,  \\n indexing is not used in analytical databases  \\n like Google BigQuery or AWS Redshift,  \\n again, they have different strategies.  \\n Now, there are several different types of indexes,  \\n we're going to pay attention to three broad categories,  \\n the B-tree, the bitmap, and the hash index,  \\n and then we're also going to look at special purpose index.  \\n So first, let's take a look at B-trees indexes.  \\n Now the B in B-tree stands for balanced.  \\n And what this means is that  \\n what we're trying to do is keep a record  \\n or information about rows of data  \\n by capturing a small amount of information,  \\n that's the attribute or attributes that we're indexing.  \\n And one of the things we want to be able to do with an index  \\n is to be able to look up a value very quickly.  \\n Well, B-trees are sort of the workhorse of indexing,  \\n they work really well in many different cases  \\n and what they give us is basically the ability  \\n to look up a value in logarithmic time.  \\n So let's see how that works.  \\n Say, we are indexing say a column that has one to 100,  \\n and the first one we index  \\n or the item right in the middle is a 50,  \\n and that makes sense because we want to be able to split  \\n sort of divide and conquer and with a balanced tree  \\n the way it works is the next time we insert a value,  \\n if the value is less than 50  \\n then we're going to put it in toward the left node  \\n and if it's greater than 50  \\n we're going to put it toward the right note.  \\n So for example, 25 would go to the left  \\n and 75 would go to the right.  \\n Now, let's say you want to insert a value  \\n say between 25 and 50,  \\n in that case, it's going to be less than 50,  \\n so we're going to go to the left, we'll go toward 25  \\n but then if it's greater than 25  \\n we're going to go to the right.  \\n So in this example, you know,  \\n 37 is placed first to the left  \\n and then to the right.  \\n And we can continue this example,  \\n and what we can see here is even though  \\n there could be a 100 different nodes in this tree,  \\n we're never going to have to go more than log n or log 100  \\n steps to actually find the value.  \\n So that's one of the big values  \\n of B-tree indexes is it gives us that order  \\n and look up time.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2424381\",\"duration\":183,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Bitmap indexes\",\"fileName\":\"2874221_02_02_XR30_bitmap_indexes\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover why bitmap indexes are used in analytical queries. Bitmap indexes can significantly improve queries over large data sets that require filtering.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5642354,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's take a look at bitmap indexes.  \\n Now, a bitmap index is relatively simple.  \\n It's quite different from a B-tree index  \\n and the basic idea is when we have a column  \\n that has a small number of possible values,  \\n we might be able to map those values  \\n or encode them in a bit string.  \\n So for example, here we have a table  \\n and there's an ID column  \\n and another column which is_union_member  \\n and you can see we list yes or no or null.  \\n And we could use literally those strings.  \\n Alternatively, we could map a yes to a one,  \\n a no to a zero and a null to a zero zero.  \\n And that's what we've done  \\n in the two columns on the right.  \\n Now, it doesn't have to be just two values.  \\n You can have a larger number of possible values.  \\n For example, in this new table here on the right  \\n where we have an ID and pay_type,  \\n we have three different kinds of pay type:  \\n salary, hourly, and contractor.  \\n Now, you'll notice the salary column has a one in it  \\n when the pay type is salary  \\n and a zero and a zero in hourly and contractor.  \\n Similarly, when the pay type is hourly,  \\n there's a one under the hourly column  \\n and a zero in the other two columns.  \\n And for a contractor,  \\n there's a one under the contractor column  \\n and a zero in the salary and hourly.  \\n So you can see, we can map these, for example,  \\n the is_union_member a simple yes/no  \\n or a list of value kind of pay_type into a bit string.  \\n And one of the reasons we would do that  \\n is because it makes it very easy to do  \\n kind of Boolean operations with bit operators.  \\n So, for example, looking for someone who is a union member  \\n and paid hourly would be a matter of doing  \\n some bit operations on the bitmap index.  \\n So we use bitmap indexes typically  \\n when there's a small number of possible volumes in a column,  \\n so a low cardinality.  \\n When we're going to be filtering by bitwise operations,  \\n such as and, or, and not,  \\n so if you're going to apply those operators,  \\n and the thing to keep in mind is that the time to access  \\n the selected rows using this index is based on the time  \\n that it takes to perform bitwise operations,  \\n which of course are very fast.  \\n Now, typically, bitmap index  \\n is when they are explicitly declared,  \\n like if you're working with a database  \\n that supports the explicit declaration of bitmap indexes.  \\n You typically save that for read-intensive use cases,  \\n like data warehousing or data science operations  \\n where there's relatively few writes.  \\n And that's because it can be expensive  \\n to build a bitmap index in some cases.  \\n So in terms of bitmap index availability, as I mentioned,  \\n some databases allow you to create them explicitly,  \\n but Postgres does not.  \\n However, Postgres does sometimes build bitmap indexes  \\n on the fly as is needed.  \\n So the decision to use bitmap indexes or not in Postgres  \\n is made by the query plan builder.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2428254\",\"duration\":80,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Hash indexes\",\"fileName\":\"2874221_02_03_XR30_hash_indexexes\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to use hash indexes for equality filter conditions. Hash indexes are used with large data sets to improve the performance of equality operations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2425332,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now a third type of index is the hash index.  \\n Now hash indexes use hash functions.  \\n Now hash functions are mappings of arbitrary linked data  \\n into a fixed-size string.  \\n Now hash values are virtually unique  \\n and the hash value is a function of the input, of course.  \\n So even slight changes in inputs will produce a new hash.  \\n So for example, if you're hashing a string  \\n and you add a space at the end,  \\n that would give you a totally different hash  \\n than the one you had originally.  \\n And here's some examples.  \\n So some things to consider.  \\n The size of the hash value depends on the algorithm used.  \\n So that's typically determined  \\n by the database management system designers.  \\n There's no ordering or preserving with hash functions  \\n so they could appear in just any random order,  \\n and similar inputs, again,  \\n can have vastly different outputs  \\n so you can't make any assumptions about the value  \\n that a hash function generates  \\n other than it's highly likely to be unique.  \\n What this means is hash indexes  \\n are only useful for equality operations.  \\n So if you're looking up, say,  \\n almost like key value kind of things,  \\n hash indexes can work very well in those cases.  \\n In the case of Postgres,  \\n hash indexes can be smaller than B-tree indexes,  \\n and they're built with at about the same speed  \\n or about the same pace as one builds B-tree indexes.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2427264\",\"duration\":113,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"GiST and SP-GiST indexes\",\"fileName\":\"2874221_02_04_XR30_gist_spgist\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how lossy indexes and space-partitioned indexes work. These specialized indexes are useful for geometric and geolocation queries.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3353274,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] GiST and SP-GiST indexes  \\n are specialized index provided by Postgres.  \\n Now, GiST stands for Generalized Search Tree,  \\n and it's basically, it's a balanced tree structure method.  \\n And GiST is used as a template  \\n to actually implement other indexing schemes.  \\n So depending on our data structure, we can use a GiST index.  \\n So for example, one could build a B-tree index,  \\n which is a self-balancing tree, or an R-trees  \\n which are used with multi-dimensional data.  \\n So these, for example, with geographic coordinates.  \\n Now, that GiST is used in Postgres for indexing  \\n things like hstore and ltree.  \\n These are two data types that we're going to delve into  \\n much more deeply, later in the course.  \\n But I just wanted to point out  \\n that GiST indexes are used for these specialized data types.  \\n Now, when we talk about indexes and data types  \\n we also have to talk about operators  \\n and what kind of objects can the operators apply to.  \\n So in the case of these indexes,  \\n we're talking about things like;  \\n operations have geometric shapes like boxes, circles,  \\n and also internet address points  \\n if we're talking about distances and dimensional planes,  \\n as well as polygons and ranges.  \\n And it also works with text queries,  \\n and that includes a data type known as text vectors as well.  \\n Now SP-GiST is a space-partitioned GiST  \\n and it supports partitioned search trees.  \\n And it's typically used for non balanced data structures.  \\n So we might use these  \\n with things like quadtrees or k-d trees,  \\n which again are used in multidimensional space.  \\n And SP-GiST can also be used to develop custom indexes.  \\n Now, the operators here include things like  \\n things for operations  \\n like K dimensional and quad operations, range operations.  \\n Again, geometric things like boxes and polygons  \\n as well as internet address data types.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2421481\",\"duration\":248,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"GIN and BRIN indexes\",\"fileName\":\"2874221_02_05_XR30_gin_brin\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover when to use generalized inverted indexes and block range indexes. These indexes are used with composite indexes and queries search for patterns within that composite value.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7452551,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] One of the reasons I like working  \\n with Postgres is that it has specialized indexes  \\n for data types that are more complex  \\n than we typically work with.  \\n So for example, there's something called a GIN index  \\n which stands for generalized inverted index.  \\n And that's used when we have to index data  \\n that are inside other elements or other pieces of data.  \\n And we call these things composite values  \\n and we use a GIN index when we need to index the values  \\n which are in that composite item.  \\n So for example, words in a document,  \\n the composite data structure is the document,  \\n but in that document, each of those words  \\n is a individual element,  \\n which we are interested in indexing.  \\n Now the index stores the data and key value pairs  \\n and the key is the element value  \\n and the posting list is a set of row IDs  \\n where that key occurs.  \\n So it's very fast to go from key to actual data location.  \\n Now GIN index has a large number  \\n of built-in operator classes  \\n that allow you to do things like operate on arrays,  \\n JSON data structures, and in particular  \\n JSONB data structures as well as text vectors.  \\n Now some tips when you're using GIN indexes,  \\n keep in mind that the insertion can be slow  \\n and this is because many keys may be inserted for each item.  \\n So if you have a large number of documents  \\n and you are indexing all of them, then each of the words  \\n and each of the documents need to be indexed.  \\n So that's why insertions can be slow.  \\n So for a very large bulk operation  \\n so if you're doing like an initial data load,  \\n it's likely faster to just drop the index  \\n and then recreate the index  \\n after you've loaded the entire dataset.  \\n An alternative way to do this is to  \\n let Postgres postpone a lot of the indexing work  \\n using something called temporary lists.  \\n Now temporary lists are eventually inserted  \\n into the index using kind of  \\n a optimized bulk insertion technique.  \\n So basically the temporary list we're keeping track of  \\n but we are not actually committing it to the index just yet.  \\n Now the disadvantage here  \\n is that the temporary list has to be searched  \\n in addition to the regular index  \\n anytime there's a lookup that uses the index.  \\n So in that case,  \\n large temporary lists will slow searches significantly  \\n so you're trading off,  \\n are your searches going to be slow  \\n or your load is going to be slow.  \\n Now, when you create index,  \\n you can disable the fast update parameter  \\n to disabled temporary lists.  \\n So if you really find that during the loads,  \\n the temporary lists are becoming problematic  \\n with respect to your query performance,  \\n try disabling the fast update perimeter.  \\n Now another index type that Postgres support  \\n is called a BRIN index, which stands for block range index.  \\n And we typically use BRIN indexes with very large tables.  \\n And within these tables, the column data  \\n has some kind of correlation with physical location  \\n like a postal code or dates where dates close  \\n to each other are going to be typically stored  \\n in the same data block near data blocks.  \\n And block ranges are pages  \\n that are physically adjacent in a table.  \\n So when we talk about a block range index,  \\n we're talking about ordering these data blocks  \\n and what the BRIN index does  \\n is it stores summary information about the block ranges.  \\n So you might have things like the max postal code  \\n and min postal code in a particular block  \\n or max statement data block.  \\n So you can end up storing very small amounts of data  \\n in a BRIN index, but still get really good performance.  \\n Some things to keep in mind with BRIN indexes,  \\n the entries are per entire block ranges,  \\n select the max and min,  \\n so it's not for individual elements.  \\n But the trade-off here is we don't have details  \\n about individual elements, but it allows us to quickly scan  \\n and skip large segments of a table when we're searching.  \\n And this is really important if you have ordered data  \\n like if you're working with time series data  \\n or you have other data where you can make use  \\n of kind of this min and max idea.  \\n Now BRIN operators have a whole bunch  \\n of operators like date min and max,  \\n also works with characters, floating point numbers,  \\n timestamps as well as UUIDs.  \\n And there are even more,  \\n which you can find in the Postgres documentation.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2421482\",\"duration\":37,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Choosing an optimal indexing strategy\",\"fileName\":\"2874221_02_06_XR30_CH30_challenge\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Given a set of data analytics requirements and a data set, define an optimal indexing strategy. This challenge assesses how well you understand how to uses different index types.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1119321,\"solution\":false,\"welcomeContent\":null,\"challenge\":true,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (bright music)  \\n - [Instructor] Alright, here's a challenge around indexing.  \\n Let's assume you have received a large dataset  \\n with insurance claims details.  \\n Now you want to be able to upload  \\n or ingest that data into an existing database  \\n that you're already using for your data science analytics.  \\n Now each claim has a unique claim identifier  \\n along with about 12 columns of data.  \\n The existing database has a table  \\n of all claim numbers ever generated.  \\n How would you index the new claims detailed  \\n to optimize a join operation on the claim ID?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2425340\",\"duration\":88,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Choosing an optimal indexing strategy\",\"fileName\":\"2874221_02_07_XR30_SO30_solution\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore the solution to the indexing challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2498580,\"solution\":true,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (bright music)  \\n - [Instructor] We have really two options we can think of.  \\n Now, by default, when we build an index in Postgres,  \\n we use a B-tree index.  \\n Now B-tree indexes are generally good choices  \\n because they're relatively fast to work with  \\n because, on average, you're going to get a time  \\n basically relative to the logarithm  \\n of the size of the table,  \\n so even a table with a large number of rows  \\n is going to be able to find  \\n the index value relatively quickly.  \\n Now because we're working with claim IDs  \\n and claim IDs are unique,  \\n that means that at most,  \\n one row in each table will have that particular claim ID.  \\n Well, in that case, since we're not going to be doing things  \\n like range scans or things like that,  \\n which work really well with B-tree indexes,  \\n we can use a hash index.  \\n A hash index uses a function that takes a column value,  \\n like a claim ID, and then converts it into a 32 bit integer,  \\n and we can use that 32 bit integer  \\n as basically like an address or an index into a table,  \\n and so, rather than kind of walking through a tree  \\n along multiple steps,  \\n we can immediately go to the storage area  \\n that has the location of the data block we're looking for  \\n or actually points to the data block.  \\n So definitely in this case, because claim ID is unique,  \\n and we're not doing any kind of range scanning,  \\n we're just doing a one-to-one lookup,  \\n that's a good use case for a hash index.  \\n \\n\\n\"}],\"name\":\"2. Data Modeling: Indexes\",\"size\":29207780,\"urn\":\"urn:li:learningContentChapter:2424385\"},{\"duration\":1767,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2425341\",\"duration\":461,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"EXPLAIN and ANALYZE commands\",\"fileName\":\"2874221_03_01_XR30_query_plan\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Discover what the ANALYZE and EXPLAIN commands do and why to use them. These two commands are used to generate execution plans and are widely used as part of the query tuning process.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14119558,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now as data scientists working with SQL,  \\n it's sometimes helpful to understand a little bit  \\n about how the database engine  \\n actually executes SQL statements.  \\n And that's particularly important  \\n if we're trying to optimize the performance of our queries.  \\n So let's start from first principles.  \\n SQL is basically a declarative language.  \\n So we specify what we want.  \\n So for example, I want these certain set of column  \\n returned from this particular table  \\n subject to this filter criteria.  \\n Now that again is stating what we want.  \\n I'm not saying in any way, how the database should go  \\n about doing that.  \\n Limiting the data we have to scan to a subset of rows.  \\n So many languages like Python  \\n or Java can be used procedurally.  \\n And when we're doing that  \\n we're specifying how to do something.  \\n So we're actually like directly manipulating data structures  \\n and determining like the sequence of steps.  \\n So one of the things that database engine does for us  \\n is it figures out those sort of procedural steps  \\n and then execute some for us.  \\n So one of the simplest kind of procedural steps  \\n is scanning a table.  \\n And basically what happens is we say,  \\n start at the beginning of table, fetch a row  \\n and then move on to the next row.  \\n And maybe we're doing a comparison.  \\n For example, we're looking at a status level  \\n and we want to filter, show me all the customers  \\n who are status level gold.  \\n Well, that's, what something like this kind of simple scan  \\n could do.  \\n We'll simply walk through each row in a table  \\n and check the particular column that's part of the filter.  \\n And then if it passes the filter, we return it.  \\n So again, the procedure is fairly straightforward.  \\n So in a lot of ways, scanning is simple.  \\n We just look at each row,  \\n fetch the data block that contains the row  \\n and apply the filter or the condition.  \\n So if we're thinking in terms of, well what's the order  \\n what's the time complexity of this?  \\n Well, the cost is basically based on the number of rows  \\n in the table.  \\n Now I should say that scanning isn't always as simple.  \\n I'm making the assumption that we're working  \\n with a database that uses a row storage  \\n or a row orientation.  \\n Now, some databases like AWS Redshift or Google Big Query  \\n use column in their storage.  \\n And again, we typically see columnar storage  \\n with data warehouses.  \\n So some of the things that I say here about scanning  \\n and row fetching certainly apply when we're using Postgres  \\n but maybe not something  \\n if you're working with say a petabyte scale data warehouse.  \\n So cost is based on the number of rows.  \\n Now that doesn't mean scanning is always bad.  \\n Some scanning can be really efficient when tables are small.  \\n So for example, rather than use an index  \\n and look up something in the index  \\n and then go fetch something from the table,  \\n it may be faster just to scan a row  \\n and not maintain an index.  \\n Now, scanning large tables can be efficient  \\n if we're only scanning them very few times.  \\n So for example, say you have a very large table  \\n and you have to scan it once.  \\n It's probably not worth building an index on that  \\n because of the cost of building the index,  \\n the storage that's required as well as the time.  \\n So even with very large tables,  \\n there may be times when scanning is efficient.  \\n But in general, with large tables that we query repeatedly,  \\n that's not efficient.  \\n So we want to find some other way  \\n to work with those tables  \\n rather than doing sequence scans or full table scans.  \\n One way to do that is to use indexes.  \\n And of course, we've already talked about this  \\n but just to recap, indexes are ordered,  \\n and they're faster to search for an attribute value  \\n than say scanning the whole table.  \\n And since the index is basically a set of attributes  \\n and an a pointer to a particular row  \\n that we can use the index say for our filter  \\n and use the index and do the filtering based on the index  \\n rather than doing the filtering  \\n based on the data in the rows.  \\n In the case of using 20 partitions.  \\n Now, another thing we want to watch  \\n when we're thinking about performance is joining tables.  \\n So of course the basic idea with joining tables  \\n is that we have data in say two tables  \\n and we want to somehow select a row from one table  \\n and link it or join it to a row in another table  \\n and basically produced a projection  \\n or a new view over that data.  \\n So the question is, how do we go about matching those rows?  \\n Well, one table is going to have a foreign key in it,  \\n and the other table is going to have of course the primary key  \\n in the other table.  \\n And so the question of how to match rows  \\n becomes how do we match keys?  \\n So there are a few ways to do this.  \\n There is a technique known as the nested loop join.  \\n And that's whether the database  \\n and Jim will compare all rows in both tables to each other.  \\n There is the hash join, in which we calculate a hash value  \\n of a key and then join based on matching hash values.  \\n And then finally, there's something called sort merge join  \\n in which we sort both tables and then join rows  \\n while taking advantage of the order.  \\n So for a nested loop, the basic sequence of steps  \\n is we loop through one table and then for each row,  \\n we loop through the other table  \\n and at each step we compare the keys.  \\n So the nice thing about nested loop joins  \\n is it simple to implement but it can be expensive  \\n if the tables are large.  \\n Nested loop join is often a good choice  \\n when the tables are small.  \\n Now a hash join computes the hash value of keys  \\n in a smaller table.  \\n It then stores those hashes in a hash table  \\n and the hash table has the hash value  \\n in some row attributes.  \\n And then we scan the larger table.  \\n And we find rows from the smaller hash table  \\n that matched with the hash value of the hash and key  \\n on the larger table.  \\n With sort merged join, we sort both tables,  \\n we compare the rows like we do with nested loop join  \\n but because both tables are ordered,  \\n we can stop when it's not possible to find a match  \\n later in the table because of sort order.  \\n And this is nice because we scan the driving table  \\n only once.  \\n So we've looked at indexes and we've looked at joins  \\n with regards to kind of things we look at  \\n with regards to query optimization.  \\n One of the things that the database engine  \\n or the the plan builder looks at  \\n is information about the tables themselves.  \\n And so the plan builder relies on statistics about the data  \\n in the tables that we're working with.  \\n Now, usually statistics are kept up to date.  \\n And in Postgres there is a process called autovacuum,  \\n which does that and also kind of cleans up  \\n after some deletes and compresses data  \\n and does some other sort of background,  \\n housekeeping kinds of things.  \\n Now, sometimes statistics can get out of date.  \\n So there is a command called ANALYZE  \\n which you can run, which will update statistics.  \\n So if for some reason you're looking at a query plan  \\n and you can't quite figure out  \\n why would the query plan builder  \\n choose this really inefficient way if it looks,  \\n like for example, it's doing a nested loop join  \\n when maybe a hash join would be more appropriate.  \\n A possible reason for that and outside chances  \\n for some reason, the statistics are out of date,  \\n so just run the ANALYZE command.  \\n And there are different parameters  \\n you can analyze whole schemas or tables.  \\n So you can definitely get more detail  \\n on the ANALYZE command.  \\n That's not something you necessarily need to use a lot,  \\n but if you do, there are some options  \\n with the ANALYZE command  \\n so you can either scan an entire schema  \\n or really target a particular table for that.  \\n And some things to keep in mind.  \\n When a table has a relatively small amount of data,  \\n the ANALYZE will look at all of the data.  \\n When it's a very large table, it will only sample the data.  \\n So it'll build statistics out of a sampling.  \\n So if you were to run ANALYZE say two times  \\n over a very large table  \\n you might get slightly different statistics  \\n but that's because of sampling.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2424382\",\"duration\":256,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Generating data with generate_sequence\",\"fileName\":\"2874221_03_02_XR30_generate_series\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to use generate_series to create a test date for an IoT time series use case.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8726836,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Okay, before we get into using explained plan  \\n let's create some data for us to work with.  \\n Now, the first thing I want to do is just paste in some code  \\n that we have seen already earlier.  \\n Basically what I want to do is recreate  \\n an IOT sensor measurement table,  \\n and create a couple of partitions.  \\n I won't go over all the details  \\n 'cause I've covered this in an earlier video  \\n but I do want to point out a couple of things.  \\n In this case, we're going to use measurement date.  \\n We're actually going to make that a timestamp  \\n so we can include hours and minutes as well.  \\n And I just want to point out in terms of creating partitions  \\n we want to keep in mind that this last value  \\n that at the end of the range is not inclusive.  \\n I always forget this.  \\n So for example, if I set this date to be 2021 01-31,  \\n that's a perfectly valid range to go from January 1st  \\n to January 31st, but it's not inclusive.  \\n I always forget that.  \\n So I just want to point that out.  \\n You want to have overlaps here.  \\n So for example, we'll go from January 1st  \\n to February 1st and then in our next petition  \\n we'll go February 1st to March 1st.  \\n Okay, and I also have a drop table statement here  \\n in case that's left around and okay,  \\n so I have created my table and it is partitioned.  \\n Now, the next thing I want to do is create some data.  \\n Now we're working with time series data.  \\n So there's a really useful set function  \\n in Postgres called generate series.  \\n So let's take a look at that.  \\n Now, generate series works with a select statement.  \\n So I'm going to say select star from generate series.  \\n We're just going to call the function.  \\n Now the arguments I pass in are integers or dates.  \\n And the reason is because generate series  \\n can work with either integers  \\n and create a series of integers  \\n say like one to a hundred  \\n or it can create a series of dates or timestamps.  \\n So we'll start by creating,  \\n let's say a hundred rows for (mumbles).  \\n And we'll say that is as T1 for table one.  \\n And what you'll notice here is it's simply generates a list  \\n of a hundred numbers.  \\n So that's perfect, that's great.  \\n That's one of the things we'll need.  \\n Those integers would make very good sensor IDs  \\n for our table.  \\n Now, the other thing we can do with generate series  \\n is to pass in dates or timestamps.  \\n So I'm going to pass in, let's see, 2021-01-01  \\n and we're going to work with hours and minutes.  \\n So I'll say zero hours and zero minutes.  \\n This is the start.  \\n Now I'm going to be explicit with Postgres  \\n and tell it to convert this string to a timestamp.  \\n And I can do that using the syntax, colon, colon  \\n and its specifying a data type, in this case timestamp.  \\n And that'll do the casting  \\n or the implicit conversion that we need.  \\n And I'm going to put as my N date 2021,  \\n let's go into February  \\n but let's only go up to say the 15th  \\n and we'll use 00 and 00, the stopping hour and minute.  \\n And again, let's cast that at a timestamp.  \\n And then, the one of the thing we need to put  \\n when we working with generate series  \\n and we're working with time series  \\n is put in some kind of interval specification.  \\n So I'm going to generate a timestamp for every minute.  \\n So I would say one minutes  \\n and let's call this a stable two, the series,  \\n let's see, kind of correct.  \\n And what you'll notice here  \\n as I'm now generating a series of timestamps  \\n that are increment in one minute steps.  \\n So as you go along, we'll see  \\n that we're still in 01, 01 and we're going down  \\n so we'll have about 60 timestamps  \\n for the first hour and so on.  \\n The reason this is going to help us  \\n is we're going to be able to use this generate series  \\n to actually create our generated data.  \\n So let's take a look at that in the next video.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2427265\",\"duration\":395,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Generating time series data\",\"fileName\":\"2874221_03_03_XR30_generate_data\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to analyze the performance of a basic query. WHERE clauses are common and SQL developers should understand how they are executed.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13997080,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now I'm starting off  \\n from where I left off in the previous video.  \\n And here we have a select statement that uses  \\n generate series to create some timestamps for us.  \\n Now, what I'd like to do at this point is actually use this  \\n and another generate series to generate a series  \\n of sensor IDs, and then a set of timestamps.  \\n But what I want is I want each sensor ID  \\n to have all of the timestamps.  \\n So let's see how we can do that.  \\n So first of all  \\n I'm going to be turning this select statement  \\n into a subquery.  \\n And just to make it a little less spread out  \\n I'll just do a little bit of formatting here, okay.  \\n Now I also want to be able to generate the sensor ID.  \\n So I'm going to put in a select star from generate series  \\n and let's go from one to 100  \\n and this is going to be a subquery.  \\n So I'm going to say, let's call that as T1  \\n and here this will be a sub query as well.  \\n And that'll be as T2.  \\n So by saying a subquery  \\n I mean a subquery in the from clause.  \\n So I'm going to select star from  \\n these two subqueries.  \\n Oh, sorry.  \\n I didn't mean to execute the query.  \\n I meant to indent these.  \\n There we go.  \\n Now, first thing I need to do is get my syntax correct  \\n by putting a comma there  \\n but you'll notice here we have two tables  \\n or two projections actually in the, from clause  \\n and there's no joint statement  \\n and that's intentional  \\n because when we don't put a joint explicit joint clause in  \\n and we have two tables,  \\n then Postgres is going to generate the Cartesian product  \\n or basically a one to all mapping  \\n for the rows in each table.  \\n So here in our first subquery  \\n we're going to generate 100 rows.  \\n Each one of those rows is going to be joined  \\n to each of the time series that are generated  \\n by the second select statement.  \\n So let's run that and see what we get.  \\n Now, what we'll notice here is  \\n we have the generated series begins with one,  \\n which is as expected.  \\n And for the one, we also have each  \\n of the time series that we saw before  \\n that's generated, you know, 60 per hour  \\n each one of those is joined to the integer that we generate.  \\n So we're going to have a large number of timestamps  \\n and generated sensor ID numbers.  \\n So this is great.  \\n Now let's take a look at our table actually  \\n and let's do a refresh to make sure it's up to date.  \\n And there's sensor management.  \\n Let's look at the columns.  \\n So we have our sensor ID  \\n and our measurement date or a timestamp.  \\n So the next thing we need to do is generate some random data  \\n for temperature and humidity.  \\n So random of course, is a good choice of random function.  \\n So let's think about this.  \\n How can we use these generated sensor IDs and timestamps  \\n but then also easily get data for temperature and humidity?  \\n Well, one way to do that is to use a common table expression  \\n and create a common table expression  \\n for this query right here.  \\n So to do that, we'll say with  \\n and let's call this sensors date times as  \\n and let's indent this a little bit  \\n and we'll close up that.  \\n So now we have something called sensor datetimes  \\n which we can use in another select statement.  \\n So we'll select something from sensors datetimes, okay.  \\n Have that spelled right.  \\n And let's say here set with ST.  \\n So the first thing I want to do  \\n is I definitely want to select everything from  \\n that's in sensor datetime.  \\n So I'll just use SD star for that.  \\n And now I want to generate some random numbers  \\n for let's start with temperature.  \\n So we're going to use the random function  \\n and random returns, a decimal between zero and one.  \\n And let's say, I want my temperatures to be  \\n say between zero and 30 degrees Celsius.  \\n So I'll multiply by 30.  \\n Now I specified temperature and humidity to be integers.  \\n So I'm going to take the floor function  \\n which just essentially drops off the decimal part.  \\n The counterpart is ceiling.  \\n If you want to, you know, have a decimal number  \\n and then go up to the next highest number  \\n that would be ceiling, but we'll just use floor here.  \\n And I'm going to alias that as temperature  \\n and we'll follow a similar pattern for humidity.  \\n We'll use a random number  \\n and let's say humidity  \\n and the up to 80%, and we'll say that as humidity.  \\n And so now what we're going to do is we're going to  \\n select everything from our sensor datetimes  \\n and then we're going to generate a random number  \\n for temperature and humidity for each row.  \\n So let's run this I'll double check commas,  \\n syntax looks correct.  \\n Let's give that a try.  \\n Okay, so let's take a look  \\n and as expected.  \\n So now what we have is we have our sensor IDs.  \\n We have all of our time series  \\n and now we have a temperature  \\n and humidity value for each row.  \\n So now we have our data, we've created our data.  \\n So the next thing we have to do is just simply save this  \\n or basically store it into our sensor measurement table.  \\n So for that, we can just use an insert statement  \\n and we can say insert into IOT schema.  \\n And the name of our table is sensor measurement, or MSMT.  \\n And then we wrap this in parentheses  \\n and wrap that in parentheses.  \\n And just for readability we'll indent this  \\n and we can just stop checking.  \\n That looks good and we'll execute this.  \\n Okay, so we have created our data.  \\n What you'll notice here is  \\n that we have about six and a half million rows inserted.  \\n So we have our data and we have it inserted into our table.  \\n So now we're ready to work with explain plan.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2427266\",\"duration\":166,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Analyzing a query with WHERE clauses and indexes\",\"fileName\":\"2874221_03_04_XR30_explain_where\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to analyze the performance of a basic query. WHERE clauses are common and SQL developers should understand how they are executed. Understand the impact of indexing on query performance.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5542777,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now let's take a look at using explained plan  \\n with a select statement that has a where clause.  \\n So to do that, we're going to explain select,  \\n let's just select star  \\n and we'll work with our time series data.  \\n So we'll say from, the IOT schema  \\n and we're going to use sensor measurement  \\n and let's say where sensor ID  \\n between 10 and 20, and let's execute that,  \\n and let's see what come up with.  \\n A couple of things to point out here.  \\n Here this top level operation, the append  \\n is sort of the outermost operation  \\n and then we can kind of drill down  \\n almost like in a modular way  \\n to the next set of operations.  \\n So the first operation under a pen, is a sequence scan  \\n and you'll notice it's scanning one of the partitions.  \\n Now this is the month one partition  \\n and below that there's another sequence  \\n scan scanning the month two partition.  \\n So these are two operations that can happen in parallel  \\n and Postgres can optimize and do things in parallel  \\n sometimes depending on configurations.  \\n But we want to notice here is we're doing a sequence scan  \\n and basically a sequence scan is basically,  \\n I'm going to look at everything in the table  \\n and see when I hit a row  \\n that satisfies a particular filter condition.  \\n Now at the top, we have a cost here  \\n which goes up to about 142, 000  \\n and it's an arbitrary like measure  \\n of computational load that Postgres uses.  \\n So the number itself  \\n or the units of measure, isn't all that important,  \\n it's more like from a relative perspective  \\n if you're comparing query plans  \\n you want to look at that number.  \\n Now we're doing a full scan  \\n or basically because we don't have any indexes  \\n so let's create an index and then see what happens.  \\n Now I'm using sensor ID in the where clause  \\n so I would create an index on that  \\n and let's call it IDX  \\n and this is on sensor measurement  \\n and it's on the sensor ID  \\n so we'll call it that on  \\n and then the table is IOT sensor measurement.  \\n And it's sensor ID that we're interested in.  \\n So let's execute the create index statement  \\n and now let's look at the explain plan.  \\n And what you'll notice is  \\n a significantly smaller number here.  \\n Let's see what's going on.  \\n Instead of a sequence scan  \\n we're actually scanning just the indexes.  \\n So this is how we can see whether  \\n or not indexes are helping us in particular queries  \\n is by just doing comparison,  \\n do an explain plan before and after creating the index  \\n and you can get a sense  \\n of how useful that particular index is.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2421483\",\"duration\":357,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Analyzing a query with a join\",\"fileName\":\"2874221_03_05_XR30_explain_join\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to analyze the performance of a query plan that uses a join. Discover when nest loop, hash, and sort-merge joins are optimally used.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12126952,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now Explain Plan is also useful  \\n for understanding how Postgres implements joins.  \\n So let's take a look at that.  \\n Now, the first step of course  \\n is we need another table to join  \\n to our sensor management table.  \\n So let's just create a simple little reference table  \\n for our sensor IDs.  \\n Let's make a table that gives a name for each ID  \\n and we'll keep it simple.  \\n We'll just call our sensors, we use the word sensor  \\n and then append the sensor ID and that'll be the name.  \\n So for that, I'm going to use the generate series again.  \\n So we'll do a select,  \\n select i from generate_series  \\n and we have 100 sensors so we'll go from one to 100  \\n and we'll alias that as i.  \\n Okay, so that gives us our list of 100.  \\n And we're going to use this  \\n as part of a common table expression.  \\n So I'll say with,  \\n let's call the sensor IDs as,  \\n and we'll say select as i,  \\n and we will select i  \\n as ID and then we'll create Sensor  \\n and we'll actually use Sensor with a space after it,  \\n we'll concatenate that sensor ID number.  \\n So concatenate i and cast it as a text.  \\n And let's alias that as sensor_name.  \\n And of course, we're going to be selecting this  \\n from our common table expression which has sensor_ids.  \\n So let's double check.  \\n Looks like the comma is in the right place.  \\n And so now what we have is a list of 100 IDs and 100 names.  \\n So that's great.  \\n So basically this just gives a table  \\n we can join to and we'll look up a name on it.  \\n So let's finish by now creating a table with this data.  \\n So we'll create table iot_sensors  \\n as  \\n with in parentheses, format that.  \\n Okay, so now we have a table called iot_sensors  \\n and I'm just going to jump over here real quickly to refresh.  \\n And let's see, we have our sensor measurements  \\n and then we have sensors.  \\n And let's just check our column names to make sure  \\n ID and sensor_name.  \\n So that looks good.  \\n All right, so we have a table now that we can join to.  \\n So now we're going to be doing an explain  \\n on a select statement.  \\n And what I want to do is I want to get a sensor name  \\n from the sensors table and then some measurements  \\n in the date from the sensor measurement table.  \\n So let's see, I'm going to select  \\n from iot.sensor measurement and let's alias that as sm  \\n and we're going to be doing a left join on iot.sensors  \\n and that's plural.  \\n And let's alias that as s,  \\n and we're going to be joining on the sensor measurement.  \\n We're going to use sensor ID and in our sensor table,  \\n and the corresponding column is ID.  \\n And so that's what our join looks like.  \\n And now let's put some columns in here.  \\n Let's get sensor name from that new table we just created.  \\n And from the measurement table  \\n let's get the measurement date time  \\n and temperature  \\n and the humidity.  \\n See if everything looks correct.  \\n So we have an explain,  \\n are from left join, okay?  \\n Check that.  \\n So what we have here, I'm going to scroll up over the text  \\n of the query so I can see what's going on here.  \\n Now, what you'll notice at the top,  \\n so the top level thing that's going on is a Hash join.  \\n So again, there are three different kinds  \\n of ways of doing join.  \\n There's a nested loop which works really well  \\n with small amounts of data, small tables.  \\n Hash join which works well when you have like large tables  \\n or you're scanning all the tables.  \\n And so we have a Hash condition here  \\n and I won't go into all of the details  \\n but we'll notice we're doing sequence scans  \\n and sequence scans tend to be expensive  \\n but we're joining on the whole table  \\n and we don't have any filtering down.  \\n So it's not unusual to see a full table scan.  \\n So here we have a join and it shows us  \\n how we're going to be able to join over an entire table.  \\n So let's make this a little bit smaller in terms  \\n of the amount of data that we're going to work with.  \\n And let's add a where clause  \\n and let's say where the sensor ID  \\n is equal to, let's pick 30.  \\n Now here, you will notice we've switched now.  \\n We're no longer doing a Hash join.  \\n And that's because we're working  \\n with a small amount of data.  \\n In fact, just one row from the sensors table.  \\n So we can switch over and use a nested loop.  \\n And so again,  \\n so we don't have to do something as complicated  \\n as a Hash join.  \\n So again, we can get a sense  \\n of which of the three techniques  \\n for implementing joins are being used by Postgres.  \\n So again, there's nested loop.  \\n That works well when you have small amounts of data,  \\n Hash joins when you're working with a lot of data  \\n and sort merge if you're working with a lot of data  \\n and you have a sort, you need to order the results.  \\n If you see yourself doing nested loop joins  \\n when you have a large amount of data  \\n that's a good indication that you might want to go back  \\n and look at your join criteria  \\n and also any use of indexes if possible.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2423377\",\"duration\":33,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Optimize a query using an explain plan\",\"fileName\":\"2874221_03_06_XR30_CH30_challenge\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Given a suboptimal explain plan, change the data model to improve query performance. This challenge tests your ability to analyze and tune a query.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":974071,\"solution\":false,\"welcomeContent\":null,\"challenge\":true,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (bright music)  \\n - [Instructor] Let's consider a challenge  \\n around query performance optimization.  \\n So imagine that a colleague has asked you  \\n to help them analyze a long-running query  \\n that they're working with.  \\n Now you take a quick look and you realize,  \\n okay, the query has three tables and two left joins.  \\n Two of the tables have over 500,000 rows  \\n and one table has 200 rows.  \\n So given that,  \\n how would you proceed to improve the query performance?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2424383\",\"duration\":99,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Optimize a query using an explain plan\",\"fileName\":\"2874221_03_07_XR30_SO30_solution\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore the solution to the query optimization challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2771851,\"solution\":true,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Now the first thing  \\n we'd want to do if we're trying  \\n to understand query performance is to run  \\n an explain plan on the query.  \\n That'll give us a breakdown of the steps  \\n that are actually being executed.  \\n Now, as you're looking at those steps,  \\n some things you might want to keep an eye out for are things  \\n like a full table scans or sequence scans,  \\n where we're scanning large amounts of data.  \\n That's often an opportunity for creating indexes.  \\n Also, you want to assess how the indexes  \\n are used with joins.  \\n So in particular, if you think that an index  \\n should be used on a particular join,  \\n and if it's not being used,  \\n then that may mean the index was dropped,  \\n or there's some other reason the index isn't being used,  \\n but it's something worth investigating.  \\n And we also want to look for opportunities  \\n to filter the dataset side.  \\n You know, a fast running query  \\n is a query that doesn't have to look at a lot of data.  \\n So if there are opportunities  \\n to reduce the amount of data  \\n that are pulled back from the large tables,  \\n that could be a help as well.  \\n And then finally, just in the off case  \\n the statistics are off,  \\n you could run the analyze command to make sure statistics  \\n are up to date.  \\n So those are just some of the solutions you can take  \\n to this challenge.  \\n Query optimization is a really in-depth area  \\n that we can dig into,  \\n and it's more than we can cover in this course.  \\n However, there is a course in the catalog  \\n on SQL query optimization and tuning.  \\n So I highly suggest you look into that course  \\n if you are interested in learning more  \\n about how to use explain plan,  \\n and how to how to really get better performance  \\n out of your queries.  \\n \\n\\n\"}],\"name\":\"3. Query Optimization\",\"size\":58259125,\"urn\":\"urn:li:learningContentChapter:2424386\"},{\"duration\":1642,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2423378\",\"duration\":186,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Extending SQL with user-defined functions\",\"fileName\":\"2874221_04_01_XR30_extending_sql\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn when to extend SQL with user-defined functions. User-defined functions can be used to implement specialized business logic in SQL.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5771646,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now we're going to shift our attention  \\n to extending SQL.  \\n And this is actually one of the more interesting topics  \\n with regards to advanced SQL,  \\n which is how do we get beyond what is given to us  \\n essentially in the box in the Postgres  \\n or whatever database we're using.  \\n How can we make it even more functional  \\n from a data science perspective?  \\n Well, the first thing to note is that of course,  \\n SQL provides many types of functions for operating on data  \\n and we use them all the time.  \\n But in terms of the universe of all possible functions  \\n we might need to work with from a data science perspective,  \\n what SQL provides is a small fraction.  \\n So there are many more functions  \\n we might want to be working with.  \\n So for example, SQL will provide aggregate functions for us,  \\n string manipulation functions,  \\n pattern matching, date/time, and geometric functions.  \\n And these are just a small number of examples.  \\n But we sometimes need custom functions.  \\n So let's think about say a retailer  \\n might have a customer loyalty program  \\n where they calculate different levels,  \\n like a one-two-three or a gold-silver-bronze  \\n based on how much a customer has purchased in the last year  \\n or the value or something like that.  \\n We also might have say some customers  \\n who are behind on payments  \\n and we want to sort of prioritize who do we focus on first.  \\n So we might want to have a priority score.  \\n In other cases, marketers or salespeople might want to predict  \\n how valuable is this customer to us  \\n over the lifetime of the relationship with the company?  \\n How much are they going to spend?  \\n Also, we might want to be able to say predict  \\n if we were a bank or a lender of some kind  \\n making predictions about what's the probability  \\n that this person will default on a loan?  \\n So when we're working with Postgres,  \\n there are five types of user-defined functions.  \\n There are query language functions,  \\n which are written in SQL.  \\n There are procedural functions,  \\n which are written in a procedural language called PL/pgSQL  \\n which is a combination of both SQL  \\n and something like a Pascal-like  \\n structured programming language.  \\n So if you're familiar with some of the older languages,  \\n ALGOL, Pascal, PL/pgSQL is similar to that.  \\n Now, another type of user-defined function  \\n is an internal function.  \\n Now, these are written in C  \\n and statically linked to the Postgres platform,  \\n the main engine.  \\n So this is something you only use  \\n if you are really working on the Postgres platform  \\n and you're trying to extend it.  \\n There are also C language functions  \\n that are available through shared libraries,  \\n so they're not necessarily linked in post compile time,  \\n but they're shared at run time.  \\n So that's another way to work with user-defined functions.  \\n But again, these are C-level functions  \\n and not something we typically use  \\n when we're say working with data science functions.  \\n There's also an extension called PL/Python  \\n which allows you to write user-defined functions in Python.  \\n And there are a lot of advantages to that,  \\n but I just want to note right off the bat  \\n that PL/Python is not a trusted language,  \\n which basically means you could really do some damage  \\n to the database if something's not quite right  \\n in your Python, so use PL/Python with caution.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2421484\",\"duration\":520,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"SQL query functions\",\"fileName\":\"2874221_04_02_XR30_sql_functions\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to write a user-defined function using SQL data manipulation language commands. These commands are used to implement specialized logic.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15730955,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now let's take a look at SQL functions.  \\n Now SQL functions are basically query language functions.  \\n So these are different from, say, writing a procedural code  \\n either in PL/pgSQL  \\n or in C  \\n or in Python.  \\n And so we're really writing in declarative language.  \\n So basically,  \\n we can execute an arbitrary list of SQL statements  \\n all at once in a single function.  \\n Now the thing to keep in mind about query language functions  \\n is that the last statement either has to be a select  \\n or the function has to be declared void.  \\n So if you're familiar with some procedural languages,  \\n you know, Java, C,  \\n sometimes, you can have a function  \\n which doesn't return anything,  \\n you declare it as void,  \\n which is basically means it's,  \\n you know, it's just a procedure,  \\n it has side effects,  \\n Similar for query language functions,  \\n you can declare it as void.  \\n Also important to note  \\n that although functions are typically functional,  \\n which means they return one value,  \\n that one value can be a set,  \\n so you can actually have multiple rows,  \\n and that would be by declaring that you are returning a set.  \\n Now there are the types of SQL statements  \\n that you can work with inside a procedure  \\n include the select, insert, update, delete.  \\n So your basic, you know,  \\n create, read, update, delete or CRUD functions  \\n are available to you.  \\n Now what's not available  \\n is anything involving things like transactions,  \\n where you'd like specifying a begin  \\n and then a commit or a rollback  \\n or a savepoint where you're doing like a checkpoint.  \\n You can't do any of those things in a function.  \\n Those are definitely side effects,  \\n but, like, from a procedural perspective.  \\n But they're not allowed.  \\n They're too big of a change within a database  \\n to allow them within a function.  \\n Now the way we create functions  \\n is with the create function statement.  \\n And no surprise here,  \\n it just creates a new function.  \\n I want to point out  \\n that Postgres has another way of creating a function,  \\n which is called create or replace function.  \\n And that's really nice,  \\n because if the function exists,  \\n it'll update the definition.  \\n And if it doesn't exist,  \\n it will create it for you.  \\n But I want to distinguish this  \\n from the drop and create function.  \\n Now if you drop a function and then create it,  \\n it sounds a lot like,  \\n well, that's the same as create or replace,  \\n but it's not.  \\n When you drop a function,  \\n you actually delete an existing function.  \\n And then you create a new function.  \\n Well, that new function essentially has a new identifier.  \\n And so if you referred to that function  \\n in rules, views, triggers,  \\n you need to drop those and recreate them,  \\n because now you need the new identifier  \\n that goes along with the new function.  \\n So again,  \\n if you are,  \\n just want to replace a definition,  \\n then use create or replace.  \\n If you really want to wipe it out  \\n and absolutely start from scratch,  \\n then drop function plus create is the way to go.  \\n Okay. Let's look at an example of a function  \\n where you might want to create.  \\n Now harmonic mean is a kind of average.  \\n It's one of the Pythagorean means.  \\n And basically, it's a reciprocal of the arithmetic mean.  \\n And we often see it used like with rates.  \\n So if you have a problem where you're,  \\n say, driving at 50 kilometers an hour for part of a trip,  \\n and then, you know, you're going back,  \\n and you're driving at 25 kilometers per hour,  \\n you know, what's the average rate?  \\n Harmonic mean is what will give you the, you know,  \\n sort of the correct average mile per hour  \\n across that whole time span.  \\n Now from a data science perspective,  \\n it's also used a lot with information retrieval  \\n in machine learning,  \\n particularly, when we combine precision and recall numbers.  \\n So we have two measures  \\n that we typically use in machine learning,  \\n precision and recall,  \\n in terms of classification problems.  \\n But sometimes, it's nice to work with just a single number  \\n that combines them both.  \\n Harmonic mean works really well for that.  \\n And in terms of information retrieval,  \\n it's often called the F-score or F1-score.  \\n Now let's imagine we are managing a hotel,  \\n and we have these two rates  \\n that we keep track of the bed occupancy rate  \\n and the room occupancy rate.  \\n Now these are really two different things,  \\n just like, you know, precision and recall,  \\n they're different things.  \\n We want to compute a harmonic mean of those.  \\n Well, we could simply spell it out  \\n by doing as we do here.  \\n We're going to round at the product of, you know,  \\n this division of a two times the bed occupancy rate  \\n times the room occupancy rate  \\n divided by bed occupancy rate plus room occupancy rate.  \\n And oh, by the way, let's cast that as a numeric.  \\n And let's name it as composite occupancy rate.  \\n So we could do that.  \\n We could write all that code in a SQL statement  \\n every time we need it to compute the harmonic mean.  \\n Alternatively,  \\n we could just create a function.  \\n So what we're doing here  \\n is we're specifying the create function statement.  \\n We give it a name called harmonic mean.  \\n It's a function,  \\n so we can have parameters.  \\n And in this case, we have two parameters, X and Y,  \\n both of which are numeric data types.  \\n We're returning a numeric type to type.  \\n And our definition is then captured  \\n in between the dollar sign dollar sign  \\n and the second dollar sign dollar sign.  \\n Basically, that dollar sign dollar sign is a way  \\n within Postgres that we can specify a literal string.  \\n So there's other ways to do it with quotes and things,  \\n but definitely,  \\n the convention is to use dollar sign dollar sign  \\n as the start of this string  \\n when working with create function.  \\n In essentially the body of the function,  \\n I'm saying select the round of the two times the product  \\n of the two parameters  \\n divided by the sum of the two parameters  \\n and cast that as a numeric  \\n and pass it back.  \\n The last statement  \\n after the closing dollar sign dollar sign,  \\n so I've closed off the string,  \\n I'm just specifying language SQL.  \\n So I could also specify language PL/SQL,  \\n or if I was working with some other language,  \\n I could specify that.  \\n But that is the, essentially, an example  \\n of how to create a function.  \\n So now let's jump over into Postgres  \\n and actually see in action.  \\n So basically,  \\n here I'm working in our iotschema.  \\n So let's say I've got a couple of measures  \\n in my central measurement table  \\n that I want to use harmonic mean for for some reason.  \\n So the first thing I'm going to do is say create function.  \\n And I'm going to give it a name,  \\n and I'll call this harmonic mean.  \\n And I'm going to use two parameters.  \\n I'm going to use X which is a numeric  \\n and Y which is also a numeric.  \\n And I want to tell it  \\n that this function is going to return a numeric value.  \\n And now I want to specify this as,  \\n so this is kind of like create views something as,  \\n this is create function something as.  \\n Now because we're working with strings here,  \\n I've got to put my dollar sign dollar sign in  \\n to say, okay, this is,  \\n I'm starting a quoted string.  \\n And I'm going to say select.  \\n And I know I need to round my number.  \\n And I'm going to use  \\n try two  \\n times my parameter X,  \\n times my perimeter Y.  \\n And I'm going to divide that  \\n by  \\n X plus Y.  \\n And I want to cast this as a numeric.  \\n And I want to round it to two decimal places.  \\n Now let me just make sure I got my parentheses correct.  \\n That closes correctly,  \\n and that closes correctly.  \\n Okay, great.  \\n Now I just need to close off my quoted string  \\n and tell Postgres I'm working with SQL  \\n and give it a run.  \\n Ah, what's happened here is  \\n that I have already defined this function in this schema,  \\n so what I should have done is written create or replace.  \\n Because essentially,  \\n what I want to do is update the function.  \\n Also, I want to point out in this error message.  \\n Note that it says the function harmonic mean already exists  \\n with same argument types.  \\n So it's not saying it simply it already exists,  \\n but it already exists with same argument types.  \\n That is a reference to a concept called overloading  \\n which we will discuss shortly.  \\n So I'm going to change this to create or replace function  \\n and execute it again.  \\n And now I succeeded.  \\n So now the next thing I want to do is,  \\n I just want to execute a select statement.  \\n I want to use this function.  \\n So let's select harmonic.  \\n And let's pass in the parameters of two and seven.  \\n And this is the only thing I want to execute,  \\n so I'll highlight that and execute.  \\n And here I get 3.11.  \\n Now this, of course, is different from the average.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2422414\",\"duration\":341,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Function overloading\",\"fileName\":\"2874221_04_03_XR30_function_overloading\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover how to use function overloading to expand the capabilities of a user-defined function. Overloading allows developers to apply functions to a wider range of use cases.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11032164,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Postgres supports the concept  \\n of function overloading.  \\n Now basically what this means is that a single function  \\n can actually have multiple definitions.  \\n Now, the way we do this is we  \\n use multiple create function statements,  \\n but those create function statements  \\n have different signatures or a different set of parameters.  \\n So for example, we could create a harmonic mean  \\n like the one we have created earlier,  \\n which takes in two arguments, both of which are numeric.  \\n We could also create a harmonic mean  \\n that takes in text values and then converts them.  \\n Function overloading is useful when the same function  \\n can apply or be applied to different data types.  \\n Now, one thing you want to keep in mind  \\n is you want to avoid anything that might be ambiguous  \\n in terms of how Postgres will interpret  \\n the particular signatures.  \\n So for example, if we called a function,  \\n let's call it ambiguous,  \\n and we passed in the value eight,  \\n well eight could be either an integer or a small integer.  \\n So if we had two create function statements  \\n which took in one parameter, one was an integer  \\n and one was a small integer,  \\n Postgres wouldn't necessarily know at the time  \\n it's processing that which of those signatures to use.  \\n So we want to make sure  \\n that we avoid those kinds of ambiguities.  \\n So let's take a look at an example.  \\n Now we've already defined a function called harmonic mean  \\n and now what I want to do is create another version  \\n or overload this function harmonic mean by specifying  \\n that someone can pass in the two parameters as text  \\n and we would simply change the definition accordingly.  \\n So in this case using text inputs,  \\n we'd want to convert the text into numeric types  \\n so we explicitly cast them as numeric.  \\n Now let's take a look at how we define overloaded functions.  \\n So I'm going to say create or replace  \\n function harmonic mean  \\n and we're going to specify the first perimeter as numeric,  \\n second as numeric.  \\n And we're going to specify  \\n that it returns a numeric string start indicator.  \\n And we'll say that we are going to select.  \\n And if you recall, the definition of harmonic mean  \\n is two times X times Y divided by X plus Y.  \\n And we want to round this to two decimal places.  \\n And just to be explicit, we can say  \\n we want to cast the results as numeric.  \\n And we'll just specify the end of the definition  \\n and of the actual function code  \\n and we have to tell Postgres which language we are using.  \\n We'll specify SQL.  \\n And let's just run that just to make sure  \\n I got that correct.  \\n So everything's correct there.  \\n Now what I can do is let's specify it with text.  \\n So here I'm going to just say create  \\n or replace function harmonic mean.  \\n So it's exactly the same name.  \\n There's no need to change that,  \\n what's changing is the parameter signature  \\n or what we put in the parameter list.  \\n I'm still going to call it X,  \\n but now I'm going to specify the data type as text.  \\n Y is a text.  \\n Now, I still want this to return a numeric  \\n so I'll specify numeric as the result.  \\n As dollar sign, dollar sign, and here I'm going to select.  \\n Now I'm going to do two times X, but X is a text  \\n so I need to cast that explicitly as numeric.  \\n And I'm also going to multiply by Y which again  \\n I have to specify explicitly as numeric.  \\n That's slightly easier to read.  \\n So that is the first part, that's our numerator  \\n so let's build our denominator here,  \\n which is simply X plus Y.  \\n And again, we'll specify a casting plus Y numeric.  \\n We'll round this up, round up to two.  \\n And we'll explicitly say numeric again.  \\n This is overkill,  \\n Postgres wouldn't have a problem figuring that out,  \\n but I'm putting it in in language is SQL.  \\n And let's see if I got that right.  \\n So this closes that,  \\n that closes correctly, that closes with the round.  \\n That looks correct so let's run that.  \\n So now we have both functions created.  \\n So now let's try select harmonic mean of 2.2 and 7.1.  \\n And we'll run that and we get an answer.  \\n Now let's add harmonic mean of quote 2.2 and quote 7.1.  \\n And let's run that.  \\n Now what you'll see is we're getting the same answer  \\n which obviously we would expect to do.  \\n So this is a little bit of a contrived case  \\n where we're passing in text values as numeric,  \\n but actually this can occur if you're doing a lot of like  \\n extraction load transform kind of work.  \\n So if you're working with say imported data  \\n or something like that,  \\n oftentimes you might have issues where you have to do  \\n either conversions or in this case,  \\n you can actually do function overloading  \\n to help you avoid some of the overhead  \\n of constantly converting different data types.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2425342\",\"duration\":242,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Function volatility\",\"fileName\":\"2874221_04_04_XR30_function_volatility\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore volatility categories and when to use each. Volatility specifications provide information to the optimizer which is used to implement the execution plan so function developers need to understand how to correctly specify volatility. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7542030,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Function volatility is something  \\n we need to consider with regards to optimizing  \\n the performance of our functions.  \\n Now, functions have a volatility classification  \\n and basically this classification is essentially  \\n like a promise or an indication  \\n of what kind of behaviors the function performs,  \\n or more practically, the kinds of things it won't do.  \\n This allows the optimizer to make certain assumptions  \\n about what optimizations are safe  \\n for that particular function.  \\n Now, there are three classifications:  \\n volatile, stable, and immutable.  \\n If you don't specify a classification,  \\n volatile is the default classification.  \\n A volatile function can perform any operation,  \\n including making changes to the database.  \\n So you can insert, you can update, you can delete.  \\n And this is because the optimizer doesn't make  \\n any assumptions about the function in terms  \\n of optimizing the way the code might run.  \\n And this is something to keep in mind,  \\n the function is reevaluated at every row  \\n the function is needed.  \\n So if you're applying this function  \\n to every row in a very large table  \\n and you haven't specified a volatility classification,  \\n it will be reevaluated at every row in that table.  \\n So, definitely use volatile functions  \\n and specify them as volatile when you have to,  \\n but use one of the other classifications if you can.  \\n Now, stable is the second classification.  \\n You cannot modify the database with a stable classification,  \\n so no deleting, updating, inserting.  \\n Another thing is it's guaranteed to return  \\n the same results given the same arguments  \\n for all rows within a single statement.  \\n So for example, if you have a select statement,  \\n and you're calling a function and that function  \\n is applied to many rows across the table,  \\n anytime a particular row has  \\n the same parameters as another row,  \\n they will always get the same result from that function.  \\n That's one of the things that we guarantee  \\n or promise to the optimizer  \\n when we specify a function is stable.  \\n And with stable, we get some optimizations.  \\n So it's definitely, if you can specify it as stable  \\n that's better than volatile in terms of performance  \\n when you're working with especially large data sets.  \\n Now immutable is the most restricted,  \\n and of course like stable, it cannot modify the database.  \\n We're guaranteed to return the same results  \\n given the same argument in all cases.  \\n it's not just restricted to a single statement.  \\n So anytime you call this function  \\n with say parameters two and three,  \\n you're always going to get the same answer.  \\n A function labeled as immutable basically tells  \\n the optimizer that it can just do as many optimizations  \\n as available for a function.  \\n So let's take a look at an example here.  \\n This is our harmonic mean function, which we've seen before.  \\n And you'll notice here in the last line,  \\n after we specify the language, which in this case is SQL.  \\n We're specifying the volatility classification,  \\n which in this case is immutable  \\n because the function is immutable.  \\n Under any circumstances if we have two parameters  \\n which are the same,  \\n and they are called at different times,  \\n they will always have the same result.  \\n Now there's some things you want  \\n to keep in mind about volatility.  \\n For performance purposes, you want to use  \\n the most strict volatility classification that's possible.  \\n That's going to give you the best performance.  \\n Now, any function with any kind of side effects  \\n that's altering something,  \\n that's changing the state of the database,  \\n that has to be volatile.  \\n Also, if a function has no side effects,  \\n but it uses another function.  \\n So for example, in our harmonic mean,  \\n if we made a call to another function,  \\n say like the random function or the time of day function,  \\n well those can change during the course  \\n of the execution of say a long statement,  \\n or random, it can change at any time  \\n and will change across rows,  \\n those functions need to be labeled as volatile.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2428255\",\"duration\":224,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"PL/Python functions\",\"fileName\":\"2874221_04_05_XR30_plpython\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to write a PostgreSQL user-defined function in Python. Python is widely used in data science and it is an option for developing user-defined functions in PostgreSQL.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6596738,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now, it is possible to write  \\n user-defined functions in Python when using Postgres.  \\n And we write Python by using the PL/Python language.  \\n So the PL/Python language is a Postgres extension.  \\n And as I mentioned,  \\n And what we would do  \\n is basically call the create extension command.  \\n And in this case,  \\n we could call the create extension command  \\n and then specify plpythonu  \\n and then either version two or version three.  \\n Now, let's look at some pros and cons.  \\n Now, there are definitely reasons to use Python.  \\n For many people who work in data science,  \\n we already know Python.  \\n It's a well-developed language.  \\n We have a lot of tools in it.  \\n We have ecosystems for developing around Python.  \\n We can also reuse code we've already written.  \\n We might be able to just copy and paste some stuff  \\n into our user deploying functions.  \\n And one of the other nice things is  \\n if we have to write procedural code for the database,  \\n you can avoid having to learn something like PL/pgSQL,  \\n which is an older procedural language.  \\n Now, there are also some cons or some reasons not to use it.  \\n So first of all, Python is not a trusted language  \\n within the Postgres database.  \\n And what that means is that the code in your functions  \\n could damage the database.  \\n There's no sort of guarantees.  \\n There's no guardrails protecting what you write  \\n from damaging the database.  \\n So that's what we mean by not trusted.  \\n Also, the installation can vary slightly  \\n by Postgres package.  \\n So yes, you always call create extensions,  \\n but depending on which package you use  \\n or which installation method you use,  \\n you may have to kind of hunt around  \\n and look for different libraries  \\n that might not be found by the create extension.  \\n So you definitely want to look at the documentation  \\n for your particular SQL package when installing this.  \\n Now, with installation,  \\n you need to use a user that has superuser privilege.  \\n Now, when you're creating PL/Python,  \\n you can specify the create extension  \\n and specify plpython3u if you want to use Python 3,  \\n or you can specify create extension pythonu  \\n and that'll use Python 2.  \\n Now, that's not recommended.  \\n Python 2 is no longer supported.  \\n So going forward,  \\n it's better to use Python 3 just in general.  \\n So here's an example of a function.  \\n Now, notice we used the create function syntax  \\n that we saw earlier.  \\n And here, we're going to specify a function called pymax,  \\n which is simply going to take the maximum of two integers.  \\n And you'll notice we have the typical function signature  \\n with create function, a function name,  \\n a list of parameters, a return statement,  \\n and then a data type as and then we have our string.  \\n The string part is what is written in Python.  \\n So here we just have a simple Python statement,  \\n so if x is greater than y, return x.  \\n Otherwise, return y.  \\n And then at the end, we specify this language  \\n and the language that you would specify  \\n is what you specified when you created the extension.  \\n So in this case,  \\n we're assuming we're working with Python 3.  \\n Now, some key points to remember about PL/Python.  \\n You need superuser privilege to install.  \\n So you might have to issue a command  \\n like alter user Postgres with superuser  \\n or you could substitute for Postgres some other user  \\n that you want to be installing this extension.  \\n And then once you have it installed,  \\n then others can write the PL/Python functions.  \\n They don't need to be superusers.  \\n And also you want to remember with PL/Python,  \\n arguments that you pass in,  \\n those are treated as global variables,  \\n and there are ways to modify them  \\n if you declare them as global within Python,  \\n but it's best to treat those arguments  \\n as read-only variables.  \\n And so if you do need to change those values or something,  \\n just copy them to a local variable,  \\n then make the changes there.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2422415\",\"duration\":34,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Write a user-defined function\",\"fileName\":\"2874221_04_06_XR30_CH30challenge\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Write a SQL user-defined function to implement a statistical function. This exercise tests your ability to develop user-defined functions in PostgreSQL.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1069595,\"solution\":false,\"welcomeContent\":null,\"challenge\":true,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] In this challenge, I'd like you  \\n to write a SQL function that returns a Boolean  \\n and the function will return true,  \\n if an input string is a palindrome.  \\n That is the string is the same  \\n in reverse as it is going forward.  \\n So, for example, the string abccba is the same  \\n in going forward and going in reverse.  \\n So, again, just write a SQL function  \\n that we will call isPalindrome.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2422416\",\"duration\":95,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Write a user-defined function\",\"fileName\":\"2874221_04_07_XR30_SO30_solution\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore the solution to the user-defined function challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3453713,\"solution\":true,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Narrator] This is our solution  \\n to the challenge to create a function called is_palindrome,  \\n which returns True when a string that's passed in  \\n is the same in reverse as it is going forward.  \\n You'll notice, we specify  \\n our create or replace function.  \\n Then we have our signature, followed by  \\n our function name, which in this case is, is_palindrome.  \\n We have a single parameter called STR,  \\n which is a type text and the function returns a Boolean.  \\n Then we specify the body of the function.  \\n Here, it's pretty straightforward.  \\n We're just going to use the reverse function.  \\n We're going to take the string, reverse it  \\n and see if it's equal to the string that was passed in.  \\n The other thing I want to point out  \\n is that because this function will always  \\n return the same value under any conditions,  \\n what if the parameter is the same at any time in the future?  \\n The results would always be the same.  \\n That means we can specify that this function is immutable.  \\n Finally, look, we can just run a test  \\n by selecting is_palindrome with an actual palindrome value.  \\n That should be True, and this value should be False.  \\n We can just execute that.  \\n As we see, the first argument actually is a palindrome,  \\n so we have a value of True returned.  \\n The second example, is_palindrome with foobarbaz, is not,  \\n so we have False return.  \\n That's basically the solution.  \\n \\n\\n\"}],\"name\":\"4. User-Defined Functions\",\"size\":51196841,\"urn\":\"urn:li:learningContentChapter:2422420\"},{\"duration\":2238,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2424384\",\"duration\":253,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Federated queries\",\"fileName\":\"2874221_05_01_XR30_federated_data\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover what federated queries are, when to use them, and their limits. Federated queries allow users to query data from multiple databases without copying or migrating data to a single database.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7677699,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now, there may be times we want  \\n to work with data that's not in the database,  \\n and we don't actually want to load the data either,  \\n but we'd still like to have access to it through SQL.  \\n In that case,  \\n we could use something known as federated queries.  \\n Now, federated data is data that's outside a database,  \\n but is still accessible by SQL.  \\n This is made possible by a SQL standard called SQL/MED,  \\n or management of external data.  \\n And it defines a number of different abstractions,  \\n but the sort of key ones  \\n are foreign data wrappers and datalinks.  \\n Now, a foreign data wrapper is a specification  \\n and a module or a package that allows us to view data  \\n that's outside the database.  \\n Datalinks compliment foreign data wrappers.  \\n They provide functionality we expect to have in a database  \\n like integrity, functions, recovery mechanisms,  \\n and authorization mechanisms.  \\n So, the way we create a federated data source is first  \\n we create an extension called postgres_fdw.  \\n Now, an extension is a module which is part of Postgres,  \\n but it is not installed by default.  \\n So when we want to install an extension,  \\n we use the CREATE EXTENSION command,  \\n and then we specify the name of the extension  \\n we want to install, which in this case is postgres_fdw.  \\n Next, if we want to create a foreign data wrapper  \\n for another database,  \\n we can use the command CREATE SERVER.  \\n And here I'm specifying a name called external_db_server.  \\n I need to specify what kind  \\n of foreign data wrapper I want to use.  \\n In this case, I'm going to use the postgres_fdw,  \\n and then I want to specify my options.  \\n Not surprisingly, the kinds of things we specify  \\n in the option are the kinds of things  \\n you would expect to see in a connect string,  \\n like name of a host and a database name.  \\n Now we also want to be able to map users.  \\n So if we're going to get data from another database,  \\n we're going to be getting that data using the roles  \\n or the permissions assigned to some user there.  \\n So the way we do that is we're using  \\n a CREATE USER MAPPING command,  \\n and we're going to map the current user  \\n to the server we just created, external_db_server.  \\n And we're going to specify in the options  \\n a username and a password.  \\n That's how we will access the data on the database.  \\n So, it's through this user, in this case analyst,  \\n which is a user on the external database,  \\n and we're going to be working through the analyst user  \\n to actually get the data out of the database  \\n when we're querying that external data.  \\n Now, another thing we can do is we can map  \\n a schema from an external database.  \\n So for example, let's say we have  \\n an e-commerce application that has some sales information  \\n and we want to reference that data,  \\n we can create a schema on our database.  \\n Let's call it external_sales.  \\n And then we import the foreign schema sales  \\n from the server that we created.  \\n Now again, when I say from the server we created,  \\n what I'm referring to is the data structure  \\n that we have on our Postgres database  \\n that references the external database.  \\n And I'm going to import that data into my schema  \\n that I just created, external_sales.  \\n Now, I can also create a foreign table.  \\n So let's say I have a log file that's CSV formatted.  \\n I can use the CREATE FOREIGN TABLE command  \\n and give the table a name.  \\n I'm going to call it page_visit_log,  \\n and it's a very simple table.  \\n It just has a timestamp, a username, and a webpage.  \\n And I can specify that as SERVER log_data  \\n and specify OPTIONS.  \\n So here, because I am working with a file,  \\n the options I'm going to specify are related  \\n to how I would access a file.  \\n So for example I need to specify  \\n the file name and the format.  \\n So, the CREATE FOREIGN TABLE command is a way  \\n of easily working with say structured data  \\n that's in a log file or some other data source,  \\n but is outside the database,  \\n and you would rather not, for whatever reason,  \\n bring that data into the database.  \\n This is how you can work with that data using SQL.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2422417\",\"duration\":278,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Bloom filters\",\"fileName\":\"2874221_05_02_XR30_bloom_filters\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover how to use the probabilistic filter on large data sets. Probabilistic filters can significantly reduce the time required to execute a SELECT query.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8421567,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now often when we work  \\n with very large data sets  \\n we often will trade accuracy for speed.  \\n So for example, we might sample a dataset  \\n and then do calculations over that sample  \\n and assume that the overall population  \\n has roughly the same measures.  \\n Well that's a probabilistic method and they're approximate.  \\n Another probabilistic method is known as a bloom filter  \\n and we can use those to create indexes  \\n which can be highly efficient in some cases.  \\n So a bloom filter is a space efficient method  \\n for determining set membership.  \\n So for example, if we need to find out  \\n which data block contains a particular piece of data  \\n or we have a complex query with multiple conditions  \\n in a where clause such as  \\n finding customers who live in a particular state or city  \\n and have been customers for less than a  \\n certain period of time and are delinquent on payments.  \\n Now, a blue filter is a lossy representation of data.  \\n So some compression methods are lossy.  \\n That means you can take the compressed version  \\n and then recreate it in an exact way the original source.  \\n Now bloom filters are similar  \\n in the sense that they're a probabilistic data structure.  \\n Now, with regards to the lossiness  \\n what that means is it may produce false positives.  \\n So a bloom filter is basically a boolean.  \\n Does this element exist in this set?  \\n And by false positives,  \\n we mean that the bloom filter may indicate,  \\n yes this element is in the set, when in fact it's not.  \\n So that can occur with bloom filters.  \\n However, we never get a false negative.  \\n If a bloom filter indicates that an element  \\n is not in a set, you know for certain it's not in that set.  \\n Now, the relative accuracy of bloom filters  \\n is a function of how many bits are used in the filter.  \\n And what people have found is that oftentimes  \\n about 10 bits per element will yield about  \\n a 1% false positive rate.  \\n Now a blue filter index is a Postgres extension.  \\n So that means it's part of Postgres  \\n but it's not installed by default.  \\n And bloom filters are particularly useful  \\n when a table has many attributes  \\n and we filter in our queries based on  \\n many different combinations or attributes.  \\n Now it's important to know  \\n that B-tree indexes are faster typically  \\n but they may require more indexes.  \\n So for example, if you have a table with 20 columns  \\n you can create a bloom filter,  \\n a single bloom filter with all of those attributes  \\n and get similar behavior that you would  \\n if you had created 20 individual B-tree indexes  \\n one for each of the columns.  \\n So the B-tree index may be faster  \\n but you're using up much more space.  \\n Now there are some limitations to keep in mind.  \\n Two in particular first is that the bloom filter indexes  \\n support only the equality operator  \\n and it only works with two data types,  \\n integers, four byte integers and text data types.  \\n If you need inequality or ranges or conditions  \\n in addition to equality  \\n then you'd probably want to use B-tree indexes instead.  \\n Now let's take a look at how we create a bloom filter index.  \\n First thing we need to do is create the extension.  \\n So basically install the bloom filter module  \\n and we use the create extension command for that.  \\n And then we just use the create index command  \\n but we specify a using clause.  \\n So for example, we might want to create  \\n an index on a table that has some locations  \\n and we might call the index IDX location bloom.  \\n And that index is being created on a table called locations.  \\n And we're using bloom, which is specification.  \\n We're using a bloom index  \\n and we're going to have three columns  \\n in that index, city, state, province and country.  \\n Now we can also specify a with clause  \\n and specify the length of the bloom filter  \\n and the longer the length  \\n the higher the accuracy of the bloom filter  \\n in the sense of reducing the number of false positives.  \\n Now because bloom filters have false positives,  \\n you can't assume that all of the data returned  \\n will actually satisfy the rule.  \\n So for example, you might have a statement such as,  \\n where attribute one equals A,  \\n attribute two equals B and attribute three equals C.  \\n It is possible to have rows returned and a result set  \\n that actually don't meet that criteria.  \\n So you want to always double-check and make sure  \\n that the results that are returned by a bloom filter  \\n actually meet the criteria that you're interested in.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2428256\",\"duration\":383,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Hstore for key-value pairs\",\"fileName\":\"2874221_05_03_XR30_hstore\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to model key-value pairs in PostgreSQL. Hstore provides a more efficient way to store key values in tables than using a normalized model.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12794479,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now, Postgres has features  \\n that may be more properly considered in the realm of NoSQL,  \\n including key-value stores  \\n and support for document structures using JSON.  \\n So let's first take a look at hstore.  \\n hstore is a data type for storing sets of key-value pairs.  \\n Now, we use hstore by first creating an extension.  \\n And an extension is a package or module  \\n that's part of Postgres but not installed by default.  \\n So we essentially install it  \\n by using the CREATE EXTENSION hstore command,  \\n and then we can create cables as we would normally.  \\n And for one of the comp source, several of the comps,  \\n we could specify the data type as hstore.  \\n Then when we insert data, we basically insert a string  \\n which is a list of key-value pairs.  \\n Now, why would you use hstore?  \\n Well, it's useful  \\n when it helps when keys are not predefined.  \\n So that is, the keys in the list can change arbitrarily.  \\n So if you have a large number of attributes  \\n and many of them are not used,  \\n so if you have sparse data, hstore may be a good option.  \\n Another advantage  \\n is you don't have to change the column definition  \\n to accomodate new columns  \\n because your column can be represented as a key-value pair  \\n in that single hstore column.  \\n Another advantage is that GIN and GiST indexes can be used  \\n to index all the keys  \\n so you can still have performant operations.  \\n So, for example, one example would be a catalog,  \\n so items in the catalog might have different attributes.  \\n So, for example, an appliance may have  \\n a power usage, a weight, height, depth dimensions,  \\n whereas a piece of clothing might have  \\n a size, color, material as attributes,  \\n and all of those can be represented as key-value pairs.  \\n Now, there are some limitations with hstore.  \\n First of all, all keys and values are stored as strings.  \\n You get no other data types.  \\n It does not support hierarchical data structures.  \\n Now, if you need hierarchical data structures,  \\n JSONB or XML data types are better options.  \\n Let's take a look at how we would use hstore.  \\n So I'm going to create a table called books,  \\n and let's have an id which is a serial  \\n and for a primary key,  \\n and we'll have the title of the book,  \\n and that will be of type text,  \\n and then we'll have a list of attributes  \\n which is of type hstore.  \\n Now, if you haven't installed hstore,  \\n you'll want to use the CREATE EXTENSION command hstore,  \\n and I'll just run this.  \\n I already have it installed  \\n so I'll get a little error message  \\n saying it's already exists.  \\n But if you haven't installed it,  \\n it would just give you a success message.  \\n So we'll get rid of it.  \\n We don't need that, so get rid of that.  \\n And your CREATE books, id serial primary key, looks good.  \\n We'll create the table.  \\n So now let's see how we would insert data into this table.  \\n So let's INSERT INTO books,  \\n and I just want to insert title and attributes  \\n because id is a serial.  \\n It will be assigned automatically.  \\n And we'll have our VALUES,  \\n and our VALUES will be the title of the book.  \\n Let's call the title SQL for Data Science.  \\n That's one value.  \\n And then our other value for attributes,  \\n let's have some attributes such as language,  \\n and we'll say that's in English.  \\n We use the equal and the greater than sign,  \\n and we'll specify English as the value.  \\n And let's say we have a page_count,  \\n and let's say that's 500.  \\n Can't forget that, okay.  \\n page_count 500.  \\n And let's say the publication year is equal to 2021.  \\n And, okay, so that inserted a row.  \\n Let's start another one.  \\n Let's just copy this and kind of mix it up a little bit.  \\n We'll change things slightly.  \\n Let's say we have SQL for Data Science 2,  \\n and the page_count is 600, the language is still English,  \\n and the pub_year is, let's say, 2022.  \\n So let's insert that.  \\n That's also inserted.  \\n And now let's do a SELECT statement,  \\n and let's simply SELECT * FROM books  \\n and see what it looks like.  \\n And what you'll see  \\n in the attributes column, the values appear as a string,  \\n but they don't behave like strings.  \\n So let's see, for example, how to work with the attributes.  \\n For example, if we wanted to,  \\n say, list the title and page count,  \\n we could do that by saying SELECT title.  \\n Now, we want page_count.  \\n Well, page_count is in the attribute's hstore  \\n so we need to specify attributes.  \\n And then here we don't use an equal sign,  \\n we use the minus sign or dash,  \\n and then the greater than sign,  \\n and then we specify what we're interested in,  \\n and we're interested in the page_count attribute,  \\n and we want to collect that from books.  \\n Now, notice I had to put these in quotes,  \\n page_count in quotes.  \\n So if I execute this,  \\n I should see title and just the page_count,  \\n which is what I have.  \\n And I could also alias this  \\n for something like pc for page_count  \\n and get something like that.  \\n I can also use attributes in a WHERE clause.  \\n For example, I could say WHERE attribute,  \\n WHERE attributes,  \\n let's see, page_count equals 500.  \\n Notice, I can't treat that as a numeric.  \\n I'm using that, making that a text value,  \\n and then I can use SELECT there.  \\n Now, if I had just put 500 here,  \\n I would get an error because it's not converted.  \\n So when you're working with values  \\n that are stored in hstore,  \\n you want to make sure to always treat them as strings.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2426335\",\"duration\":514,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"JSON for semi-structured data\",\"fileName\":\"2874221_05_04_XR30_json\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn to model semi-structured data in PostgreSQL. JSON allows for the use of document model storage in addition to relational storage.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17485347,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now PostgreSQL provides data types  \\n for supporting JSON,  \\n which is ideal for semi-structured data.  \\n So with PostgreSQL, we have options to use both relational  \\n and NoSQL type features.  \\n Now JSON is particularly good  \\n at modeling document databases.  \\n Now, document databases are used  \\n when a use case demands a flexible schema,  \\n nested structures,  \\n and the ability to query an index keys  \\n throughout the structure.  \\n Since PostgreSQL 9.2, we can have both relational  \\n and document structures in a single database.  \\n So here's an example of a document structure.  \\n Let's imagine we have a table called Customer Summary  \\n and we have the name of a customer, their address,  \\n and then some information about purchase history.  \\n Well, each of those could itself be a complex structure.  \\n So for example, name could be a pair,  \\n first name and last name.  \\n Address could be made up of a street, city,  \\n province, postal code.  \\n And finally, the purchase history could say,  \\n be an array of annual purchase values,  \\n so how much did that customer purchase each year,  \\n and also a lifetime value estimate,  \\n which might be just a scale or a single numeric.  \\n Well, both JSON and JSONB would work well for that.  \\n Both data types validate that a string is valid JSON,  \\n both allow querying and JSONB supports indexing  \\n of elements in a JSON structure.  \\n Now since 9.2, JSON represents the actual JSON structure  \\n in plain text.  \\n But starting in 9.4, we have the option of using JSONB.  \\n Now the B stands for better,  \\n and part of the better is that it's a binary representation  \\n and it's more efficient than the JSON data type.  \\n So JSONB supports nested JSON structures.  \\n We can use GIN indexes with them  \\n which index all the key value pairs in a structure.  \\n Now in general, if you're trying to choose  \\n between a key value hstore  \\n or JSON, which stores JSON and plain text, or JSONB,  \\n almost always the best option is JSONB.  \\n Okay, let's work a little bit with that JSON structure.  \\n First thing I want to do is create a table  \\n and let's call this customer_summary.  \\n And let's use id which is a serial  \\n as our primary key.  \\n And let's call our JSON column customer_doc,  \\n and that will be a JSON.  \\n And we're going to use JSONB since that's more efficient.  \\n And we will create that table.  \\n So now we have a table called customer_summary.  \\n It has an id as a primary key.  \\n And it has a single other attribute, which is customer_doc.  \\n So let's insert some data into that table.  \\n So I'm going to insert into customer_summary,  \\n and I'm just going to insert the customer_doc  \\n because the serial value will be set automatically.  \\n And we'll specify the values.  \\n And the values we're going to be specifying,  \\n we are going to specify a JSON structure.  \\n So that will be a string  \\n which is delimited using the curly brackets.  \\n And within that,  \\n we're going to have our three high-level structures,  \\n which is customer_name and address  \\n and purchase_history.  \\n Now, each of these, these are keys,  \\n so it's going to be followed by a value.  \\n And in JSON,  \\n we use the colon to separate the key from the value.  \\n Now the customer name, you'll remember,  \\n is also a structure  \\n and it includes a first_name  \\n and a last_name.  \\n And again, we're going to specify a value for each key.  \\n So let's say the first_name of this customer is Alice  \\n and the last_name is Johnson.  \\n And we need to put our commas in,  \\n put a comma there, put a comma there.  \\n And so now we've specified customer_name.  \\n So now let's specify address.  \\n So again, we're going to build another JSON structure.  \\n And an address has a street address,  \\n we'll just call it street though.  \\n It has a street,  \\n a city  \\n and let's say state, typo there, correct that.  \\n So those were our three keys.  \\n Now we specify the values,  \\n the street, let's say it's 5432 Port Ave,  \\n and the city is Boston  \\n and the state is Massachusetts,  \\n which we can use a two letter abbreviation MA for that.  \\n And so now we've specified our address as a key,  \\n and this is the value of that key,  \\n so I'll put a comma after that.  \\n And now the last thing we want to specify is  \\n purchase_history.  \\n And purchase_history, as you recall, has two elements.  \\n And that is annual_purchase_value  \\n and the other key is lifetime_value.  \\n And so we have our keys, we'll specify a value for each.  \\n Now the annual_purchase_value,  \\n let's assume it's an array  \\n and the first element of the array is the total value  \\n of the customer's purchase their first year as a customer.  \\n So let's say that's $100.  \\n And the second is the second year,  \\n let's say that's 200.  \\n And let's say at year three, it went up to 350, okay.  \\n So that completes our array of values.  \\n So in addition to the nested structures  \\n we can have with JSON,  \\n we can also have a RASON.  \\n And for lifetime_value, we wanted just a scalar.  \\n Let's say we estimate the lifetime value will be 1500.  \\n Okay, so that closes off purchase history.  \\n And then we need to close off customer_doc, which we do,  \\n and we'll close that.  \\n Actually, we need to put it right there and now close off.  \\n And now let's run this.  \\n Let's just do a quick check for missing commas.  \\n Nope, looks good,  \\n so we will execute that insert statement.  \\n That's an extra in there.  \\n Let's execute again.  \\n There we go.  \\n So we now have one row in that table.  \\n And let's clear this out  \\n and work with some select statements now.  \\n Now first, let's see what the data looks like.  \\n So I'm simply going to do a select star,  \\n customer_summary.  \\n And as we expect, we see the JSON structure  \\n and it contains what is essentially text values  \\n or strings of attribute value pairs  \\n and some of them are hierarchical.  \\n Also notice that the way  \\n that the key value pairs are ordered in the database  \\n is not necessarily the same way  \\n that we entered them using the insert statement.  \\n Now, if I want to just look at the customer_doc column,  \\n I could specify customer_doc  \\n and that gives me the entire JSONB structure.  \\n Now I could also essentially look inside  \\n by using the dash arrow.  \\n And what that will do is  \\n it will return the value of some key.  \\n So let's look at the customer_name and execute that.  \\n What you'll notice, it returns just the customer_name.  \\n Now the operator here I'm using is a single dash  \\n and then an arrow or greater than sign.  \\n Now, when you use that,  \\n that returns the results as a JSONB type.  \\n You'll notice there it's a JSONB type.  \\n And one reason we might want to do that  \\n is because we want to treat that JSONB structure  \\n as a hierarchical thing  \\n and we want to extract something from that.  \\n So for example, if we wanted to extract a first_name,  \\n we could do that.  \\n Now the first name is going to be a text.  \\n So I'm going to specify first_name  \\n and let's run that.  \\n And what you'll see here, we get back a text item.  \\n So the result is text and it's first_name.  \\n So that's how we reference items  \\n within the particular structure.  \\n And of course you can use the same syntax  \\n in the where clause as well.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2422418\",\"duration\":719,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Hierarchical data and ltrees\",\"fileName\":\"2874221_05_05_XR30_ltree\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to use the ltree extension to efficiently store hierarchical tree data and use specialty operators to perform matches over hierarchies.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":22993358,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Occasionally, we need to work  \\n with hierarchical data.  \\n There are different ways of working with hierarchical data  \\n and one is to use an extension in Postgres called ltrees.  \\n So, here's an example of a simple tree  \\n and oftentimes one of the things  \\n we need to do with these trees is work with paths.  \\n So for example, if I were to start at the root  \\n and go from node A, which is the root, to node B,  \\n and then to node D, and then to node H,  \\n we'd have a path from the root to one of the leaves.  \\n And we can depict that as a string of characters,  \\n such as A.B.D.H.  \\n Now, we can model this in different ways.  \\n A common way to do this is to use a table  \\n that has one row per node.  \\n So for example, in this table, node A is the root,  \\n so it's parent node is null.  \\n A is the parent of B and C,  \\n so what we see in the parent node column is the foreign key  \\n or the ID of the parent.  \\n So in the case of B and C, that would be ID one  \\n because node A is the parent.  \\n In the case of D and E, both of those are below B,  \\n and so B is the foreign key in the parent node table.  \\n Now, when you're querying hierarchical data,  \\n we often use recursive common table expressions  \\n but if you're working with Oracle,  \\n you can use a Connect By clause in your SQL query.  \\n Now, another way to represent paths  \\n is to use a pattern called materialized paths.  \\n In this case, each node in the tree is in its own row  \\n and each row representing a node has a path column.  \\n So we're no longer have a parent column,  \\n that's essentially been changed to a path column,  \\n and the path column contains a string  \\n which represents the path from the current node  \\n back through all of the ancestors to the root.  \\n And the way we query this and work with the path  \\n is by using string function.  \\n So for example,  \\n we might use regular expression, pattern matching,  \\n or use sub strings to match something  \\n in that particular path.  \\n Now, as you can imagine,  \\n if we're working with very large data sets  \\n and doing a lot of string operations  \\n across all many different rows,  \\n it can be rather inefficient,  \\n because for example, without indexes,  \\n we have to apply operations repeatedly over each row.  \\n So, wouldn't it be better if we had a mechanism  \\n that has allowed us to work with materialized paths,  \\n but gave us the advantages of indexes.  \\n And in Postgres, we can do that  \\n and that's by using the ltree extension.  \\n So, the ltree extension is part of Postgres,  \\n and it's used for working with trees and paths.  \\n It comes with many operators  \\n which we can use for operating on and comparing paths.  \\n It supports both B-tree and GiST indexes.  \\n Although GiST indexes are typically favored  \\n because you can use more operators,  \\n or more operators are available with the GiST index.  \\n So, some of the common ltree operators  \\n that we'll often work with are the ancestor  \\n and descendant pattern matching,  \\n the concatenate for concatenating tree paths,  \\n and the Tilda which basically answers the question,  \\n does an ltree match an ltree text query?  \\n So this is a basic pattern matching operation.  \\n And as I mentioned, there are quite a few operators.  \\n There are about 20 more in addition to these four.  \\n So I suggest if you'd like to learn more  \\n about ltree operators to consult the Postgres documentation  \\n on the ltree extension.  \\n Okay, so let's work with ltrees.  \\n So the first thing I'm going to do is create a table  \\n and let's call it something like paths_to_nodes,  \\n and  \\n we'll follow a pattern of having an ID  \\n which is a serial as our primary key,  \\n and we will have a node,  \\n which is a text type,  \\n and we will have a path which is an ltree.  \\n Now, I have already installed the ltree extension.  \\n So, I don't need to specify, create extension ltree.  \\n So I'm just going to execute that.  \\n Now, if you haven't installed the ltree extension  \\n then you can simply add the command, create  \\n extension  \\n ltree,  \\n and you can execute that.  \\n Now I'll execute it  \\n and Postgres will just tell me it's already installed.  \\n So it already exists so I don't need to do that.  \\n But first time you're using it,  \\n you want to put that create extension in.  \\n So we have our table,  \\n let's create an index on this table.  \\n And we'll call it  \\n idx_paths_to_nodes,  \\n and we're going to create this on paths_to_nodes,  \\n and we're going to specify a using clause  \\n because we want to use a GiST index here and not a B-tree,  \\n which is the default if we don't specify a using clause,  \\n and we want that GiST to be applied to the path column.  \\n So I will just execute that.  \\n Okay, so now we have our table.  \\n We have a GiST index on our path column.  \\n So, I've previously saved these insert statements  \\n so I'm just going to paste them in to save time.  \\n And so now we have data, so just take a look at it.  \\n Paths_to_nodes,  \\n and we'll execute that,  \\n and let's take a look.  \\n Okay, what would you expect?  \\n 11 rows, each with paths from the root  \\n down to the various leaves.  \\n So the first thing I want to do,  \\n I want to use the ancestor operator.  \\n So I'm going to select paths_to_nodes.  \\n I'm going to add a where clause here.  \\n And let's say, I want to find all of the descendants  \\n that have  \\n A.B at the beginning of their path.  \\n And to do that, I'm going to specify the path,  \\n which in this case has just A and B,  \\n and then I'll use the at greater than sign operator,  \\n which is the ancestor,  \\n and I want to apply that operator to the path column.  \\n So I will execute that,  \\n and what we'll see here is everything that is returned  \\n has the pattern A.B in the path  \\n at the beginning of the path.  \\n So that's pretty simple.  \\n And once we do this, we can do things like count the nodes.  \\n So things that you would normally do  \\n with other aggregation patterns or other things like that,  \\n you can of course do here as well.  \\n So now, let's say I want to kind of loosen up the criteria  \\n and I want to search for anything where B is an ancestor.  \\n Well, one way to do that  \\n is instead of using the ancestor path,  \\n we'll use a pattern matching operator, which is the Tilda,  \\n and now I can specify, for example,  \\n I want everything with a B in it.  \\n I can  \\n specify star  \\n dot B dot star.  \\n Now this should return anything that has a B in the pattern.  \\n We'll get rid of the count, we'll go back here,  \\n and here again, we have anything which has B in the pattern,  \\n but there needs to be one B.  \\n We can add a specification using a curly bracket  \\n and then specify one.  \\n So that means any number of ancestors can proceed B,  \\n we want to have one node following the B in the pattern.  \\n Now let's see what happens there.  \\n Well notice, we lost A.B itself  \\n but now we have A.B.D and A.B.E, which is what we'd expect.  \\n Now, we can also use a zero-based operators,  \\n and we could, for example say,  \\n we want to have at least zero and at most two.  \\n So we can specify both a min and a max here.  \\n So let's do that.  \\n So we should see this first row shows just A.B.  \\n Now, so there are zero nodes after the B.  \\n The other rows in the table show at least one  \\n and up to two nodes in the path.  \\n All right, let's do a little work  \\n with concatenation operator.  \\n Let's clear this out.  \\n And  \\n first of all, let's just start,  \\n we'll build this query incrementally.  \\n So we'll select, let's see,  \\n we're going to select from paths_to_nodes,  \\n and I'm going to alias set as p1.  \\n And,  \\n let's select from p1,  \\n the ID, the p1 node  \\n and the p1 path.  \\n And let's add a where clause.  \\n And let's say where  \\n p1 path  \\n matches  \\n the ltree query  \\n star dot B dot star.  \\n So the one we just used.  \\n So we should get results  \\n similar to what we've seen before  \\n when we're querying for a star B star.  \\n Okay, so that's working as expected.  \\n Now, what I want to do is I want to concatenate  \\n some other sub trees to those rows.  \\n So let's build another query,  \\n and in this case, I'm going to use select,  \\n and I'm going to be selecting from same table,  \\n paths_to_nodes, but this time I'm going to alias it as p2,  \\n and so I want to select, we'll select star.  \\n Here, we'll keep it simple from p2  \\n and I want to say  \\n where  \\n path  \\n matches a query,  \\n such that there is a C in the path.  \\n Now, let's just execute this select statement,  \\n and I'll do that by highlighting it.  \\n Okay, as we expect,  \\n everything that is returned has a C in it.  \\n Okay.  \\n So now let's build these two together.  \\n So we've got our two building blocks.  \\n I'm going to copy this select statement  \\n and move it up here into a with statement.  \\n So I'm going to say with, we'll call this paths_to_concatenate,  \\n or simply to concat as.  \\n So I'm creating a common table expression,  \\n and I'm going to paste in what I had below,  \\n and I'll just indent this a little bit for me to build them.  \\n Okay, so what I've done here  \\n is I've created a common table expression,  \\n and I'm calling paths to concatenate the table  \\n or that's the relation where there's are the Cs in the path.  \\n And what I'm wanting to do now is I want to join p1  \\n to  \\n paths_to_concat  \\n p2.  \\n Now, I want to do a Cartesian product  \\n so I'm not going to specify a joint clause.  \\n So each row in p1 is going to get matched up  \\n with each row in p2.  \\n And we'll leave the paths the same.  \\n So again, let's just review this one more time.  \\n We have a common table expression  \\n which allows us to build this query in a modular way.  \\n So all of the paths that have C in the path  \\n are going to be in the relation called paths to concatenate.  \\n I'm going to use paths to concatenate  \\n to do a Cartesian product with paths_to_nodes  \\n and I'm going to  \\n concatenate  \\n p1  \\n path  \\n to  \\n p2  \\n path.  \\n So this is how I'm going to combine the B paths  \\n with the C paths.  \\n So let's execute that query, and see what we have.  \\n What we'll notice here is that all of the rows,  \\n starting with the A.B paths,  \\n and then afterwards we have C paths.  \\n So for example, we have A.C, A.C.F, A.C.G.  \\n So this is how we can build ltrees through concatenation.  \\n Now, you'll notice  \\n that the concatenation operator is a double pipe,  \\n which is the same as what we use with strings,  \\n but this column that is being returned  \\n is actually ltree type, it's not a string.  \\n So we have all the advantages  \\n of having the ltree entry indexed.  \\n So, ltrees look a lot like strings,  \\n and even the operator, like the concatenation operator,  \\n looks familiar and the Tilda operator looks familiar,  \\n if you're familiar with working with strings, but again,  \\n there's potentially a significant performance difference  \\n by using ltrees instead of strings.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2427267\",\"duration\":33,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Design a table to support unstructured data\",\"fileName\":\"2874221_05_06_XR30_CH30_challenge\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Given a set of business requirements, determine the best data types to use for semi-structured data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":962234,\"solution\":false,\"welcomeContent\":null,\"challenge\":true,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (light techno music)  \\n - [Instructor] Okay, here's a challenge  \\n around data modeling and design choices.  \\n Let's imagine you're working with an ecommerce company  \\n who wants to create a new catalog of products.  \\n Now different kinds of products  \\n are going to have different attributes  \\n and it's critical  \\n that users be able to query and filter on any attribute.  \\n And of course, performance is a key consideration.  \\n So how would you design a table  \\n to support this type of product model?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2423379\",\"duration\":58,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Design a table to support unstructured data\",\"fileName\":\"2874221_05_07_XR30_SO30_solution\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore the solution to the special-purpose functionality challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1622291,\"solution\":true,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Now, in this solution,  \\n we need to consider a number of factors.  \\n Clearly we're working with semi-structured data  \\n and we need indexes to support query performance.  \\n Now, we could use a column for each attribute,  \\n but that's really not practical,  \\n particularly in a relational database.  \\n Now, that's not the case with another type of database,  \\n in particular the type of NoSQL database  \\n known as a wide-column database.  \\n Now, Apache Cassandra, Query Google Bigtable  \\n are both examples of wide-column tables,  \\n where having a column for each attribute  \\n is an appropriate way to design it.  \\n But when you're working with Postgres  \\n or other relational tables, that's really not a good option.  \\n A better option is to use a JSONB column,  \\n because with JSONB, we have the semi-structured document  \\n data structure to work with  \\n and JSONB supports indexing.  \\n \\n\\n\"}],\"name\":\"5. Special-Purpose Functionality\",\"size\":71956975,\"urn\":\"urn:li:learningContentChapter:2428257\"},{\"duration\":63,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2427268\",\"duration\":63,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"2874221_06_01_XR30_next_steps\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2192568,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Dan] Now that we've completed  \\n the Advanced SQL for Data Science course,  \\n there's some other courses you may be interested  \\n in taking a look at.  \\n SQL query tuning and performance optimization  \\n is a critical skill anytime you're working  \\n with large data sets.  \\n I'd highly recommend studying query tuning  \\n and performance optimization.  \\n I'd also suggest learning more about data modeling,  \\n including modeling NoSQL databases,  \\n because the lines have begun to blur  \\n between relational or SQL and NoSQL  \\n and so some of the techniques that you might learn  \\n in NoSQL data modeling may be applicable  \\n when you're working, for example, with JSONB data.  \\n Now, depending on the type of data you're working with,  \\n you may want to look into some specialized courses,  \\n like data science for time series data.  \\n And also, as you have seen,  \\n SQL for data science is much more  \\n than just writing queries.  \\n There's a lot of thought that needs to go in  \\n around how you structure your data,  \\n how you structure systems.  \\n So any work you can do with data architecture  \\n or learning data architecture patterns  \\n will almost certainly be helpful  \\n in your data science career.  \\n \\n\\n\"}],\"name\":\"Conclusion\",\"size\":2192568,\"urn\":\"urn:li:learningContentChapter:2421485\"}],\"size\":285263903,\"duration\":9038,\"zeroBased\":false}]}"