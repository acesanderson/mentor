"{\"title\":\"\",\"courses\":[{\"course_title\":\"Hands-On Introduction: SQL\",\"course_admin_id\":3086685,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":3086685,\"Project ID\":null,\"Course Name\":\"Hands-On Introduction: SQL\",\"Course Name EN\":\"Hands-On Introduction: SQL\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"&lt;p&gt;Join instructor Deepa Maddala as she uses hands-on lessons to teach you the tools, techniques, and know-how that you need to start using SQL from day one. Deepa guides you through using the Select statement to fetch and filter data, creating and adding to tables and data, modifying existing tables, and what to use in different scenarios. She includes simple examples with each topic she covers.&lt;/p&gt;&lt;p&gt;The best way to learn a language is to use it in practice. That\u00e2\u20ac\u2122s why this course is integrated with GitHub Codespaces, an instant cloud developer environment that offers all the functionality of your favorite IDE without the need for any local machine setup. With GitHub Codespaces, you can get hands-on practice from any machine, at any time\u00e2\u20ac\u201dall while using a tool that you\u00e2\u20ac\u2122ll likely encounter in the workplace. Check out the \u00e2\u20ac\u0153Using GitHub Codespaces with this course\u00e2\u20ac\u009d video to learn how to get started.&lt;/p&gt;\",\"Course Short Description\":\"Get the tools, techniques, and know-how to write SQL queries through hands-on lessons where you\u00e2\u20ac\u2122ll code along with the instructor.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":20533045,\"Instructor Name\":\"Deepa  Maddala\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Experienced Consultant and PL/SQL Developer in Information Technology\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2022-11-09T00:00:00\",\"Course Updated Date\":\"2024-04-19T00:00:00\",\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/hands-on-introduction-sql\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Databases\",\"Primary Software\":\"SQL\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":4761.0,\"Visible Video Count\":17.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":166,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4362477\",\"duration\":63,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Introduction to SQL\",\"fileName\":\"3086685_en_US_00_01_WX30\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"This video gives a short introduction to SQL and how it is used with various databases.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2073781,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Deepa] You've probably heard of databases\\nsince you were in grade school,\\nand maybe you already know databases are used to store data.\\nBut how does the data get there?\\nWhat can you do with the data that's stored there?\\nIf you don't know the answers to those questions\\nbut want to learn,\\nthat's where I can help you.\\nHi, my name is Deepa Maddala.\\nI'm a database developer.\\nI've designed this course to get you up and running\\nwith database tables using the SQL language.\\n\\nI'll show you how to use the common statements\\nin order to manipulate data within tables.\\nThis includes modifying, deleting,\\nmerging, and creating data,\\nalong with tips on transaction controls.\\nWhile this isn't a fully comprehensive primer on SQL,\\nit should help you get started and maybe answer some\\nof your immediate questions regarding databases.\\nSo if you're ready to dive in,\\nlet's go ahead and get started.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5912212\",\"duration\":103,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Using GitHub Codespaces with this course\",\"fileName\":\"3086685_en_US_00_02_FY24Q4_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":152,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false,\"updateType\":\"ADDED\"},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3341846,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] On your course interface,\\nfind the GitHub Codespaces Open button.\\nClick on it to go to GitHub.\\nYou then click on Create Codespace on Main button\\nand wait for it to fully load.\\nThis might take some time.\\nNow once it's fully loaded, we can see an Install button.\\nClick on that.\\nOnce it's installed, navigate to File Explorer.\\n\\nThis is where we have all the files\\nthat are needed for our project.\\nI will be adding all the SQL queries here\\nunder the queries folder.\\nThese three are example SQL statements.\\nThis is just for your reference.\\nCurrently we are on the Main branch.\\nWe will have to navigate to chapter one, video one branch.\\nSo I click on Main\\nand let's try to find 01_01 branch.\\n\\nThere it is, chapter one, video one branch.\\nSo this is the one, I click on it,\\nand now we can see that there are two folders here.\\nThe first one is to create the entire database\\nthat is needed for our course.\\nSo we will not be looking into that.\\nThe folder that we will be looking into is 01_01,\\nwhich corresponds to our video.\\nSo I click on that\\nand we have all the SQL statements\\nthat are needed for this specific video.\\n\\n\"}],\"name\":\"Introduction\",\"size\":5415627,\"urn\":\"urn:li:learningContentChapter:4358590\"},{\"duration\":2567,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4362478\",\"duration\":383,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Fetch data\",\"fileName\":\"3086685_en_US_01_01_XR30\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to write different kinds of select statements to fetch data in different forms.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11286712,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Facilitator] The first thing we need to learn\\nis how to fetch data from different tables in our database.\\nWe use the SELECT statement to fetch data from the tables.\\nWe can fetch data by columns.\\nThat means we can fetch one single column\\nor multiple columns from the table\\nby specifying it in the SELECT statement.\\nWe can fetch data by filtering the rows in a table\\nby specifying a condition.\\nWe can also fetch data from one\\nor multiple tables by using joints, which we learn later.\\n\\nSQL statements are not case sensitive,\\nbut the best practice is to write the keywords\\nin uppercase and the table names, column names,\\nand all others in lowercase.\\nKeywords cannot be abbreviated\\nor cannot be split across lines.\\nWe can write the complete SQL statement in one line.\\nHowever, we use indentation to improve readability,\\nespecially when we are writing long SQL statements.\\nNow this is the syntax for the SELECT statement.\\n\\nIf we give star, it'll fetch all the columns from the table,\\nbut if I specify the column names explicitly,\\nonly those columns data will be fetched.\\nDISTINCT keyword is optional.\\nIt is used only when we need to fetch distinct values\\nin a particular column.\\nWe can also ask to fetch an expression\\ninstead of a column name.\\nWe give the alias name\\nif we do not want the expression name to be shown\\nas the column name.\\nWe'll understand this better when we look at an example.\\n\\nThen we give the FROM keyword followed by the table name.\\nSo now let's look at a few examples in codespace.\\nOn your codes interface,\\nfind the GitHub codespace's open button.\\nClick on it to go to GitHub.\\nYou then click on create codespace on main button\\nand wait for it to fully load.\\nThis might take some time.\\nNow once it's fully loaded, we can see an install button.\\n\\nClick on that.\\nOnce it's installed, navigate to File Explorer.\\nThis is where we have all the files\\nthat are needed for our project.\\nI will be adding all the SQL queries\\nhere under the queries folder.\\nThese three are example SQL statements.\\nThis is just for your reference.\\nCurrently, we are on the main branch.\\nWe will have to navigate to chapter one, video one branch.\\n\\nSo I click on main and let's try to find 0101 branch.\\nThat is, chapter one, video one branch.\\nSo this is the one, I click on it.\\nAnd now we can see that there are two folders here.\\nThe first one is to create the entire database\\nthat is needed for our course.\\nSo we will not be looking into that.\\nThe folder that we will be looking into is 0101,\\nwhich corresponds to our video.\\n\\nSo I click on that and we have all the SQL statements\\nthat are needed for this specific video.\\nSo since we are in chapter one, video one,\\nso I go there and click on the SELECT statement .SQL.\\nSo this is a basic SELECT statement,\\nwhere I'm fetching all the rows and columns from EMP tab.\\nSo when I run this, I right click\\nand go to run selected query.\\nClick on that and it fetches all the rows\\nand all the columns from EMP tab table.\\n\\nNow in the second SELECT statement,\\nI am specifying the columns that I want to fetch.\\nI want only EMP number and name columns from EMP tab table.\\nSo when I run this, it only fetches those two columns.\\nNow, if I want to give an expression\\nalong with the column names and want to fetch that data,\\nI give the column names first,\\nfollowed by an expression where I want to calculate\\nthe annual salary for all employees.\\n\\nSo I'm multiplying it with 12 and then I run this.\\nSo now we got the annual salary,\\nbut the column name looks a little weird\\nlike salary times 12.\\nSo for it to look a little nicer, we give,\\nsuppose I give it as salary\\nand run this,\\nit shows it as salary.\\nBut if you want to give a more appropriate name,\\nI would like to call it as annual salary.\\n\\nNow if I run this, it's going to throw me an error\\nbecause there are two words in the alias name.\\nWhen there are multiple words in the alias name,\\nwe have to put them in quotes.\\nSo now when I run this, it shows it annual salary\\nand this looks much more nicer.\\nNow let's learn about the different operators that we have.\\nWe have arithmetic operators and character operators.\\n\\nAny arithmetic expression uses arithmetic operators.\\nThat is, plus, minus, multiplication, and division.\\nThe order of precedence is multiplication,\\ndivision, addition, and then finally, subtraction.\\nThe parenthesis overrides the order of precedence,\\nwhich means that anything in the parenthesis\\nwill be calculated first\\nand then the other operators outside the parenthesis\\nin the order of precedence.\\n\\nCharacter strings use character operators.\\nThose are two pipelines that is for contamination\\nand single quotes and double quotes\\nfor giving character strings.\\nSo here we have learned about the basic SELECT statements,\\nhow to use alias names,\\nand about arithmetic and character operators.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4363675\",\"duration\":725,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filtering data\",\"fileName\":\"3086685_en_US_01_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to fetch the right data based on different scenarios.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":22216204,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We do not need the entire tables data\\nevery single time.\\nSo let's learn how to filter the data from tables\\nbased on our requirement.\\nWe use the WHERE condition to filter the data.\\nThis is the syntax for the WHERE.\\nAlias names cannot be given in the WHERE condition.\\nIf we have character values\\nor date values in the conditions,\\nthen we need to give those in single quotes.\\nLet's look at some examples to understand this better.\\nLet's go to chapter 1 video 2, and go to WHERE condition.\\n\\nSo here, I'm trying to SELECT empno, name,\\nand salary from emp_tab WHERE salary&gt;2500.\\nSo it's going to retrieve all the employees\\nwhose salaries are greater than 2,500.\\nSo when I run this,\\ngets all the employees\\nwhose salaries are greater than 2,500.\\nIn the second SELECT statement,\\nthe condition is using a character value.\\n\\nSo I have to enclose those in single quotes.\\nSo here we are checking if an employee's name is KING\\nand getting their details.\\nSo when I run this,\\nthere is an employee named KING and these are his details.\\nThere are two types of conditions,\\none is comparison and the other is logical.\\nComparison conditions, as the name implies,\\nare used to compare the table data\\nwith the values that we provide.\\n\\nThese are the different comparison operators.\\n= to checks if the column value\\nis equal to a specific value,\\n&lt;&gt; to check if they're not equal to a specific value.\\nSimilarly, we have &lt;, &gt;, &lt;=, and &gt;=.\\nLIKE checks for a specific character pattern.\\nIN checks if the column value\\nmatches with any value in our set.\\nBETWEEN AND checks if the column value\\nis between a range of values.\\n\\nAnd IS NULL checks for null values in column.\\nSo let's look at some examples for these.\\nNow that we are on chapter 1 video 2,\\nwe will have to navigate to the corresponding branch.\\nWe will have to do this step once we start a new video\\nso that we have the appropriate SQL statements.\\nSo let's navigate to 01_02 Branch.\\nI click on this and let's find 01_02 branch.\\n\\nAnd I click on that.\\nSo now, we have a new folder, 01_02,\\nwhich has all the SQL statements\\nthat are needed for this particular video.\\nSo these are all the SQL statements\\nthat we will cover in this video.\\nSo this is a very important step\\nto navigate to the correct branch\\nbefore we start working on a specific video.\\nLet's go to ComparisonConditions.\\n\\nHere, we are checking for the name\\nto match this specific pattern.\\nSo _ represents one single character\\nand % represents multiple characters.\\nSo we are trying to look for this pattern\\nand we'll see if it returns any employee details\\nwith that pattern.\\nSo ADAMS is a name which matches this pattern\\nbecause the first letter is A,\\nit could be any letter so that doesn't really matter,\\nbut the second letter has to be an uppercase D,\\nwhich matches, and then we have other letters.\\n\\nSo it matches the pattern\\nso it returns this employee details.\\nIN checks for this column value to be in this set of values.\\nSo it checks if the deptno is in this set of values\\nand gets those employee details for us.\\nSo now let's run this.\\nAnd these are all the employees\\nwho belong to department 30 and 40.\\n\\nNow BETWEEN AND is used to check for a column value\\nto be between a range of values,\\nincluding these values as well.\\nSo when I run this, these employees' salaries\\nare within the range 2,000 and 6,000,\\nso that's fetched for us.\\nNow here, we are checking if the manager column IS NULL\\nand we are getting those employee records\\nso that we can update that manager details\\nfor that employee.\\n\\nOnly empno 7001 manager is set to null.\\nSo that's been returned for us.\\nNow let's take a look at the logical conditions.\\nWe use these when we have multiple comparison conditions\\nin WHERE.\\nAND returns true only if both the conditions are true,\\notherwise, it returns a false.\\nLet's take an example.\\nLet's go to LogicalConditions.\\n\\nHere, we have two conditions,\\nWHERE name has to match this pattern\\nAND the deptno has to be 30.\\nSo only if both the conditions are true,\\nonly then the rows will be fetched.\\nSo when I run this,\\nthis is the only employee\\nwhose name matches with the pattern here\\nand who belongs to department number 30.\\nSo that has been fetched for us.\\n\\nNow OR works in a different way.\\nSo even if one of these conditions satisfies,\\nwe will get a row fetched.\\nSo let's see how the output changes\\nwhen we use OR in between.\\nAs we can see, we have multiple rows\\nand not just the single row that we got\\nin the first SELECT statement\\nbecause even if one of these satisfies, we get some output.\\nSo here, there are two employees whose names start with S\\nand there are employees whose deptno is 30.\\n\\nThis one has been fetched because the name pattern matches.\\nSo that's the reason why we have that record fetched.\\nNOT checks for the department\\nnot to be in this set of values.\\nSo when I run this, it checks for any employees\\nwho are not in this specific departments that is 30 or 40.\\n\\nSo since it belongs to department 70, we got that record.\\nSo this is the order of precedence for all the operators.\\nThe order of precedence can be changed by using parenthesis.\\nLet's look at some examples.\\nSo here, when I run this particular SELECT statement,\\nit first checks for AND\\nbecause AND has a higher order of precedence than OR.\\n\\nSo it checks for these two conditions first\\nand then looks for this deptno 30.\\nSo it checks for deptno 40,\\nand also checks if the salaries are greater than 2,500.\\nSo those rows will be fetched,\\nand we will also have employees\\nwho belong to department 30 as well.\\nSo let's see.\\nHere, as we can see,\\nthe only employee who has a salary greater than 2,500\\nin department 40 is KING.\\n\\nSo that row has been fetched.\\nAnd then it also looked for deptno 30\\nand fetched all the employees\\nwho belong to that particular department.\\nSo first, these two are considered, and then the OR.\\nNow, if I want to do it in a slightly different way,\\nI have to put the department numbers in parenthesis\\nif I want to consider the departments first\\nand then check for the salaries.\\n\\nSo it first checks for all employees\\nwho belong to either deptno 30 or deptno 40,\\nand then it looks for the salaries\\nif they are greater than 2,500.\\nSo now when I run this,\\nit gets all the employees\\nwhose salaries are greater than 2,500\\nand those who belong to departments 40 and 30.\\nSo that's how we can change the order of precedence.\\n\\nWe can sort data in ascending or descending order.\\nIf we do not specify the order,\\nthen the default would be taken as ascending.\\nLet's look at an example here.\\nLet's go to ORDERBY.\\nSo here, I'm fetching all the employees\\nwho belong to deptno 30,\\nand I'm going to sort them by salary.\\nAnd since I haven't specified in which order I want,\\nit's going to take it as ascending by default.\\n\\nSo when I run this,\\nhere, these are all the employees who belong to deptno 30\\nand their salaries are sorted\\nfrom minimum to maximum in ascending order.\\nAnd we can also sort by two different columns as well.\\nSo when we do that,\\nit first sorts by deptno in ascending order\\nbecause we haven't specified anything here,\\nand then it looks for the salaries in descending order.\\n\\nSo let's look at the output to understand this better.\\nSo here, if we see,\\nit first sorted by the deptno in ascending order,\\nthat is starting from 30 all the way up to 70.\\nAnd then, in each department,\\nit sorted the salaries in descending order,\\nthat is from the highest to the lowest.\\n\\nThat is if we look at department 30,\\nit started from 3,000 all the way to 800.\\nAnd then it moves to department 40, 5,000 to 1,100.\\nAnd then department 70,\\nwhich has only one employee with salary 3,000.\\nThis is how we can sort by multiple columns as well.\\nSo this is about the WHERE condition,\\nthe types of conditions, order of precedence,\\nand the ORDER BY clause.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4362479\",\"duration\":324,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Types of functions, part 1\",\"fileName\":\"3086685_en_US_01_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn about different single row functions that can be used to fetch data from tables.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9791670,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Now let's learn about the types of functions.\\nThere are two types of functions that can be applied\\non rows in a table, single row and multiple row functions.\\nSingle row functions work with one row at a time\\nand returns one result for every row.\\nMultiple row functions work with a group of rows at a time\\nand return one result for every group.\\nLet's take a look at different single row functions.\\nFirst one is number functions.\\n\\nNumber functions perform arithmatic operations on the data.\\nLet's look at an example for this.\\nSo let's go to number functions,\\nSELECT ROUND(92.427,2),\\nthis is how I've given it.\\nThis rounds to 2 decimal places\\nbecause I've given 2 as the second argument.\\nSince 7 is greater than 5,\\nit rounds off the decimal places to 92.43.\\nSo let me run this.\\n\\nSo it rounds off to 92.43\\nand if this is a value lower than 5,\\nsuppose it is 92.423\\nthen when it rounds off, it would just be 92.42\\nbecause the last value is less than 5.\\nThe next one is conditional functions.\\nNow let's take a look at how case function works.\\nLet's go to conditional functions\\nand here, we are trying to change the salary\\nbased on the job of an employee.\\n\\nSo here what we did is,\\nselect and we are trying to fetch name, job, salary\\nand the fourth column would be revised salary\\nbecause that is being given as the alias name here\\nand what content would be there in that column\\nis we are trying to check for the job of the employee\\nthat is whether he's a manager,\\nanalyst, clerk or something else\\nand then calculating the revised salary for every employee\\nand displaying that in the fourth column.\\n\\nSo let me run this.\\nSo here, we have all the employees\\nand the revised salaries for all of them based on their job.\\nSo these were their original salaries\\nand now after making these changes,\\nthese are the revised salaries based on their job.\\nNext, let's look at the character functions.\\nUPPER converts the entire string to upper case,\\nLOWER converts everything to lower case\\nand SUBSTR extracts a string from a starting position\\nto fetch a certain number of characters\\nfrom that character value.\\n\\nLet's look at an example for that one.\\nSo let's go to character functions\\nand here UPPER will convert everything to upper case\\nand LOWER would convert everything to lower case\\nand then here, in SUBSTR, starting from the sixth position,\\nI want to extract 10 characters from this character value.\\n\\nSo it extracts 'world12345' from this statement.\\nSo when I run this, it gives us 'world12345'.\\nLENGTH is used to calculate the length\\nof the character value.\\nSince there are 10 characters, our output would be 10.\\nINSTR is used to search for an expression\\nand identify its position in the actual character value.\\n\\nSo here it looks for the first occurrence of E\\nand gets that as the output for us.\\nSo since it's in the second place,\\nour output would be 2.\\nTRIM trims 'World' from the 'Hello World' string,\\nso what's left is 'Hello',\\nso that'll be our output.\\n\\nREPLACE replaces 'Hello' with 'Good morning',\\nso this is our character value\\nand this is the one which we want to replace\\nand this is the new string that we want to replace it with.\\nSo instead of 'Hello',\\nit's going to replace it with 'Good morning'\\nin our character value.\\nSo it changes to 'Good morning World',\\nthat would be our output.\\n\\nSo when I run this, it gives us 'Good morning World'.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4360476\",\"duration\":541,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Types of functions, part 2\",\"fileName\":\"3086685_en_US_01_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn about different group functions and how these functions can be used to fetch the required data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17230348,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that we've learned\\nabout single row functions,\\nlet's take a look at GROUP BY clause\\nand GROUP functions.\\nWhat are multiple row functions?\\nThese are also called group functions.\\nThese functions work on a group of rows\\nand give one output for every group.\\nThe columns in the SELECT statement\\nthat are not used in the group function\\nmust be in the GROUP BY clause.\\nThese are the various multiple row functions.\\nSUM returns the sum of all n values.\\n\\nAVG returns the average.\\nMAX and MIN return maximum and minium values\\nwithin a group of values pulled by an expression.\\nCOUNT returns the count of number of rows that we need.\\nVARIANCE and STDDEV functions are used\\nto calculate the variance\\nand standard deviation for a set of values.\\nLet's look at some examples.\\nLet's head to chapter one, video four, GroupFunctions.\\nSo this fetches the SUM of all salaries in emp_tab table.\\n\\nThis fetches the average of all salaries.\\nNow, this gets us the lowest\\nand highest salaries from emp_tab\\nwhere department number is 30.\\nSo it's going to check for the highest\\nand lowest salaries of employees belonging\\nto department number 30.\\nSo that's the lowest and the highest salaries\\nof employees belonging to department number 30.\\n\\nNow, when I give MIN of hiredate\\nand MAX of hiredate, it's going to get us the oldest\\nand the most recently joined employees hire dates\\nin emp_tab table.\\nSo this is the hiredate of the employee\\nwho's been there the longest tenure\\nand this gives us the employee who's joined most recently.\\nThen this gives us the first\\nand last names in the list\\nif the names are sorted in alphabetical order.\\n\\nSo the MIN gives us the first name\\nand MAX gives us the last names\\nif they're sorted in alphabetical order.\\nSo when I run this,\\nAdams is the first name in the list\\nbecause it begins with A.\\nAnd the last name in the list is Smith\\nas there are no other names after Smith.\\nCOUNT of star counts the number of rows\\nthat are there in the table.\\n\\nThere are eight rows in the table.\\nSo this is going to count the number\\nof employees reporting to manager 7330\\nsince we have given the WHERE condition as well.\\nSo we can give the column name\\nin the COUNT function as well.\\nSo there are three employees reporting\\nto manager 7003.\\n\\nThe DISTINCT keyword is to eliminate any duplicate values.\\nSo if there are any duplicate employee number values,\\nthen those will be eliminated\\nand it counts the distinct values in emp_tab table.\\nSo there are eight distinct employee numbers.\\nSo that's what's being fetched.\\nSo these are a few examples of our group functions.\\n\\nThe GROUP BY clause is used to group multiple rows.\\nSo instead of using the entire table,\\nsometimes we need small groups of rows\\nto perform operations individually.\\nSo we can use the GROUP BY clause to do so.\\nWe can only use column names\\nand not alias names in the GROUP BY clause.\\nWe have to keep in mind\\nthat any columns in the SELECT statement\\nthat are not used in the group functions\\nmust be given in the GROUP BY clause.\\nNow, HAVING clause is used to filter the groups' results.\\n\\nThis is because we cannot restrict groups\\nby using the WHERE condition the way we do\\nfor a regular SELECT statement.\\nSo we have to give the group functions\\nin the HAVING clause.\\nNow, this is the syntax for the GROUP BY clause.\\nThe GROUP BY clause comes after the WHERE condition\\nand before the ORDER BY clause.\\nFirst, we write the SELECT columns from table name.\\n\\nThen the WHERE condition.\\nThen GROUP BY,\\nfollowed by HAVING\\nand group_conditions if needed.\\nAnd finally, ORDER BY to sort the filter data.\\nThis order is always the same\\nfor any SELECT statement.\\nEverything that is enclosed in square brackets\\nis optional and used only as needed.\\nSo now let's take a look at some examples\\nfor understanding this better.\\n\\nSo we just now saw how to find the employee\\nwith highest and lowest salaries in every department.\\nNow I want to find out the employee\\nwith the highest salary under every manager.\\nTo do this, we need to group rows\\nin the table by using GROUP BY clause.\\nAnd we use the GROUP BY clause with the manager.\\nSo when I run this,\\nit's going to give me the highest salaries\\nunder every manager\\nand then it is also sorting these salaries\\nin descending order\\nbecause I have given ORDER BY MAX salary\\nin descending order.\\n\\nSo that's the reason why the salaries\\nare sorted in descending order.\\nNow as I mentioned earlier,\\nwe cannot give the group functions in a WHERE condition,\\nlike we did over here.\\nIf I give a group function like this,\\nit's going to throw us an error.\\nSo let me just run this.\\nIt says that we cannot use the MAX function\\nin the WHERE condition.\\n\\nSo instead of giving this condition here\\nin the WHERE condition,\\nwe have something called HAVING clause,\\nwhich we can use to give these group functions conditions.\\nSo first of all, let's try to fetch the highest salary\\nin every department.\\nSo for that, I give SELECT department number, MAX salary\\nfrom emp_tab, GROUP BY department number\\nbecause I want the highest salary in every department.\\n\\nSo that's the reason why I'm grouping them by departments\\nand getting the maximum salary in every department.\\nSo when I run this,\\nit's getting the department number\\nand the maximum salary in each department.\\nSo now if I want to get the highest salary\\nin every department,\\nand then I also want to check\\nif that MAX salary is greater than 3,000.\\n\\nSo I also have a group function condition over here.\\nSo I need to put that group function condition\\nin the HAVING clause and not in the WHERE condition.\\nSo I give SELECT department number, maximum salary\\nfrom emp_tab GROUP BY department number,\\nwhich is exactly the same as we did earlier\\nbut since I have a condition to check\\nif the maximum salary is greater than 3,000,\\nI give the HAVING clause and put the condition in there.\\n\\nAnd I run this.\\nSo there is only one department,\\nwhich has a maximum salary greater than 3,000.\\nSo this is about GROUP BY and group functions.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4365498\",\"duration\":594,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Using multiple tables\",\"fileName\":\"3086685_en_US_01_05_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Explore the various joins and subqueries that can be used to get data from multiple tables at a time.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":19996173,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now let's learn about the different kinds\\nof joins and subqueries.\\nJoins are used to fetch data\\nfrom one or more tables at a time.\\nBased on the data we need,\\njoins are categorized into four types:\\ninner, right, left and full outer join.\\nThis is the syntax for using joins.\\nNow let's take a look at each of these.\\nInner join retrieves all rows matching in both tables.\\nSo here in this example,\\nit's going to get the matching rows\\nfrom country_tab and states_tab.\\n\\nLet's look at an example for this.\\nLet's go to chapter one, video five.\\nJoins.\\nSo this is the join.\\nI'm using the INNER JOIN first.\\nSo here, I give SELECT * FROM country_tab c.\\nC is the alias name that I'm giving for the table name\\nbecause I don't want to give the table name\\nevery single time I'm using it in any condition\\nbecause that's going to make it very lengthy.\\n\\nSo I just want to use the alias name\\ninstead of the table name.\\nSo I give that.\\nSo country_name, followed by the alias name for the table.\\nAnd then the join name\\nand the second table.\\nAnd its alias name.\\nON keyword, followed by the join condition.\\nSo here we are comparing the country_ids\\nfrom country_tab and states_tab.\\nSo let's run these three\\nto understand the join output better.\\n\\nSo this is the country_tab\\nand this is the states_tab.\\nSo here if we see,\\nwe are trying to retrieve only the matching rows\\nfrom country_tab and states_tab.\\nSo in country_tab,\\nwe have three country_ids\\nbut we have matching states\\nonly for one and two but not three.\\nSo Canada does not have any associated states over here.\\nSo that would be excluded from our output.\\n\\nAnd similarly, in the states_tab.\\nTexas, California and Baja California\\nhave associated country_ids\\nwhile Kerala does not.\\nSo Kerala also does not have a matching value.\\nSo that's the reason why Kerala also would be excluded.\\nSo in our output, we will have USA and Mexico.\\nAnd we will have all the states,\\nthat is Texas, California and Baja California.\\nAnd Canada and Kerala have been excluded.\\n\\nThis is because it's the inner join.\\nNow let's go take a look at the other outer joins.\\nNow, SQL supports three kinds of outer joins,\\nthat is left, right and full outer joins.\\nBut SQLite supports only left join,\\nso we will take a look at that first,\\nand then I'm going to explain how the right\\nand full outer join would work like.\\nSo if we take a look at the left outer join,\\nit gets all the rows from left table\\nand only the matching rows from the right table.\\n\\nSo if we look at this example here,\\nI'm using the left outer join and I run this.\\nSo it's going to get all the rows\\nfrom the left table\\nand only the matching rows from the right table.\\nThe reason I say this is the left table\\nis because when I put it like this,\\nthe country_tab is to the left side\\nso that's the left table,\\nand states_tab is to the right,\\nso that's the right table.\\n\\nI just indented it like this for readability purpose.\\nSo here we retrieve all the rows from the left table.\\nSo in our output, we would have USA, Mexico and Canada.\\nAnd only the matching rows from the right table.\\nThat is Texas, California and Baja California.\\nAnd Kerala is excluded\\nbecause it does not have an associated country_id.\\nAnd since Canada does not have any state associated to it,\\nthose columns are set to null.\\n\\nNow if I give RIGHT JOIN here,\\nthen it would change to something like this.\\nWe'll get only the matching rows from the left table\\nand all the rows from the right table.\\nSo we will have USA and Mexico,\\nand we'll have all the states from the states_tab.\\nOnly Canada will be excluded\\nbecause that's from the left table.\\n\\nAnd that doesn't have any matching rows here.\\nAnd if I'm using the OUTER JOIN,\\nit fetches all the rows\\nfrom both the tables irrespective\\nof whether they have matching rows or not.\\nSo we will have USA, Mexico, Canada\\nand all the four states.\\nWhen we write a SELECT statement in a clause\\nof another SELECT statement, it is known as a subquery.\\nIt's also called inner SELECT or nested SELECT.\\nThe inner SELECT statement will be executed first.\\n\\nThen its output is taken as input\\nfor the outer SELECT to retrieve data.\\nSubqueries must be enclosed in parenthesis.\\nLet's look at an example.\\nSo in the first example,\\nthis is my outer SELECT.\\nAnd the one in the parenthesis here\\nis my subquery.\\nSo this gets executed first.\\nSo it gets the department number for employee 7001.\\n\\nAnd then it fetches all the employees\\nwho belong to that particular department number.\\nSo when I run this,\\nsince 7001 belongs to department 40,\\nit has fetched all the employees belonging\\nto that particular department.\\nSo this is another example\\nwhere we are using the same tables\\nin both the outer and the inner join.\\nSo here we are getting the country_id for California\\nand then we are trying to fetch all the states\\nassociated with that particular country_id.\\n\\nSo when I run this,\\nthese are the two states\\nthat are associated with this country.\\nSo since the inner query gets executed first,\\nwhen we get the country_id for California,\\nit returns an output of one.\\nAnd then the outer query gets executed.\\nSo we get the states associated\\nwith that particular country_id.\\nWe can also use different tables\\nin the outer and inner queries.\\n\\nHere I'm using the country_tab\\nand inside, I'm using states_tab.\\nSo what I do is I get the country_id\\nfor California from the states_tab, which is one.\\nAnd then I am running the outer SELECT statement,\\nthat is SELECT * FROM country_tab.\\nSo it's going to fetch the country details\\nfor this particular country_id.\\nSo depending on our requirement,\\nwe can use different kinds of subqueries.\\n\\nSo when I run this.\\nIt's going to get me the country details\\nfrom the country_tab\\nbased on the country_id of California.\\nThere are two types of subqueries:\\nsingle row and multiple row.\\nSingle row subqueries return single rows\\nand we use single row comparison operators,\\nlike equals to, not equals to, less than,\\nless than, equals to,\\ngreater than, greater than, equals to.\\nWe can have multiple single row subqueries.\\nThat is inner queries, unlike what we have seen\\nin our examples.\\n\\nAs we have seen, inner and outer queries\\ncan fetch data from different tables.\\nNow, when it comes to multiple row subqueries,\\nwe can use operators like IN, ANY, and ALL.\\nAnd we can also use group functions in subqueries.\\nSo let's look at some examples for these.\\nSo here's how we can use a group function.\\nIn the inner query, I'm using MAX function.\\nSo it's going to fetch the maximum salary\\nfrom the emp_tab.\\n\\nAnd then it's going to populate the employee details\\nfor that particular salary.\\nSo when I run this,\\nthis is the employee with the highest salary.\\nSo that's what's going to be fetched\\nbecause it's going to get the max salary\\nfrom emp_tab, that is 5,000\\nand then it's going to run this outer SELECT statement.\\nNow, this is how we can use IN, ANY or ALL.\\nIn this example, we are taking IN.\\n\\nSo here, first the inner SELECT statement gets executed.\\nSo here we get the salaries of all employees\\nwho belong to department 30.\\nSo when this gets executed,\\nwe get these salaries\\nand then, once we have the list of salaries,\\nwe are going to run the SELECT statement,\\nthat is the outer SELECT statement.\\nSo it's going to fetch all the employee details\\nwhose salaries are in this set of values.\\n\\nSo when I run this,\\nthese are the employees whose salaries\\nare in this set of values.\\nSo this is all about joins and subqueries.\\n\"}],\"name\":\"1. The Select Statement\",\"size\":80521107,\"urn\":\"urn:li:learningContentChapter:4361518\"},{\"duration\":1385,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4362480\",\"duration\":337,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Create new tables\",\"fileName\":\"3086685_en_US_02_01_XR30\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to create new tables using different techniques based on the information you have.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10716172,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that we have learned\\nhow to fetch and filter data from existing tables,\\nlet's learn how to create new tables.\\nFor this, we use the create statement.\\nIt is a DDL statement,\\nthat is, DDL stands for Data Definition Language,\\nand all DDL statements are auto commit,\\nwhich means that once they are executed\\nthe changes will be permanent and cannot be reverted back.\\nThis is the syntax for the create statement.\\n\\nWe can check if the table is created by giving\\ndescribe table name or desc table name.\\nTables which do not belong to this user\\nare not in this user's schema.\\nIf we are creating a table in a different schema,\\nwe will need to specify the schema name\\nin the create statement.\\nWe can also create a table using a sub query.\\nWe use this method when we want to create a table\\nby using another table's data.\\n\\nBut this feature is available\\nin a few databases like Oracle.\\nIf column specifications are given,\\nthen number of column specifications\\nand number of columns in the sub query must be equal.\\nWhen using sub query method,\\ncolumn definition can contain only column names\\nand default values, but no constraints.\\nNow, let's understand what are constraints.\\nConstraints are like rules to follow\\nat table level or column level.\\n\\nWe can define these constraints\\nat the time of creation of the table,\\nor after the table has been created.\\nConstraints that are defined must be satisfied\\nto completely execute the statements.\\nOtherwise, the statements will not be executed\\nand it throws some errors.\\nWe have to name the constraints for easy identification.\\nNow, let's take a look at some of the constraints.\\nPrimary key is a unique and not null value for every row.\\n\\nThis helps identify every row in the table.\\nNot null constrained, as the name implies,\\nspecifies that a column cannot contain a null value.\\nLet's look at an example.\\nLet's go to chapter two, video one.\\nSo here, we are giving create table countries and then\\nwe are giving all the column names.\\nHere, if we observe carefully, the not null constraint\\nis given with the column specification,\\nas it is a column level constraint.\\n\\nIf we see here, we are giving it along with the column.\\nSo, we gave it the column name as country code,\\nthe data type, followed by the constraint,\\nthe constraint name, and what kind of constraint it is.\\nSo, we are defining it near the column specification.\\nSo, it is a column level constraint.\\nThe primary constraint is a table level constraint,\\nas it is specified after all the column specifications.\\n\\nIf we notice carefully here in the code, this constraint,\\nwhich is the primary key constraint, is given\\nall the way after the column specifications are done,\\nand not like the not null constraint,\\nas we give with the column specification.\\nSo, the primary key constraint is a table level constraint.\\nThe third constraint is foreign key\\nor referential integrity constraint.\\n\\nThis assigns one or more columns as foreign key\\nand establishes a connection with the primary key\\nof the same or different table.\\nForeign key values must match\\nwith a value in the parent table or must be null.\\nLet's look at an example for the foreign key.\\nSo here, for this table, the primary key is the state ID.\\nThe foreign key is defined on country ID.\\n\\nIf you take a look over here,\\nthe primary key for states table is state ID,\\nand the foreign key here is imposed on the country ID.\\nSo, this establishes a connection between\\nthe country ID from this table\\nand the country ID from the countries table.\\n\\nSo if we see, it is referencing the country ID\\nfrom countries table.\\nSo, this means that we can insert a country ID\\nin the states table only if that country ID\\nis in the countries table.\\nOtherwise, it throws an error.\\nSimilarly, we cannot delete a country ID\\nfrom the country table while that country ID\\nis mapped with some states in the states table.\\nIt again throws an error, since there are\\nsome dependencies on that country ID in the states table.\\n\\nSo, we call countries table as the parent table\\nand states table as the child table.\\nSo, this is how primary key and foreign key\\nwork hand in hand.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4364476\",\"duration\":216,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Change table structure\",\"fileName\":\"3086685_en_US_02_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to change the table structure like adding new columns to a table and so on.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6350006,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Let's learn how to change the table structure\\nby adding or modifying columns, renaming them,\\ndefining a default value for the new column,\\ndropping columns from a table.\\nWe can do this by using the ALTER statement.\\nALTER statement is a DDL statement like \\\"create\\\"\\nso it auto commits.\\nThis is the syntax for ALTER statement to add new columns.\\nIf you think that there is a need for a new column\\nat a later point of time\\nthen we can use this ALTER statement to do so.\\n\\nLet's try to add a new column to \\\"states\\\" table.\\nLet's go to chapter two, video two.\\nHere, ALTER TABLE states, that's the table name,\\nADD COLUMN,\\nthis is the new column that I want to add,\\nand the data type for it.\\nBefore running this I would like to show you\\nhow it looks like in the database first.\\nLet's go to \\\"states\\\" table.\\nAnd if you take a look at the columns,\\nwe do not have test column present.\\n\\nSo now let's try to add this new column\\nthere.\\nSo we have that column added.\\nLet's go to the database again,\\ngo to \\\"states\\\" table.\\nAnd if we scroll all the way to the right,\\nwe can see our new column.\\nThis is how we can add a new column.\\nThis is the syntax to rename a column name.\\nThis is to rename a table name\\nand this one is to drop a column.\\n\\nLet's look at some examples for these in Codespace.\\nTo rename the table, I give something like this.\\nALTER TABLE,\\ntable name,\\nRENAME TO,\\nand this is the new table name, that is \\\"new states.\\\"\\nSo now when I run this,\\nso the table name has changed.\\nLet's go to the database and if we take a look over here,\\nwe can see that the table name has changed to \\\"new states.\\\"\\nLet's go back to our SQL file.\\n\\nNow, if I want to rename the column name,\\nI have to ALTER TABLE,\\nand since we have modified the table name to \\\"new states\\\"\\nI give that name, RENAME COLUMN.\\nThis is the column name which I want to modify,\\nTO,\\nand this is the new column name, that is \\\"testing.\\\"\\nSo we have to see if the column name\\nhas renamed to \\\"testing\\\" or not.\\n\\nSo first, let's run this.\\nLet's go to the database.\\nAnd under \\\"new states,\\\"\\nthe column name has been renamed to \\\"testing.\\\"\\nThe drop feature is not supported in SQLite\\nbut we can use the syntax to use with other databases.\\nSo this is the syntax, it's ALTER TABLE, table name, DROP,\\nand the column name.\\n\\nSo this is how it works.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4365499\",\"duration\":389,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Add new rows to a table\",\"fileName\":\"3086685_en_US_02_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to insert new rows in a table like inserting a single row or inserting multiple rows as a script.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11904695,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that we know how to create tables,\\nlet's learn how to add new rows into the table.\\nWe use the INSERT statement to do that.\\nThere are three approaches to insert rows into a table.\\nThe first one is to insert one row at a time\\nusing the INSERT statement.\\nThe second approach is by using a script.\\nSo we can insert multiple rows at a time\\nby using ampersand substitution\\nin the INSERT statement\\nto prompt for runtime values.\\n\\nWe'll understand this a little better in the next slides.\\nThe third approach is to copy rows from an existing table\\nto a new table.\\nThis way, we can copy multiple rows at a time\\nby using a subquery.\\nNow let's take a look at each approach.\\nThe syntax to insert one row at a time\\nis INSERT INTO table_name,\\nfollowed by the columns names, VALUES keyword,\\nfollowed by the values corresponding\\nto the columns that we list down here.\\n\\nSo now let's take a look at a few examples for this.\\nLet's head to the folder chapter two, video three,\\nand here let's take a look at the table first.\\nIf we observe, these are the column names,\\ndepartment number, department name,\\nmanager ID and location_id.\\nSo now when I try to insert a row into the table,\\nif I'm not changing the order of the columns,\\nand leaving them as is in the table,\\nthen we don't have to specify\\nthe column names explicitly here\\nin the INSERT statement.\\n\\nI can just give the values\\nand it is intelligent enough\\nto identify that these values belong to those columns.\\nSo when I run this,\\nthe row has been inserted.\\nAnd now when I run the SELECT statement,\\nwe can see that row has been inserted over here.\\nNow in this INSERT statement,\\nI'm changing the default order\\nof the column names.\\n\\nSo after department number,\\ninstead of giving department name,\\nI have put location_id.\\nSo I have to explicitly mention the column names\\nfor it to understand that these are the values\\nfor these column names.\\nAnd I'm giving the values also corresponding\\nto this columns.\\nSo now when I run this,\\nit's been inserted.\\n\\nLet's look at the table.\\nThe new row has been inserted.\\nNow, the third example is to add null values.\\nSo when I run this,\\nthe new row has been inserted.\\nAnd here we are trying to insert some NULL values\\nin some of the columns.\\n\\nSo when I run this,\\nthe new row has been inserted\\nwith some null values over here.\\nAnd also remember that null value\\nis not the same as a blank space.\\nSo when there is no value specified for a column,\\nit is taken as a null value as well.\\nNow this is the syntax for inserting multiple rows\\nat a time using a script.\\n\\nSo when I run this particular statement,\\nit'll ask us to enter values\\nfor these columns at runtime.\\nSo in the second approach,\\nwe are trying to insert multiple rows at a time\\nby using a script.\\nSo let's take a look at the syntax.\\nINSERT INTO table_name VALUES,\\nfollowed by the column names.\\nBut if you observe,\\nwe have given the ampersand symbol\\nbefore the column name.\\n\\nSo what this implies is it's going\\nto prompt us to enter values\\nfor these column names at runtime.\\nSo all we need to do is give the values at runtime,\\nand it takes multiple rows, values at a time\\nand it inserts those into the table.\\nSo this is going to save us a lot of time\\nbecause we don't have to type the INSERT statement\\nover and over again.\\n\\nInstead, we run the same INSERT statement\\nand just type in the values every single time.\\nAnd we should be able to add more rows\\nin the same amount of time.\\nThe other way to do so\\nis by creating a script file.\\nA script file is nothing but a collection\\nof SQL statements put together and stored in a file.\\nAnd that file is executed whenever it's needed.\\nThe third approach is to copy multiple rows\\nfrom another table by using a subquery.\\n\\nWhen we are using a subquery in the INSERT statement,\\nwe do not have to give VALUES keyword.\\nNumber of columns in the subquery\\nmust be equal to the number of columns in the INSERT.\\nFor example, if we want to create an exact replica\\nof an existing table,\\nwe just give INSERT INTO new_table\\nSELECT * FROM old_table_name\\nand WHERE condition.\\nThe WHERE condition is optional.\\n\\nIf we do not give the WHERE condition,\\nall rows from the old table are copied to the new table.\\nBut if a condition is specified,\\nonly those rows which satisfy that criteria\\nwill be copied to the new table.\\nINSERT statement is a Data Manipulation Language statement,\\nalso known as a DML statement.\\nWe need to commit the INSERT statement changes\\nto make them permanent.\\nSo this is about the INSERT statement.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361514\",\"duration\":186,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Delete rows in a table\",\"fileName\":\"3086685_en_US_02_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn about deleting rows from a table.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6119093,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this video,\\nwe will learn about the DELETE statement.\\nIt is used to delete rows from a table.\\nThere are two approaches to delete rows.\\nOne is a simple WHERE condition.\\nJust like other DML statements,\\nif we give the WHERE condition,\\nit's going to delete only specific rows\\nwhich satisfy that criteria.\\nOtherwise, all the rows in the tables will be deleted.\\nSo this is how we will give it,\\nDELETE FROM table_name WHERE condition.\\nThe second approach is by using subqueries.\\nWhen we want to delete rows\\nbased on another table,\\nwe use subqueries.\\n\\nLike, DELETE FROM table_name\\nWHERE column name equals a subquery in the parenthesis.\\nIt need not be an equals to sign.\\nIt can be any other operators\\nthat we have learned earlier.\\nSo let's take a look at few examples for this.\\nLet's go to chapter 02, video 04, Delete.sql.\\nClick here and run the SELECT statement.\\nSo this is our table emp_tab.\\n\\nNow here, I'm trying to delete the employee\\nwith employee number 7,007.\\nSo it's executed.\\nNow let's take a look at emp_tab table.\\nSo that row has been deleted.\\nNow, to understand the subqueries,\\nlet's take a look at an example.\\nSo here, SELECT * FROM old_emp_tab.\\nThis is another table\\nwhich is an exact replica of emp_tab table.\\n\\nSo it has all these row.\\nNow I'm using the employee_tab table\\nto get the salary\\nwhich is less than thousand.\\nAnd then I want to delete\\nthat particular row from old_emp_tab,\\nand not from emp_tab.\\nI'm going to run this.\\nSo there's only one record\\nand that record has been deleted.\\n\\nThere was only one salary\\nwhich was less than thousand.\\nThat was employee 7,006.\\nSo that row has been deleted now.\\nIf we try to delete a row\\nwhich contains a primary key\\nthat is being used\\nas a foreign key in another table,\\nit's going to return an integrity constraint error.\\nIt does not allow us to delete departments\\nif some employees are already assigned\\nto that department number,\\nas there are some dependencies\\nfor that department number in other tables.\\n\\nSo that is about integrity constraint error.\\nDELETE statement is also a DML statement,\\nso it needs to be committed explicitly,\\nand it does not auto commit by itself.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4362481\",\"duration\":257,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Commit and rollback\",\"fileName\":\"3086685_en_US_02_05_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn about the difference between DDL and DML commands. Also, learn when and how to use commit and rollback.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7169718,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now we learn\\nabout Transaction Control Language\\nor also called as TCL.\\nThere are three TCL statements;\\nthat is Commit, Rollback, and Savepoint.\\nA transaction is a set of statements\\nwhich begin when the DML statement is executed\\nand ends when Commit or Rollback is issued,\\nor when a DDL or a DCL statement is executed,\\nauto commit will occur,\\nor if the user exits the system or the system crashes.\\n\\nSo these are the possible scenarios when a transaction ends.\\nNow let's learn\\nabout the first TCL statement, that is Commit.\\nCommit ends the current transaction\\nand saves all the data changes,\\nthus, making them permanent.\\nFor DML statements, we need to give Commit\\nin order to save the changes,\\nunlike DDL and DCL statements, which are auto committed.\\nAuto commit also occurs when the user exits the system\\nnormally without explicitly executing a Commit statement.\\n\\nNow let's compare the data before and after Commit.\\nBefore Commit,\\nthe data changes can be reverted back to the previous state.\\nAfter Commit, all data changes are made permanent\\nand cannot be reverted back.\\nBefore Commit, the current user can view results\\nby using the SELECT statements,\\nbut others cannot view these changes made by the user.\\nAfter Commit, all the users can view the changes\\nmade irrespective of who makes the changes.\\n\\nBefore Commit, affected rows are logged,\\nwhich does not allow other users\\nto make any changes to the affected rows.\\nAfter Commit, locks on affected rows are released\\nand those rows are available to other users\\nto make any other changes.\\nThe next one is Rollback and Savepoint.\\nThey both work hand in hand.\\nRollback discards all data changes\\nand ends the current transaction.\\nSavepoint creates a marker point within a transaction.\\n\\nBy using Commit and Rollback,\\nwe can preview the data changes\\nbefore making them permanent.\\nAuto rollback occurs when there is a system failure\\nor abnormal termination of the system.\\nBy using Commit and Rollback,\\nwe can preview data changes before making them permanent.\\nNow as we can see in this diagram,\\nfirst I issued a Commit to save all the pending changes.\\nNow I start a new transaction.\\n\\nI give a DELETE statement followed by Savepoint A.\\nThen I give a DML statement\\nand then give Savepoint B\\nas a marker at that point.\\nAfter that, I give a few other DML statements.\\nIf I just give Rollback,\\nit'll end this current transaction\\nand discard all the changes.\\nIf I give Rollback to Savepoint A,\\nthen it'll rollback current transaction to the Savepoint.\\n\\nThereby discarding all changes made after Savepoint A.\\nSimilarly, if I give Savepoint B,\\nit'll discard all changes made after Savepoint B.\\nNow here is a comparison of the data\\nbefore and after Rollback.\\nBefore Rollback, changes made cannot be reverted.\\nAfter Rollback, data changes can be discarded.\\nBefore Rollback, the affected rows are logged\\nso that other users cannot make any changes to data\\nwithin the affected rows.\\n\\nAfter Rollback, all locks on affected rows are released.\\nBefore Rollback, the current user can review results\\non data changes by using SELECT statements.\\nOther users cannot view the results\\nof the data changes made by this user.\\nAfter Commit, since the data changes are discarded,\\nnobody can see those changes in their database.\\n\"}],\"name\":\"2. New Tables and Data\",\"size\":42259684,\"urn\":\"urn:li:learningContentChapter:4359514\"},{\"duration\":596,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4358589\",\"duration\":371,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Modify rows and columns in tables\",\"fileName\":\"3086685_en_US_03_01_XR30\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to update existing rows and columns based on the requirements.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13129741,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this video,\\nwe will learn about UPDATE statement.\\nIt's a DML statement.\\nWe use this to modify data in a table.\\nNow, this is the syntax for UPDATE statement.\\nUPDATE table_name SET column_name equals to some value.\\nAnd if you're trying to update multiple columns,\\nthen we give column names\\nand assign some values to them, separated by commas.\\nAnd then the WHERE condition.\\nJust like other DML statements,\\nif we specify the WHERE condition,\\nthen it updates only those rows\\nthat satisfy that criteria.\\n\\nIf we do not give the WHERE condition,\\nit updates all the rows in that particular table.\\nSo now let's try to modify one row in emp_tab table.\\nWe are trying to update emp_tab table\\nby setting the salary to 8,000 for emp number 7001.\\nFirst, let's take a look at the table.\\nI give SELECT * FROM emp_tab.\\n\\nRun this.\\nSo if you take a look at the first record,\\nthat is emp number 7001,\\nhe has a salary of 5,000.\\nNow I want to update that salary to 8,000.\\nSo I run this.\\nAnd when I take a look at the table now,\\nhis salary has updated to 8,000.\\nNow when we are updating only one row,\\nit's always important to identify the row\\nwith the column value that is unique for that specific row.\\n\\nIn this case, we are using the emp number\\nas it is unique for every employee.\\nInstead of employee number,\\nif we are trying to find by the name,\\nit might update multiple rows at a time\\nbecause names cannot be unique\\nfor all employees.\\nThere might be more people with the same name.\\nSo it is very important\\nthat we identify the unique value for every row.\\n\\nWe can modify multiple columns at a time\\nusing subqueries in the SET clause.\\nFirst, let's take a look at the emp_tab table.\\nSo now this is our emp_tab table.\\nAnd if we observe the first two rows,\\nwe will be modifying Clark's manager\\nand salary to King's manager and salary.\\nSo here, if we take a look at the UPDATE statement,\\nI'm trying to modify multiple columns in emp_tab table\\nfor employee number 7002.\\n\\nThat is for Clark.\\nAnd I want to modify them to the manager\\nand salary of employee number 7001,\\nthat is King's manager and salary.\\nSo when I run this UPDATE statement,\\nand we go to the table,\\nso now when we take a look at Clark's manager,\\nit's set to the same value as King's manager.\\n\\nAnd the salary is also set to the same as King's salary.\\nSo now we have modified Clark's manager and salary.\\nWe can modify rows in same tables\\nor another table as well by using these subqueries.\\nSo here, in this example that we've already seen,\\nwe are using the same table here,\\nas well as in the subqueries.\\nIn the next example,\\nwe'll be using two tables,\\nthat is old_emp_tab table and emp_tab tables.\\n\\nSo first, let's take a look\\nat our old_emp_tab table.\\nSELECT * FROM old_emp_tab table.\\nAnd I run this.\\nSo if we take a look\\nat the first row,\\nKing belongs to department number 10.\\nIt hasn't been updated for quite some time.\\nSo now I would like to update it\\nto the latest department\\nto which he is assigned,\\nwhich is in emp_tab table.\\n\\nSo what I do is I give something like this.\\nUPDATE old_emp_tab table SET department number to.\\nWe are fetching the department number\\nfor King from the new table\\nand we are assigning it to the department number\\nin the old table.\\nSo now when I run this,\\nand now let's take a look at our old_emp_tab table.\\n\\nFor King,\\nthe department number is now set to 40,\\nwhich is the latest department\\nto which he belongs currently.\\nNow, if we try to modify a row to a value\\nthat does not exist in the parent table,\\nit returns integrity constraint error.\\nFor example, if I'm trying to update emp_tab table\\nand I want to set the department number to 500\\nwhere the department number is 10,\\nit throws an error.\\n\\nThis happens because department number 500\\nis not there in the ept_tab table,\\nwhich is the parent table for department\\nin the emp_tab.\\nThis is about the UPDATE statement.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4362482\",\"duration\":71,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Merge rows in a table\",\"fileName\":\"3086685_en_US_03_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to merge multiple rows in a table.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2205365,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this video,\\nwe will be learning about the last DML statement.\\nThat is merge.\\nIt updates or inserts data in tables based\\non a condition.\\nIf the row is already there in the table,\\nit updates it,\\notherwise it inserts a new row in the table.\\nInstead of giving loops,\\nwe just use one merge statement.\\nHence, it improves performance.\\nIt's useful in data warehouse systems.\\nAs the data comes from various sources\\nand there is a possibility\\nfor duplicate records to be inserted into the table.\\n\\nTo prevent that, we use the merge statement.\\nThis is the syntax.\\nThis table name is the table\\nwhere all the rows must be inserted or updated.\\nThis is the source from where we get the data\\nto be inserted or updated.\\nON clause has the condition\\nwhich determines whether to insert or update.\\nWhen matched or when not matched clause\\ninstructs the server what to do\\nwhen the condition is matched or not matched.\\n\\nSo this is how the merge works.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361515\",\"duration\":117,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Drop or truncate a table\",\"fileName\":\"3086685_en_US_03_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to drop an existing table and know the different constraints that might prevent us from deleting tables. Also, learn the difference between drop and truncate table.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3598689,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's learn about\\nthe three DDL statements\\nthat is drop, truncate, and rename.\\nThe first one is DROP statement.\\nDROP statement will delete everything,\\nthe data and the structure of the table.\\nIt's auto committed,\\nso changes made will be permanent.\\nA person should have drop any table\\nor he should be\\nthe creator of the table\\nto drop the table.\\nNow let's try one of these examples.\\n\\nHere, I want to drop the table\\ndept_copy_tab table.\\nFirst, let's take a look\\nat the table in our database.\\nSo we have a table,\\nempty table, which is dept_copy_tab.\\nIt does not have any rows.\\nSo now,\\nI try to drop this particular table.\\n\\nNow let's take a look at the database.\\nIf we see, dept_copy_tab has been dropped.\\nThe next one is TRUNCATE statement.\\nIt'll delete all the rows from the table\\nand it'll also release the storage space\\nthat was used by the table.\\nYou might say that\\nwe can just use the DELETE statement\\nto delete the rows.\\nWhy use TRUNCATE?\\nTRUNCATE is a DDL statement\\nand the changes made are permanent,\\nand the previous state cannot be recovered.\\n\\nBut when we use DELETE,\\nwe can still revert back the changes\\nby using rollback.\\nThe other difference\\nbetween TRUNCATE and DELETE is that\\nTRUNCATE will release the storage space\\nthat was used by the table,\\nbut DELETE does not do so.\\nIt just deletes all the rows from the table.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361516\",\"duration\":37,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Rename a table\",\"fileName\":\"3086685_en_US_03_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to rename a table.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":907614,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We will be learning\\nabout the last DDL statement that is rename statement.\\nUsing this, we can change the name of a table, view,\\nsynonym, or sequence, but to do so,\\nwe should be the owner of the object.\\nSo this is the syntax for rename.\\nIt says: RENAME table1 TO table2.\\nSo this is the name, original name of the table,\\nwhich I want to change to this one.\\n\\nSo this is going to be syntax for the rename statement.\\n\"}],\"name\":\"3. Modifying Existing Tables\",\"size\":19841409,\"urn\":\"urn:li:learningContentChapter:4361519\"},{\"duration\":47,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4361517\",\"duration\":47,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"3086685_en_US_04_01_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":0,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Explore the next steps for learners after this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1217035,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Thank you so much\\nfor joining me for this course.\\nI hope you enjoyed learning about SQL statements\\nand how they can be used to create,\\nedit and retrieve information from databases.\\nIf you wish to continue your database journey,\\nI suggest you learn a bit more\\nabout database objects other than tables,\\nsuch as views, synonyms, index, et cetera.\\nYou could also dive deep and learn more about constraints.\\n\\nI recommend checking out some books like,\\n\\\"Getting Started with SQL\\\", by Thomas Neild\\nand \\\"SQL Practice Problems\\\", by Sylvia Moestl Vasilik,\\nwhich helps you practice more.\\nThanks again and I hope you have a wonderful day.\\n\"}],\"name\":\"Conclusion\",\"size\":1217035,\"urn\":\"urn:li:learningContentChapter:4361520\"}],\"size\":149254862,\"duration\":4761,\"zeroBased\":false},{\"course_title\":\"SQL Practice: Basic Queries\",\"course_admin_id\":4431564,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":4431564,\"Project ID\":null,\"Course Name\":\"SQL Practice: Basic Queries\",\"Course Name EN\":\"SQL Practice: Basic Queries\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"In this hands-on, interactive course, join instructor David Gassner to practice writing basic queries in SQL, the popular programming language that\u00e2\u20ac\u2122s widely used for relational databases. This course includes Code Challenges powered by CoderPad. Code Challenges are interactive coding exercises with real-time feedback, so you can get hands-on coding practice to advance your coding skills. David helps you boost your skills as a SQL programmer with seven specific, query-focused coding challenges. Practice retrieving sorted data from a specific table or multiple tables, retrieving data filtered on a range of values, retrieving aggregated data with SQL functions, retrieving data filtered with a string pattern, and more.\",\"Course Short Description\":\"Practice writing basic queries in SQL in this hands-on, interactive course with coding challenges in CoderPad.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":87,\"Instructor Name\":\"David Gassner\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Author of 60+ video-based training courses for software developers\",\"Author Payment Category\":\"NONE\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2023-09-13T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"No\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/sql-practice-basic-queries,https://www.linkedin.com/learning/sql-coding-labs-basic-queries-coderpad\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Beginner\",\"LI Level EN\":\"Beginner\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Databases\",\"Primary Software\":\"SQL\",\"Media Type\":\"Interactive\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":1057.0,\"Visible Video Count\":11.0,\"Contract Type\":\"STAFF_INSTRUCTOR\"},\"sections\":[{\"duration\":262,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3331109\",\"duration\":41,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Talking to your database in SQL\",\"fileName\":\"4431564_en_US_00_01_WX30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"SQL is the universal language for relational databases. These code challenges will help you test your SQL skills.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2563798,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [David] SQL or structured query language  \\n is used by everyone from software developers  \\n to business analysts.  \\n It's the universal language for accessing  \\n and manipulating data in a relational database,  \\n and it's an essential tool for anyone who works with data.  \\n SQL is supported  \\n by everything from enterprise server-based products  \\n to tiny file-based systems.  \\n This course will help you test your SQL knowledge.  \\n In each challenge,  \\n you'll be given a data set and instructions,  \\n and then it's up to you to write your own  \\n database queries right in your browser.  \\n Good luck and have fun.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3332093\",\"duration\":94,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"4431564_en_US_00_02_MM30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"This course is designed for anyone working with databases that support SQL. This video shows you what you need to know to get the most from this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2540177,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] This course is designed  \\n for anybody using a relational database,  \\n that is, a database that can support multiple tables  \\n that can be joined at runtime using queries.  \\n Different database products use different versions of SQL.  \\n This particular course is built around code challenges  \\n that run on something called the H2 Database Engine.  \\n This is an open source database product  \\n that can mimic the behavior of various database products.  \\n These code challenges work with H2 Database Engine  \\n in its compatibility mode,  \\n which means that its syntax is the same  \\n as with the product MySQL and MariaDB.  \\n When working through these code challenges,  \\n you'll be using a database schema,  \\n a set of tables that are designed to link together.  \\n They're built for a fictional restaurant brand,  \\n and so you have customers of the restaurant,  \\n dishes that the restaurant serves, orders,  \\n a table that links the orders and dishes tables together,  \\n and then reservations and events.  \\n None of the code challenges  \\n are going to use all of these tables,  \\n and at the beginning of each challenge,  \\n I'll show you which tables in this schema will be used.  \\n Before you try these challenges,  \\n you may want to watch some instructional content,  \\n and I highly recommend the course SQL Essential Training,  \\n which takes you through the basic concepts of SQL  \\n and the most common commands that you'll use  \\n when you work with relational databases.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2110106\",\"duration\":127,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Exploring the lab environment\",\"fileName\":\"4431564_en_US_00_03_C_MM30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"This course's code challenges are hosted in CoderPad. After watching this video, you'll know how to navigate the CoderPad environment.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3748656,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] This course consists of automated  \\n code challenges that appear when you click  \\n on the challenge links in the course's table of contents.  \\n Each challenge includes instructions  \\n and a code editor you can use to create  \\n and test your own solution to the challenge.  \\n These challenges are hosted by CoderPad,  \\n and they appear in the same area of the course page  \\n where you watch the course's videos.  \\n We recommend using a desktop browser  \\n for the best experience with code challenges,  \\n but you can use the LinkedIn Learning mobile app  \\n if you prefer.  \\n The code challenge has three areas.  \\n Instructions in the top left,  \\n a console for output in the bottom left,  \\n and a code editor for your answer on the right.  \\n You can use these drag handles to reallocate space  \\n for the code editor or any other content.  \\n To get even more horizontal space for the code editor,  \\n you can collapse the course's table of contents on the left.  \\n Each challenge has instructions that include a description  \\n of the challenge and the challenge's desired result.  \\n In this example, the desired result  \\n has a single value named price.  \\n Create your answer in the code editor.  \\n When you click Test my code,  \\n you'll see a message indicating  \\n whether your code returned a correct result.  \\n If you don't see that result immediately,  \\n make sure that you've allocated enough space.  \\n In this example, I'm seeing incorrect output  \\n because I'm querying everything from a particular table  \\n in the database.  \\n Now, I'm going to change this to the correct answer.  \\n I'll use the max function and pass in price  \\n and then I'll rename the result price.  \\n I'll test the code again  \\n and this time I'll see that I got the correct output.  \\n If any messages are too long to fit in the console,  \\n you can scroll sideways to see all of the text.  \\n And when you finished each code challenge,  \\n return to the course's table of contents  \\n and click the next video to see my solution.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":8852631,\"urn\":\"urn:li:learningContentChapter:2109120\"},{\"duration\":748,\"entries\":[{\"urn\":\"urn:li:learningContentAssessment:64e3cd49498e3906e81577f8\",\"duration\":900,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Get sorted data from a table\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:836669\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:3331108\",\"duration\":95,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Get sorted data from a table\",\"fileName\":\"4431564_en_US_01_02_C_MM30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Retrieving and sorting data from a database is the most common use of SQL. This video shows the instructor's solution to this code challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3236250,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] This code challenge  \\n uses just one of the tables in this database,  \\n the Customers table.  \\n This table has many columns,  \\n starting with the primary key of CustomerID  \\n and continuing with a whole bunch of different columns  \\n for first name, last name, and so on.  \\n In this challenge,  \\n you're asked to return just a few of the columns  \\n sorted by the last name and then the first name.  \\n The beginning code  \\n selects everything from the Customers table,  \\n and when you test the code with this solution,  \\n you'll get incorrect output  \\n and it'll show that you're returning everything  \\n in that database table sorted by the primary key,  \\n the CustomerID.  \\n Now I'm going to paste in my solution.  \\n This time, I'm only returning three columns:  \\n the FirstName, the LastName, and the Email column.  \\n I'm getting the data from the same table,  \\n the Customers table,  \\n but now I'm sorting the data by the LastName  \\n and then the FirstName columns.  \\n Notice that I specify asc for ascending on both columns.  \\n That isn't actually necessary because that's the default,  \\n but if you want to be explicit, you can put those in.  \\n And when I test my code,  \\n I get back the expected response,  \\n the same data but now sorted alphabetically.  \\n And this time, I'm only getting back  \\n to three requested columns:  \\n FirstName, LastName, and Email.  \\n So that's my solution to this challenge.  \\n What's yours?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentAssessment:64e3cd49498e3906e81577fa\",\"duration\":900,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Retrieve data from multiple tables\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:836670\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:2109119\",\"duration\":89,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Retrieve data from multiple tables\",\"fileName\":\"4431564_en_US_01_04_C_MM30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Joining multiple tables in a database query is an essential SQL skill. This video shows the instructor's solution to this code challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2974876,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] This code challenge  \\n is going to use two tables for my database,  \\n the Customers table and the Reservations table.  \\n The tables are linked on a foreign key,  \\n a one-to-many relationship where each customer  \\n can have one or more reservations.  \\n In the beginning code,  \\n I'm just selecting from the Reservations table,  \\n and when I click Test my code,  \\n I get back a whole lot of data,  \\n but not too much context.  \\n Now, I'm going to paste in my solution.  \\n First of all, I'm getting data from two different tables,  \\n the Reservations and the Customers table,  \\n and I'm using an INNER JOIN,  \\n meaning I'm connecting the two parts  \\n of the foreign key together,  \\n and I'm connecting on the CustomerID columns  \\n of each of the two tables.  \\n I'm sorting the return to data  \\n by the Date in descending order,  \\n so I'll see the newest reservation first.  \\n And I'm only going to retrieve two reservations.  \\n That's done with the TOP 2 clause in the SELECT statement.  \\n Now let's test that code.  \\n As instructed, I retrieve two rows.  \\n These are the newest reservations,  \\n and because I have the inner joins,  \\n I'm connecting that data  \\n to the data from the Customers table,  \\n and retrieving the FirstName, LastName  \\n and Email address of each customer.  \\n So that's how I solved this code challenge.  \\n What was your solution?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentAssessment:64e3cd49498e3906e81577fc\",\"duration\":900,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Retrieve data filtered on a range\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:836671\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:2110105\",\"duration\":101,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Retrieve data filtered on a range\",\"fileName\":\"4431564_en_US_01_06_C_MM30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Filtering data using ranges of values lets you get just the data you need. This video shows the instructor's solution to this code challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3078882,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] This code challenge uses just one table  \\n from my restaurant's database, the dishes table.  \\n This table has a primary key column,  \\n and then three string columns  \\n for name, description, and type,  \\n and a decimal column for the price.  \\n This challenge asks you to filter the data  \\n so you only retrieve dishes  \\n where the price is between a value of eight and nine,  \\n and only to show the dish's name and price.  \\n Here's my solution.  \\n The beginning code just retrieves  \\n everything from the dish's table,  \\n so if I run that code, I'll see incorrect output  \\n and I'll see that there are 22 rows in the table.  \\n Now I'm going to paste in my solution.  \\n I'm only retrieving the dish ID,  \\n that is the primary key, the name, and the price.  \\n And I'm filtering using the between keyword.  \\n This means get everything with a value greater than  \\n or equal to eight, and less than or equal to nine.  \\n And I'm sorting the results by the name column.  \\n This results in retrieving six rows from the table,  \\n and they're alphabetized by the dish's name,  \\n starting with A and ending with P.  \\n That's my solution to this code challenge.  \\n Now, you could've solved this in a couple of different ways.  \\n Instead of the between keyword,  \\n you could've said the price has to be greater than  \\n or equal to eight. and less than or equal to nine,  \\n but the between keyword is a great way to filter  \\n for a range of values using SQL.  \\n It's more concise and it gets the job done.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentAssessment:64e3cd49498e3906e81577fe\",\"duration\":900,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Retrieve aggregated data\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:836672\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:3331106\",\"duration\":132,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Retrieve aggregated data\",\"fileName\":\"4431564_en_US_01_08_C_MM30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"SQL's aggregate functions let you perform calculations based on values from multiple rows of data. This video shows the instructor's solution to this code challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4315732,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this code challenge,  \\n you're asked to find the three customers  \\n that have the most orders historically over time.  \\n You'll work with two tables in this database,  \\n the customers table and the orders table.  \\n These two tables are linked together  \\n by the CustomerID columns.  \\n That's the primary key in the customers table  \\n and the second column in the orders table.  \\n The initial SQL statement just selects data  \\n from the customers table.  \\n I'll test the code  \\n and see, incorrect output, which is expected  \\n but see that the customer table has those three columns.  \\n CUSTOMERID, FIRSTNAME, and LASTNAME.  \\n Now I'm going to select everything from the orders table.  \\n The orders table has 1,000 rows  \\n and only some of them are displayed here.  \\n But you'll see that there's an ORDERID, a CUSTOMERID  \\n and an ORDERDATE.  \\n Now I'll show you my solution to this challenge.  \\n The challenge asked me to return the three customers  \\n who have made the most orders.  \\n So I'm starting with the TOP 3 clause.  \\n That's an easy way to get the three top rows  \\n of a result set.  \\n And then I'm retrieving the CustomerID,  \\n the Customer.FirstName and the Customer.LastName.  \\n To get the CustomerOrderCount,  \\n I'm using an INNER JOIN to link the two tables together  \\n on their CustomerID columns.  \\n And then to get the aggregate,  \\n that is the total count per customer,  \\n I'm using the GROUP BY clause  \\n and I'm grouping on the CustomerID.  \\n And then finally,  \\n to make sure I'm getting the three customers  \\n with the most orders,  \\n I'm sorting by CustomerOrderCount in descending order.  \\n And notice that CustomerOrderCount is the name  \\n that I've assigned to that result column.  \\n So now when I test my code, I get a correct output  \\n and I see the four columns that were requested,  \\n the CUSTOMERID, the FIRSTNAME, the LASTNAME  \\n and the CUSTOMERORDERCOUNT,  \\n which is the result of the count of the OrderID.  \\n That's my solution to this code challenge.  \\n How did you solve it?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentAssessment:64e3cd49498e3906e8157800\",\"duration\":900,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Get data filtered with a string pattern\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:836673\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:2109118\",\"duration\":91,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Get data filtered with a string pattern\",\"fileName\":\"4431564_en_US_01_10_C_MM30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"SQL's wildcard characters can be used to filter data based on string patterns. This video shows the instructor's solution to this code challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3005830,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] This code challenge is only going to use  \\n one table from this dataset, the Customers table.  \\n This table has a primary key of a CustomerID, an integer,  \\n and then a number of string-based columns  \\n including first name, last name,  \\n and for this exercise, the city.  \\n The task in this challenge is to retrieve  \\n all customers who live in a city  \\n where the name of the city starts with the letter R.  \\n The initial query retrieves everything  \\n from the Customers table.  \\n So when I click Test my code,  \\n I see that this table has 100 rows.  \\n Only 12 of them are displayed  \\n but I'm seeing all of the columns from the table.  \\n Now I'll paste in my solution to this challenge.  \\n My solution retrieves just four columns, the CustomerID,  \\n the first name and last name, and the city,  \\n and then the filter is in the WHERE clause.  \\n And I'm using the LIKE operator.  \\n The LIKE operator allows me to use a wild card.  \\n So I start with a pair of single quotes.  \\n Then the starting letter that I'm filtering on  \\n and the percent character as the wild card.  \\n The filtered data will come back  \\n and then I want to sort it.  \\n So I use the ORDER BY clause  \\n and say that I want to sort by city.  \\n So now when I test my code,  \\n I get back five rows and four columns,  \\n and all of these customers live in cities  \\n starting with the letter R.  \\n That's my solution to this challenge.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentAssessment:64e3cd49498e3906e8157802\",\"duration\":900,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Find the most expensive order\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:836674\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:3327138\",\"duration\":115,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Find the most expensive order\",\"fileName\":\"4431564_en_US_01_12_C_MM30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Aggregate functions and sorting can help you find exactly the data you're looking for. This video shows the instructor's solution to this code challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3500037,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this code challenge,  \\n you're asked to retrieve the most expensive order  \\n in this dataset and display the result  \\n including the name of the customer who made the order.  \\n The Orders table in this dataset  \\n doesn't have any information about the value of the order,  \\n so you have to calculate it on the fly.  \\n And I'm going to do that by linking  \\n to the Dishes table that has a price  \\n through the OrdersDishes table that has foreign keys  \\n for both the OrderID and the DishID.  \\n Now, there isn't any information in this data set  \\n about the quantity of dishes in each order  \\n so we're going to assume for this challenge  \\n that each dish in an order has a quantity of one.  \\n And then I'm going to link from the Orders table  \\n to the Customers table by the CustomerID  \\n from Orders to Dishes through the OrdersDishes table  \\n and calculate the total value  \\n returning the most expensive order.  \\n Here's my solution to this challenge.  \\n Notice that I start with a TOP 1.  \\n So I'm only going to retrieve one row  \\n from this resulting dataset.  \\n I'm linking from the Orders table  \\n to all the other tables using INNER JOINS.  \\n and then I'm grouping by the OrderID  \\n so I'm summing the total of the price for each order.  \\n And then to make sure that I'm only getting back  \\n the most expensive order,  \\n I'm going to order by  \\n and I'll sort by the order total column  \\n in descending order.  \\n So the largest number that is the most expensive order  \\n will be at the top of the dataset  \\n and my top one clause means that's all I'm going to display.  \\n So when I test my code, I get back a single row  \\n showing that this customer had an order total of 58.95.  \\n And if I go back and look at the entire dataset,  \\n I'll find that that is indeed the most expensive order.  \\n That's my solution to this code challenge.  \\n How did you solve it?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentAssessment:64e3cd49498e3906e8157804\",\"duration\":900,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Find the average of orders\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:836678\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:3332091\",\"duration\":125,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Find the average of orders\",\"fileName\":\"4431564_en_US_01_14_C_MM30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Aggregate functions can be applied to datasets created with nested SELECT statements. This video shows the instructor's solution to this code challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3506430,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this code challenge,  \\n you're asked to retrieve the average  \\n of all of the orders in this dataset.  \\n To do this, I'm going to use three tables, orders, dishes,  \\n and orders dishes as an intermediary table,  \\n because it has foreign keys to the other two tables.  \\n The initial code retrieves everything from the dishes table,  \\n and when I test that code, I see that there are 22 rows,  \\n and these are all the individual dishes.  \\n But if I select everything from orders dishes,  \\n I see that there are many more items, over 4,000,  \\n because there's a one to many relationship  \\n from order to dish.  \\n Here's my solution to this challenge.  \\n Notice that I have two select statements,  \\n one nested within the other,  \\n and I'm going to start with the nested select statement  \\n that retrieves the sum of all dishes for all orders.  \\n The nested select retrieves two columns,  \\n an order ID and the sum of the price from the dishes table.  \\n I'm joining the tables from orders to dishes  \\n through the orders dishes table as I described,  \\n but then I'm grouping by the order ID,  \\n and that's going to result in a dataset  \\n that has one column for the order ID  \\n and then the sum of the price for that order,  \\n named order total.  \\n Next, I use an outer select,  \\n and I get the average of that order total column,  \\n and the result will be a single row  \\n that has a numerical value  \\n that's the average order for everything in the dataset.  \\n If you prefer, you could rename that column,  \\n I'll say as order average,  \\n and then I'd get back an order average column,  \\n and that would satisfy the requirements  \\n of this code challenge just as well.  \\n So that's my solution to this more complex challenge.  \\n How did you solve this one?  \\n \\n\\n\"}],\"name\":\"1. Code Challenges\",\"size\":23618037,\"urn\":\"urn:li:learningContentChapter:3332094\"},{\"duration\":47,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3331105\",\"duration\":47,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"4431564_en_US_02_01_MM30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"There are many resources available to help you build your SQL skills. After watching this video, you'll know about other SQL courses including those with more advanced code challenges.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1068564,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Narrator] Thank you for joining me  \\n for this set of SQL Code Challenges.  \\n If you liked this course,  \\n you can watch more code challenge courses.  \\n There's one on intermediate queries  \\n and a more advanced course  \\n on the topic of SQL Window functions.  \\n To learn more about the topic,  \\n you can watch one of many available courses on SQL.  \\n SQL Essential Training takes you through the basics  \\n of the language and then to keep up-to-date with new ideas,  \\n you could keep track of the serial course, SQL Weekly Tips  \\n and then this course on data reporting  \\n and analysis is one of many courses available on this topic.  \\n These courses and more are available  \\n as you continue to build your SQL skills.  \\n \\n\\n\"}],\"name\":\"Conclusion\",\"size\":1068564,\"urn\":\"urn:li:learningContentChapter:3332095\"}],\"size\":33539232,\"duration\":1057,\"zeroBased\":false},{\"course_title\":\"SQL for Data Analysis\",\"course_admin_id\":3271025,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":3271025,\"Project ID\":null,\"Course Name\":\"SQL for Data Analysis\",\"Course Name EN\":\"SQL for Data Analysis\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"&lt;p&gt;If you\u00e2\u20ac\u2122re looking to use Structured Query Language in your application development but your story is more prequel than SQL, you need to build a solid foundational knowledge of SQL analysis. In this course, instructor Nikiya Simpson starts with a quick review of SQL basics, then focuses on fundamental SQL queries used in beginning data analysis. Nikiya explains the importance of asking the right questions about your data and how those questions translate into SQL. She also shows how these concepts can be used to derive quick insights from your data to help drive effective decision-making. Nikiya finishes the course by demonstrating how to incorporate queries into tools like Jupyter Notebook to help bring your data to life.&lt;/p&gt;&lt;p&gt;This course is integrated with GitHub Codespaces, an instant cloud developer environment that offers all the functionality of your favorite IDE without the need for any local machine setup. With GitHub Codespaces, you can get hands-on practice from any machine, at any time\u00e2\u20ac\u201dall while using a tool that you\u00e2\u20ac\u2122ll likely encounter in the workplace. Check out both Codespaces videos to learn how to get started.&lt;/p&gt;\",\"Course Short Description\":\"Learn fundamental SQL data analysis techniques especially useful for developers. Explore querying relational database values, filtering results, leveraging functions, and more.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":20535004,\"Instructor Name\":\"Nikiya Michelle Simpson\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"MBA, Developer, Expert in Data-Driven Web Applications\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2023-07-17T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/sql-for-data-analysis-22645200,https://www.linkedin.com/learning/sql-for-data-analysis-revision-2023\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Databases\",\"Primary Software\":\"SQL\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":4257.0,\"Visible Video Count\":19.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":457,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4484707\",\"duration\":36,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Using SQL for data analysis\",\"fileName\":\"3271025_en_US_00_01_WX30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1490339,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - SQL is the most widely used language  \\n for managing data from a relational database.  \\n It's also essential in organizing  \\n and accessing data to get insights into your business.  \\n I'm Nikiya Simpson.  \\n I'm a full stack engineer and consultant.  \\n And this LinkedIn Learning course, we'll focus  \\n on building fundamentals to get you started  \\n with data analysis.  \\n We'll cover SQL skills like using various data types  \\n like dates and strings, utilizing keywords and clauses  \\n and a few easy SQL functions like count and averages,  \\n so you can get started on analysis right away.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2706665\",\"duration\":67,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"3271025_en_US_00_02_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1502586,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Before we begin,  \\n let's discuss some skills you may need to know first.  \\n This is an introductory course for relational databases.  \\n However, you may be more comfortable with this course  \\n if you have some previous experience working with data.  \\n For example, if you have experience working with Excel,  \\n that can be helpful for understanding  \\n the concepts in this course.  \\n During this course, we'll use Visual Studio Code  \\n and GitHub Codespaces to practice writing queries  \\n that will demonstrate the concepts in each section.  \\n The tools and database needed for the course  \\n will populate for you in the Codespace environment.  \\n We'll also use the MariaDB version of MySQL  \\n as the database management system for this course.  \\n You won't need to be specifically familiar with MariaDB.  \\n There will be some commands used  \\n that will be specific to this database management system,  \\n but mostly the queries used in this course can be performed  \\n in any relational database management system.  \\n Before starting the practice materials in this course,  \\n I'm going to hand it over to Ray Villalobos  \\n to discuss how to use Codespaces.  \\n Then, I'll be back for more on this course.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4483693\",\"duration\":299,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Setting up Codespaces\",\"fileName\":\"3271025_en_US_00_03_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12458764,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Ravi] This is Ravi De Lobos,  \\n senior staff instructor at LinkedIn Learning,  \\n and I'm going to show you how to work  \\n with LinkedIn Learning courses using GitHub Codespaces.  \\n Codespaces is a code editor in the cloud  \\n with the full power of Visual Studio Code.  \\n It allows for real-world hands-on practice  \\n that mirrors software development in the workplace.  \\n This course was created using GitHub Codespaces.  \\n Using Codespaces, you have everything you need to get going  \\n without needing complex installations or build tools.  \\n One click and you're ready to go.  \\n First, make sure you've signed up  \\n and that you've logged in.  \\n You just see your profile icon right here.  \\n When you go to the repository for one of our courses,  \\n look for the Code button.  \\n Click on Create codespace.  \\n The first time you open up a codespace,  \\n it might take a few minutes  \\n to create the virtual machine.  \\n Once it's done, you'll enter the code editor environment.  \\n If you're familiar with Visual Studio Code,  \\n this is a special version  \\n of that editor running on GitHub's servers.  \\n The cloud editor's menu lives in a hamburger icon  \\n inside the activity bar.  \\n The menu will let you see the keyboard shortcuts  \\n for your operating system.  \\n Let's find the shortcut for the command palette.  \\n Look in the hamburger menu under View,  \\n and then look for the keyboard shortcut  \\n for your operating system.  \\n The activity bar has the most common tools  \\n you'll need to work with projects.  \\n You can, for example, show or hide the Explorer,  \\n do a search and replace for content,  \\n manage features of the repository, and much more.  \\n By the way, if for any reason,  \\n you lose the activity bar,  \\n you can get it back using the command palette.  \\n Your course can have one  \\n or more extensions pre-installed  \\n based on the course you're taking.  \\n Those can be found in the Extensions panel.  \\n Because this is a browser,  \\n it's easy to inadvertently close  \\n the browser window and lose the editor.  \\n If you do that, don't panic.  \\n Codespaces saves everything you're doing  \\n on a virtual machine.  \\n You can always get the codespace back  \\n for this repo by going to the Code menu  \\n and finding your codespace right here.  \\n Notice that it even remembered  \\n that I had opened the Extensions panel.  \\n The sample course I'm looking at  \\n is what is known as a flat branch repository.  \\n You can tell because there is a single branch  \\n when you click on the branch icon on the status bar.  \\n You can also tell because there are different folders  \\n for each of the videos in the course.  \\n Your instructor will let you know  \\n what the folder structure is for your course.  \\n Forking let's you create your own copy  \\n of the repository on your account  \\n so that you can keep any changes that you've made  \\n even if you've deleted the codespace.  \\n To create your own fork, you can click  \\n on the Fork button on the repository.  \\n I'm going to hit Create fork.  \\n Now, I have my own copy of this repository.  \\n And if I want to, I can start  \\n a new codespace on that fork.  \\n This fork version is almost exactly  \\n like the original repository,  \\n but it will let you push your own changes.  \\n Notice that the URL of the repository is slightly different.  \\n Let's go ahead and make a simple change  \\n to this file right here.  \\n I'm going to save it.  \\n You'll note that there is an M right here  \\n as well as an asterisk on this branch.  \\n In the Source Control panel,  \\n you can also see a 1  \\n for the change that we just made.  \\n Let's go ahead and try to commit this change.  \\n I'm going to hit the Commit button  \\n and I'll ask it to go ahead and state the changes.  \\n And then I'm going to hit this button here  \\n to commit this onto my own version of the repo.  \\n I'll hit OK.  \\n You can also let it go ahead and run git fetch.  \\n That way, it'll automatically sync  \\n with your forked repo.  \\n Now, that change will be stored  \\n in your own version of this repository.  \\n Don't worry, if you forget to fork a repo  \\n and then try to push changes,  \\n Codespaces will also ask you  \\n if you want to create a fork automatically.  \\n Look for additional course specific tips  \\n from the instructor.  \\n Now, let's get back to the course.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4480728\",\"duration\":55,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Getting started with a database in Codespaces\",\"fileName\":\"3271025_en_US_00_04_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1751465,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now that we know how to use Codespaces,  \\n let's get you familiar with how to access the database  \\n we'll use in this course.  \\n Start by opening your Codespace from GitHub.  \\n Go to Code, we'll go to Codespaces,  \\n and click on Create a new codespace on main.  \\n After loading the repository in the Codespace,  \\n you'll notice lots of options here  \\n on the left hand side of your browser.  \\n Let's click on SQL Tools.  \\n Under Connections, you should see H+ Sport  \\n and this is the database we'll use within this course.  \\n You can click on arrows or carets  \\n next to H+ Sport to explore the database tables.  \\n We'll get into more about how to use this extension  \\n as we go along with the course,  \\n but feel free to pause the video and look around.  \\n Now that we have our database ready,  \\n let's get started with learning more about SQL  \\n for data analysis.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":17203154,\"urn\":\"urn:li:learningContentChapter:4479650\"},{\"duration\":975,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4480727\",\"duration\":248,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"A quick review of Structured Query Language (SQL)\",\"fileName\":\"3271025_en_US_01_01_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Review how SQL has developed over time and discover its relevance and popularity in today's job market. When you understand the importance of SQL in today's business applications and in various industries, you can invest in learning more about the language and incorporate using SQL into your work.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8047537,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - In this course, we'll focus on building fundamentals  \\n to get you started with using a database for a project.  \\n But, what is a database anyways?  \\n Well, a database is simply a collection of data  \\n arranged for retrieval of the data by a computer system.  \\n Let's say you're a business owner  \\n and you're selling a product.  \\n Most likely, you want to keep track  \\n of the amount of orders you receive.  \\n You may also want to know the dates of these orders,  \\n so that you can see trends  \\n or know how many orders you get each quarter.  \\n You should also keep track of your customers  \\n so that you can see what they like and what they don't like.  \\n A database is often organized as a collection of tables.  \\n A table is a two-dimensional grid  \\n consisting of rows and columns.  \\n A row is also known as an instance or a record.  \\n A row can be a student, a product, a customer, or an order.  \\n The column is also known as a field or an attribute,  \\n as it describes something about the row.  \\n In the case of a product,  \\n it could be the product's size, color,  \\n product name, or category.  \\n The most widely used language for managing data  \\n from a relational database is known as SQL.  \\n Structured Query Language, or SQL, or S-Q-L,  \\n is the primary language we use to manage  \\n and access structured data  \\n in a Relational Database Management System.  \\n It's how we communicate with the database.  \\n A Database Management System, or DBMS,  \\n is a software application  \\n that helps us store, load, and update data.  \\n We can also use the DBMS to retrieve data  \\n or query directly.  \\n In addition to the relational database model,  \\n there are different types of database models.  \\n Some of the models include the entity-relationship,  \\n or ER model,  \\n and object-oriented relationship models  \\n that we'll discuss later in another section.  \\n Time for a brief history review.  \\n The SQL language was first developed in the 1970s  \\n by IBM researchers Raymond Boyce and Donald Chamberlain  \\n based on the relational model  \\n developed by Edgar F. Codd in 1970.  \\n He wrote the paper,  \\n \\\"A Relational Model of Data for Large Shared Data Banks,\\\"  \\n which provided a theoretical base  \\n for working with structured data.  \\n This research led to the first Commercial  \\n Relational Database Management Systems,  \\n Oracle in 1979, SQL/DS in 1981, and DB2 in 1983.  \\n As you can see, SQL is an old programming language,  \\n so why is it still relevant and why do I need to learn it?  \\n There's an easy answer to this.  \\n Many companies still use it.  \\n According to the 2020 Insights from Stack Overflow,  \\n SQL is the third most commonly used programming language  \\n behind JavaScript and HTML and CSS.  \\n Having a good database understanding  \\n is essential in today's job market.  \\n And it's not standing still.  \\n New concepts in database technology  \\n include processing big data.  \\n Big data is a concept of collecting, organizing,  \\n and analyzing massive amounts of data  \\n from a variety of sources.  \\n You may have heard of technologies,  \\n such as Hadoop MapReduce, NoSQL, and NewSQL.  \\n Big data concepts involve analyzing  \\n and extracting information from enormous data sets  \\n that will require multiple servers  \\n and lots of compute power to process.  \\n As application developers,  \\n we develop code that calls the Database Management System  \\n to perform operations or retrieve data from the database,  \\n and this course will help H+ Sport  \\n display their sales dashboards  \\n and give you essential SQL skills to put in your toolbox.  \\n Let's get started.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4480726\",\"duration\":363,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"What is a relational database?\",\"fileName\":\"3271025_en_US_01_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore the relational database model and interpret a simple Entity Relationship Diagram (ERD). When you understand one of the most popular database models used in business applications today, you find ways to organize data in a way that best suits your organization or project.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9527660,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - A relational database is a type of database  \\n that stores and provides access to data points  \\n that are related to one another.  \\n In most instances, tables are organized by entities.  \\n An entity is a single object,  \\n like a person, place, or thing.  \\n This could be a customer, a store,  \\n an employee, or a product.  \\n The key is that each entity is unique.  \\n We infer relationships between the different entities  \\n to gain insights into our data.  \\n Let's use a cake example. Who doesn't like cake?  \\n Topsy Turvy Baking Company sells beautiful custom cakes  \\n that ship all across the United States.  \\n They have many customers, and I'm one of them.  \\n I want to order a birthday cake for my friend.  \\n They ship me a cake, and it's so good.  \\n I think I'll order another one for me too.  \\n So I place a different order.  \\n So we can see the relationship between customer and orders.  \\n Each customer can make more than one order.  \\n A cake can also have many different flavors.  \\n The company can have one or more locations  \\n to pick up orders in person.  \\n There are lots of different relationships  \\n between the entities, as we see here.  \\n When we talk about relationships between entities,  \\n there are three main different types.  \\n One-to-one, one-to-many, and many-to-many.  \\n A one-to-one relationship may be a customer  \\n and the username they use to log into your website.  \\n The customer name may be Mary Smith,  \\n and their username is Mary Smith without any spaces.  \\n They both refer to the same person or entity.  \\n Therefore, the relationship is one-to-one.  \\n A one-to-many relationship is more like  \\n a relationship between a customer and an order.  \\n One customer can have many orders.  \\n So that relationship is one-to-many.  \\n And a many-to-many relationship is like  \\n the relationship between salespersons and customers.  \\n A salesperson can serve many customers.  \\n And a customer can request orders  \\n for more than one salesperson.  \\n Many-to-many relationships can be  \\n difficult to work with sometimes.  \\n If each salesperson and customer can work with anyone,  \\n we can't really define that relationship.  \\n So we usually use some type of entity in between  \\n to help clarify what that relationship is.  \\n For example, if we want to define  \\n the relationship between a salesperson and a customer,  \\n we can look back at the order they placed.  \\n This order clarifies that the salesperson worked  \\n with a specific customer, and vice versa.  \\n The order is the intermediary between the two.  \\n Clearly, we can see that things  \\n can get complicated really fast.  \\n So how do we visualize these relationships?  \\n Well, we use what's called  \\n an entity-relationship diagram, or ERD.  \\n This is a graphical representation of entities  \\n and their relationships to each other.  \\n It provides the guide to our data,  \\n and a quick view of what data is available to us.  \\n To see an example, check out the GitHub repo  \\n for the ERD in the H+ Sport database.  \\n Just so you know, some people call this  \\n entity-relationship model.  \\n Both are fine, but I like the diagram better,  \\n so that's what we're using.  \\n Each entity has a unique identifier called a primary key.  \\n The primary key is used to identify that specific record.  \\n It doesn't contain null values,  \\n and there has to be something there to identify the record.  \\n It can also be a combination of more than one field,  \\n which is also known as a composite key.  \\n Again, this has to be a unique identifier,  \\n so some composite keys might seem like the easiest option,  \\n but would actually cause some problems.  \\n Let's say a developer is wanting  \\n to use first name and last name as the composite key.  \\n This might seem simple, but it wouldn't work  \\n if you have two John Smiths.  \\n However, a composite key with the last name  \\n and address may work, if there is  \\n only one customer per household.  \\n Deciding on how you want to make your primary key  \\n is really up to how complicated you think  \\n your database is going to be.  \\n It's not really about if it works now.  \\n You have to look at the future,  \\n and how complicated you think  \\n your database is going to get.  \\n Trust me, it's a real pain to change a primary key.  \\n You don't want to do that.  \\n Let's look at the ERD for H+ Sport.  \\n Some examples of a primary key include  \\n the CustomerID, a ProductID, and the OrderID.  \\n Each table can refer to another table  \\n using a key called a foreign key.  \\n The foreign keys refer to the primary keys of another table.  \\n Let's take a look at this example.  \\n Customer 101 makes a purchase on January 1st.  \\n It has an Order ID of #123.  \\n The order table shows us the Customer ID  \\n of the customer who purchased the order,  \\n and the Salesperson ID of the person who sold the order.  \\n We can join the order table  \\n to the order item table by Order ID,  \\n and see that the customer ordered Product ID A,  \\n along with the quantity that they ordered.  \\n Then we can join the Product ID A to the product table  \\n to find out the product name,  \\n and other attributes as associated with the product.  \\n From the order table, I can join  \\n the salesperson table by Salesperson ID.  \\n Then get more information, like the name and address  \\n of the person who sold the order.  \\n Each foreign key must have a matching primary key.  \\n This is known as referential integrity.  \\n It ensures the accuracy and consistency  \\n of data with your database.  \\n It also maintains the relationship between the two tables.  \\n Database administrators and programmers work together  \\n by putting constraints in place  \\n to ensure the integrity of the data.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4484705\",\"duration\":364,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Common SQL structures\",\"fileName\":\"3271025_en_US_01_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore the basic SQL syntax covered in this course, develop questions in your own language, and convert those questions into a simple SQL query using information from the ERD for reference. When you understand how to develop good questions about your data, you can use those questions to make sure the data supports your project goals.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9839588,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Now let's take a look at how to ask the right questions  \\n to get the results you want using some common SQL syntax.  \\n Let's start by opening up your code space environment  \\n and going to visual studio code.  \\n Let's make sure we are connected  \\n to the H plus Sport database,  \\n and can view the tables in the explore window.  \\n I see all five of the tables  \\n from the entity relationship diagram,  \\n customers, orders order item, product,  \\n and salesperson tables.  \\n When developing a good SQL query,  \\n it's important to determine a detailed question  \\n that you want to ask of the data.  \\n If not, you may end up with the wrong results.  \\n This could be the difference between  \\n asking about the number of times  \\n a customer has been to your website,  \\n and the number of times a customer  \\n has ordered from your website.  \\n Questions that can help you develop  \\n a more detailed query are,  \\n what tables do you need to access?  \\n What are the fields of interest?  \\n What data do we need to filter out,  \\n or is there a specific time period that we're looking for?  \\n Do we need to order our data  \\n in a specific way to get the right results?  \\n Let's start with the basic syntax for retrieving data.  \\n SQL statements are structured in subsections  \\n known as a clause,  \\n and each clause contains at least one SQL syntax keyword.  \\n The select keyword is part of the select clause,  \\n where we define the fields  \\n that we want to be returned back to us.  \\n From is a keyword that is part of the from clause  \\n that identifies the tables in which we're pulling data from,  \\n like the order table, or the customer table.  \\n The foundation of a simple query looks like this.  \\n Select your field name from a table name.  \\n Let's look at how we can use the SQL tools  \\n and Visual Studio code.  \\n I'll demonstrate the code here,  \\n and feel free to pause and work in the code space  \\n along with me.  \\n Let's start by going to menu.  \\n Go to view and command palette.  \\n We'll type in SQL tools, colon, and then space,  \\n new SQL file.  \\n You can just push in,  \\n and go down to new SQL file.  \\n In the editor, you'll have a new SQL window  \\n to start writing your queries.  \\n All these options have shortcuts as well,  \\n but this should help you get started  \\n with knowing how to access all the commands.  \\n So, how do I retrieve all the information  \\n from the customer table?  \\n I'll start by typing in select star from customer,  \\n and then I end with the semicolon here.  \\n I can click on run on active connection,  \\n and the results should appear  \\n on the right side of your screen.  \\n This retrieves all the rows and attributes  \\n from the customer table.  \\n So, what if I just want to select the state  \\n for all customers?  \\n Then, instead of using the star here,  \\n I can hit select state from customer,  \\n leave everything the same.  \\n Note, this doesn't give me a unique list of states  \\n where our customers live.  \\n It gives me the state field  \\n for every row in the customer table,  \\n and we'll go over creating a unique list later.  \\n Let's say the company website wants to display  \\n a map of customers from around the country.  \\n Instead of manually entering each location  \\n in the code for our website,  \\n we can add a call to the database  \\n to display the total amount of customers in each state.  \\n There are also great geographic information systems  \\n that use SQL to perform visualizations like this.  \\n Most likely, we'll need to join tables together  \\n to gain more insights.  \\n A join statement will return the records  \\n or the keys on the tables in the join match.  \\n There are different types of joins.  \\n Some join types include inner join,  \\n left join, and outer join.  \\n The inner join returns only data that matches both tables.  \\n The left join returns all the data from the left table,  \\n and the data for the matching rows on the right.  \\n The full outer join returns the rows  \\n from both the left and right tables.  \\n There are more join types that are available  \\n depending on your database system.  \\n We'll use these join types  \\n depending on the relationships between entities  \\n and the question that is being asked  \\n to make sure our query returns the correct information.  \\n We will focus on the inner join for now.  \\n Let's look at another example.  \\n So we want to find all of our order IDs  \\n by the customer's last name.  \\n So we're going to start by hitting select,  \\n last name, order ID,  \\n and we'll use our from clause,  \\n type in customer for our table,  \\n join to our orders table,  \\n and when we join we're going to join on the key.  \\n So we're going to use customer dot customer ID  \\n equals orders dot customer ID.  \\n This gives us our last name,  \\n the order ID from the orders table,  \\n and we can click on run on active connection.  \\n And this will give us all the order IDs by last name.  \\n To further define our queries,  \\n we can add on additional clauses like where,  \\n order by, group by, having, and limit,  \\n and the additional clauses  \\n help us to further define our query  \\n so that we can display the correct information.  \\n \\n\\n\"}],\"name\":\"1. SQL Introduction and Asking the Right Questions\",\"size\":27414785,\"urn\":\"urn:li:learningContentChapter:4481718\"},{\"duration\":1152,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4482704\",\"duration\":346,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Using data types and identifying the wrong data types\",\"fileName\":\"3271025_en_US_02_01_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Learn how to identify values stored with incorrect data types and use CAST and CONVERT to change data types within your SELECT statements. When you understand how incorrectly stored data types affect how the application works, you are motivated to work with data architects and designers in the early phases of development projects to make sure the correct data types are applied to your projects.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10627689,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Now, let's take a look at what data types are  \\n and how we can prevent storing  \\n and processing the wrong data.  \\n Let's review.  \\n A relational database is an organization of related tables.  \\n A table consists of rows and columns  \\n with each row representing a record or an instance.  \\n A column represents an attribute of each instance.  \\n However, all fields are not made the same.  \\n The data type of a column defines  \\n what value the column can hold.  \\n Much like other programming languages,  \\n each attribute or column has a data type  \\n that helps the system and the programmer  \\n know how to interpret the value and how to process it.  \\n Here's an example.  \\n Let's say I want to order a pet carrier.  \\n Depending on the type of pet I have, like a dog, cat  \\n or bird and the size of my pet,  \\n I'm going to buy the carrier that best fits.  \\n Also, depending on their needs  \\n I might need different features.  \\n Data types work the same.  \\n They help us store the right value  \\n to make sure the application works correctly.  \\n There are many different data types to choose from,  \\n and depending on the system,  \\n data types and their corresponding functions  \\n can perform differently.  \\n Some common data types in SQL are INT or integer,  \\n a VARCHAR, or you see NVARCHAR as a derivative of that,  \\n DATE, DATETIME, FLOAT, Decimal and DOUBLE.  \\n Also note that different systems offer  \\n different data types.  \\n When defining a column's data type,  \\n for some systems, you can include the maximum number  \\n of characters or digits allowed in the column.  \\n These restrictions are known as constraints.  \\n This is defined when the table is created.  \\n The developer or administrator can also define  \\n additional column constraints.  \\n When defining tables, we can define  \\n whether the data can be NULL or NOT NULL in each column.  \\n We can also ensure that all values in the column are UNIQUE.  \\n This is also where we define PRIMARY KEYs, FOREIGN KEYs  \\n and set DEFAULT values for each column.  \\n Here's an example of a CREATE TABLE statement.  \\n Don't worry about the syntax for right now.  \\n We'll go over that in more detail later.  \\n Let's look at the table's data types and constraints.  \\n Here we have the CustomerID column.  \\n You can specify an integer of 4 data type for CustomerID.  \\n That'll be good for about 9,999 customer IDs.  \\n If the system tries to auto increment greater than 9,999  \\n or a customer ID has more than four digits,  \\n the process will error.  \\n It's important to work with the database team  \\n to ensure enough space for customer IDs  \\n that are larger than four digits,  \\n such as an INT or a SMALLINT data type.  \\n For example, in my SQL,  \\n the constraints on our integer values are TINYINT,  \\n SMALLINT, MEDIUMINT and BIGINT.  \\n Data types and constraints ensure that the data  \\n is kept consistent and has good quality.  \\n Let's look at some more examples.  \\n I want to calculate the total amount a customer  \\n has ever spent with H+ Sport.  \\n I've typed out a query in advance so we can go over  \\n the main concepts you need to know for now.  \\n If there are some key words you don't recognize, it's okay.  \\n I've selected the CustomerID, FirstName,  \\n LastName from the Customer table,  \\n and the TotalDue from the Orders table.  \\n And the results from that query gives us  \\n the total due for each customer.  \\n However, that's not exactly the question we want to answer.  \\n We need to apply a function called SUM  \\n to add the total due for each customer ID.  \\n We can call this function using the GROUP BY clause  \\n and the SUM function.  \\n This allows us to aggregate the total due  \\n for each customer ID.  \\n Again, we'll cover aggregates in a minute.  \\n It's a great feature,  \\n and I won't forget to come back to this.  \\n Let's click on Run on active connection,  \\n and here are our query results.  \\n Be sure that when using the SUM operator,  \\n that you're working with a number data type.  \\n Let's try the same query with a text data type  \\n like customer email.  \\n Looks like we didn't encounter an error,  \\n but let's look at the results.  \\n We have zeros for the value for some email.  \\n The data type for email will not error  \\n but will also not give us the correct results.  \\n In up database management systems such as SQL Server,  \\n this would result in an error.  \\n The data type character does not allow  \\n for the use of the SUM function.  \\n There are other functions we can use  \\n to count the number of emails in this case.  \\n I'll click on the Top10Customers, that SQL file.  \\n I'm going to close this query result.  \\n I'll add some additional code to select the top 10 customers  \\n by adding the LIMIT 10 and order by keywords.  \\n Scroll over a little bit to see the results.  \\n It works.  \\n We have our top 10 customers ordered by total due.  \\n Also note the IDs that we used to join the data together  \\n should also be of the same data type.  \\n For example, you can store customer ID 001  \\n as VARCHAR or INT.  \\n Whatever you decide, the data type needs to be consistent  \\n across tables to prevent any future errors that might occur.  \\n Take a look at a few of the links for data types  \\n for MySQL and SQL Server for more information  \\n on data types that best fit the data  \\n that you are working with.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4483692\",\"duration\":316,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Finding missing values\",\"fileName\":\"3271025_en_US_02_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover missing data using common SQL code syntax for finding NULL, blanks, and spaces. Missing data can cause major issues for your applications, issues in decision-making, and ultimately affect the success of your organization. When you understand how to identify missing data, you can prevent help errors that cause projects and organizations a lot of time and money.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9888933,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Next, we'll take a look  \\n at how we can identify missing data in our database.  \\n Missing data can have major implications  \\n for your development project,  \\n affecting your customer's experience  \\n and your organization's reputation.  \\n Let's look at some ways to identify data  \\n that may be missing.  \\n I'm going to go to the chapter two directory  \\n finding missing values,  \\n and first we're going to take a look  \\n at the salesperson's table.  \\n The database team has added a new column called Status.  \\n Here's a script we can run to add that new column  \\n to the salesperson table.  \\n Let's click on Run on Active Connection again,  \\n that adds the column to our table.  \\n Now, when we run select status from salesperson,  \\n we see that all the values for status are listed as null.  \\n Null represents an empty value.  \\n The value could be null upon creation of the record  \\n or it can be assigned a null value from the program.  \\n Of course, that depends on if our column allows null values  \\n as we discussed before in the column constraints.  \\n Null is not the same as a zero value or even a blank value.  \\n Sometimes data values can contain spaces  \\n and the value may look like it's empty, but it's really not.  \\n The WHERE clause is used to help us filter records  \\n to a specific set of data that we want to explore  \\n based on what we specify in select.  \\n The WHERE clause also comes after the FROM clause.  \\n To find values that contain blank spaces,  \\n we can use the keyword LIKE.  \\n we use LIKE along with a wildcard,  \\n which is the percent sign to find characters  \\n like blank spaces within the text.  \\n It takes the place of the equal sign.  \\n The percent sign is used in place of any character string  \\n that we want to search for.  \\n The wildcards are enclosed in single quotation marks.  \\n Searches within the text are not case sensitive,  \\n so searching for capital MW can return MW or lowercase MW  \\n or even a mixed case in the string.  \\n And the wildcard with space SQL file, let's open that.  \\n In this example, let's look at the following query.  \\n We select first name on line one  \\n from customer, our customer table on line two,  \\n where a first name looks like  \\n and you see the percent signs with a space in the middle  \\n on line three.  \\n Let's click on run on active query,  \\n and the following query returns zero records.  \\n So we don't have any records that contain a space  \\n in the first name.  \\n Let's look at the products table.  \\n The company has a new orange mineral water.  \\n Let's run a script that will add a new product  \\n to the product table.  \\n We'll go to our AddNewProduct.sql file.  \\n This is syntax we haven't covered yet,  \\n but will help us set up for the next part of the analysis.  \\n Let's click on Run.  \\n Now it's just been entered into our products table  \\n and the product is now available in the database.  \\n Let's click on our SelectNewProduct.sql file.  \\n Let's click on Run.  \\n You'll notice that we can't see a name for the product,  \\n so let's go to our SelectBlankProduct.sql file.  \\n Let's try to run this one.  \\n There is one record that returns that lets us know  \\n that the product code contains spaces  \\n and is not a null value.  \\n There is a record for product ID 17 listed,  \\n but how much does it cost?  \\n There also looks like there may be some data missing.  \\n Let's go back to the SelectNewProduct.sql file,  \\n we can see that the product name is still null  \\n and the cost is listed at $0.  \\n Let's perform some queries that find null values  \\n using the WHERE clause.  \\n You can filter records by adding is null  \\n or is not null to the WHERE criteria.  \\n Let's go to our StateIsNull.sql file.  \\n To identify customer records with a missing state value,  \\n we can select customer ID, first name, last name,  \\n first customer, and on line five,  \\n we have where state is null.  \\n Let's run this.  \\n This query should return zero records since the column state  \\n does not allow null values.  \\n If it does return records,  \\n we can take this information to our team  \\n to identify any issues that may be causing missing values  \\n and impose a proper constraints.  \\n Next, let's go to the StateIsNotNull.sql.  \\n As a check, we can also run this following query  \\n on line five, you see, where state is not null.  \\n We will want this number to be the same number  \\n as the records in the customer table.  \\n That means that no state values are null.  \\n Again, it's still important to look at the data  \\n to make sure that there are no blanks or spaces and values  \\n in the state column, as they will not be identified as null.  \\n This is a great practice for enforcing quality in your data.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4482703\",\"duration\":245,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Finding possible inaccurate data values\",\"fileName\":\"3271025_en_US_02_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to identify incomplete or incorrect data based on a set of valid values. When you understand these common string functions, you are able to identify bad data that can cause loss of money to your project as well as find data values that are important to your business applications.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7092848,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Presenter] Next, we'll take a look at using SQL functions  \\n to find some possible bad data values.  \\n Sometimes, we'll run across data  \\n that doesn't look right at all,  \\n like a misspelled word  \\n or a product that was giving the wrong product code.  \\n A great tip in application development  \\n is to search for possible anomalies in our data  \\n and be prepared for dealing with it in our app.  \\n There are several different operators you can use  \\n to gain more insights about your data.  \\n To do this, we'll look at three types of SQL operator:  \\n comparison, logical, and arithmetic.  \\n Comparison operators compare values  \\n and return a true or false to evaluate the statement.  \\n Some common comparison operators in SQL are equal,  \\n not equal, greater than or greater than or equal to,  \\n less than or less than or equal to.  \\n If I compare an apple and an orange by color and taste,  \\n would they be the same?  \\n No, so the operation would turn a result of false.  \\n We compare two values to evaluate  \\n whether or not we want to return the record.  \\n In this code example,  \\n we can return all the orders  \\n where the order date is greater than January 6, 2016.  \\n Depending on the date of format that your data is stored in,  \\n the structure and format of the date value  \\n may vary slightly.  \\n Logical operators combine multiple Boolean values  \\n and return a single Boolean output of true or false.  \\n Some common logical operators in SQL are listed here  \\n including and, or, like, and between.  \\n Say you want to check for multiple conditions,  \\n such as if a product variety is Blueberry  \\n and the price is below $2.  \\n In this example, we selected all the records  \\n from the product table where the variety is Blueberry  \\n and the price is less than $2.  \\n We can see a simple example of the use of and  \\n as a logical operator here.  \\n Now, let's take a look at arithmetic operators.  \\n These are our operations that we use  \\n to perform calculations.  \\n We can use basic arithmetic operators to our data  \\n like addition, subtraction, multiplication, division,  \\n and modulo, which we'll find the remainder.  \\n Let's check the total due column  \\n by also checking the product price and quantity  \\n in the order item table.  \\n Let's open our check order total SQL file.  \\n We'll have to join the order item and product tables  \\n by the product ID,  \\n and we'll multiply the quantity and price.  \\n Next, to get the total for the orders,  \\n we will use the sum function  \\n to aggregate the order item records by order ID.  \\n In order to aggregate,  \\n we have to use a group-by clause  \\n that allows us to group data that is similar.  \\n When we look at our results,  \\n our total due is the same as our new total due  \\n that we have calculated from multiplying quantity and price.  \\n We want to see if there are any discrepancies here  \\n that we may need to discuss with our team.  \\n Are we missing some additional calculations  \\n that we should consider, such as sales tax or discounts?  \\n This is all part of making sure  \\n that we're asking the right questions  \\n or let someone know that we have some bad data.  \\n A couple of things to note here:  \\n This is where good data documentation comes into play.  \\n Most databases will also come with a definition  \\n of what is in each column.  \\n It's also good to document  \\n where the data originally comes from  \\n and how it's derived if it's a calculated field.  \\n It's also great to list  \\n or define valid values for the column.  \\n This could be a list or a range  \\n like store A, store B, and store C.  \\n Good documentation is essential in identifying bad data  \\n that can mess up the integrity of your application.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4484706\",\"duration\":245,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Finding duplicate rows\",\"fileName\":\"3271025_en_US_02_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to identify duplicate values in your data and how they can affect the data in your applications using the GROUP BY and HAVING clause. When you understand how to find duplicate values in data, you can help to prevent serious errors in your business applications that can affect the business reputation and customer experience.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8244444,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's look at some ways  \\n to use SQL to identify duplicate values in our data,  \\n as well as how to prepare a summary  \\n that will give us even more insight.  \\n Now let's add two new clauses.  \\n They are the GROUP BY and HAVING clauses.  \\n The GROUP BY clause allows us  \\n to introduce aggregation to our SQL query.  \\n By using GROUP BY,  \\n we can group together rows  \\n that have the same values  \\n and summarize those groups with aggregate functions,  \\n as we have seen in some previous examples.  \\n Aggregate functions include  \\n COUNT,  \\n SUM,  \\n AVG,  \\n MIN,  \\n We will look at some  \\n of those functions a little later in this course.  \\n For now, let's focus on the COUNT function  \\n that will help us identify the duplicate records.  \\n Let's start by going to duplicate 1 dot SQL.  \\n All of fields in the select statement  \\n must also be in the group by.  \\n Now, here's an example of a summary.  \\n I want to count the number of orders for each customer.  \\n I'll use the customer ID from the customer table.  \\n Next, in our select statement,  \\n I'll add the COUNT function.  \\n Inside the COUNT function,  \\n you can add a field  \\n that you want to count the number of values  \\n as seen on line 4.  \\n A common practice,  \\n we'll write count one,  \\n and that will count the number of values  \\n in the first column.  \\n We can also use an alias to rename our calculated field  \\n to something else like num underscore orders.  \\n Next, I'll include some more information  \\n from the customer table by joining by customer ID.  \\n We'll go over more about joins here soon.  \\n However, this is a simple join.  \\n I'll add the group by clause  \\n and add customer ID, first name, and last name.  \\n You can also add order by  \\n to order the account in ascending or descending order.  \\n Let's run this query to see what's returned.  \\n Now we have a summary that contains the number  \\n of orders for each customer.  \\n As you can see here,  \\n we have some num orders that have two  \\n and some that have one.  \\n A duplicate row happens if the record is not unique.  \\n This means that all the attributes for a row are the same.  \\n This can cause the application to count something twice  \\n or overestimate.  \\n Let's add the HAVING clause.  \\n The HAVING clause allows us  \\n to use some additional filtering based  \\n on the results of the aggregate functions.  \\n It will always come after the group by.  \\n Let's take a look at duplicate 2 dot SQL.  \\n On line 4, we add the following,  \\n having count of one greater than one.  \\n Let's run this,  \\n and ah, we have some duplicate values.  \\n By checking if the count is greater than one,  \\n we can identify where there may be one  \\n or more rows that are the same.  \\n Of course, first and last name combinations  \\n can be very common.  \\n However, if we were to add address,  \\n city, state, and zip code  \\n that makes it more likely  \\n that these are different people and less likely  \\n that it's a duplicate record unless you have two people  \\n at your address with the same name.  \\n If that's the case,  \\n we can try to add even more fields  \\n to help us identify the truly unique records.  \\n Let's go to duplicate 3 dot SQL.  \\n Here, we've added city, state,  \\n and zip code to our existing query.  \\n Let's run that.  \\n No duplicate values there.  \\n Some key points to remember,  \\n the WHERE clause allows us  \\n to filter non aggregate data columns.  \\n The HAVING clause is only used to filter aggregated data  \\n like SUM and COUNT,  \\n and has to be accompanied  \\n by the GROUP BY clause that's listed first.  \\n Identifying duplication  \\n in our data can save us database space.  \\n It also helps us to prevent errors in our application  \\n or in our data analysis,  \\n ultimately leading to better decision making  \\n and a better customer experience.  \\n \\n\\n\"}],\"name\":\"2. Using Data Types\",\"size\":35853914,\"urn\":\"urn:li:learningContentChapter:4480729\"},{\"duration\":423,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4483690\",\"duration\":155,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Working with dates\",\"fileName\":\"3271025_en_US_03_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to construct a SQL SELECT statement with data functions to find data by MONTH, DAY, and YEAR. When you understand how to use date functions, you can filter and aggregate data to identify trends and display relevant information in your applications.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4909273,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Most database management systems  \\n will allow for many different data types for dates,  \\n so let's look at some different methods.  \\n In MySQL, you have DATE, DATETIME, TIMESTAMP, and YEAR.  \\n There are lots of different options.  \\n They may seem very similar,  \\n but there are some unique differences.  \\n In MySQL, the DATE datatype will store the value  \\n as YYYY-MM-DD.  \\n While the TIMESTAMP stores both the date and time  \\n that includes the hour, minute,  \\n and second of the time.  \\n With most business applications,  \\n the timestamp in SQL will be more than enough precision.  \\n Datatypes allow the database  \\n to have different levels of precision  \\n based on the needs of the application.  \\n Let's take a look at the order date.  \\n Let's open the SelectOrderDate.sql file.  \\n Date is in both the name of the field and the datatype,  \\n and sometimes that may be confusing.  \\n In practice, make sure that the fields used  \\n are not the same as keywords.  \\n In this instance, we use CreationDate instead of Date.  \\n This query returns the date for every order in the database.  \\n Let's add some more to this query.  \\n Open the SeparateDates.sql file.  \\n Here, we'll separate the month, day, and year  \\n of the order date into three separate columns.  \\n The Year returns a year in an integer,  \\n Month returns an integer 1 through 12  \\n for the month of the date,  \\n and Day returns an integer  \\n that represents the day of the month.  \\n I can also add aliases  \\n to make my columns easier to read for my audience.  \\n Open the SeparateDatesAlias.sql file for an example.  \\n Let's click on run.  \\n Now we have successfully created  \\n three columns of data from one.  \\n There's also a couple of ways  \\n to capture the current date and time.  \\n Let's again look at a few DBMS methods to do that.  \\n In MySQL, let's use the NOW function.  \\n We'll open the CurrentDateTime.sql file.  \\n The value is derived from the operating system  \\n where the database is running.  \\n These functions are great for when you're ready  \\n to insert a new order into the order table.  \\n There are so many date functions available  \\n to make the most of our data.  \\n Each database management system  \\n will contain documentation that is useful for helping us.  \\n There are so many date functions available  \\n to make the most of our data.  \\n Each database management system  \\n will contain documentation that's useful  \\n in helping us query effectively.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4481717\",\"duration\":268,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filtering data by dates\",\"fileName\":\"3271025_en_US_03_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to identify data from dates within a specific range and construct SQL SELECT statements using BETWEEN, Greater Than (>), Less Than (<), Equal (=), and Not Equal (<>) to. When you understand how to use date functions, you can filter and aggregate data to identify trends and display relevant information in your applications.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9321054,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - In a larger database,  \\n we wouldn't want to return all of the orders.  \\n For one, it could get pretty inefficient;  \\n and 2, we most likely aren't interested in every record.  \\n Instead, we're going to identify a subset of the data  \\n by using dates.  \\n I need to show the sales team a list  \\n of all the orders from May 2016.  \\n Let's incorporate both logical and comparison operators  \\n and the date functions from our previous section.  \\n Let's open the ComparisonOperators.sql file.  \\n We have the following query,  \\n SELECT OrderID and CreationDate FROM Orders,  \\n where the month is equal to 5 and the year is equal to 2016.  \\n A couple of things to note here.  \\n Month and year are returned as integers,  \\n and the values can be noted in single quotes or not.  \\n That doesn't matter, since they are integers.  \\n Also, we use the AND operator  \\n so that both conditions must be true  \\n in order for the records to be returned.  \\n If either condition is false,  \\n we will not get the records we want.  \\n Let's click on run, and we see a order ID and creation date.  \\n We can also use BETWEEN to get a section of orders  \\n in a specified time period.  \\n Let's look at the same question  \\n of returning orders for May 2016.  \\n Let's open the BetweenDates.sql query.  \\n Close this result.  \\n Notice the WHERE clause that has the BETWEEN keyword.  \\n In line 4, you see \\\"BETWEEN '2016-05-01' and '2016-05-31'\\\".  \\n Click on run.  \\n The query returns 17 records.  \\n We'll also order the results by adding an ORDER BY clause  \\n that sorts our data by the order date in ascending order.  \\n By adding the DESCENDING keyword in our ORDER BY clause,  \\n we can order the order date in descending order.  \\n The ascending order keyword is ASC,  \\n but is also the default method in ORDER BY  \\n and does not need to be implied.  \\n We can also use comparison operators  \\n together with date functions to filter the results by dates.  \\n Let's select dates greater than today's date.  \\n Open the WithCurrentDate.sql file.  \\n Yep.  \\n Let's move this over here,  \\n and we'll close out the old result set.  \\n In the where clause, you'll see  \\n where we're comparing the creation date  \\n with the current date using the now function.  \\n Let's run this.  \\n No records are returned, which is what we expect,  \\n since we don't have any data in 2023 or in the future.  \\n Instead, let's find an order date  \\n where it's greater than 10 years.  \\n So let's open the WithCurrentDateMinus10years.sql file.  \\n Here we also add some arithmetic operators  \\n to our where clause.  \\n Let's run this query.  \\n The query now returns 200 orders  \\n with a order date greater than 10 years ago.  \\n Next, let's try to return our order data from 2015  \\n by month and year along with our other order information.  \\n Open the CompareYear.sql file.  \\n Let's close out our previous results.  \\n On line 6 and 7, in the where clause,  \\n you'll see the year function  \\n and we pass in the creation date as a parameter.  \\n We've used our year function to filter by just a year,  \\n and we're able to use our column alias to order our date  \\n by month and year in ascending order.  \\n Let's run this query.  \\n We have 87 records returned with the dates  \\n between June and December of 2015.  \\n We're also able to use our alias in the ORDER BY clause.  \\n This is allowed because of the order  \\n in which SQL processes each clause.  \\n \\n\\n\"}],\"name\":\"3. Working with Dates\",\"size\":14230327,\"urn\":\"urn:li:learningContentChapter:4481719\"},{\"duration\":830,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4483691\",\"duration\":315,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Common SQL string functions\",\"fileName\":\"3271025_en_US_04_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to identify common SQL functions that can be used to modify character-based data values by learning the different string functions, you can display data in your applications that work best for your apps and the users.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9000347,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Different relational database management  \\n systems offer built-in SQL functions that allow us to  \\n retrieve data for our applications.  \\n This is especially helpful when working with text.  \\n Now let's take a step back  \\n for a moment and talk about what is a function.  \\n SQL functions are built  \\n into the database system and return values based  \\n on the operation, functions within SQL work  \\n much like functions in other programming languages  \\n they can take in one or more arguments and return a value.  \\n Some functions are specific to a data type  \\n and some functions can cross different data types.  \\n For example, we can use the MIN function to  \\n find the minimum total of products ordered  \\n and we can also use the minimum order date  \\n which would be the oldest date in the results.  \\n However, the sum function is for adding numerical values.  \\n You won't be able to use the sum of apples and oranges.  \\n The sum function will not work on strings  \\n Let's look at some common string functions.  \\n String functions are used to modify character  \\n and text space data.  \\n Some common string functions are CONCAT  \\n UPPER, LOWER, TRIM, REPLACE, and SUBSTRING.  \\n Using these functions  \\n within our select statement temporarily changes how  \\n the values are returned to us without actually  \\n changing the data in the underlying database.  \\n The SUBSTRING function in this case takes three arguments.  \\n The value of the field to use, the first position  \\n of where you want the substring to begin  \\n and the number of characters to include in the substring.  \\n Some examples would be where, substring of the last name  \\n one for the first position, and three for the number  \\n of characters to include in the substrate.  \\n Will return last name that begins with Smi.  \\n Like Smith.  \\n The CONCAT function allows us to join strings together.  \\n In this example, we can add the first  \\n and last name together to get the customer's full name.  \\n The upper function will return your string  \\n in all capital letters, and the lower function  \\n will return the whole character string in lowercase.  \\n These functions are great for standardizing the result set.  \\n For instance, if you want to standardize a mailing address  \\n this is a great method to do that.  \\n The replace function can quickly replace all  \\n of the occurrences of a substring  \\n within a string with another substring.  \\n Let's take a look at these functions in action.  \\n Let's open the CONCAT and a upper.SQL file.  \\n This is a quick example for listing mailing address.  \\n We will use the CONCAT and upper functions to  \\n show the customer's full mailing address.  \\n Let's run this query and check out the results  \\n of the concatenated new address.  \\n Notice, we combine two string functions here along  \\n with the limit 10 to only include the first 10 records  \\n in our result set.  \\n The trim function will remove spaces  \\n from the beginning or the end of a string.  \\n This is extremely helpful if we have some users that may  \\n enter text in a web input field and forget  \\n that there is an extra space at the end.  \\n The trim function can be used to  \\n remove these trailing spaces.  \\n The trim function can also remove specified characters  \\n from a string, for instance, with the dollar sign  \\n in a total due field in our invoices table.  \\n Let's look at the trim.SQL file.  \\n Let's close our previous query results.  \\n We can remove the M in front of a product code  \\n with a trim function with a following code.  \\n So clearly this is a lot easier to try to implement  \\n than the substring function  \\n with trimming characters from text.  \\n Some derivatives of the trim function include LTRIM  \\n which is a function that can remove leading spaces  \\n from the character on the left side and the RTRIM function  \\n which will remove the trailing spaces.  \\n The REPLACE function will replace a set  \\n of specified characters with another set  \\n of characters similar to find and replace in excel.  \\n Let's open the replace.SQL file.  \\n Let's close this previous query result  \\n and let's say I'm interested  \\n in replacing product code B-L-U with B-L-E,  \\n you see on line two, we use replace, product code  \\n with BLU to BLE and an alias of new product code.  \\n Let's run this query.  \\n We can see the original product codes  \\n with the new product codes using the replace function.  \\n Of course  \\n these are not all the string functions available to us.  \\n Find more useful string functions on document websites  \\n like MySQL for more string manipulation options.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2705664\",\"duration\":185,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Common SQL aggregate functions\",\"fileName\":\"3271025_en_US_04_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to differentiate between COUNT and DISTINCT COUNT to find the frequency of data values as well as other useful aggregate functions. When you understand some simple aggregate functions, you can incorporate those functions to assist users within your web applications.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6250922,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now let's look  \\n at the many mathematical functions  \\n we have available in SQL  \\n to translate our data into some meaningful information.  \\n Some common aggregate functions include count, average,  \\n min, max, and sum.  \\n Count is a commonly used function within SQL  \\n to find the frequency of values in a data set.  \\n For example, we may want to know how many customers we have.  \\n We can perform that calculation  \\n in the Count Customers.sql file.  \\n Let's take a look at that syntax, which should be familiar.  \\n Let's run this query.  \\n This query will return the number of customers  \\n in our database.  \\n Here, we see that we have a thousand customers  \\n in our database.  \\n We can also count the number of customers  \\n that have made purchases  \\n by counting the number of customer IDs in the order table.  \\n Let's go to Count Orders.sql.  \\n Run this query, I get a count of 200.  \\n However, we know that customers  \\n can make more than one order,  \\n so that will not give us a unique count of customers.  \\n In order to get the unique count of customers  \\n from our order table, we can use the distinct keyword.  \\n Let's open the Count Distinct.sql file.  \\n We'll click on Run.  \\n Now I get a count of 179,  \\n which is really the number of customers  \\n that have actually ordered something from our company.  \\n Let's say we also want to find the average amount of orders.  \\n The average function finds the average of a set of numbers.  \\n In order to provide account summary  \\n for a group of similar rows,  \\n again, we can use the Group By statement.  \\n Let's work with some code examples.  \\n Open the Min Max Functions.sql file.  \\n We can join the customer table to the orders table  \\n to find the average total due by state.  \\n Let's build on this query  \\n to add some additional aggregate functions.  \\n We'll add the max function to find the maximum order amount,  \\n and the min function to find the minimum total due.  \\n And we'll also add the sum function  \\n to add all the orders for each state.  \\n Let's run this query.  \\n You can see in the results,  \\n all the aggregate values are the same for Utah.  \\n That's because there's only one order recorded in Utah.  \\n If we scroll down to Florida,  \\n we can see an average, minimum, maximum  \\n and some that are different.  \\n This is great summary information  \\n about the H+ Sport customers.  \\n Let's save this query as a view.  \\n Open the Create Views.sql file.  \\n Let's get this over here.  \\n Close out our previous SQL results.  \\n We'll call this view V_CustomerOrderSummary,  \\n and we can use it to incorporate into our web app for later.  \\n Just remove the Order By Class  \\n to prevent errors in the Create View statement.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4483689\",\"duration\":330,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to data manipulation\",\"fileName\":\"3271025_en_US_04_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore CRUD operations and learn how they are used in software and web development. When you understand this methodology, you can use it to apply to more complex SQL in back-end web and software development.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9295689,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] All of the syntax  \\n we've covered so far in this course  \\n are geared toward data retrieval.  \\n We've mainly used the select statement  \\n to retrieve data that we want to display.  \\n As a developer, you'll also have to create processes  \\n for creating, updating, and removing data.  \\n This is part of the SQL language called data manipulation.  \\n Data manipulation language or DML  \\n allows you as a developer to change the actual values  \\n of the data that is stored within the database tables.  \\n Common data manipulation functions include  \\n insert, update, and delete.  \\n The insert, update, and delete operations,  \\n along with select, are used to do CRUD operations,  \\n create, read, update, and delete.  \\n The select statement is a read or retrieval operation.  \\n We have used this operation in several examples already.  \\n It returns a result set of data from our database.  \\n The insert statement allows us  \\n to add new records to our tables,  \\n like adding a customer or adding a new order.  \\n Insert is one of the keywords in our create operation.  \\n We also use the create keyword when creating new tables  \\n or views in the data definition language.  \\n The update statement updates an existing row  \\n or set of rows in a table,  \\n like updating a customer's address or phone number.  \\n The update keyword is the keyword for our update operation.  \\n The delete statement allows us to delete rows of data.  \\n For example, if we need to delete  \\n an old customer or user from our database.  \\n The delete keyword is used for our delete operations.  \\n Let's look at some examples.  \\n Let's start with the insert operation.  \\n We're going to add a new customer to our customer table.  \\n The syntax for inserting a new record  \\n is insert into the table name, the columns,  \\n and then they use the values keyword  \\n and we enter the values.  \\n To add a customer,  \\n we'll add the following code from the insert.sql file.  \\n You see we have insert into our customer table,  \\n our column names and the values for each column.  \\n Let's click on run, no errors in the console,  \\n but we can check the table  \\n to make sure the record was added.  \\n Let's open confirm.sql.  \\n Let's click on run.  \\n We can see that there's one record added here,  \\n to let us know that the row  \\n has been inserted into the table.  \\n I want to update the same record with an updated address,  \\n we'll use the update statement  \\n and also include a where clause  \\n to identify the record that we want to update.  \\n We'll also introduce the keyword set  \\n that sets the new value of the attributes  \\n that we're updating.  \\n Let's open the update.sql file.  \\n Here, we're updating the customer table,  \\n we're setting the phone number  \\n where customer ID is 9999.  \\n Here's the old phone number.  \\n Click on run.  \\n There are no errors here.  \\n Let's run the select statement again to confirm  \\n that the record is updated with the new phone number.  \\n Notice if we do not include a where clause here,  \\n we'll update the attribute for every row in our database  \\n and most likely that's not what we want to do.  \\n Say our customer decides to no longer do business with us,  \\n we may want to delete that customer's record.  \\n Note that we don't have to include the field names here  \\n as the entire row will be deleted,  \\n depending on what is in the where clause.  \\n Let's open the delete.sql file.  \\n We'll delete from customer where customer ID is 9999.  \\n You can include the quotation marks or not.  \\n Let's click on run.  \\n We'll go back to confirm.  \\n Click on run and confirm that the record has been deleted.  \\n We can also run account check  \\n to make sure that we are only down one row.  \\n If we delete more than one record here,  \\n we'd be in some serious trouble.  \\n Luckily, in SQL, we can perform transactions.  \\n Transactions are a single unit of work  \\n where data modifications can be made  \\n and committed to the database  \\n By default, when we perform SQL modification transactions,  \\n they are automatically committed to the database.  \\n However, you can explicitly run a transaction  \\n by adding begin transaction  \\n to the beginning of the statement  \\n and to the end of the statement  \\n with either commits or rollback.  \\n If an error occurs, we can rollback changes  \\n to what they were before the statement.  \\n If everything is good, we can commit  \\n and continue with the next set of processes.  \\n This is known as transact SQL  \\n and is an important part of the development process,  \\n especially when dealing with large transactions  \\n that involve cascading changes across the database.  \\n Transact SQL is helpful in ensuring  \\n the integrity of the database  \\n by preventing processes from continuing  \\n if an error has occurred in our data processing.  \\n \\n\\n\"}],\"name\":\"4. Easy SQL Functions\",\"size\":24546958,\"urn\":\"urn:li:learningContentChapter:4479651\"},{\"duration\":295,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4483688\",\"duration\":216,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Getting started with data visualization\",\"fileName\":\"3271025_en_US_05_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover how to incorporate a SQL statement into a data visualization project. When you can incorporate data visualization in your applications, you can bring data to life and make decision-making easier for visual you and those who need to read and interpret data quickly.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7291579,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's bring it all together  \\n using the code we've learned in this course.  \\n We'll use fundamental concepts to create a simple,  \\n web-based dashboard using Jupiter Notebook and Plotly.  \\n In software applications,  \\n we use the CRUD operations to create, read, update,  \\n and delete data in the application code.  \\n Developers use a combination  \\n of these concepts with other advanced ones  \\n to create a successful data-driven application.  \\n Now, let's put some of our code into practice.  \\n I'm going to incorporate a dashboard using Plotly,  \\n an open source graphing library for Python.  \\n We'll build the dashboard using Jupyter Notebook  \\n and Python and display it  \\n on a webpage that shows how each salesperson is performing.  \\n If you aren't familiar with Python, no worries.  \\n Python is a programming language that can be used for lots  \\n of applications from software development to data analysis.  \\n Let's start by looking at the view V orders.  \\n It includes the sum of the total due,  \\n the count of orders by month, year,  \\n and the salesperson's full name.  \\n Let's start by opening the dashboard dot IPYNB file  \\n that contains our Jupiter notebook.  \\n Let's move this down a little bit and start from the top.  \\n Feel free to follow along with me  \\n as I go through each step in the Jupiter notebook.  \\n We'll begin  \\n by importing the Python libraries needed to run our code  \\n and connecting to the H plus sport database in the repo.  \\n Just go ahead and click  \\n on the recommended Python environment.  \\n If the step is finished, you'll see that green check.  \\n This says it's okay to go ahead.  \\n We'll connect to the H plus sport database.  \\n Let's take a quick peek  \\n at our V orders table by running DF dot head.  \\n This is the head of our data frame in Python.  \\n We'll see our first five records in the table.  \\n Now let's plot this on a bar chart.  \\n I've already created a Python script  \\n that will demonstrate the total due by order date  \\n and salesperson.  \\n I also added a filter by salesperson  \\n so that the user can filter the data to show the sum  \\n of the total due for each salesperson.  \\n Let's click to run.  \\n You'll see a message that says your application is running  \\n on port 8050, so let's go there.  \\n You can go there by scrolling  \\n down and going to dash is running  \\n on http://127.0.0.1 on port 8050.  \\n You can click on the link  \\n and now you should see your dashboard running in an app.  \\n This could take a while to load, so be patient.  \\n It took me about two or three minutes to load.  \\n Here you can use your dropdown box to change the salesperson  \\n and you can see an order summary  \\n by salesperson here and interact with the data.  \\n Many business intelligence tools  \\n will have embed capabilities or APIs that we can call  \\n to add visualizations to a website.  \\n Some can have cost associated with it  \\n like a Tableau or Power BI.  \\n Plotly is open source and you can use it just  \\n like you did today in Python and Jupiter Notebook.  \\n To learn more about Python tools,  \\n check out the other courses in the library  \\n and also documentation on python.org/about and plotly.com.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4484704\",\"duration\":79,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating data visualization\",\"fileName\":\"3271025_en_US_05_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to embed a simple dashboard on a webpage. When you can combine a simple dashboard into a webpage, you can bridge concepts of application development and disseminate data to users and other stakeholders.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3229328,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now it's your turn to create.  \\n In the repository for the course, you'll run the code  \\n for Create a dashboard.ipynb.  \\n This Jupiter Notebook contains the code  \\n to create a new dashboard  \\n from the V customer order summary view  \\n that contains the total sales, minimum due,  \\n maximum due and average due by state.  \\n Run through each step in the Jupiter notebook  \\n and note how we can incorporate SQL  \\n from this course into Python, Pandas  \\n and plot with Plotly to create a new web-based dashboard.  \\n I've updated the header to H plus Sport Sales Dashboard,  \\n and you can also look at the Plotly documentation  \\n if you're more interested in more ways  \\n that we can format the dashboard  \\n and make it even more interactive.  \\n Click on the link again from Jupiter notebook,  \\n and you've created your own web-based dashboard  \\n that you can share with the sales team for H Plus Sport.  \\n We can see that our webpage now includes the dashboard  \\n with our SQL queries that we created in this course.  \\n We can also use the filters that we created  \\n in Plotly right here on the webpage.  \\n Note that I'm using Plotly and Jupiter notebooks,  \\n but you can embed a dashboard  \\n into a webpage using Power BI, Tableau, Looker  \\n and many other data visualization software systems.  \\n \\n\\n\"}],\"name\":\"5. Presenting Your SQL Results in Data Visualization\",\"size\":10520907,\"urn\":\"urn:li:learningContentChapter:4480730\"},{\"duration\":125,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2705663\",\"duration\":125,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Leveling up SQL\",\"fileName\":\"3271025_en_US_06_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5198262,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - We've reached the end of the course.  \\n By now, we've talked about what SQL is and how to use it.  \\n We've gone over some basic syntax  \\n to get information from a table  \\n and common functions to use that data.  \\n Then we brought it all together  \\n in a web visualization project using Python.  \\n We've done a lot, but this is only the beginning  \\n of incorporating SQL into your data driven web applications.  \\n So where do we go from here?  \\n If you're newer to data development, you should know  \\n that most calls to your data database occur on the backend.  \\n So if you're interested in using SQL for web development,  \\n learning more about backend  \\n or even full stack web development may be a good idea.  \\n Also, this course was a high-level overview.  \\n You can create web apps to enter customers, update orders  \\n or allow customers to place their own orders  \\n but that's something we wouldn't do from a Jupyter Notebook.  \\n To do that, you should learn more about how to incorporate  \\n other more advanced CRUD operations  \\n and these can get really complex.  \\n Finally, there are a bunch of common relational databases  \\n used in web development.  \\n There's PostgreSQL, SQLite, SQL Server,  \\n and that's not even scratching the surface.  \\n Good news, most of the SQL syntax is the same  \\n in various database management systems  \\n but how you apply it changes  \\n depending on the database that you're using.  \\n You can check out some of our other courses  \\n on LinkedIn Learning for a deeper dive into the differences  \\n and more advanced SQL operations.  \\n SQL is still incredibly relevant and useful in both software  \\n and web development, analytics and data science.  \\n Good SQL fundamentals is a must have for any developer  \\n and a great skill to add to your resume.  \\n So congratulations!  \\n Again, I'm Nakia Simpson  \\n and here's to many successful development projects  \\n using your new SQL skills.  \\n \\n\\n\"}],\"name\":\"Conclusion\",\"size\":5198262,\"urn\":\"urn:li:learningContentChapter:2706666\"}],\"size\":134968307,\"duration\":4257,\"zeroBased\":false},{\"course_title\":\"SQL: Data Reporting and Analysis\",\"course_admin_id\":2494129,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":2494129,\"Project ID\":null,\"Course Name\":\"SQL: Data Reporting and Analysis\",\"Course Name EN\":\"SQL: Data Reporting and Analysis\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"Do you rely on IT to get the data you need? Are you often stuck waiting in line for data, and wish you could just retrieve it yourself? In this course, learn how to get the data you want by writing a bit of SQL code. You won't just be able to pull data out of the database, you'll be able to manipulate it\u00e2\u20ac\u201dmerging it, grouping it, and relabeling it to get just the report you want. Join Emma Saunders as she shows how to write simple SQL queries for data reporting and analysis using a publicly accessible online database. Learn how to filter, group, and sort data, using built-in SQL functions to format or calculate results. Discover how to perform more complex queries, such as joining data together from different database tables. Last but not least, she introduces views, procedures, functions, and variables.\",\"Course Short Description\":\"Get the data you need for analysis and reporting by writing your own SQL code. Learn how to write basic SQL queries, sort and filter data, and join results from different tables.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":7094528,\"Instructor Name\":\"Emma Saunders\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Data Consultant specializing in data visualization on the web\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2022-07-19T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/sql-data-reporting-and-analysis-18082247,https://www.linkedin.com/learning/sql-data-reporting-and-analysis-2022-revision\",\"Series\":\"Deep Dive (X:Y)\",\"Limited Series\":null,\"Manager Level\":\"General\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Data Science\",\"Primary Software\":\"SQL\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":9156.0,\"Visible Video Count\":38.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":311,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3086803\",\"duration\":39,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Don't let your reporting system hold you back\",\"fileName\":\"2494129_en_US_00_01_2817027_WX30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"This beginner course is aimed at technically-able business people who have no previous programming knowledge, rather than IT staff. It is aimed at users and producers of reports who are looking to increase their efficiency at work.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2422976,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Emma] If your reporting system is holding you back  \\n SQL could help.  \\n This course will teach you to access data directly  \\n from your database, merging tables  \\n and reporting exactly the data you want.  \\n It's much quicker than downloading it  \\n and then manipulating in Excel or Access.  \\n SQL can also be used in the advanced settings  \\n of many reporting systems.  \\n Luckily it's intuitive even for non-programmers.  \\n My name is Emma Saunders, and I've worked  \\n with data and databases for more than 20 years.  \\n I'm going to teach you the SQL you need to build reports  \\n including grouping, filtering, and analyzing.  \\n So let's get reporting.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3083809\",\"duration\":20,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Who this course is for\",\"fileName\":\"2494129_en_US_00_02_2817027_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"SQL is a programming language used to access data from databases. There are several versions of SQL but they are all similar at a basic level. In this course, learn about the four most common kinds.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":643048,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Narrator] This course is aimed  \\n at technically able business people rather than IT staff.  \\n You don't need previous programming experience  \\n to follow along.  \\n You don't need to know how a database works,  \\n and you don't need any special software or machinery.  \\n You just need an interest in data and analysis,  \\n and an internet connection,  \\n which I'm guessing you already have.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3085862\",\"duration\":252,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What is SQL?\",\"fileName\":\"2494129_en_US_00_03_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11069796,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] SQL stands for Structured Query Language  \\n and it's used to retrieve data from databases.  \\n You'll often hear people refer to SQL as sequel.  \\n When we talk about databases,  \\n in this course, we mean relational databases.  \\n They can be thought of like a spreadsheet,  \\n in that they have tables and columns.  \\n Relational databases are the most popular kind  \\n and they've been around since 1970.  \\n Now, there's some good news and some bad news.  \\n The bad news is that there are lots of versions of SQL.  \\n In fact, each company that makes database software,  \\n such as Oracle or Microsoft,  \\n uses a slightly different version of SQL.  \\n The good news is that they're all similar at a basic level  \\n and this course covers all the main ones,  \\n Oracle, Microsoft, PostgreSQL, and MySQL.  \\n There is an ISO standard that acts  \\n as a template for these vendors.  \\n In fact, there's nine, I think,  \\n and this blog post is a good place to start  \\n if you're interested in that.  \\n Within each type of SQL, there are some different versions,  \\n as the language has changed over the decades.  \\n For Microsoft, I've tested my queries  \\n against SQL server 2017.  \\n For Postgre I've used 9.6.  \\n for Oracle I've used 11g Release 2.  \\n And if you are using the demo MySQL database,  \\n then I'm using version 8.03.  \\n If you're using a slightly different version,  \\n it's possible that there will be differences  \\n with this course, but those should be small.  \\n We're going to be using an online demo database  \\n for this course and I'm going to show you how to find it.  \\n There's a reason I'm doing this  \\n rather than giving you a permanent link.  \\n And there it is, sakila try phpMyAdmin.  \\n That's the name of the software,  \\n web-based software that we're going to use.  \\n So a word of warning here.  \\n Anyone across the whole world can access  \\n and alter this database,  \\n including deleting the entire thing.  \\n They can also move it.  \\n I believe that the database is recreated each day  \\n so it's possible one day that you try and find a database  \\n or a table within it, and you can no longer see it.  \\n If this happens, try a few hours later or the next day.  \\n Don't consider this link to be a permanent link.  \\n It's best that you Google for yourself  \\n in case the link address changes.  \\n If the entire database is unavailable,  \\n come up here where it says Current server,  \\n top left, and change to MariaDB.  \\n And that should work just as well.  \\n You don't need to use the demo database.  \\n You can follow along with your own company database.  \\n You'll have to adapt the queries and listen out  \\n when I tell you that the keywords or syntax differs  \\n between different versions.  \\n If this is you, it will be useful for you  \\n to know which version or which type of SQL you're using  \\n or which vendor has made the database.  \\n You can't always tell by looking at the reporting system.  \\n You could be using a Microsoft reporting system  \\n that's connected to an Oracle database.  \\n We can find out typically using code,  \\n but unbelievably, there is no simple command  \\n that works across all types of SQL,  \\n so we have to do some detective work.  \\n First of all, work out where to run your queries.  \\n In phpMyAdmin, that is under the SQL tab here.  \\n Then you can try select version  \\n and click go, and you can see the version  \\n is returned here as 8.03.  \\n Now, if select version works,  \\n you'll be using MySQL or Postgre,  \\n because those commands only work in those types.  \\n And if you get a number of around 12, you're using Postgre,  \\n and a number around eight you're using MySQL.  \\n If that doesn't work, try select @@version.  \\n And if that works, you're using Microsoft.  \\n And if that still doesn't work,  \\n try select star from the dollar version.  \\n And if that works, you're using Oracle.  \\n If none of these has worked,  \\n you could be using a database from another vendor.  \\n And there's quite a handy list  \\n of relational database management systems here on Wikipedia.  \\n As you can see, there's an awful lot of them.  \\n If you can, try asking your IT department  \\n which system is yours, or follow along using the demo site.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":14135820,\"urn\":\"urn:li:learningContentChapter:3080935\"},{\"duration\":319,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3082834\",\"duration\":225,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Get familiar with phpMyAdmin\",\"fileName\":\"2494129_en_US_01_01_XR15\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"In this video, learn about phpMyAdmin, the online software used to access the demo database. Explore the databases listed on the left, the tabs along the top, and the breadcrumb bar.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7041207,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] You already know that we are going  \\n to use a demo database online,  \\n and to find that we can Google demo Sakila database  \\n in any order, and there it is.  \\n The reason I'm asking you to Google,  \\n instead of giving you a permanent link is  \\n because sometimes the link changes.  \\n This is a free resource to help people learn and use SQL.  \\n So, here, is the demo that we are going to be using.  \\n Let's have a look around the screen.  \\n In the top left, we have a logo for phpMyAdmin.  \\n This is the name of the software that we're going to use.  \\n I'm not sure who named it.  \\n It doesn't trip off the tongue,  \\n but it is an open source database software that you  \\n can actually download and use on any database that you like.  \\n If we look at the top left, where it says current server,  \\n underneath there's a dropdown.  \\n Dropdown gives us two options.  \\n They're both demo databases.  \\n One says Maria DB, and the other one says MySQL,  \\n and I just want to show you something in Maria DB.  \\n On the left here, are a list of all  \\n of the databases available.  \\n And you can see that the Sakila database is available  \\n in Maria DB.  \\n Now, we're not going to use that one.  \\n We're going to use the Sakila database  \\n in the MySQL server,  \\n but if you were to turn up one day  \\n and not be able to find the Sakila database in MySQL,  \\n you could change the current server to be Maria DB,  \\n and your queries ought to work exactly the same.  \\n Maria DB and MySQL are virtually identical.  \\n This database list on the left will change all the time  \\n because remember that anyone  \\n in the whole world can edit this database.  \\n We want the Sakila database, which I've clicked,  \\n and you can tell I've clicked it  \\n because there's a breadcrumb bar at the top,  \\n and it says server, demo MySQL,  \\n within which database equals Sakila.  \\n So, we have now selected the Sakila database.  \\n Along the top, underneath the breadcrumb bar,  \\n we can see a number of tabs, structure, SQL,  \\n search query, et cetera.  \\n The main ones we are going to be using are the structure  \\n and SQL tabs and a third tab called browse,  \\n which we shall see in a minute.  \\n The structure tab is really important.  \\n At the moment, it's listing all of the tables  \\n and views that occur within the Sakila database,  \\n but it's context dependent.  \\n So, if we click on the actor table,  \\n we are then taken to a browse tab,  \\n but if we go back to the structure tab  \\n you can see it's changed substantially.  \\n And now, instead of listing all the tables in the database,  \\n it lists all the fields in the table.  \\n So, it's drilled down for us,  \\n and the breadcrumb bar reflects that too.  \\n Within the Sakila database it tells us  \\n we have now selected the actor table.  \\n So, this breadcrumb bar is really helpful  \\n to stop you getting lost.  \\n Now, in the structure tab, we can see  \\n that we have four fields in the actor table,  \\n actor ID, first name, last name, and last update.  \\n If we go back to the browse tab,  \\n you can see these field names shown again.  \\n This time they're shown horizontally instead of vertically,  \\n actor ID, first name, last name, last update.  \\n The purpose of the browse tab is to let you see  \\n the actual data inside of the table.  \\n So, under these field names, we can see names, IDs,  \\n and dates in the respective columns.  \\n The final tab to look at is the SQL tab,  \\n which is where we enter our queries  \\n and run them by clicking go.  \\n So, that's a really quick tour of phpMyAdmin,  \\n which is the interface we're going to use  \\n in this course.  \\n Coming up we take a quick look at the Sakila database,  \\n which is populated with fake data,  \\n and I'll explain to you what that fake data is meant to be.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3086804\",\"duration\":94,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand the demo database\",\"fileName\":\"2494129_en_US_01_02_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"This course uses the freely available Sakila demo database. This database contains fake data for a DVD rental store.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3253090,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Sukila database contains fake data  \\n for a DVD rental store.  \\n So this was created in 2006, back in the day  \\n when you would physically go to a shop,  \\n look at a selection of DVDs on a shelf,  \\n and take one home and watch it.  \\n Some of the tables that are in here  \\n we would entirely expect of a sort of retail business model.  \\n So there's a table on stores or shops data.  \\n There's a list of staff,  \\n and we have a customer table,  \\n which contains obviously customer name,  \\n whether that customer is still active, et cetera.  \\n Then we have a series of more frequently updating tables.  \\n We have the infantry table,  \\n which is a stock check effectively,  \\n payment table for transactions,  \\n and a rental table,  \\n which ties lots of these other data points together.  \\n So this one will tell you which customer rented  \\n which film on which date.  \\n So far so ordinary,  \\n but the Sukila database is quite a lot more  \\n powerful than that.  \\n It contains a lot of extra data about the movies themselves.  \\n We can tell from this database, which actors played a part  \\n in which film, what type of film the film is.  \\n So is it a comedy or horror?  \\n Even what language the film was originally recorded in.  \\n So the Sukila database would let you identify  \\n which customers had ever rented a comedy  \\n in Japanese containing a particular actor.  \\n Altogether, pretty powerful.  \\n So let's get running some queries on it.  \\n \\n\\n\"}],\"name\":\"1. Prepare to Code in SQL\",\"size\":10294297,\"urn\":\"urn:li:learningContentChapter:3083815\"},{\"duration\":3020,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3082835\",\"duration\":331,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Retrieve data with SELECT\",\"fileName\":\"2494129_en_US_02_01_2817027_XR15\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"When you run your first query you should first return all fields and then specific fields from a table. In this video, Emma discusses when you should use special characters around table names.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9582272,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] When you ask a database  \\n to give you some data,  \\n you write a piece of code called a query  \\n and that's the queue in SQL.  \\n To retrieve data from a database,  \\n we use what's called the SELECT query.  \\n So if I click on the actor table there,  \\n you can see that the database has run its own query for us.  \\n SELECT * FROM actor.  \\n SELECT queries always take the basic form,  \\n SELECT field name FROM table name.  \\n And the asterisk there just means all.  \\n So this means SELECT ALL FIELDS FROM actor.  \\n And we know from what we've seen previously  \\n that the four fields we can see here horizontally  \\n are indeed all of the fields available in the actor table.  \\n So our query has worked.  \\n Although, we can see all four fields,  \\n we can only see 25 rows.  \\n You see it says 25, KEVIN BLOOM.  \\n And if we go up in the yellow bar with a little tick,  \\n it says showing rows 0 to 24 of 200 total.  \\n So there's 200 rows in this table,  \\n but the database is only showing us the first 25.  \\n Notice that it says here 0 to 24, and not 1 to 25.  \\n This is called zero indexing.  \\n And it means that the database starts counting  \\n at 0 instead of 1.  \\n So even though the data in the table,  \\n the actor ID data says 1, 2, 3, and so forth, up to 25,  \\n the database itself counts these rows  \\n in a way that isn't shown here on the screen, from 0.  \\n Let's change this query.  \\n So click Edit in line, if you haven't already,  \\n and replace the * with first name.  \\n So you have SELECT first_name FROM actor.  \\n And I'm going to just pop the FROM section of this query  \\n onto another line.  \\n You can keep it all on the same line, if you like.  \\n I just think it's easier to see the query,  \\n the parts of the query that way.  \\n So our screen changes when we click Go.  \\n Because instead of seeing all four rows now,  \\n all four fields, sorry,  \\n we can only see one, which is the first name column.  \\n Now, we can click Edit in line again  \\n and say SELECT first_name, last_name.  \\n And can you see it's offering us field names  \\n that we might want to use,  \\n and I've just pressed Tab to take advantage of that.  \\n So we've now got SELECT first_name, last_name FROM actor.  \\n Then we click Go.  \\n And unsurprisingly, perhaps, we now have two fields showing.  \\n And you can see how the fake data works here, by the way,  \\n because we have PENELOPE GUINESS here, which, I think,  \\n the real actor would probably be Alec Guinness.  \\n So they've scrambled the first  \\n and last names of different actors in this database.  \\n We can select or retrieve  \\n as many fields as we want to, by the way,  \\n just by listing all the fields with commas in between them,  \\n as we've done here.  \\n Now notice that the actor table is enclosed in back ticks.  \\n This is an oddity of MySQL.  \\n Back ticks aren't the same as single quotation marks  \\n or apostrophes, by the way.  \\n On a Mac, you find the back tick, bottom left,  \\n just below the tilde sign of your keyboard.  \\n They are optional here.  \\n And I want to try and quote the queries  \\n in this course without back ticks in them,  \\n because that way, the queries will work  \\n across Microsoft Oracle, MySQL, and PostgreSQL.  \\n Back ticks should be used in MySQL,  \\n where the table name has a special character  \\n or a space or a carriage return,  \\n or uses a reserved word such as date in the table name,  \\n because those things would confuse the database.  \\n So if we had say an @ sign there,  \\n and the database maybe would get confused.  \\n In MySQL, you would use back ticks  \\n to quote your system identifier,  \\n that's the proper name for it, your table name.  \\n And that tells that MySQL database  \\n that anything inside the back ticks is the table name.  \\n And we don't want to do anything special with our query.  \\n It's just a table name that we're using.  \\n If you're using Microsoft,  \\n you quote your system identifies using square brackets.  \\n And if you're using PostgreSQL,  \\n you quote them using double quotation marks.  \\n This is compliant with the standard, actually,  \\n the way that Postgre does it.  \\n Now, from what I've read,  \\n Oracle also uses double quotation marks.  \\n But when I have tried to make that work  \\n in my own testing, it hasn't worked properly.  \\n So in this course,  \\n we're going to try avoiding using any system identifiers.  \\n And we're just going to use the table names like that.  \\n Where possible, if you are creating a database,  \\n don't use any special characters, whitespace symbols,  \\n or reserve names when naming your tables  \\n because it makes the whole thing really complicated.  \\n But if you didn't create the database  \\n that you are using, you'll have no choice.  \\n And then it's good to know how to quote system identifies  \\n for your version of SQL.  \\n Notice also that I've capitalized the keywords here,  \\n SELECT and FROM, and it's just to show the structure  \\n of the query more clearly.  \\n They can all be lowercase, and it works perfectly well.  \\n So coming up, we're going to look  \\n at how to filter our new query.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3083810\",\"duration\":304,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filter results with the WHERE clause\",\"fileName\":\"2494129_en_US_02_02_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to filter your results with the WHERE keyword including when you should put single or double quotation marks around your filter criteria.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8927765,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] So far, we've changed the number  \\n of fields in our query,  \\n but we've always returned all 200 rows,  \\n but what if we only wanted to see particular actors,  \\n such as those whose name starts with Penelope,  \\n then we would place a filter on the data using  \\n the WHERE keyword.  \\n Going to say WHERE first name equals Penelope and hit Go,  \\n and we see we have four rows this time,  \\n instead of 200, and it's still showing the two fields  \\n or columns that are in the SELECT part of our query,  \\n and now what if we wanted all actors except  \\n those called Penelope.  \\n For that, we need a not operator,  \\n which can be a less than, followed by a greater than sign.  \\n Let's hit Go.  \\n So now we've got 196 rows, which sounds about right  \\n and in all versions of SQL that this course covers,  \\n you could also use an exclamation mark,  \\n followed by an equals sign.  \\n That is also a not operator.  \\n Now let's have a look at this query.  \\n Notice that the word Penelope is  \\n within single quotation marks.  \\n Single quotes are needed to enclose text,  \\n and they're not the same as the back ticks  \\n that we saw earlier, and they're not the same  \\n as double quotation marks either.  \\n These things all have different meanings  \\n in the world of databases.  \\n Back ticks were optional earlier around the table name actor  \\n but leaving out the single quotation marks  \\n on the word Penelope is not an option.  \\n Without quotation marks, your database will think  \\n that Penelope is a command like SELECT or FROM,  \\n or it might think that it's a field name,  \\n and then it will throw an error.  \\n So if we take them out and click Go, hey, Presto.  \\n There's that error. So let's pop 'em back in.  \\n Some types of SQL can accept single or double quotes here,  \\n and other types will only accept single quotes,  \\n and I believe the standard says only to use single quotes.  \\n So that is good practice,  \\n to get into the habit of just using that.  \\n Now, what if we want to filter  \\n on something other than a string or text.  \\n Perhaps, we want to see all actors  \\n with an ID of less than five,  \\n and we would write that like that.  \\n Now, there's lots of new operators you can use  \\n with numeric data types.  \\n So we've just used less than.  \\n We could have less than or equal to, like so,  \\n and greater than or equal to is similarly  \\n a greater than sign, followed by an equals sign.  \\n If we wanted a range of numbers here.  \\n Let's just add actor ID into our field list,  \\n so we can see what we're getting.  \\n If we wanted a range of numbers here,  \\n we could do that in two different ways.  \\n The first way would be to say, we have an actor ID  \\n of, say, greater than three, AND,  \\n an actor ID of less than five.  \\n Now, notice we've had to repeat the field name  \\n in each section of our WHERE clause there,  \\n and we hit Go, and we just get the one row back  \\n with the ID of 4.  \\n Now, the other way of doing this,  \\n which I personally never use,  \\n is to say WHERE, actor ID, BETWEEN 3 AND 5,  \\n and you might think that would give us the same result set  \\n that we already have down here, but actually it doesn't.  \\n So when we use BETWEEN and AND, the numbers are inclusive.  \\n We get IDs 3, 4 and 5 back,  \\n and that ambiguity is why I avoid using BETWEEN and AND  \\n because I feel it's more precise just  \\n to use the operators and the AND.  \\n Now, notice that we've not needed any quotation marks  \\n or indeed, anything at all around the numbers,  \\n and numeric values never need anything  \\n around them, in any form of SQL.  \\n We can also make up our filters using  \\n the OR statement as well as the AND statement.  \\n So if we go back to first name is Penelope,  \\n we can say OR actor ID is less than five,  \\n and you can tack on as many AND  \\n and OR statements as you like, or first name equals Nick.  \\n Now, we had seven rows returned previously,  \\n and we're adding an OR.  \\n So we'd expect that number to go up,  \\n and we found indeed two Nicks there.  \\n We've added two Nicks, I should say,  \\n 'cause we've now got nine rows back.  \\n Do be careful when combining ANDs and ORs  \\n because you can get yourself in a muddle pretty quickly,  \\n and the way to get around this is to use brackets  \\n around part of your code, and what this does  \\n is it separates your query into sections  \\n which run independently of each other.  \\n This query is using an OR and an OR.  \\n Putting the brackets here, actually,  \\n makes no logical difference at all,  \\n but once we start mixing ORs with ANDs,  \\n it makes the world of difference,  \\n and we'll see more on this later in the course.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3086805\",\"duration\":339,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Use LIKE, IN, and wildcards with WHERE\",\"fileName\":\"2494129_en_US_02_03_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to save time by using IN or NOT IN within your WHERE statement. You can also use wildcards for individual or multiple characters when using the LIKE keyword.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10256360,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Here we have a query,  \\n select star, select all fields, from the actor table  \\n where the first name is Penelope or Nick or Ed.  \\n And as you know, we have to repeat the column name  \\n which is a bit tedious for each of our filter conditions.  \\n And this query returns us 10 rows as you can see up here.  \\n There is a quicker way of doing this  \\n and that's to use the in keyword.  \\n So where first name in, and then Penelope, Nick, Ed  \\n and that's a lot quicker to type it out.  \\n And you can see it returns us the same 10 values there.  \\n Note from this query that we have single quotation marks  \\n around each of these names because they're all text items.  \\n So they all need their single quotes.  \\n So in means matches any of the following  \\n and you could also have, or alternatively have not in  \\n which gives us 190 rows, which makes sense, doesn't it?  \\n That 190 and 10 adds up to the 200 rows  \\n that we know are in that actor table.  \\n You can put as many values as you like  \\n in these parentheses, by the way,  \\n as long as they're separated by commas.  \\n SQL will also let you use wild cards  \\n to match part of a text.  \\n So let's say we wanted any actor whose name begins J O H N.  \\n Then we could say like John, and then use the percent sign.  \\n Now that's returned us three rows, a John and two Johnny's.  \\n So you can see at once  \\n that the percent sign is quite a flexible operator.  \\n It will match there being nothing at all after the N,  \\n and also, it will match two letters  \\n or indeed, an almost infinite number of letters after the N.  \\n So the percent sign will match anything.  \\n There's another wild card character  \\n for matching just a single letter  \\n and that's the underscore.  \\n So if we change this to Jane, so J, spelled like that,  \\n so the underscore is in the middle between the A and the N.  \\n And you can see that's returned us three Jaynes with a Y.  \\n The underscore is a bit more restrictive.  \\n It will match any kind of character,  \\n but there has to be one there.  \\n It won't match a blank.  \\n So if I were to change that  \\n to like Jane without a wild card,  \\n which is the same as saying equals, by the way.  \\n I'll prove it to you by running it.  \\n You can see there is a Jane,  \\n Jane Jackman without a Y in the database  \\n and that was not returned when we used the underscore.  \\n If we had used the percent sign there instead,  \\n it would've returned us all four Janes  \\n with, or without Y's.  \\n One thing to note is that you can't combine like with in  \\n and it really feels like you should be able to.  \\n It feels quite tempting to write where first name in,  \\n and then Penelope percent,  \\n or let's use the ones we know now, John percent or Jane.  \\n We need some single quotes there.  \\n That feels like we ought to be able to do that.  \\n And it doesn't throw us an error  \\n but neither does it give us any useful results.  \\n Because we haven't used the like keyword,  \\n the database has actually searched  \\n for a string that is Penelope percent  \\n and of course, it hasn't found one.  \\n If you're using Oracle or Microsoft,  \\n there is a function called contains  \\n that achieves this effect, combining like with in,  \\n but sadly, that's not part of MySQL.  \\n Now let's look at combining and's and or's.  \\n We're going to run this query  \\n on the address table rather than the actor table  \\n but we don't need to click on the table name to run it.  \\n Here, we returned three rows.  \\n Let's just have a look at that query.  \\n Select everything from the address table  \\n where the district is Buenos Aires  \\n and the address is either like EL  \\n with anything on the either side  \\n or like AL with anything on either side.  \\n So we're filtering on the address column  \\n and that's this one here.  \\n And we can see in the first row, we have El Alto Avenue.  \\n In fact, that one would match on both criteria, wouldn't it?  \\n 'Cause it's got an el and an al.  \\n This one's got an al and another al  \\n and Dallas Manor has an al in there,  \\n which is lowercase, incidentally  \\n but still matches this setting.  \\n So these sorts of inquiries are case insensitive  \\n but I have come across situations  \\n where they're not case insensitive,  \\n particularly with PostgreSQL  \\n and I'm not sure whether that's a setting of the database  \\n or a feature of the language, but take care.  \\n If you're not using MySQL, the case might matter.  \\n Notice that we've used brackets  \\n or parentheses around our or condition.  \\n Without brackets, the database will read  \\n and apply the filters in the order they appear.  \\n So let's take them off.  \\n Remember that we've returned three rows here.  \\n And now we've returned 73 rows.  \\n And if we look at the district,  \\n you can see lots of the districts are not Buenos Aires.  \\n We're all over the world here, aren't we?  \\n Khartum, Missouri, Uttar Pradesh.  \\n The database has lumped the where and the and together.  \\n So it's returned any row  \\n that has a district of Buenos Aires  \\n and an address like el,  \\n then the database has applied the or.  \\n So a row is returned if the address contains AL,  \\n regardless of whether the district is Buenos Aires.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3082836\",\"duration\":102,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Sort SQL results with ORDER BY\",\"fileName\":\"2494129_en_US_02_04_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Database results are typically ordered by the date and time that row was entered into its table. This is generally unhelpful in reporting. In this video, learn how to sort your results using ORDER BY.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3029695,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Earlier we ran this query.  \\n Select first name, last name from actor,  \\n where first name is Penelope.  \\n And we have our four results there.  \\n Now, if you look at the last name shown,  \\n they appear to be in a random order.  \\n In fact, they're given to us  \\n in the same order that they appear in the table.  \\n And to make that a bit more obvious,  \\n let's add in actor ID  \\n and you can see the actor ID is in order.  \\n Rows in the database are typically  \\n in the order they were entered.  \\n So data that's entered later is lower down in the table,  \\n but that's no good to us.  \\n We don't care when the name was added to the database.  \\n We want to sort our results alphabetically on last name.  \\n And to do that, we use the order by keyword.  \\n I'll say order by last name  \\n and then ask for ascending,  \\n and press go.  \\n And there we have, that's better CGMP.  \\n So that's worked.  \\n Note that we put the filtering clause where  \\n before the order by clause.  \\n SQL requires you to write your queries  \\n in a particular order.  \\n It doesn't mind if you have random spaces between words  \\n like this,  \\n or you can have extra carriage returns.  \\n It's not fussy about any of that, but it does mind  \\n that the query is written in the right order.  \\n So from now on, in this course,  \\n we will be using this standard setup for select statement,  \\n select fields, from table, where any filter,  \\n which might include some ands and some ors,  \\n and then order by followed by ask or desk, by the way,  \\n if you wanted it to be descending.  \\n Every select statement we look at now  \\n will be a variation on this theme.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3084823\",\"duration\":401,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Use string functions on your data\",\"fileName\":\"2494129_en_US_02_05_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"A string function transforms a text result in some way. For example, the LENGTH function returns the number of characters in a word, while CONCAT adds different strings together. In this video, learn how to use LENGTH, CONCAT, UPPER, LOWER, LEFT, and RIGHT.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11073218,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - In this section we're going to look at string functions.  \\n A function is just a formula, and string just means text.  \\n So a string function is a formula that you can use on text.  \\n SQL provides lots of functions to manipulate your data,  \\n which saves you having to download it  \\n and then manipulate your data in Excel  \\n or some other package.  \\n But it's really important to know which data type  \\n you're dealing with  \\n because some of the functions will only work on  \\n numbers or dates or strings.  \\n And we are focusing ourselves on strings here.  \\n So the first one we're looking at is length,  \\n and like most functions or formulas,  \\n you use the function name,  \\n which can be in upper or lower case,  \\n then parenthesis, and the thing you're trying to measure,  \\n typically a field name, goes inside the brackets.  \\n So if we click go, we can see Penelope has eight characters,  \\n Nick has four, Ed has two, et cetera.  \\n Notice that if you're using Microsoft  \\n the function is LEN and not LENGTH.  \\n So using this function has created us a whole new column.  \\n We have the original column, which is a field, first_name,  \\n and then we have this second column  \\n which processes information from the first one.  \\n It's kind of synthetic data.  \\n And it's quite normal for SQL queries and reports indeed  \\n that you may wish to run to return only synthetic data.  \\n Data that describes other data that's in the tables.  \\n Another common string function is CONCAT.  \\n And CONCAT joins together two bits of text.  \\n So if we can concatenated rain and bow,  \\n we would get rainbow.  \\n So here we can say CONCAT first_name,  \\n and then we'll put a space in using single quotes,  \\n and then last name.  \\n So we're going to return the full name from actor. There we go.  \\n Penelope Guinness, Nick Wahlberg, et cetera.  \\n Notice that with Oracle, you can only put two parameters,  \\n two arguments into the brackets.  \\n So this wouldn't work in Oracle  \\n because we have three parameters here,  \\n even though one of our parameters is only a space  \\n and not even a field.  \\n That would fall over in Oracle.  \\n But all is not lost because Oracle, Postgre and Microsoft  \\n all offer a shortcut for CONCAT.  \\n Instead of using the word CONCAT,  \\n you can use special symbols to indicate that  \\n you want to join the text together.  \\n So in Oracle and Postgre it's a double pipe,  \\n and in Microsoft it's a plus sign, like so.  \\n And then you can put as many parameters in as you want.  \\n We can also combine string functions  \\n by placing one inside the other.  \\n So let's combine the two we've already seen.  \\n SELECT CONCAT,  \\n and I'm copying that expression and pasting it  \\n to save a bit of time and the inevitable typos.  \\n Okay, so we've got select full name basically,  \\n and length of full name.  \\n So Penelope Guinness has 16, et cetera.  \\n We can still use order by on this query by the way.  \\n Order by will work on numeric or text or date.  \\n It works out which data type it's dealing with  \\n and orders accordingly.  \\n Now when we ordered last time  \\n we just did it on a field name.  \\n Something like order by first_name,  \\n which we could still do here.  \\n But it might be more interesting to order by  \\n the number of characters, for example.  \\n And there we could just copy and paste this expression  \\n down here  \\n and then put, say, descending,  \\n which is a quick way of finding out  \\n what's the longest name in the database.  \\n Michelle McConaughey, with 20 characters.  \\n There are quicker ways of doing this by the way,  \\n with slightly more advanced SQL which we will come onto.  \\n The different types of SQL  \\n have different string functions available by the way.  \\n And they can vary quite wildly  \\n so if you're not using MySQL, or even if you are in fact  \\n it's a good job to Google string functions  \\n followed by your database vendor name,  \\n Oracle, Microsoft, et cetera.  \\n So let's have a look at some more common string functions  \\n UPPER and LOWER, which change the case of our strings.  \\n You might have noticed in the actor table  \\n that all the actor names were given in upper case.  \\n So let's say LOWER first_name.  \\n And there we can see Penelope in lower case.  \\n It's more likely though  \\n that you'd want to return proper case  \\n where the first letter of each word is capitalized.  \\n In Oracle and Postgre you could do INITCAP,  \\n but sadly that's not available in the other two.  \\n So for those, we would have to use LEFT and RIGHT.  \\n So we could go LEFT first_name, one character.  \\n So we're saying return me the first character from the left  \\n of the first_name field.  \\n So with Penelope, that would be a P  \\n and indeed we can see a P.  \\n And an N for Nick,  \\n and an E for Ed,  \\n and so on.  \\n Similarly, we could say, RIGHT first_name, seven  \\n and then we get ENELOPE.  \\n So if we wanted to create a proper name  \\n we would concatenate part of our LEFT  \\n and part of our RIGHTS.  \\n And then we would have that in lower case  \\n to create proper case overall.  \\n And we can see we've got Penelope there.  \\n Now, if we look at the next line down  \\n we can see a shortcoming of this approach.  \\n We have hard coded in the length of the string effectively  \\n because we've said we want one from the left  \\n and seven from the right,  \\n which is great if we have an eight character word.  \\n But here we have a four character or a two character words,  \\n and they look pretty awful.  \\n To make this statement work irrespective of the length  \\n we need to use LENGTH itself,  \\n or a more advanced function called SUBSTRING  \\n which we will come onto.  \\n But let's just finish this query off first.  \\n We're always going to want the first character from the left  \\n to be an upper case.  \\n But then it's this number that's going to vary.  \\n And what we want is the length, minus one of that,  \\n which then gives us proper case for Penelope, Nick and Ed,  \\n and all the others.  \\n Coming up, we will see some quicker ways  \\n to achieve the same effect.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3086806\",\"duration\":515,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"More advanced string functions\",\"fileName\":\"2494129_en_US_02_06_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"More advanced string functions look inside or transform the string in some way. In this video, learn how to return part of a text value using SUBSTRING or remove certain characters from the start or end of a value using TRIM. Emma also shows you how to find the location of a specific set of characters using LOCATE\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14326123,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Recently, we've used left and right  \\n to return parts of string  \\n from the beginning or end, respectively.  \\n But what if you wanted to return a partial string  \\n from the middle?  \\n Here we could use substring unless you're using Oracle,  \\n in which case you need substra.  \\n Substra works in all of the four versions except Microsoft.  \\n So none of them work across the board.  \\n Anyway, we're going to use substring.  \\n Select substring, first name,  \\n and this one takes three arguments.  \\n 1,1, like that.  \\n To get the first letter of the first name,  \\n we give the field name  \\n then the starting character one, not zero.  \\n And then the length, the number of characters  \\n we want to return.  \\n So this should return us the same as left, 1 returns us,  \\n and you can see it does 'cause there's our P.  \\n Now let's play around with this a little bit.  \\n What if we tried 1,2, then we'd have PE  \\n for Penelope instead of P  \\n because we've said that we still want to start  \\n at the first character,  \\n but now we want to return two characters.  \\n And if we tried 2,1,  \\n we'd have E instead of P  \\n 'cause we're saying start at the second character  \\n and go on for just one character.  \\n Now let's try select substring first name,2  \\n and get rid of our third parameter all together.  \\n It should look a bit odd, really.  \\n We're using substring,  \\n but we've passed in fewer arguments.  \\n In fact, substring can accept commands in a number of ways  \\n in most versions of SQL, but not Microsoft,  \\n from what I've found.  \\n Again, we've told the database  \\n that we want part of first name returned  \\n and the two says we want to start at position two.  \\n The lack of a third parameter, the length,  \\n tells phpMyAdmin we want the rest of the word,  \\n however long it may be.  \\n So substring has an advantage over left and right  \\n in that it doesn't need to know how long the word is.  \\n So you don't need to couple it with the length function  \\n and you can see that's worked,  \\n 'cause it's returned us enelope and ick and d and ennifer.  \\n For MySQL and Oracle,  \\n you could also use a negative number here,  \\n - 4, to return characters from the end of the string  \\n rather than the start.  \\n So you can see it's given us the last four characters,  \\n lope from Penelope.  \\n Now back to our original statement,  \\n which was first name, 1, 1, which gave us the P.  \\n So now we can concatenate our P  \\n with a lowercase version of enelope  \\n and there we have it.  \\n This query works in MySQL and Postgre.  \\n If you are using a different version,  \\n try to spot why it might fail.  \\n For Microsoft, the substring function  \\n must always have three parameters.  \\n It's not as flexible as the example that we've seen here.  \\n And for Oracle, substring is not a function.  \\n Can you remember what it is?  \\n For Oracle, it's substra.  \\n The final two string functions in this section  \\n are trim and locate.  \\n Used in its simple form,  \\n trim removes spaces from a word, if they're there,  \\n and returns the word unmodified, if they're not.  \\n Let's just have a look, select star from actor  \\n where first name is Grace.  \\n Now I know there is an actor in the database called Grace.  \\n However, this query has returned us an empty result set  \\n and that's because I have gone in  \\n and modified this word and added some spaces before  \\n and some spaces after.  \\n Spaces before are called leading spaces, by the way.  \\n And those after are called trailing.  \\n So if we change the query  \\n and say where trim first name equals Grace,  \\n we're saying if you take off all the spaces,  \\n can we find a match?  \\n And there, there we have it.  \\n Grace has popped up.  \\n Notice that I'm using capitals here, by the way,  \\n for my actor name and MySQL doesn't really mind  \\n but Oracle and Postgre can be more picky.  \\n Trim can be really useful for removing unwanted spaces  \\n where your data quality isn't great,  \\n but it's much more advanced than just shifting spaces.  \\n Click on the film text table  \\n and look at the description column.  \\n And can you see that they all start with A,  \\n A Epic Drama, A Astounding Epistle, and so on.  \\n Every description starts with a capital A and a space,  \\n even when the next word begins with a vowel.  \\n And therefore that should be an An Epic Drama,  \\n An Astounding Epistle.  \\n In fact, I checked and every single row begins this way.  \\n So we could use trim to remove the A and the space.  \\n Let's get rid of those bothersome back ticks.  \\n And let's say select description as original  \\n and trim leading a space from description  \\n as modified and hit go.  \\n So we've returned two columns, original and modified.  \\n The first one has got all those As with spaces.  \\n And the second one has stripped them out.  \\n Let's have a look at the query.  \\n The leading keyword tells the database  \\n only to look at the start of the text string.  \\n Then we have an A and a space within single quotes  \\n and we use single quotes because we're supplying text.  \\n It's just the same as if we were to be filtering  \\n on a string.  \\n And you could specify any set of characters  \\n that you wanted here.  \\n So you could use trim on strings, numbers, or dates.  \\n If your database was storing a unique ID  \\n with leading zeros, for example,  \\n you could remove all of those to return the actual number.  \\n This leading or trailing syntax  \\n works in MySQL, Oracle, and Postgre.  \\n Microsoft wanted to be a little bit different.  \\n In Microsoft, you don't use these keywords at all.  \\n In fact, you use different functions.  \\n You use Ltrim to trim from the left  \\n and Rtrim to trim from the right.  \\n There is a trim function now in Microsoft as of 2017.  \\n And that trims from both ends at the same time.  \\n The final string function we're going to look at  \\n in this section is locate.  \\n So let's say select locate lope,  \\n and I'm doing that in lowercase  \\n which might not work in Postgre.  \\n And then first name from actor,  \\n which means we don't need that bit anymore.  \\n And click go.  \\n In MySQL this would return five for Penelope  \\n because the L of lope occurs at position five  \\n and it's returned zero for the others  \\n 'cause it's looked for lope within Nick and unsurprisingly,  \\n it hasn't found it.  \\n Unfortunately, this useful function is different  \\n for each different type of SQL.  \\n In Postgre, it's select position.  \\n And then you would say lope in first name  \\n and then the table name.  \\n In Microsoft, it would be select charindex  \\n and then it would be lope, first name.  \\n And in Oracle it's select instra.  \\n And it's the other way around just to be awkward,  \\n first name and then lope like that.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3087942\",\"duration\":148,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Change report headings with an alias\",\"fileName\":\"2494129_en_US_02_07_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"You can assign a new column heading to your results, using ALIAS. In this video, learn how to use an alias, which parts of your query can use the alias, and why.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4552384,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's revisit a query from a previous section  \\n because we're going to update it and make it easier to read.  \\n So we've got select full name basically,  \\n and then length of full name  \\n and order alphabetically, descending.  \\n So if we look down here, we've got Zero Cage,  \\n with nine characters, and that's at the top,  \\n because alphabetically, it comes last.  \\n Remember that length would be len in Microsoft,  \\n and that concat in Oracle would only accept two parameters.  \\n You'd have to use the shorthand,  \\n which is the double pipe, for Oracle.  \\n Now this query looks a bit long, and worst of all,  \\n bearing in mind, that this table down here is our results,  \\n and therefore our report, these column headings are awful.  \\n They're unreadable, really, to most people.  \\n You'd be pleased to know though,  \\n that there's a way of making this better,  \\n and it makes our query shorter and more efficient,  \\n and it's called an alias.  \\n So we can say, select concat first name, last name, as name.  \\n That might be a reserved word outside of mySQL,  \\n so if it doesn't work for you,  \\n think of a different name.  \\n As name, as len, and those aliases then pop up  \\n as our column headings, which is extremely useful,  \\n if you're trying to build a report.  \\n So now we've got name and len, which is much better,  \\n and there's one other benefit,  \\n which is instead of repeating this long formula here,  \\n we can say order by len descending,  \\n and you can see there,  \\n we've got Michelle McConaughey with 20 characters.  \\n Now, what if we wanted to return only actors  \\n who had more than 17 characters in their name?  \\n You might think that we would be able to go,  \\n where len greater than 17,  \\n but we can't do that.  \\n We get an error.  \\n Click to get rid of that,  \\n and it says \\\"unknown column len in where clause.\\\"  \\n And the problem basically is,  \\n that this part of the query all sort of runs together,  \\n and the database hasn't necessarily worked out  \\n what len means by the time it gets to the where clause,  \\n which doesn't mean we give up,  \\n we just have to approach it longhand, is one solution.  \\n So we could say where,  \\n and I'm just copy pasting the len,  \\n length, sorry, greater than 17.  \\n And there we have just three results,  \\n so that one has worked.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3083811\",\"duration\":530,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Use date functions with WHERE\",\"fileName\":\"2494129_en_US_02_08_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Dates frequently go wrong in any programming language, and SQL is no exception. In this video, learn about the common problems involved in retrieving date and time data from your database. Emma also shows you how to reformat your date and how to extract specific information such as the year.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14960321,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this section,  \\n we're going to look at date functions  \\n and a little bit more about the data type of date as well.  \\n Date functions just mean a formula that can be run  \\n on fields that have a data type of a date kind.  \\n I've switched to the address table here  \\n from the actor table  \\n and that's because the actor table has just disappeared  \\n which is one of the problems of dealing with an online demo  \\n that anybody can use and something you may come  \\n across yourself as you follow the course.  \\n So the address table has an ID column,  \\n address as in street address,  \\n a district, a city ID, postcode and phone  \\n which appeared to be optional  \\n and then a last update column, which is a date.  \\n So it's got year, then month, then day.  \\n And then it has a time as well, hours, minutes, and seconds.  \\n Now it may seem obvious to you and I  \\n whether something is a number, text, or a date,  \\n but a database has to be told explicitly  \\n and in far more detail  \\n than those three categories would suggest.  \\n So let's have a look at the data type  \\n which is in the type column of the structure tab  \\n if you're using phpMyAdmin.  \\n So the address ID you'll remember looked numeric  \\n but the type given is smallint(5)  \\n and that means small integer  \\n with a maximum number of characters of five.  \\n So you could have 99,999, but you couldn't have  \\n a hundred thousand stored in this field,  \\n the database wouldn't let you do that  \\n because you'd gone over the maximum.  \\n Underneath we have what appear to be text fields  \\n and we can see these are varchars, variable characters  \\n with a maximum character limit of 50 or 20 here.  \\n Then we have another smallint,  \\n a couple more varchars, and last but not least  \\n we have our date column, which is a timestamp.  \\n Now, timestamp is a date typically generated  \\n by the database itself.  \\n So it's stored in a kind of database language  \\n even though it's presented to you and I in the front end  \\n looking very much like a date,  \\n behind the scenes that is stored  \\n in kind of database sort of speak.  \\n And we know that this one is specifically generated  \\n by the database because it has a special behavior attached  \\n to it on update CURRENT_TIMESTAMP.  \\n So if somebody comes in and edits a row in this table,  \\n this last update column will change.  \\n So it's worth just dwelling for a second on this date  \\n once again, we could describe this as European date format  \\n couldn't we, because it's it's year and then month  \\n and then day, which is the European way of doing things.  \\n In most forms of SQL you can store timezone data as well,  \\n along with your date and time,  \\n and that's a different data type.  \\n So it would be clear in the structure tab that you had  \\n a data type that supported timezones down here,  \\n it would have a special name.  \\n You'd also be able to tell whether you had a timezone  \\n by looking in the browse tab.  \\n There's normally a plus sign at the end and then a number  \\n of hours or a minus sign indeed, and a number of hours.  \\n So just look at this date,  \\n which is the 15th of February, 2006,  \\n and let's try running a query on it  \\n where last_update = notice I'm using single quotation  \\n marks here, 2006-02-15.  \\n So we're saying where the date is the 15th of February  \\n and we've got zero rows.  \\n The problem is that this field in the database  \\n contains time as well as date  \\n and we've not specified the time,  \\n with no time supplied the database will assume  \\n we mean midnight entering the 15th,  \\n which of course fails to match.  \\n So if we were to try greater than because four o'clock  \\n in the morning is greater than midnight,  \\n we see that we get all 603 rows there,  \\n but it's not terribly precise is it,  \\n because any update since 2006  \\n would be captured by this query.  \\n So to get an exact match,  \\n we would have to actually type in the time, 04:34:33  \\n and sometimes by the way, you get a partial second  \\n as well, which is point and then some numbers,  \\n and obviously doing it like this is rather tedious.  \\n So at the risk of repeating myself,  \\n the parts of a date are separated by hyphens,  \\n according to the SQL standard  \\n and the parts of time are separated by colons.  \\n Now we are dealing with a timestamp here,  \\n just a note if you're using Microsoft  \\n that the word timestamp refers  \\n to a completely different data type,  \\n and it's now been deprecated  \\n possibly because Microsoft realized it was confusing.  \\n So the nearest data type in Microsoft  \\n now is called date time.  \\n So we can provide an exact match  \\n like this for dates and times,  \\n but as I said, pretty tedious,  \\n just as left and upper are string functions,  \\n so we have some date functions  \\n which are going to help us out.  \\n For instance, in my SQL we could say  \\n WHERE YEAR of last update is 2006,  \\n and we get 603 rows back and notice  \\n that we didn't need any single quotes there  \\n because the database has evaluated a number  \\n from our date column for us,  \\n year doesn't work in Postgre SQL by the way,  \\n where you would use extract, so you'd say WHERE EXTRACT  \\n and then I think it's YEAR FROM, like that.  \\n In Postgre and Microsoft, you could also use date part,  \\n date_, or one of them has an underscore  \\n and one of them doesn't actually  \\n but you can use date part to extract the year  \\n out of a field.  \\n Along with year, you can use month, week, day, hour, minute,  \\n and second in a similar way across all these languages.  \\n And in some of them  \\n you can extract data down to the nanosecond.  \\n You can also return day of the week, Monday to Sunday,  \\n day of the month, one to 31,  \\n the quarter or the week of the year,  \\n but the most likely requirement  \\n and the one that can take the longest  \\n because it can be really fiddly to do  \\n is just to extract the date out of a field  \\n which contains both date and time.  \\n In my SQL, you can simply use date, which is brilliant  \\n and I wish it existed in all of them.  \\n So we say WHERE DATE =  \\n and here we do need single quotes again, '2006-02-15'  \\n like so, and there we've got 603,  \\n with Microsoft 2008 and later  \\n you would need to use the convert function  \\n and in Oracle you would need to use TO_DATE,  \\n in Postgre you'd use date, part, or extract.  \\n Another key date function you should know  \\n about is date format  \\n which show your data in a different format  \\n and this can save you hours in Excel.  \\n So you, for instance, if your dates are stored  \\n in a European date format and you're in the states  \\n and you want to report them in an American style date format  \\n this is how you would do this.  \\n So we would say SELECTED DATE_FORMAT,  \\n field name which is last_update  \\n and then we use a sort of mini language  \\n which may differ between types of SQL  \\n and we are saying %m-%d-%Y  \\n and the capitals in the lowercase matter here, by the way.  \\n And you can see that that has returned us month  \\n and then day and then year separated with hyphens.  \\n You can do all sorts of weird  \\n and wonderful things with this mini language by the way,  \\n we could say %D, get rid of the hyphen,  \\n %M, get rid of the hyphen  \\n and we could put a comma in there maybe, and click go,  \\n and then it's given us, sort of long form there  \\n which is really helpful  \\n if you're working in an international group  \\n and people are used to seeing dates in different formats.  \\n The bad news is of course  \\n that different types of SQL use different keywords  \\n and different formatting conventions.  \\n So for Oracle and Postgre  \\n you would use TO_CHAR,  \\n just write that down so you can see what I mean,  \\n TO_CHAR, like that.  \\n And the formatting inside might be rather different,  \\n and in Microsoft, as long as you're using SQL server 2012  \\n or later, you can use FORMAT.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3085863\",\"duration\":67,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Query a table\",\"fileName\":\"2494129_en_US_02_09_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, Emma challenges you to query a table. In doing so you reinforce how to write a SELECT query using IN.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2018487,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - As you already know,  \\n the Sakila Demo Database is for a movie rental store.  \\n That's an old-fashioned shop that you go into  \\n and you hire a DVD from.  \\n For our first challenge,  \\n I want you to imagine you work at such a store  \\n and a customer has asked for your advice to find a film  \\n that's either rated, PG parental guidance,  \\n or G, generally suitable for all.  \\n The only description they give is  \\n that they are looking for a drama.  \\n And for this challenge,  \\n I want you to confine your search to the film table.  \\n Next, I'm going to give you the same challenge  \\n in more query-like terms.  \\n If you like, you can pause the video now  \\n and try to build the query,  \\n or you can wait for a few more clues.  \\n So in other words, to help your customer,  \\n I want you to return all the records from the film table  \\n where the rating is either G or PG  \\n and the description field contains the word drama.  \\n There's more than one correct solution here.  \\n And building a work inquiry  \\n should take you no more than five minutes.  \\n Good luck.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3088187\",\"duration\":83,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Query a table\",\"fileName\":\"2494129_en_US_02_10_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, explore the solution to \\\"Challenge: Query a table.\\\" Using the examples given, explore the differences in your results.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3183117,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] For this query, we can interpret each line  \\n of our objective into a line of code  \\n so we can build the query up line by line.  \\n The first part was to return all records  \\n from the film table.  \\n And that's what happens when we click  \\n on the film table anyway, select star from film.  \\n So we run that and we have a thousand rows returned.  \\n Next, we need to place a filter using the where claws.  \\n We can either use rating in brackets with the in key word  \\n or we could use the or statement,  \\n which I'm not going to show you.  \\n Either of these is fine,  \\n but if you use the or statement  \\n you might need parentheses around part of your where clause.  \\n So select star from film where rating in G, PG,  \\n and we can run that.  \\n Finally, for the second filter,  \\n we use the and keyword and a wild card,  \\n which is the percent sign in this case.  \\n So we're going to say  \\n and description like percent drama percent.  \\n And let's run that as well.  \\n So we have 36 records returned.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3080933\",\"duration\":50,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Convert case\",\"fileName\":\"2494129_en_US_02_11_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, Emma challenges you to convert cases. In doing so you reinforce how to use string functions in a query.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1450508,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (bright music)  \\n - [Narrator] For our next challenge,  \\n you're still working at the Movie Rental Empire,  \\n but this time you are building a report  \\n for senior management.  \\n This challenge is only going to return  \\n the very first bit of data for that report,  \\n and that is the title field from the film table.  \\n You're going to report all the titles from the field table,  \\n the thing is you want them in proper case.  \\n That means you need to capitalize the first letter  \\n and make the remaining letters of the title field lowercase.  \\n Let's have a look at an example here.  \\n So the objective is to convert the title field  \\n of the film table to proper case,  \\n and underneath you can see a table  \\n where we start with everything in uppercase,  \\n and it's been converted to proper case.  \\n This should take you about five minutes.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3082837\",\"duration\":150,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Convert case\",\"fileName\":\"2494129_en_US_02_12_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, explore the solution to \\\"Challenge: Convert case\\\" so you can compare your results to the examples given.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5086392,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] As a quick recap,  \\n the challenge here is to return  \\n the title field of the film table in proper case.  \\n For this challenge you're going to need to use SUBSTR  \\n and or a combination of left and right,  \\n these are all string functions that return  \\n part of a text field.  \\n Let's start by returning the part of the string  \\n that we want capitalized.  \\n We can say SELECT LEFT(title,1)  \\n and that's saying the left most character  \\n of the title field.  \\n And I dunno if you remember,  \\n but this top row was academy dinosaur  \\n and it's returned A, which is correct.  \\n Now these are already an uppercase.  \\n So just in case somebody comes along and changes it,  \\n we can put up around here,  \\n but strictly that shouldn't be necessary.  \\n Okay, so we have the first character, now we need the rest.  \\n Now, if we were to use the string function called right here  \\n we would also need to use the length of function  \\n to work out how long the title field is in each case.  \\n And it becomes a bit unwieldy.  \\n It's absolutely fine if you did that by the way  \\n but we are going to use SUBSTR.  \\n This is short for substring and it turns part of a string.  \\n Now there is an optional third argument  \\n that we could put here, which is for the length  \\n of the string that we want to return.  \\n But happily, if we leave that out,  \\n it will just take all of the rest of the string  \\n and that is what we want.  \\n So at the moment we're going to return two fields  \\n as an intermediate step, and we can see  \\n that we have returned the first letter  \\n and then the rest of the title, like so.  \\n Currently still in upper case.  \\n So we need to make that lower case.  \\n If you're not using MySQL by the way,  \\n some other versions of SQL have a proper function  \\n which of course makes this far easier.  \\n So now all that's left is to put these together  \\n and we can do that using CONCAT.  \\n And purist among you might want to rename your field as well  \\n because that's rather unwieldy if you're building a report.  \\n So I'm going to say, \\\"as title,\\\" like so.  \\n And there we have it.  \\n \\n\\n\"}],\"name\":\"2. Use SQL to Report Data\",\"size\":88446642,\"urn\":\"urn:li:learningContentChapter:3084828\"},{\"duration\":1321,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3083812\",\"duration\":347,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Use GROUP BY with count\",\"fileName\":\"2494129_en_US_03_01_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"So far you have returned all the rows in the table. But sometimes you want to group data and describe those groups in some way. In this video, Emma demonstrates how to create groups. They also discuss common pitfalls of grouping data, such as failing to select the field on which you are grouping.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9938283,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Sometimes you want to  \\n return information about a table  \\n rather than just returning a subset of information  \\n from within the table.  \\n So for instance, you might want to know  \\n how many times something occurs or how many rows you've got.  \\n Let's have a look at the address table.  \\n If all you want is a count of rows,  \\n you can do that with count star, like so.  \\n We hit go and it gives us the number 603,  \\n which means there's 603 rows in our table.  \\n Now, just a quick note on the asterisk,  \\n where we have seen that until now  \\n it's been like this, hasn't it?  \\n Select star from address,  \\n and that has meant select all fields.  \\n But when we use it with select count,  \\n it can be read really  \\n as select the count of all rows, in a way.  \\n We can limit our count to just one field.  \\n So if we take the address field  \\n which is the street address,  \\n generally it gives the same result and this is 603.  \\n But I don't know if you noticed,  \\n there was a field called address two  \\n which is less populated.  \\n And when we count that field,  \\n we get an answer of 599, which is obviously four short.  \\n The reason for that is that there are four null values.  \\n So we say select from address, where address two is null.  \\n Not an equal sign, but an is.  \\n And let's just select everything there.  \\n And there we have four rows  \\n and you can see the nulls in this column  \\n are written in and italicized.  \\n Null in a database means absence of data.  \\n It doesn't mean zero or blank or a space.  \\n It means nothing here.  \\n There may be times  \\n when you want to exclude null values from your count,  \\n in which case counting a specific field  \\n as we did with count address two.  \\n That's the way to go for you.  \\n But if you just want a straightforward count of the rows,  \\n stick with count star.  \\n Now, what if we try to retrieve a field  \\n and a count at the same time?  \\n Let's try select district comma count star.  \\n And that gives us an error and rightly so.  \\n I don't know if you can read the text  \\n in the warning message,  \\n but it says we're trying to pull both grouped  \\n and ungrouped data at the same time.  \\n Earlier versions of MySQL actually let us do this  \\n but it was very misleading.  \\n So just click that to get rid of it.  \\n So what you would see was the first district  \\n which was Alberta in the database  \\n that listed against the number 603.  \\n And it would look as though  \\n those two results were connected,  \\n but of course they weren't.  \\n It was grossly misleading.  \\n So this is much better, better to get an error.  \\n So let's say we want to see the district  \\n against the number of rows  \\n that district has in the database.  \\n To do this, we need to group our data on district  \\n and do that, we use the group by command  \\n followed by the field name, like so.  \\n And there we have 378 rows.  \\n We can see Alberta pops up twice,  \\n QLD once, California nine times, and so on.  \\n So aggregating functions such as count,  \\n should be used with group by  \\n unless you want to return information about the entire table  \\n as we did to begin with.  \\n So which district has the most rows against it?  \\n We can still use order by in our query.  \\n Let's give an alias to the count there  \\n and say group by district.  \\n Order by ct desc,  \\n descending count.  \\n So we can see that the most common district  \\n is in fact Buenos Aires in the database.  \\n So a little word on that alias here.  \\n Note, we used an alias for count  \\n which was given the as keyword  \\n and then we referred to that further down.  \\n Now the as keyword in MySQL actually is optional,  \\n but in some languages, you need it.  \\n We can still use a where clause to filter our statement  \\n and it would go here.  \\n So select from and where sort of stick together  \\n and then your group by comes next.  \\n So we say where address ID is less than 10,  \\n and now we've returned seven rows  \\n and we can see that Alberta and QLD are the most frequent.  \\n Our SQL statements from now on in  \\n need to follow this basic structure.  \\n SQL is quite forgiving about things  \\n like random spaces or carriage returns  \\n but they do require things to be in sequence.  \\n So we have select fields from table  \\n and any other tables you wanted to join in,  \\n more on which later, and then where and any filters,  \\n so you might have some ands and some ors,  \\n then group by, and then order by.  \\n We can't use the where keyword on our grouped field though,  \\n only on an original field like address ID  \\n which actually occurs in the database.  \\n So let's just try if we'd said where count is less than two,  \\n this query should fail.  \\n It says unknown column ct in the where clause  \\n and that's because the database hasn't,  \\n by the time it gets to this line,  \\n the database hasn't quite assembled all the information yet.  \\n So it doesn't know what ct is when it gets to here.  \\n That doesn't mean that you can't filter on this column.  \\n We just have to use a new keyword called having  \\n and we'll be looking at that next.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3084824\",\"duration\":271,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filter grouped results with HAVING\",\"fileName\":\"2494129_en_US_03_02_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to filter your grouped results with HAVING. WHERE runs before the data are grouped, to filter after grouping you must use HAVING.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7740132,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this section, we're going to look  \\n at how we would filter on an aggregated column,  \\n such as count.  \\n And to do this, we need to use a keyword called HAVING.  \\n You should really read GROUP BY statements in sections.  \\n So we have SELECT, FROM, and WHERE altogether.  \\n Let's say SELECT district count *, ct from address.  \\n And then we'll say WHERE district LIKE.  \\n So the district needs to have a B in it, basically.  \\n So that's one section.  \\n And then we would say GROUP BY the district  \\n HAVING a ct account more than 8.  \\n So we want to see the most frequent districts only.  \\n And then we can keep our ORDER BY statement there as well.  \\n So there we go.  \\n The only district that container B  \\n and have more than eight rows in the sakila database  \\n in the address table are Buenos Aires and West Bengali.  \\n So GROUP BY has its own special filter,  \\n basically called HAVING.  \\n Now, let's leave count behind,  \\n and try some more mathematical aggregate functions.  \\n Let's go to the film table now,  \\n which contains a fair bit of numeric data.  \\n Have a look here, scroll across.  \\n It's got release here, got some ID columns,  \\n and we've got rental duration,  \\n which appears to have different numbers in it,  \\n which is quite helpful.  \\n 'Cause if you take an average of a field  \\n and it's only got the same number in,  \\n it's a pretty boring calculation.  \\n So let's start using MIN rental_duration FROM film.  \\n So the MIN rental duration is 3,  \\n which I'm guessing is in days, and the MAX is 7.  \\n So this should work just great.  \\n We've seen MIN and MAX. So now let's try AVG.  \\n So the average of those values,  \\n which is between 3 and 7, 4.985 there.  \\n Now, I have run an average  \\n or a standard deviation calculation once in Microsoft,  \\n and it rounded this number up, which was quite misleading.  \\n So watch out for that, that could be down to data types  \\n or database settings, or just a peculiarity of the language.  \\n But if you get a suspicious average  \\n or a suspicious standard deviation  \\n that's a whole number or appears rounded,  \\n then do investigate.  \\n We could also run SELECT rating,  \\n and then we could SUM the rental_duration.  \\n I know this is a bit spurious.  \\n Why would you sum a rental duration?  \\n But it's a numeric column. So it will work.  \\n And then GROUP BY the rating field.  \\n So we are grouping and summing.  \\n So we can see that, all in all in our database,  \\n the rating of PG-13 has had the most sort of days rental,  \\n if you like.  \\n Many numeric functions aggregate or group our data.  \\n They work best like that.  \\n If we are using a string or date function,  \\n we typically input a row,  \\n perform a function, and output a row.  \\n So if we've got 603 rows in the table,  \\n our output will also have 603 rows.  \\n So they tend to work on one data point at a time.  \\n Numeric functions tend to work on a whole set of data points  \\n and then return a sort of, a calculation, basically.  \\n Now, we can use numeric functions  \\n in conjunction with string or date functions.  \\n So for example, we could, from the actor table,  \\n do you remember that we can extract the year  \\n from a date column?  \\n And when we do that, that is giving us a numeric result.  \\n So we can then run a numeric function, like so.  \\n And the average is 2006.  \\n Then you might think that's suspiciously rounded,  \\n but actually it's because the last update column  \\n only contains the 15th of February, 2006,  \\n in every single row.  \\n Remember that there are differences  \\n across different types of SQL, unfortunately here.  \\n So for example, in Postgre,  \\n you would need to use EXTRACT instead of YEAR.  \\n So it would be EXTRACT,  \\n and then it would be YEAR FROM, like so.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3086807\",\"duration\":390,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Deduplicate with SELECT DISTINCT\",\"fileName\":\"2494129_en_US_03_03_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Sometimes you only want to see the unique values and not all values. In this video, learn how to use SELECT DISTINCT to return unique values from a field or a unique combination of values across multiple fields.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10986646,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Previously, we were introduced to group by  \\n and used this with count star  \\n to see which values were the most popular in a field.  \\n So, this query returned us 378 rows  \\n with a sort of frequency value against them.  \\n Now we know that there were 603 rows altogether  \\n in the table, so what this tells us is,  \\n that there are 378 unique values,  \\n and sometimes all you need is a list of those unique values.  \\n You don't actually need any counting or clever stuff.  \\n And for that, we can use select distinct district,  \\n and you can see that gives us 378 rows,  \\n just unique values there.  \\n You can use select distinct  \\n on as many fields as you like, actually.  \\n If you wrote select distinct,  \\n and then every field in the table, for example,  \\n then you would simply have a list of all the fields  \\n and all the data in the table.  \\n So let's say select distinct district and address,  \\n and because address is broadly, well, probably unique,  \\n we have now 603 rows returned,  \\n which is the number of rows in the table.  \\n Now let's click onto the rental table.  \\n You can see it's got a lot more rows than previous tables.  \\n It has 16,000 odd rows.  \\n Now let's see among all of those rows,  \\n how many staff we have,  \\n and there are only two.  \\n There's two staff IDs there.  \\n We could look up those staff IDs in the staff table  \\n to find out the names of the staff members,  \\n and more on that in later sections in this course.  \\n But there's just two staff.  \\n Now how many customers are there?  \\n 599.  \\n What if we wanted to know which customers  \\n had been served by which staff?  \\n Then we could combine the fields  \\n after the distinct keyword.  \\n Select distinct staff ID, customer ID,  \\n and we see we have 1,198, which is double 599.  \\n So what this is telling us is,  \\n that every customer has been served  \\n by both members of staff, at some point.  \\n It seems a bit unlikely to me,  \\n but then this is a sample database.  \\n So select distinct is used to remove duplicate values.  \\n Now let's see  \\n if any customer has ever rented the same DVD twice.  \\n Now remember, there are 16,044 rows in the database.  \\n Select distinct  \\n customer ID,  \\n inventory ID this time,  \\n which is a unique reference for the DVD itself,  \\n and that gives us 16,044.  \\n And what that tells us is  \\n that no customer has ever rented the same DVD twice.  \\n We can combine select distinct with functions as well.  \\n So we could, for instance  \\n use concat customer ID inventory ID,  \\n and here we're creating a unique ID  \\n from a combination of two other IDs.  \\n And this really ought to return us much the same  \\n as our previous query.  \\n We're saying select a distinct unique ID list,  \\n but when we run it, it gives us 16,040, rather than 16,044.  \\n So we are four down.  \\n Let's investigate.  \\n Let's look at which four combinations  \\n of customer and inventory were duplicated.  \\n So we'll take out the distinct,  \\n select concat customer ID, inventory ID,  \\n as, and we'll call that conc,  \\n count star as ct from rental,  \\n and we want to group by our unique ID,  \\n and then order by  \\n the count descending, 'cause there's a lot of rows here,  \\n and if we don't do that,  \\n we'll be here all day clicking through them.  \\n So here's our unique ID that we've created,  \\n for example 462706,  \\n and here's our count,  \\n and here are our mystery four rows  \\n that have a count of two, which we weren't expecting,  \\n because select distinct had told us  \\n that the combination of these fields was unique  \\n in every case.  \\n And can you see what's happened?  \\n Is this unique ID 462 matched with 706,  \\n or could it be perhaps 46 matched with 2706?  \\n We haven't separated our IDs out sufficiently  \\n to be sure of the answer to that question.  \\n So we can add an underscore, like that,  \\n and that means we can see both parts  \\n of our unique ID clearly,  \\n and then we get the 16,044 rows that we were expecting.  \\n We can also use date functions with select distinct.  \\n We could say select distinct year, last update,  \\n and there we go.  \\n It's much quicker than checking this manually  \\n or having to download all of those rows  \\n to perform the the check in a different software.  \\n And if you just need a quick count  \\n of unique values in a column, by the way,  \\n you can also combine count with distinct.  \\n So we could say count distinct, let's say district,  \\n bit more interesting than getting an answer of one,  \\n distinct district from address,  \\n which gives us 378.  \\n Coming up, we're going to look at how  \\n to merge rows together using group by.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3086808\",\"duration\":144,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Merge rows with GROUP BY\",\"fileName\":\"2494129_en_US_03_04_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to merge multiple text data points into a single cell using GROUP_CONCAT or similar functions in languages other than MySQL. This can save a lot of time using VLOOKUPs once you have downloaded your data to Excel.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4296830,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] So far, we've looked at  \\n numeric grouping functioned,  \\n but there is one string function  \\n that you can run on grouped data,  \\n and that's called group_concat.  \\n Postgre has string agg.  \\n Microsoft offers this function too now,  \\n as of SQL server 2017, and Oracle has list agg.  \\n So let's take a look at what this one does.  \\n Select district, group concat, phone.  \\n So, perhaps you want to call all of your customers,  \\n one city at a time.  \\n This query would help you to return  \\n all of your phone numbers, grouped by city.  \\n So there we have, where the district is blank,  \\n we have one number,  \\n you see that it's separated with a comma there,  \\n another number and so on.  \\n And there may be more than three numbers,  \\n it may not be showing us everything there.  \\n And some of them just have one value,  \\n and so on and so forth.  \\n And that could be useful if, for instance,  \\n you need to prepend dialing codes, or something like that,  \\n to the numbers that you're ringing.  \\n By default, this returns to phone numbers  \\n in the order they occur in the table.  \\n So 99800, blah, blah, blah,  \\n came before 93435 in the table.  \\n This function can be really useful  \\n if you're trying to create a data array,  \\n for another language, such as JavaScript.  \\n And you can usually change the separator,  \\n so instead of a comma, you could have a pipe,  \\n or a semicolon, or something.  \\n In my SQL you could say,  \\n select district, group concat, phone,  \\n we could say order by phone ascending, separator.  \\n And then because it's a string, we use single quotes,  \\n so we're asking it to separate the numbers  \\n with a semicolon instead of a comma, and order the results.  \\n So it may be rather hard to spot,  \\n but I can tell you now that they're separated by semicolons,  \\n and where previously we began with 998,  \\n we now begin with 282, because we've organized our data.  \\n In Microsoft, PostgreSQL, and Oracle,  \\n you would set the separator, or the limiter,  \\n as an optional second argument within the functions  \\n I told you previously, which were string agg, or list agg.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3087943\",\"duration\":77,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Cost analysis\",\"fileName\":\"2494129_en_US_03_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, Emma challenges you to perform a cost analysis. In doing so you reinforce how to use number functions and aggregations in a query.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2389516,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (energetic music)  \\n - [Instructor] Imagine now that you work  \\n in a central administrative function  \\n for this movie rental empire  \\n that's represented by the Sakila database.  \\n You are an analyst advising the finance  \\n and strategy departments on things  \\n like which movies are becoming obsolete and trends  \\n in which movies are becoming more or less popular,  \\n or trends in which movies are more  \\n or less cost-effective to buy.  \\n And that one is the purpose of this challenge.  \\n You've been asked to analyze your movie data by rating,  \\n such as G or PG,  \\n as we saw before.  \\n You want to know  \\n whether certain ratings have a cheaper replacement cost  \\n than others.  \\n But you are only interested in cheap movies,  \\n those with an average cost of less than 20,  \\n that are relatively short as well,  \\n 90 minutes or less.  \\n And you want to know how many you have  \\n of each rating in this category.  \\n Stop this movie now  \\n if you want to try this query with no further input  \\n and keep going for a few more clues.  \\n In query terms then,  \\n we want to return the rating and row count  \\n from the film table where the length is less than 90  \\n and the average replacement cost is less than 20.  \\n This is quite a complex query  \\n but it should be possible in five minutes.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3080934\",\"duration\":92,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Cost analysis\",\"fileName\":\"2494129_en_US_03_06_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, explore the solution to \\\"Challenge: Cost analysis.\\\" This helps you learn by comparing your process with the one given in an example.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3065356,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Narrator] Let's start with the main part of the query  \\n which includes a group by.  \\n So we'll say select star from film,  \\n change that to select rating  \\n and count of rows from film.  \\n Group by rating.  \\n And there we have the count of rows against each rating,  \\n and you can see that those numbers are about 200  \\n in each case.  \\n Next we're going to add  \\n a filter using the where clause for length  \\n is less than 90.  \\n Now, length was a field in the table.  \\n And this filter applies to every row in that table.  \\n And that is why we use the where clause,  \\n and not the having.  \\n So let's run that.  \\n And you can see the numbers have reduced to about 60.  \\n Now, we have one more filter.  \\n And this uses an average function,  \\n which is an aggregate number function.  \\n And that is a giveaway  \\n that this filter operates on the grouped data,  \\n not at the individual row level.  \\n So we use having for that,  \\n having average replacement cost  \\n less than 20.  \\n And there we have our result.  \\n \\n\\n\"}],\"name\":\"3. Group Your SQL Results\",\"size\":38416763,\"urn\":\"urn:li:learningContentChapter:3087946\"},{\"duration\":2516,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3084825\",\"duration\":107,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Make your queries faster\",\"fileName\":\"2494129_en_US_04_01_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to test a query or check the data types, save time, and help your colleagues by limiting the size of your report set.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4876143,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] If we click on the payment table,  \\n you can see that there are 16,000 odd results.  \\n And this query took 0.0009 seconds.  \\n Now we're going to limit our query to 25 rows,  \\n and do that by limit 25 and click go.  \\n Now you can see that the query took a little bit less time,  \\n 0.007 seconds to run.  \\n But when we're dealing with millions of rows,  \\n this sort of thing can make a substantial difference.  \\n The limit keyword works in MySQL and PostgreSQL,  \\n but unfortunately the keywords differ in different versions.  \\n So for Microsoft, the keyword is top,  \\n and you use it like this.  \\n You'd say select top 25, field name,  \\n whatever that field name may be from the payment table.  \\n And obviously you don't need that bit.  \\n And for Oracle, it would be select, I'll show you.  \\n Star from the table.  \\n And then it's used as a filter  \\n where ROWNUM is less than or equal to 25.  \\n And ROWNUM isn't a field, it's Oracle's own way  \\n of counting the rows in the database.  \\n It's like an automatically assigned ID.  \\n Now these queries are going to return rows  \\n in the same order they appear in the table.  \\n We're going to move on to look  \\n at joining tables and merging data.  \\n And when we do that, even in a sample database  \\n like this one, we can quickly produce millions of rows.  \\n So your computer and your colleagues will both thank you  \\n for using this limit keyword,  \\n if you're using MySQL to limit the size  \\n of the reports you produce  \\n while you're testing out your queries.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3088188\",\"duration\":436,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand SQL indexes and keys\",\"fileName\":\"2494129_en_US_04_02_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Some fields are special; they have an index on them. In this video, Emma explains primary and foreign keys, indexes, and unique constraints. Looking at these can help you make sense of an unfamiliar database and find which fields do what.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12602220,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In the up and coming sections,  \\n we want to join data together for more than one table.  \\n And for this, we need to understand a bit more  \\n about data types, keys, and indexes.  \\n Data type is whether a field is numeric or date or text,  \\n but as we've seen, there are far more categories than that.  \\n Keys and indexes are special kinds of field,  \\n and constraints are like rules  \\n that the database must enforce.  \\n So let's look at the customer table,  \\n and you can see we're on the Browse tab,  \\n and we can see all of the fields laid out horizontally,  \\n and the data beneath them.  \\n And then if we go to the Structure tab,  \\n we can see the same fields, but laid out vertically,  \\n and without the data showing underneath them.  \\n We have three ID columns,  \\n customer_id, store_id, and address_id.  \\n And these are all int type or integer type data,  \\n so these are all numeric.  \\n And they are, smallint, tinyint, and another smallint.  \\n And the number in the brackets here,  \\n indicates the number of characters,  \\n the length that that ID can be at maximum.  \\n The difference between smallint and tinyint then  \\n is simply the number of characters  \\n that that value can have.  \\n Active is a tinyint with a maximum length of one.  \\n And in fact, it's probably just a binary field  \\n that contains a zero or a one  \\n to indicate the status of the customer.  \\n Then we have first_name, last_name, and email,  \\n which are all varchar or variable character columns.  \\n Again, with a character length in brackets after them.  \\n And then we have two date type fields.  \\n We have create_date, which is a datetime  \\n and last_update, which is a timestamp.  \\n Datetime is for storing date and time together.  \\n Timestamp does the same thing,  \\n but in a less human-readable form.  \\n And timestamp is often automatically generated,  \\n in fact by the database, and that's the case here.  \\n In very small font down here,  \\n it says, on update Current_Timestamp.  \\n So if anybody comes along and modifies,  \\n one of the rows in this table,  \\n the database would automatically log  \\n the time and date that they did that.  \\n Some data types are better than others  \\n for joining tables together.  \\n Almost always, we will join tables together  \\n using an integer or whole number field.  \\n You see that as we've said  \\n all the ID fields here are integer types,  \\n and it is the ID fields that we will use.  \\n You see that with the customer table  \\n we have a gold key against customer_id.  \\n And if we hover, it says Primary, this is a primary key.  \\n It tells the database that this field  \\n uniquely identifies the row for this table.  \\n We typically use primary keys to join tables together.  \\n Can you see if we scroll to the right,  \\n it says auto_increment.  \\n So if we were to insert a new row into this table,  \\n the database would automatically populate this field  \\n with a number one higher than the current maximum value.  \\n And this preserves data integrity,  \\n which is why primary keys are so good  \\n for joining tables together on.  \\n Then we have store_id and address_id.  \\n These are the primary keys  \\n of the store table and the address table respectively,  \\n but they're showing up in the customer table,  \\n and when this happens, it's called a foreign key.  \\n Just deselect those.  \\n If we scroll down now we see there is a box called Indexes,  \\n and it has four rows in it.  \\n The first one is primary, for our primary key.  \\n Then we have two indexes  \\n with fk for foreign key in the name,  \\n and then we have another index idx meaning index,  \\n which is on last_name,  \\n and if we scroll back up, you can see indeed  \\n last_name does have an index applied to it.  \\n An index tells the database to optimize this field  \\n for searching.  \\n So search is performed on an indexed field  \\n will be quicker than those performed on a regular field.  \\n But creating indexes takes memory,  \\n so we don't want to make every field an index.  \\n In MySQL, when you make a field into a primary key  \\n the database automatically creates an index for you,  \\n but that's not true of all types of SQL.  \\n So why do we have indexes on these three fields then?  \\n Two of them are IDs,  \\n and when we join two tables together, as I've said,  \\n we tend to join them on IDs.  \\n So the database then has to search through one table,  \\n finding a match for those IDs in the other table.  \\n It's like a giant VLookup function.  \\n So it's pretty smart  \\n that the primary and foreign keys would be indexed.  \\n And for last_name, it's probable that searches  \\n are being performed manually by staff  \\n on the last name of customers,  \\n so that's why we would index that field.  \\n Now, if we click on Relation View,  \\n we can see a bit more  \\n The constraint names have fk at the front there,  \\n which stands for foreign key.  \\n And this is just convention as the index box was as well.  \\n It's a very useful convention,  \\n but you could call them what you like.  \\n So on the left is the name,  \\n and the behaviors that should be triggered  \\n when a relevant row is deleted or updated.  \\n So when deleting the master data,  \\n data in the foreign key data in other tables  \\n ought to be restricted.  \\n And when the master data is updated,  \\n those changes should cascade throughout the database  \\n to different tables.  \\n If we tried to update the store_id in the customer table  \\n to an ID that doesn't exist in the store table,  \\n the database would throw an error.  \\n So it's crosschecking for us to preserve data integrity.  \\n This section on the right,  \\n links the foreign keys in the customer table  \\n to the fields in their original table  \\n containing the master data.  \\n So this says the address_id field from the customer table  \\n should match to the address_id field in the address table.  \\n And similarly, the second row,  \\n the store_id field in the customer table  \\n should match data from the store_id field  \\n in the store table.  \\n There are other types of constraint  \\n that you can place on your data.  \\n Commonly now, if we were to create this database  \\n we would expect email to be a unique value.  \\n And if you wanted to create a unique constraint  \\n on the email column,  \\n you could highlight, check the box next to email,  \\n and then click on Unique.  \\n That would work, if all of the values are already unique.  \\n You might come across some problems,  \\n if it turns out that they're not.  \\n It's likely that they wouldn't be unique,  \\n in fact, because it says that  \\n the default value of the email field is null,  \\n which would be very unusual these days.  \\n It's normally a mandatory field now, it's not optional.  \\n Coming up, we're going to look at  \\n how to visualize your database graphically,  \\n and then we'll move on to joining tables.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3082838\",\"duration\":478,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"EDIT: Visualize your database\",\"fileName\":\"2494129_en_US_04_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"If you want to make sense of your database, draw a diagram of all the tables and key fields. In this video, Emma shows you one approach to doing this.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15066298,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now that we understand a bit more  \\n about keys, indexes, and constraints,  \\n we're going to move on to see how our tables link together.  \\n We're going to look at a diagram that shows  \\n all of the tables and all of the links.  \\n And we're going to do that using a common database diagram.  \\n Here we are representing the rental table as a box,  \\n and in the shaded area at the top  \\n you can see the name of the table, it's rental,  \\n and then within the paler region underneath,  \\n we have rental_id, which is the primary key,  \\n P for primary, and three foreign keys  \\n listed with an F for foreign.  \\n If we go and have a look at the rental table  \\n in the structure tab, we can see we have a rental_id,  \\n which is a primary key, inventory_id,  \\n customer_id, and staff_id, so three foreign keys,  \\n so that's where I've got that information  \\n from to draw that box.  \\n I'm not listing any other indexes,  \\n such as rental date, in my diagram,  \\n because they are indexes and unique constraints  \\n are kind of internal to their table,  \\n they don't tell us anything  \\n about how the table's linked together.  \\n Now we know that the rental table  \\n links to the inventory table,  \\n so let's take a look at that one next.  \\n The inventory table has two foreign keys, film and store.  \\n So let's show that one next.  \\n Now we want to show how the rental table  \\n and the inventory table link together,  \\n and I've done that with a line here  \\n that points to the specific fields in each table  \\n to which it relates.  \\n So it's not just linking the boxes together,  \\n it's linking specific fields.  \\n And it's easy to spot which,  \\n 'cause they're both called inventory_id.  \\n The inventory_id field in the rental table  \\n is a foreign key, of course, and a primary key  \\n of the inventory table.  \\n Now we can see the rental table also has connections  \\n to the customer and staff fields,  \\n so let's take a quick look at those.  \\n Customer, which has two  \\n foreign keys and a primary key,  \\n and that's store and address,  \\n and then the staff table also has  \\n associated store and address data.  \\n So we can add the customer table and its connector,  \\n the customer and rental tables connect on customer_id.  \\n Next, we can look at the staff table  \\n and they connect to the rental table on the staff_id.  \\n We can see that both the customer  \\n and staff tables contain address_id,  \\n which is a foreign key from the table address.  \\n So both customers and staff  \\n have their addresses stored in the same table.  \\n We can also see that both customer and staff tables  \\n have an associated store.  \\n And this tells us something about the business model  \\n of this DVD rental company, actually,  \\n because a customer is linked to a single store,  \\n it may just be a primary store,  \\n or it may be that customers can only borrow  \\n films from one store.  \\n I've had gym memberships like that.  \\n So a well designed database can imply certain things  \\n about the organization it's for.  \\n Next up we can see the address related table of city.  \\n The address table is going to be updated fairly frequently,  \\n every time a new customer is set up,  \\n but it relies upon a city_id field,  \\n which links to the city table,  \\n and the city table relies on a country_id field,  \\n which relates to the country tables.  \\n So the city and country tables effectively contain lists.  \\n And if we take a quick look at those,  \\n there's the city table, lots of different cities,  \\n and then corresponding country_id's.  \\n And the country table has 109 countries in it there.  \\n The city and country lists probably don't change very often.  \\n So why bother having these two tables of city and country?  \\n Why not just stick the city and the country_ids  \\n into the address table?  \\n Basically it's down to data integrity,  \\n the way this would show this data, the city data  \\n would show up in the front end is probably in a dropdown.  \\n So staff, when they're entering a customer_id,  \\n would select the city and the country  \\n from two different dropdowns,  \\n meaning that they haven't got to type  \\n the values in and potentially get them wrong,  \\n and it's also quicker for them.  \\n The city and country table  \\n almost belong to the address table.  \\n They don't really have any other purpose,  \\n they're like supporting reference tables.  \\n Notice that the store table also has an address stored  \\n in the address table, so let's add that link in.  \\n And notice too that we have a field called manager_staff_id  \\n in the store table, which links to the field  \\n called staff_id in the staff table.  \\n Now how do I know that given that they have different names?  \\n How do I know that those two are linked?  \\n If we go to the store table,  \\n and look at the structure tab,  \\n and go to relation view, we can see here  \\n we have a column manager_staff_id,  \\n and if I scroll over to the right,  \\n this is telling us that it relates to master data saved  \\n in the staff_id column of the staff table.  \\n So that's how I knew that there was genuinely a link,  \\n even though those two fields have different names.  \\n Next up we have the payment table.  \\n Between them, the rental payment and inventory tables  \\n are like the prime movers of this database.  \\n These are the ones that update frequently,  \\n every time a movie is borrowed.  \\n Now all of our remaining tables concern films,  \\n they come in three mini groups.  \\n We have the film table itself,  \\n this is supported by the language table.  \\n Language is a reference table containing a pretty  \\n static list of languages,  \\n such as English, French, Japanese.  \\n Film uses this list to identify what language  \\n the film is in, but it also uses language_id  \\n to drive another field called original_language_id.  \\n Presumably if a French film is dubbed to be in English,  \\n the language would be English  \\n and the original language would be French.  \\n So that was the first of our three mini groups,  \\n the second one is film category,  \\n which is dependent on category.  \\n And these categories are things like  \\n sci-fi, horror, romance and comedy.  \\n And the final mini section we have there  \\n is film_actor, and actor.  \\n Now the film table contains one row per film,  \\n and you might wonder why film category  \\n and film actor are needed,  \\n because couldn't this information  \\n go straight into the film table and save us a bit of bother?  \\n I suspect that this was done to solve  \\n a common problem in database design,  \\n what I think of as the one to many problem.  \\n There will be multiple actors in each film,  \\n and there are probably multiple categories too.  \\n So you could have a romance,  \\n which is also a comedy, for example.  \\n To save this information in the film table,  \\n we'd have to have multiple values in one cell,  \\n or we'd have to repeat the film listing  \\n once for each actor or category,  \\n that would be quite poor database design.  \\n So hopefully by now you can see  \\n that not all tables are created equal,  \\n logically they are, but they are used  \\n for really different purposes.  \\n There are reference tables, which are rarely updated,  \\n and basically a glorified list.  \\n These are used by other tables  \\n to help maintain data integrity.  \\n There are semi-static tables,  \\n which update for a new customer, new staff member, or store.  \\n And the remaining tables updates every time  \\n a rental is made or a new film is brought out,  \\n so they are updated much more frequently.  \\n I have created this database diagram  \\n just by following the trail of foreign keys,  \\n and you can do the same with your database.  \\n It's the fastest way to get well acquainted  \\n with the data that's in it.  \\n Now we're going to move on to joining tables together.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3086809\",\"duration\":302,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Merge data across tables with JOIN\",\"fileName\":\"2494129_en_US_04_04_2817027_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to JOIN different tables using aliases to make your query more readable. JOIN returns all the fields side by side.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11771414,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Sometimes you need data  \\n from more than one table.  \\n What if we want a report  \\n that shows all customers and their addresses,  \\n where the district falls in Buenos Aires.  \\n We can't do this from the customer table  \\n because if we have a look at all the fields in here,  \\n there's no address data.  \\n All we've got is an ID of an address that exists  \\n in the address table.  \\n To run this query,  \\n we have to merge or join data from two tables.  \\n The basic structure of a join is straightforward.  \\n Select star from customer,  \\n Join the address table on customer.address ID  \\n equals address, address ID, and hit go.  \\n Now, if we look down here,  \\n these fields should look familiar,  \\n customer, blah, blah, blah, email, address ID,  \\n and active create date and last update.  \\n So all of those fields are our customer table.  \\n And then from address ID,  \\n we start with our address fields  \\n that also go over to last update.  \\n And if we compare the address ID field  \\n from the customer table  \\n and the address ID field from the address table,  \\n we see that they are always identical  \\n and they have to be because that's what we joined on.  \\n We told the database to match these two fields.  \\n So we need to read this query in two parts,  \\n select the fields, and then from table one,  \\n join table two on.  \\n The from applies to the whole thing that follows.  \\n Now notice that we haven't just typed address ID here  \\n in our on field.  \\n We've put customer.address ID and address.address ID.  \\n And what we're doing here is supplying the table name  \\n before the column name.  \\n So we're saying join these two tables on the address ID  \\n in the customer table equaling the address ID  \\n in the address table.  \\n We couldn't leave those table names out  \\n because the column name is ambiguous.  \\n MySQL would reach the first address ID,  \\n look it up, find two occurrences, and throw an error.  \\n So we repeat the table name with a dot or a point.  \\n We could use table aliases here  \\n to make this a bit neater and easier.  \\n And then our on statement, similarly,  \\n can just have C and A and that gives us the same results.  \\n In MySQL, you don't need to have the as keyword  \\n when declaring an alias.  \\n You can have it, it's optional.  \\n So let's look at the theory, the principles,  \\n of what we are doing.  \\n Here we have two tables, the address table  \\n and the customer table, and I've put some fake ID in.  \\n So the address table has six rows with IDs one to six  \\n and the customer table also has an address ID column  \\n and it has five rows.  \\n When we make a join using our SQL,  \\n we are returning every row that has a record in both tables.  \\n This is a specific kind of join called an inner join.  \\n There are other joins  \\n and we will be looking at that in forthcoming sections.  \\n So when we join these two tables together,  \\n one of six rows and one of five,  \\n how many rows do you think we're going to get?  \\n We just get four  \\n because there are only four records that exist  \\n in both tables.  \\n In the customer table,  \\n we have an address ID of 18  \\n which doesn't have a match in the address ID table,  \\n so that one's been left out.  \\n And in the address table, we have an ID of four and five  \\n which don't have corresponding items in the customer table.  \\n So those ones have been left out.  \\n And that's how we find the four.  \\n This principle of only returning records  \\n that exist in both tables can also be shown  \\n with a Venn diagram.  \\n Only the dark shaded area in the middle  \\n is going to be returned by this query.  \\n More Venn diagrams are going to pop up  \\n in coming sections with different kinds of join.  \\n Now, going back to our query,  \\n we only wanted customers with an address of Buenos Aires.  \\n So we need to filter with where.  \\n But where does where go?  \\n Join and on are all part of the from section of our query  \\n and where comes after from.  \\n So it goes down here.  \\n So we could say where district equals Buenos Aires.  \\n And there we have our 10 results that we would expect.  \\n If we wanted to just return particular fields,  \\n we could use those table aliases again.  \\n Sometimes it's quicker to include the table aliases  \\n than to try and figure out which ones are ambiguous  \\n and which ones aren't.  \\n So there's our report.  \\n There's our customer name and their address.  \\n Next up, more on different types of join.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3084826\",\"duration\":256,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand SQL JOINS\",\"fileName\":\"2494129_en_US_04_05_2817027_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to merge data across different tables using JOIN. Inner joins show results that exist in both tables, while outer joins show results from one table with or without a corresponding record in the other. Emma explains what some abbreviations mean in SQL, such as JOIN meaning INNER JOIN.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7966912,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now we're going to look at different kinds  \\n of join: inner join, outer join, left join, and right join,  \\n using some fake data.  \\n We've already seen that an inner join  \\n and a join means the same thing.  \\n It's where rows are returned where they exist  \\n in both tables and it's like the middle of a Venn diagram.  \\n Now, here we have table A and table B.  \\n Table A contains days of the week  \\n with five rows named Monday to Friday.  \\n And table B contains four people's names.  \\n Let's say on each day one person  \\n is required to work at a shop.  \\n You can see the query in the arrow.  \\n How many rows do you think an inner join  \\n is going to give us from this?  \\n It gives us four rows.  \\n Friday has disappeared because there's no person ID  \\n of seven in table B.  \\n And we don't see Fatima in the results  \\n because there's no person ID of four in table A.  \\n But Fred is down for two days' work.  \\n So while only IDs one, two, and three occur in both tables,  \\n we output four rows.  \\n So that's an inner join. Now for an outer join.  \\n An outer join returns all rows from table A and B  \\n matched together where possible.  \\n It's probably the least useful of the joins  \\n and it's not supported in MySQL.  \\n Outer joins are also called full joins and full outer joins.  \\n So how many rows are we going to get this time?  \\n We've got the same two tables that we had before.  \\n The only difference is this keyword that we've changed.  \\n This time we get six rows, all the matches  \\n that we had before plus Friday and Fatima  \\n shown against null values.  \\n A left join is the same as a left outer join.  \\n Here we see table A is on the left.  \\n If we look at the query in the arrow,  \\n you can see table A is listed first  \\n and then we have the keywords left join  \\n and then we have table B.  \\n When you format your query in this way  \\n with the from and the join all on one line,  \\n it becomes easier to see what's what.  \\n A left join will give us everything from the table  \\n to the left of the join keyword.  \\n Here, that is table A.  \\n So how many records will we get?  \\n Table A has five rows.  \\n We output five rows.  \\n The database takes all the rows from table A  \\n whether or not there are corresponding records in table B.  \\n A right join or right outer join does the same  \\n as a left join but from a different table.  \\n Just a quick note here.  \\n If you think about the Venn diagram, you can see  \\n that a right inner join isn't a thing.  \\n It wouldn't make sense.  \\n The inner part of the Venn diagram  \\n is neither right nor left.  \\n I wouldn't generally use a right join.  \\n I would rearrange my table names and use left joins  \\n because otherwise it can get really confusing  \\n when you're joining more than two tables.  \\n Anyway, they do exist.  \\n So to cover them briefly, the right join  \\n will return everything from table B  \\n because table B is to the right of the keyword join.  \\n So how many rows this time?  \\n Again, we have five rows even though table B  \\n has just four rows.  \\n That's because poor old Fred is down to work two days.  \\n And we see Fatima this time even though  \\n she has no corresponding record in table A.  \\n We finish with a brief note on a useful technique.  \\n It's not another kind of join but it can be useful  \\n to show all records where there isn't a match.  \\n Generally, I would use this kind of query to spot errors.  \\n So I would hope to return a zero result set  \\n with this kind of query.  \\n What we're saying is select the day and the person ID  \\n from our tables joined as they are joined normally  \\n where the person ID is null.  \\n We would hope with data integrity being good  \\n that you wouldn't have any null values  \\n and therefore that you get a zero result set.  \\n But we have one row returned here, which is Friday  \\n because there's one case in which the person ID  \\n is missing from table B.  \\n Of course, you could do this as a right join as well.  \\n You'd have to change the where clause slightly  \\n and you'd get Fatima.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3086810\",\"duration\":289,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Combine rows with SQL UNION\",\"fileName\":\"2494129_en_US_04_06_2817027_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"UNION returns all the rows from different tables, one set under another. It can be very misleading to use UNION or UNION ALL. In this video, Emma explains some common pitfalls.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10944705,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In previous sections,  \\n we've learned how to merge tables with a join.  \\n Joins allow us to merge columns from different tables.  \\n It's like a sort of horizontal merge.  \\n So if we start off with two tables of three columns  \\n and six rows, as we show in the diagram  \\n and we use a join,  \\n we would expect to output  \\n one table of six columns and six rows,  \\n assuming that there was a perfect match  \\n in each table for records in the other table.  \\n Unions are also a way of merging tables,  \\n but they work vertically instead of horizontally,  \\n and they don't care about matching records.  \\n So in this case where we have the same two input tables  \\n which are three columns and six rows,  \\n a union would output three columns, but 12 rows.  \\n So let's have a look at this query.  \\n Here we have a query that says,  \\n select the text address  \\n which is there in single quotes,  \\n as column name table.  \\n You can see the column name here, table.  \\n And then the date, extract the date  \\n from the last update column from address.  \\n And we can see that's returned us 603 rows,  \\n and they all appear to be quite similar.  \\n The text string address,  \\n and then the date, 15th of February, 2006.  \\n Now let's just change that.  \\n We're selecting the actor as the tbl.  \\n And we've got the same field,  \\n last update exists in the actor table.  \\n We click go there.  \\n So now the table is actor,  \\n and the date appears to be the same, 15th of February, 2006,  \\n and that's returned us 200 rows.  \\n Now, if we wanted to join these two together,  \\n we simply copy, paste and use union.  \\n We're going to start off using something called union all  \\n and I'll show you why in a minute.  \\n So, how many columns are we going to get?  \\n We're just going to get two when we do our union  \\n because this is a vertical merge.  \\n So what we've got now is 803 rows.  \\n The first 200 rows are from the actor table.  \\n Let's see if we can find the address ones.  \\n If we go to page nine,  \\n then we start to get the address results,  \\n also with the date next to them.  \\n So union all is literally showing you 200 results from one  \\n and then 603 results from the other.  \\n Now, if we change this to union and run that,  \\n we get just two rows back when we do that  \\n and that's because the union keyword  \\n effectively runs a select distinct by default.  \\n And you might remember that that means  \\n it only returns us unique values.  \\n So what this is telling us is that the last update column  \\n only contains the 15th of February, 2006 in either table.  \\n Now, the query that we've run  \\n is pulling a very similar field, two very similar fields,  \\n into the same field heading,  \\n but you can in fact pull completely different data  \\n into the same column and it can be really misleading.  \\n So here, let's say select address as tbl  \\n and instead of pulling the date,  \\n let's pull in the city ID from address  \\n and that's still worked.  \\n Let's just change that to union all  \\n so that we should get 803 rows.  \\n There we go.  \\n So we have the actor table with the date  \\n and then if we go to page nine,  \\n then we've got the address is showing  \\n this information is coming from the address table.  \\n And now instead of a date,  \\n even though it's in a column called date last update,  \\n we now have a set of city IDs, which obviously are numeric.  \\n This can be really risky.  \\n You need to take a lot of care with union and union all.  \\n It's absolutely fine, in fact, often necessary  \\n to combine two columns of different origins into one column.  \\n But I find it's best to include an alias column  \\n to help describe them and stop you making errors.  \\n You can still filter statements within union all  \\n because essentially,  \\n you have two completely separate SQL queries here  \\n with a keyword in between them.  \\n So if you wanted to place a filter on the actor table,  \\n you would do it here.  \\n And if you wanted a filter on the address table,  \\n you would do it here, like so.  \\n And you can see,  \\n that's returned us just eight rows there.  \\n Coming up, we're going to look at  \\n how to merge tables using the in keyword.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3085864\",\"duration\":131,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Merge data across tables with IN\",\"fileName\":\"2494129_en_US_04_07_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"There are alternatives to using JOIN and one of them is IN. In this video, learn how to use the IN keyword to filter one table based on the results of a previous query.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3964095,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] I want to show you another way  \\n of using data from two tables.  \\n It's not a join. It's not a union.  \\n And it's a bit of a hack, to be honest.  \\n I wouldn't generally use it.  \\n But if you are looking at queries  \\n that other people have written,  \\n you may have come across this.  \\n So you need to know that it exists.  \\n So we're looking at the customer table  \\n where the first name is Mary,  \\n and you can see we've got MARY SMITH down here,  \\n and she has a customer ID of 1.  \\n And what we're going to do  \\n Now, let's say, we just returned the customer ID of Mary,  \\n which as we know is 1.  \\n And then what we're going to say  \\n is SELECT everything FROM the rental table.  \\n So we want to see all the transactions that have happened,  \\n WHERE the customer_id is IN.  \\n We've seen the IN keyword before.  \\n But last time, we supplied a set  \\n of text items in the bracket.  \\n I think it was Penelope Nick and the Ed  \\n that we had inside the IN before.  \\n So previously, we used IN  \\n to mean matches any of the following.  \\n And it still means the same thing here.  \\n Select everything from the rental table,  \\n where the customer ID matches any of the customer IDs  \\n returned by this query.  \\n Now, as it happens,  \\n we know there's only one customer returned by this query,  \\n but it could return 58, that would be fine.  \\n So here we have 32 records.  \\n So Mary has clearly a film fanatic  \\n and has rented lots of DVDs in her time.  \\n You could still add filters in here.  \\n You could add further filters onto our rental query.  \\n So say, WHERE field_name =, whatever, 5,  \\n I'm making this up, obviously.  \\n AND, so there we've got two queries placed  \\n onto the rental table.  \\n And you could place small queries,  \\n of course, on the customer table.  \\n What you can't do with this method  \\n is display results from multiple tables.  \\n We are using one query just to filter the other table,  \\n and we will be using this  \\n in the very last chapter of this course.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3084827\",\"duration\":176,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Select from a selection with subqueries\",\"fileName\":\"2494129_en_US_04_08_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Using subqueries is generally not preferable to using JOINS. In this video, learn how to use JOINS to filter results based on a query that returns a calculated value, particularly where that value is run on a different table.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4808385,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We've already seen a nested query  \\n or a subquery where we use a SELECT statement  \\n to place a filter on a broader SELECT statement.  \\n And in this section we're going to do something  \\n slightly more complex where we use a SELECT statement  \\n to return results to another SELECT statement.  \\n So here we have a familiar query,  \\n SELECT first_name FROM actor  \\n and we're going to call that result set f  \\n and then we say SELECT F.first_name FROM, like that.  \\n And it's returned us the same first names  \\n that we had before, as you would expect.  \\n So on its own, this query isn't useful  \\n but the syntax is useful.  \\n And if you come across a query like this  \\n you should always try to make sense of it  \\n from the inside out.  \\n So let's start with the middle part of the query,  \\n the bit in brackets,  \\n SELECT this column first_name FROM actor  \\n and call that result set f.  \\n And then from the result set f, return me the first name.  \\n If we didn't use an alias here,  \\n this result would fall over.  \\n Even though this query looks as though it ought to work.  \\n There we go.  \\n Every derived table must have its own alias.  \\n The field first_name that we are trying to ultimately select  \\n would appear to be ambiguous to the database in this query.  \\n So alias or no go.  \\n Obviously you could call the alias anything.  \\n You don't need to call it f.  \\n Now what happens if we gave the field name in the subquery  \\n an alias as well?  \\n So SELECT first_name as fn for first name,  \\n and again you can call that anything.  \\n So now we're using two different kinds of alias.  \\n We're using f to refer to the results set  \\n or in MySQL terminology, a derived table,  \\n and we're using fn to return, to refer, sorry,  \\n to the field name, first_name.  \\n If we run this, it throws an error  \\n and that's because f.first_name is no longer recognized.  \\n That's 'cause we've renamed it, haven't we.  \\n Our sub query returns a single column  \\n and it's now called fn.  \\n So the outer query is looking for first_name  \\n and not finding it.  \\n If we wanted to return the results set  \\n we would say f.fn.  \\n So the column fn from the results set called f.  \\n And there we go. We can see that's worked.  \\n Now in practice, of course,  \\n nobody would use this sort of query  \\n because the same result can be achieved much more neatly  \\n and efficiently with the first statement.  \\n The main time you would want to use this  \\n is when you want to return different calculations  \\n into different fields.  \\n Particularly where the calculations  \\n are based on different tables.  \\n And we see more on this in later sections.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3085865\",\"duration\":64,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Join tables\",\"fileName\":\"2494129_en_US_04_09_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, Emma challenges you to join tables. This reinforces how joining works in a query.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2073612,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Conceptually, this challenge is very simple.  \\n You want to return all data from the film,  \\n film_category, and category tables, filtering for comedies.  \\n Last time we filtered for a film type in a challenge,  \\n we used the like keyword and the description field  \\n from the film table to filter for drama types.  \\n But that was a bit of a hack, really.  \\n The category table is what stores actual film type.  \\n You can see we have action, animation,  \\n and then number five there is comedy,  \\n and this is what we're going to filter on.  \\n So this challenge is all about joining tables together  \\n in order to perform a proper filter.  \\n In query terms, we want to return all records  \\n from the film, film_category and category tables,  \\n filtering on category.name = \\\"comedy\\\".  \\n You should be able to complete this in five minutes.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3086811\",\"duration\":277,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Join tables\",\"fileName\":\"2494129_en_US_04_10_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, explore the solution to \\\"Challenge: Join tables.\\\" This helps you solidify your understanding of the concept by comparing your answers to an example result.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9988109,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] We can start by joining  \\n just two tables together.  \\n This could be film and film_category  \\n or film_category and category  \\n because there isn't a shared field directly  \\n between the film and category tables.  \\n So whichever way you do it is fine  \\n but I'm going to join SELECT *  \\n FROM film f  \\n JOIN film_category  \\n ON film_category fc.  \\n I'll give them aliases to make life easier later on.  \\n F.film_id equals fc.  \\n So we're telling the database  \\n how to knit together the records that we're finding.  \\n Now, notice here that there are 1,000 rows returned.  \\n And notice also that we have this message up here,  \\n which doesn't grab the attention  \\n 'cause it's not in red.  \\n But it does become very significant later.  \\n The current selection does not contain a unique column.  \\n So we've got two film_id columns returned, for example.  \\n And they are not prefaced in the results table  \\n with f and fc.  \\n So the database knows  \\n that it might get a bit confused later on  \\n and indeed, it will.  \\n So for now, let's carry on building our query.  \\n We're going to join the next table.  \\n I'm going to say JOIN the category table, c,  \\n ON c.category_id  \\n equals fc.category_id.  \\n And let's return that.  \\n And once again, we have 1,000 rows  \\n and once again, we have this warning up here.  \\n Now we're going to try and filter our results.  \\n So we're going to say WHERE c.name  \\n is equal to comedy.  \\n And it appears to have worked  \\n but we still have 1,000 rows.  \\n And that should ring some alarm bells.  \\n So I'm just going to copy that.  \\n I want to have a quick look in the category table.  \\n So it's unlikely that all 1,000 films  \\n are down as comedies  \\n because you can see, there's 16 different categories  \\n and if we look in the film_category table,  \\n you can see it's about maybe 1 in 10  \\n or even fewer that are actually comedies.  \\n So we've got a problem with our query.  \\n Let's run that again.  \\n It shouldn't be returning us 1,000 rows.  \\n And the problem with the query  \\n is what the database has flagged up here.  \\n It's getting confused  \\n because there isn't a unique column.  \\n So one way around this,  \\n instead of returning all of the rows,  \\n if we return the description field,  \\n which only exists in one table,  \\n and then return the category name.  \\n Let's take this filter off for a second.  \\n We can see we have a lot of actions to begin with,  \\n and I'm not going to scroll through all 1,000  \\n but we can see we've got 1,000 in there now.  \\n And then we're going to add that filter back in  \\n WHERE c.name is equal to comedy.  \\n And this time, we have 58 rows returned,  \\n which is rather better.  \\n So this wasn't meant to be a trick query.  \\n In fact, I only noticed the issue myself  \\n as I came to do the solution.  \\n But it is a really good learning point  \\n to look at the messages that you are given  \\n and to understand the significance for the database  \\n on having a unique column.  \\n And it's still telling us well,  \\n I don't have a unique column,  \\n so I'm upset about that.  \\n So I'm going to give it one  \\n just to make the message go away.  \\n And we'll say concat the film_id, underscore.  \\n \\n\\n\"}],\"name\":\"4. Merge Data from Multiple Tables\",\"size\":84061893,\"urn\":\"urn:li:learningContentChapter:3082839\"},{\"duration\":1607,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3086812\",\"duration\":508,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Save your queries using a VIEW\",\"fileName\":\"2494129_en_US_05_01_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn what a view is and how to create one. A view is a way to store a query and you can query a view in the same way you would query a table. Emma works through a complex stored view from the Sakila database.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15817930,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this section,  \\n we're going to look at views.  \\n The word view has a really specific meaning  \\n in the world of databases.  \\n A view is a stored query.  \\n If you look at the breadcrumb bar,  \\n we are at the database level in our hierarchy.  \\n And if you come to the navigation on the left  \\n and we close the table's hierarchy, you can see  \\n we have functions, procedures, and views as well as tables.  \\n And I should say, in this chapter where we look at  \\n these three things as well as variables,  \\n what follows relates just to MySQL.  \\n Views, functions, procedures and variables are dealt  \\n with quite differently on the different types of SQL,  \\n but the principles are the same.  \\n So, for MySQL,  \\n we're going to expand the hierarchy for views,  \\n and you can see, we have seven views stored already for us.  \\n You can create and save your own view,  \\n but we're going to look at the stored views  \\n that SeQuiLa offers us.  \\n So, to run a view, you can click on it.  \\n I've just clicked on actor underscore info,  \\n and it returns us some data.  \\n If we scroll down, we can see  \\n we have tabular data returned to us.  \\n It looks very much like a table.  \\n We have actor ID, first name, last name, and then film info.  \\n So, it looks as though we might be looking at a table,  \\n but don't be fooled.  \\n With tables data is stored in fields and rows,  \\n but with a view it is just the query that is stored,  \\n and it runs when you click on it.  \\n In this case, it runs to the actor table and the film table  \\n by the look of things, gathers data from both,  \\n joins them together and returns them to us in a table,  \\n and we're going to look at how it does that.  \\n So, let's take a look inside the query and to do that,  \\n we can say, show, create view,  \\n click go.  \\n It may be that you can only see a little bit of this query,  \\n which is in the create view column.  \\n If that's the case, click options,  \\n and you've probably got partial texts,  \\n which is the default selected.  \\n So, click full texts and then go,  \\n and you'll be able to see all of the query in one go,  \\n and we're most interested in the create,  \\n and then, from the word view, down.  \\n So, what I've done is copy paste that into Atom,  \\n a text editor, to make it a bit bigger,  \\n and a bit easier for us to look at it.  \\n The first thing to notice is that all of the tables  \\n have been given aliases, which is really important.  \\n So, it says, create view, actor info, as select a.actor ID.  \\n And then, there's an alias for that column name as well.  \\n So, it's not select actor ID, it's select a.actor ID.  \\n And if you scoot down to the bottom of the query,  \\n it says from bracket, bracket, bracket actor a.  \\n Every single table here has an alias.  \\n Without these aliases the field name  \\n would often be ambiguous.  \\n The way to read this query is in three sections.  \\n At the top, we have ordinary column names  \\n after the word select.  \\n So, they are the columns that we want to return.  \\n You will remember that there are four columns  \\n in our query when we clicked on it.  \\n Then we have a middle section beginning  \\n with group, concat bracket, and ending with, as film info.  \\n And that's probably the most important  \\n and complicated section of the query.  \\n And all of this part of the query is dedicated  \\n to one column, which is the film underscore info column.  \\n Just remind you of that.  \\n Just run the query again.  \\n And we have actor ID, first name, last name and film info.  \\n Film info is clearly where the important stuff is happening.  \\n So, the middle section is dedicated to that fourth column.  \\n And then, the final section, we have the from  \\n and join statements, which list all the tables  \\n and how those are connected to each other.  \\n So, let's have a look at the output of this again,  \\n which is kind of the target,  \\n so we know where our query is trying to get to.  \\n And we're just interested in the film info column  \\n for the moment.  \\n We've got an actor name, Penelope Guiness,  \\n and against that, we've got all the films  \\n that she appears in, in the world of SeQuiLa that is,  \\n and they are organized in a certain way.  \\n So, the films are organized into film categories,  \\n animation, children, classics, comedy, et cetera,  \\n which are followed by a colon and separated by semicolons,  \\n and they are organized in ascending alphabetical order.  \\n So we have A, and then some Cs and then a D  \\n and then an F and so forth.  \\n Within each film category, we have film names  \\n in the world of SeQuiLa.  \\n So, if you look at classics, we have Color Philadelphia,  \\n and then a comma, and then Westward Seabiscuit.  \\n So, there's two films there separated by commas  \\n and they are also arranged alphabetically ascending.  \\n So, this is an example of the one to many  \\n relationship I mentioned earlier,  \\n which often happens in databases.  \\n For every one actor, there are multiple film categories  \\n and for every film category, there are multiple films.  \\n This query has completely reorganized our tabular data.  \\n It's taken two tables, one with 200 actors  \\n and another with a thousand films  \\n and joined them together in a new way.  \\n This would take you a long time to do in Excel,  \\n if you just downloaded all the data  \\n and then tried to match them up.  \\n So, let's look again at how the query achieves this.  \\n The fourth column, remember, is obviously key.  \\n All of this middle section is dedicated  \\n to returning values into the fourth column,  \\n and we can see it uses two group concat statements,  \\n one nested inside of the other.  \\n You may remember that group concat merges the contents  \\n from multiple rows, and it's the text contents usually.  \\n Now, with all complex queries, we start in the innermost  \\n part, the middle, and we work our way out.  \\n So, if we look at the second group concat statement,  \\n it says select group concat F dot title,  \\n that's film title, order by film title ascending  \\n with a separator of comma.  \\n So, where we saw classics and Westward Seabiscuit,  \\n and I think animation, Anaconda Confessions, I think,  \\n and they were separated by a comma.  \\n That is what this part is returning.  \\n It's saying merge all of our film titles together,  \\n all of them alphabetically ascending with just a comma.  \\n Now, we can actually just run parts  \\n of this query on their own.  \\n And then, you get to see which parts  \\n of the code are performing,  \\n which functions a bit more clearly.  \\n So, I'm going to copy and paste that  \\n and I'm going to select full text so I can see what's going on.  \\n So, by doing this, we get a list of all films  \\n in alphabetical order, separated by commas.  \\n At the moment, they're all in one cell  \\n and they're not grouped.  \\n If you look at the bottom right of the cell,  \\n you can see Alamo.  \\n So, we haven't even got past the As.  \\n There's so much text in here it's got cut off,  \\n and you can see in the top left of the cell,  \\n Academy Dinosaur, probably 10 times, at least there.  \\n So, it's repeated.  \\n So, we take this mass of film names.  \\n That's what this very middle section is doing.  \\n And then, the distinct concat C dot name colon,  \\n is taking the unique values,  \\n that's the distinct, and it's adding  \\n in the film category name and a colon.  \\n It's imposing a bit of order on this mass of films.  \\n Once we've done that, we perform a final group concat,  \\n which organizes by film category name  \\n using a separator of a semicolon.  \\n And this is how we get the reorientated  \\n and merged data that we see in the results page.  \\n Now, if you had gone to the trouble of creating this query,  \\n you may well want to reward yourself by saving it.  \\n And that way you can run it whenever you like,  \\n just by clicking on it.  \\n You can also join views to tables or views to other views.  \\n So, you can reference it in your queries.  \\n To do that, you could just to create view  \\n and then you would put the view name as  \\n and this is MySQL.  \\n And then, you would begin your query.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3083813\",\"duration\":317,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Use variables\",\"fileName\":\"2494129_en_US_05_02_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"If you are writing longer queries and saving them, it can be useful to use variables so it's easier to change parameters in a single location. In this video, Emma shows you how to declare and use a simple variable in MySQL.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8544107,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now we're going to look at variables.  \\n A variable is a user defined keyword  \\n that's used to store data  \\n and that data might just be a number  \\n or a piece of text, a date,  \\n but it could also be a whole result set from a query.  \\n So with variables there are two obvious ways  \\n that you could use them, one is as a filter  \\n which is what we're going to look at first.  \\n And then the other way is to store results into a variable  \\n with select into, and the reason,  \\n looking first at the using a variable as a filter,  \\n the reason you would want to use a variable  \\n is if you've got a particularly long query  \\n and you are making reference to an ID say,  \\n which is liable to change  \\n and perhaps you're making reference to it multiple times  \\n then with a variable, you can just update the variable  \\n in one place at the top of your code,  \\n nice and neat, and you are done.  \\n You don't have to go sifting through your code.  \\n So in MySQL, to declare a variable  \\n you use the SET keyword, SET, and you use  \\n the at sign, @variable1, and I've just made that up.  \\n You could call it Fred or Mary, whatever you fancy,  \\n SET @variable1 = 5; and then we put a semicolon  \\n at the end because that is the end of one query  \\n and your database needs a semicolon to tell it  \\n to sort of execute that query  \\n before moving on to the next one.  \\n Underneath we've got our familiar SELECT *  \\n FROM actor WHERE actor_id is less than five  \\n and now we want to change that  \\n to our variable name and hit go  \\n and you can see it's returned us the same result set  \\n and it split it into two queries for us as well.  \\n Now, incidentally, if you can see here that we have #MySQL,  \\n hashtag in MySQL is a way of commenting out a line,  \\n so that isn't a piece of code and it won't run.  \\n That was just to let you know and let me know  \\n that that was MySQL way of declaring a variable  \\n because of course they're all different.  \\n So the Microsoft way of doing it, and this only works  \\n in what's called procedural SQL, which is where  \\n you have different lines of SQL, different queries  \\n that run one after the other and are related to each other.  \\n You have to use a sort of more complicated  \\n set of extensions to the language.  \\n So in Microsoft, that's called Transact SQL.  \\n Obviously I can't run this query, but the format is similar.  \\n You would say DECLARE instead of SET,  \\n you'd use the at sign again, so @variable1,  \\n you give the data type,  \\n in this case varchar with a character length of 30  \\n and that's the first query.  \\n The second one is setting your variable to be,  \\n in this case, text 'PENELOPE'  \\n and then your familiar looking query  \\n SELECT * FROM actor WHERE first_name is @variable1;,  \\n which can have a semicolon too.  \\n Now, the other thing that you could do  \\n with Microsoft is put this all on one line here.  \\n So DECLARE your variable, data type,  \\n and what it's equal to all in one go.  \\n That's how you do it in Microsoft.  \\n In Postgre, again, you have to use the procedural extension  \\n PL/pgSQL, rolls off the tongue.  \\n And here again you use the DECLARE keyword,  \\n DECLARE variable1 as text.  \\n In this case, that's the data type in Postgre,  \\n and then you have a BEGIN and an END statement,  \\n SELECT * FROM employees WHERE the first_name is equal to  \\n and you would say variable1.  \\n And then finally in Oracle,  \\n again we would say DECLARE variable1,  \\n there's no at sign this time, still need a data type,  \\n you've got your BEGIN and END again,  \\n SELECT * FROM my_table WHERE field_name > variable1,  \\n for all of these queries it wouldn't be worth it  \\n because clearly it's easier just to modify your filter.  \\n But if you think of the view statement that we had,  \\n how long that was, if that was the sort  \\n of statement that had a changeable ID field  \\n or name field in it, you could save yourself  \\n or your colleagues a lot of time by using a variable.  \\n And we'll be seeing more  \\n on variables in the coming sections.  \\n Now I said that we would look  \\n at two different cases for using variables.  \\n So the first one was variables as filters  \\n and that's what we've just done.  \\n And the second one was storing your results  \\n set into a variable and for that, we use SELECT INTO  \\n and SELECT INTO works across all the different forms of SQL.  \\n This example I'm showing you here is a MySQL version  \\n but you can use SELECT INTO across all the main versions.  \\n So let's have a look here, SELECT column1, column2  \\n or field one, field two, INTO @x, @y,  \\n these are your variables,  \\n FROM table1 LIMIT 1;.  \\n Now it's important that your query,  \\n your SELECT part of your query returns just one row  \\n and that the number of variables  \\n you have is equal to the number of columns.  \\n Now in the later sections  \\n we will be building on this knowledge now of variables.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3087944\",\"duration\":484,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Use functions\",\"fileName\":\"2494129_en_US_05_03_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"A function is a formula, which typically returns a value, such as a number or text, and is comparable to Excel formulas such as SUM or AVG. In this video, learn about the MySQL functions and learn how to create your own. This video focuses on the MySQL approach, although all main versions of SQL allow you to create functions.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12875968,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now we're going to look at functions.  \\n You may be familiar with functions already  \\n from Visual Basic or JavaScript  \\n but if you've never used a programming language,  \\n you've probably used a function in Excel.  \\n So if you've typed in something like equals sum  \\n and then you've given a range of cells  \\n and you've hit enter, you've been using a function.  \\n And Excel will go off and count up all the values  \\n in the range of cells and return you a value.  \\n That's what functions do, typically.  \\n You give them an input  \\n and they calculate some sort of output  \\n along rules that you specify  \\n and return you with usually a number or a text or a date.  \\n So what they don't do is they don't run off and edit data.  \\n They don't delete rows of data.  \\n You give them an input and they calculate an output for you.  \\n You can see from the breadcrumb bar  \\n that we are at the top level of the sakila database  \\n and we have functions, procedures, tables, and views.  \\n And this time we're looking at functions.  \\n And the sakila database has three for us already defined.  \\n We're going to look at inventory held by customer.  \\n So I've just clicked on it there.  \\n And if we scroll down, there's a definition there.  \\n Now we could have returned this definition  \\n programmatically in MySQL by saying,  \\n show create function  \\n and then using the function name,  \\n but this is just as easy to do.  \\n And just a reminder that what follows  \\n is specific to MySQL.  \\n So I've just copied that and pasted it into atom  \\n so that we can look at it a bit more easily.  \\n So, first of all, we have a login data root at local host  \\n which we can largely ignore.  \\n And then we have function and the function name,  \\n inventory held by customer.  \\n After inventory held by customer,  \\n we have brackets, P inventory ID, int  \\n and int means integer.  \\n P inventory ID is a variable name.  \\n So what does it mean to have that in brackets  \\n after the name of the function?  \\n When we use this function in SQL,  \\n we're going to use it much like this.  \\n Select inventory help our customer.  \\n And then we're going to supply a number  \\n in the brackets and that's the P inventory ID.  \\n And the number needs to be an integer.  \\n And then this line says returns int.  \\n So this function is expecting an integer as an input  \\n and it's going to give us an integer as an output as well.  \\n That's the whole purpose.  \\n So let's get rid of that.  \\n You can see that we have begin and end declarations  \\n to this function.  \\n And after that, we have a variable declaration  \\n and then an exit handler at the top.  \\n Now this might look a bit odd because we are  \\n declaring a variable using the declare statement.  \\n And we've just seen that we can use the set statement  \\n with an @ sign to declare a variable, like so.  \\n So why are these two things different?  \\n They're both variables in MySQL  \\n but they're different kinds.  \\n Where you see the set keyword followed by the @ sign,  \\n you're using a user defined variable  \\n and these are sort of longer lasting variables, if you like.  \\n And let me tell you a bit more about that.  \\n If you tried to reference the customer ID  \\n after the end keyword,  \\n MySQL wouldn't know what you were talking about.  \\n Where you have a variable name and the declare keyword  \\n with no @ sign, you're using a local variable  \\n and these cease to exist after the end keyword.  \\n If instead, we wanted to make reference down here  \\n to the user defined keyword, that would be fine.  \\n They're longer lasting.  \\n They exist outside of the begin and the end declarations.  \\n But in a function like this,  \\n it's nice and tidy to have those variables disappear  \\n after the end statement.  \\n So we don't need to use these larger user-defined ones,  \\n we can use local variables.  \\n In our second declaration statement,  \\n it says declare exit handler for not found return null,  \\n which looks like really bad grammar.  \\n And what it's saying is  \\n if the query finds no rows at all, then return a null.  \\n So show on screen the word null.  \\n After that we have what looks like  \\n a regular select statement  \\n except that this one has an into keyword.  \\n And what it's saying is select the customer ID  \\n and pop it into a variable called V customer ID.  \\n V Customer ID is what is going to be returned,  \\n as in shown on screen.  \\n And you can see that further down the query  \\n next to the end declaration.  \\n It says return V customer ID.  \\n So that is what we're going to see on screen.  \\n So what's this select statement doing?  \\n It says select the customer ID  \\n from the rental table where the return date is null,  \\n that is to say the DVD is still on loan,  \\n and for the inventory ID matches the number  \\n that's been given in the parameters.  \\n So let's run this  \\n and make a bit more sense of what's going on.  \\n Now I haven't just made that number up.  \\n I've had a look through the tables  \\n to try and work out a sensible inventory ID to use.  \\n So select inventory held by customer 2047,  \\n returns us 155.  \\n So have a quick look back at our query.  \\n We have supplied an inventory ID,  \\n that is to say a piece of stock or DVD,  \\n with a number 2047 in the database.  \\n And this query is returning us the customer ID  \\n of the person who still has that on loan.  \\n What if we tried a different number?  \\n And again, here's one I prepared earlier.  \\n Going to try 367.  \\n You can see we returned a null.  \\n So now that could be a couple of different things.  \\n It may be that there's no customer has ever hired DVD 367  \\n or it may be that no customer has it currently on loan.  \\n So let's have a quick look at what's going on.  \\n Select everything from the rental table  \\n where the inventory ID is 367 and we get five rows.  \\n So this DVD has been rented before,  \\n but if we loo, the return date is filled in in every case,  \\n which means all of those DVDs have been returned.  \\n There's no outstanding item.  \\n And that's why we got the null when we ran our function.  \\n Our declare exit handler came into play.  \\n Now on its own, this function is a little bit unhelpful.  \\n Knowing that customer 155 has a movie  \\n that's still on loan is not really human readable  \\n but functions can be useful as part of a query.  \\n And that's really how they're used.  \\n So let's say you're chasing up on an item  \\n of stock and you want the email address  \\n for the customer who still has a movie on loan.  \\n Then we could say select email from the customer table  \\n where inventory held by customer 2047  \\n is customer ID and inventory held by customer 2047  \\n is not null.  \\n So this time we're using our function in a query  \\n to get the email address of anybody  \\n who still has an item of stock on loan.  \\n And we know that that was customer 155  \\n and there we have the email  \\n of the offending customer, as it were.  \\n This tells us two things at once then.  \\n The customer does indeed have the movie  \\n still in her possession.  \\n And there's the email address to contact her.  \\n Using a function here is a substitute really  \\n for a subquery or joining tables together.  \\n It can be a lot quicker.  \\n And for common queries,  \\n this is a way of storing them  \\n so that you and other people can use them easily.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3087945\",\"duration\":298,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Use procedures\",\"fileName\":\"2494129_en_US_05_04_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"A stored procedure is like a function, except it typically performs an action rather than just returning a value. Procedures can also edit or delete data. In this video, Emma shows you a read-only example. This video focuses on MySQL, but all main versions of SQL offer the ability to create and use stored procedures.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8321017,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Unlike functions,  \\n stored procedures tend to do something.  \\n So typically, they perform an action,  \\n like edits in data or deletes some.  \\n We're going to look at a very simple one  \\n that just returns a value.  \\n We're going to look at the film not  \\n in stock stored procedure,  \\n and this is used to work out which copies  \\n of a film are on loan from a particular store.  \\n Now, we've taken this definition from,  \\n stored procedures film not in stock,  \\n and to click on that.  \\n And it has a definition section,  \\n just like the others.  \\n And note that this what follows is specific to MySQL.  \\n There's also this section above called parameters,  \\n and you can see there are three parameters,  \\n two input parameters and one output parameter.  \\n Now, unfortunately, you can't see the full title here.  \\n It says, p film id, so that's the first input,  \\n p store id which is the second input,  \\n and then an output parameter which we've not seen before,  \\n p film count.  \\n So let's see, we've got film id, store id, and count,  \\n and let's pop over and have a look at our query.  \\n We're selecting an inventory id from the inventory table,  \\n where the film id is equal to the first parameter,  \\n the store id is equal to the second parameter,  \\n and the inventory is not in stock.  \\n So that line there is using a function.  \\n And it's worth noting, you can use functions  \\n in stored procedures in MySQL, but you can't do the reverse.  \\n So we can't call a stored procedure from a function,  \\n but we can call a function from a stored procedure.  \\n And then it says, select any rows that are found  \\n into our output parameter, p film count.  \\n Let's try using it.  \\n To make sense of all of that, we use the call keyword,  \\n call film  \\n not in stock.  \\n And I'm going to look up film with an id of 39,  \\n a store of two, and then our output parameter,  \\n I'm providing a user defined variable at count,  \\n and you could use at Fred, or  \\n at anything, there, that's fine.  \\n And the purpose of that is if you had a multi-line query,  \\n you could then say, select star save  \\n from the inventory table,  \\n where inventory.  \\n You could write lots of queries here.  \\n I'm just showing an example.  \\n Equals at count,  \\n so you could use your var, your output parameter,  \\n your variable, you could use on a second line  \\n of SQL and subsequent lines.  \\n So, that's there to help you out.  \\n So, we want to know for film 39 and store two,  \\n are any of those currently on loan?  \\n I click go, and we've got an inventory id  \\n of one seven seven.  \\n So, I'm going to write some more normal queries  \\n to try and puzzle out how we've got  \\n to this one seven seven and make sense of it.  \\n What we can do is say,  \\n select everything from the inventory table, where,  \\n film id is 39,  \\n and the store id is two.  \\n So let's start with that.  \\n Now we've got four rows.  \\n Now the inventory table remember,  \\n is all about what stock the DVD rental store has,  \\n if it were full, if it had all of its DVDs.  \\n So, there are four copies of this film,  \\n available to store two in general.  \\n The rental table is the one that tell us,  \\n whether they're currently with a customer or in the shop.  \\n And this function, inventory in stock, inventory id,  \\n is running off to the rental table,  \\n and checking in with each of those inventory ids,  \\n one seven seven, one seven eight,  \\n one seven nine, and one 80,  \\n is that currently in or out of stock  \\n according to the rental table?  \\n So, here we have one seven seven to one 80.  \\n Let's have a look.  \\n So let's start now from the rental table,  \\n where,  \\n the inventory id,  \\n is in,  \\n one seven seven, seven eight, one seven nine,  \\n and one 80.  \\n Now, we've got 12 rows there.  \\n And if we look along,  \\n only one of them has a return date of null,  \\n and that tells us that only one  \\n of these copies is currently loaned out  \\n and not been given back.  \\n And you can see that that's the one  \\n with an inventory id of one seven seven.  \\n So that's how this stored procedure worked out the number,  \\n one seven seven to return it to us,  \\n and I think you'll agree,  \\n it's a lot quicker running that stored procedure,  \\n than running all of these separate queries  \\n through the different tables ourselves.  \\n \\n\\n\"}],\"name\":\"5. More Advanced SQL\",\"size\":45559022,\"urn\":\"urn:li:learningContentChapter:3086813\"},{\"duration\":62,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3083814\",\"duration\":62,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps and additional resources\",\"fileName\":\"2494129_en_US_06_01_2817027_XR15\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1548270,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Emma Saunders] Thank you for participating  \\n in this course on SQL.  \\n I hope that you are now confident to design your own queries  \\n and run reports on your database directly.  \\n You should experiment now.  \\n That's what's really going to make the learning sink in.  \\n And design your own queries  \\n and joins and views and functions.  \\n A good way of doing this is to use the Sakila database  \\n and look at the functions and procedures  \\n and things that we haven't looked at yet.  \\n Making sense of this more complex SQL could really help you.  \\n If you're going to be using SQL at work,  \\n you should also work out which type  \\n of SQL it is such as Microsoft or Oracle  \\n and refine your learning now to that particular vendor.  \\n Because once you move past the basics,  \\n the SQL language can become wildly different.  \\n There is an awful lot more SQL to learn.  \\n This course was just the beginning  \\n and it's focused on read-only SQL,  \\n the select statement.  \\n The next steps for you would be to learn how to edit  \\n and update and delete data in a database,  \\n if you dare. Good luck.  \\n \\n\\n\"}],\"name\":\"Continuing Your SQL Learning Journey\",\"size\":1548270,\"urn\":\"urn:li:learningContentChapter:3082840\"}],\"size\":282462707,\"duration\":9156,\"zeroBased\":false},{\"course_title\":\"Advanced SQL for Data Scientists\",\"course_admin_id\":2874221,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":2874221,\"Project ID\":null,\"Course Name\":\"Advanced SQL for Data Scientists\",\"Course Name EN\":\"Advanced SQL for Data Scientists\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"Many data scientists know how to work with SQL\u00e2\u20ac\u201dthe industry-standard language for data analysis. But as data sizes grow, you need to know how to do more than simply read and write from a database. This course provides a more sophisticated approach to designing data models and optimizing queries in SQL. Instructor Dan Sullivan begins with the logical and physical design of tables\u00e2\u20ac\u201dwith particular focus on very large databases\u00e2\u20ac\u201dand then presents a deep dive review of indexes, including specialized indexes and when to use them. The next section introduces query optimization and shows how to optimize basic, multi-join, and more complex queries. The course also covers SQL extensions, including user-defined functions and specialized data types. The techniques taught here enable more efficient analysis of large data sets using SQL, statistics, and custom business logic.\",\"Course Short Description\":\"Learn advanced techniques for analyzing large data sets with SQL. Find out how to build sophisticated data models, optimize queries, extend SQL with user-defined functions, and more.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":7382682,\"Instructor Name\":\"Dan Sullivan\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Data Architect, Author, and Instructor\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2021-05-27T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"Yes\",\"LIL URL\":\"https://www.linkedin.com/learning/advanced-sql-for-data-scientists-13972889,https://www.linkedin.com/learning/advanced-sql-for-data-scientists-2021-revision\",\"Series\":\"Persona\",\"Limited Series\":null,\"Manager Level\":\"General\",\"LI Level\":\"Advanced\",\"LI Level EN\":\"Advanced\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Data Science\",\"Primary Software\":\"SQL\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":9038.0,\"Visible Video Count\":38.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":95,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2422413\",\"duration\":51,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Advanced SQL techniques for data science\",\"fileName\":\"2874221_00_01_WX30_welcome\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2609341,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Dan] Welcome to this course  \\n on Advanced SQL for Data Scientists.  \\n In this course,  \\n we'll go beyond writing complex select statements  \\n to learning how to design performant data models  \\n using denormalization and read replicas.  \\n We'll review how and when to use various types of indexes,  \\n including GiST and GIN indexes,  \\n which we use for composite data types.  \\n Next, we'll move on to see how to optimize queries  \\n by analyzing query execution plans.  \\n SQL is extensible,  \\n so we'll spend time learning  \\n how to create user defined functions  \\n that can streamline working with SQL for data science.  \\n We'll also see how to take advantage  \\n of specialized features for working with JSON,  \\n as well as specialized data types  \\n for tree structures that can provide  \\n an order of magnitude speed up  \\n over conventional hierarchical queries  \\n based on recursive common table expressions.  \\n So let's get started  \\n and learn some Advanced SQL for Data Science.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2424379\",\"duration\":44,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"2874221_00_02_XR30_wysk\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1307361,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] This is an advanced course,  \\n so I make some assumptions about what you already know  \\n or at least familiar with,  \\n and that starts with assuming that you're fairly comfortable  \\n with writing complex SQL statements.  \\n So that includes working with different types of joins,  \\n applying different filtering operations,  \\n such as where and having clauses,  \\n applying windowing functions,  \\n and working with common table expressions.  \\n I also assume you're capable  \\n of installing Postgres or PostgreSQL,  \\n and as well as any kind of graphical base query tools  \\n you would like to use,  \\n and also, I assume you have some familiarity  \\n with data structures.  \\n So when we're talking about things like balanced trees  \\n and hashes in the index topics that you will be comfortable  \\n and be able to follow along.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":3916702,\"urn\":\"urn:li:learningContentChapter:2423380\"},{\"duration\":2252,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2425338\",\"duration\":379,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Rules of normalization\",\"fileName\":\"2874221_01_01_XR30_normalization\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"In this video, explore the first three rules of normalization and learn when they should be used. These are foundational design patterns data scientists use regularly.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11188963,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] When we talk about relational databases  \\n in SQL and data modeling,  \\n we're almost always going to be talking  \\n about normalization to some degree.  \\n Now, normalization is a practice in which  \\n we follow a set of rules for designing database tables  \\n which help us minimize the risk of data anomalies.  \\n Now, normalization is an important practice  \\n and it's widely adopted, but it's not always essential  \\n and there are cases where we actually don't  \\n want to normalize.  \\n So let's take a look at normalization  \\n and then we'll take a look  \\n at when we actually break the rules of normalization.  \\n So data anomalies are basically errors  \\n or inconsistencies in data that we really want to avoid.  \\n And there's three types we'll talk about here.  \\n There is an update anomaly.  \\n And an update anomaly occurs when you have redundant data  \\n and you only partially update that data.  \\n So for example, if we had a database of books  \\n and in that we tracked books and authors,  \\n and for authors we track things like their address.  \\n Well, if every time an author had a book entry,  \\n we captured the author's address along with the book,  \\n then we might have multiple addresses,  \\n say an author has 10 books published,  \\n there would be 10 copies of the address.  \\n Now, if for some reason we updated that address  \\n but only updated five of them,  \\n then we'd have five with old data and five with new.  \\n That's an example of an update anomaly.  \\n An insertion anomaly basically results  \\n when we're not able to add data to the database  \\n due to the absence of other data.  \\n So for example, if to add a new book  \\n I also have to be able to enter the author's address  \\n and I don't have the author's address,  \\n then I'm not able to insert that data.  \\n That's an example of an insertion anomaly.  \\n And then a deletion anomaly is when  \\n we unintentionally lose data  \\n because we've deleted other data.  \\n So for example, we might lose an author's address  \\n because we deleted the author's only book  \\n that we had in the database.  \\n Now we may have wanted to delete the book  \\n but actually keep the author and author information  \\n in the database for future use.  \\n So that's an example of a deletion anomaly.  \\n So the way we avoid anomalies like that  \\n is we follow normalization rules.  \\n Now there are many normalization rules  \\n but really the most important and the most widely used  \\n are the first three.  \\n The first normalization rule  \\n which is called first normal form,  \\n basically states that each value in a column  \\n is an atomic unit.  \\n So it's going to be a particular number or a unit string  \\n or a Boolean or some scalar values, some unit like that  \\n that can't be broken down further.  \\n The second normal form or second rule of normalization  \\n states that any attributes or any column we have in a table  \\n is dependent on the key.  \\n So it's a function of that key.  \\n So there's nothing in the table that's not related  \\n to that particular key.  \\n That gets us to second normal form.  \\n And then third normal form says we don't have  \\n any transitive dependencies  \\n or there's some pieces that are dependent  \\n on something in the table, but not the key.  \\n And that something else might be,  \\n for example, an author's address is dependent on the author  \\n and the author depends on, for example, the book,  \\n if we had that kind of relationship,  \\n that would violate third normal form.  \\n So we want to avoid that kind of transitive dependency.  \\n Now, oftentimes when we visualize normalized databases,  \\n we have diagrams which show rectangles representing tables  \\n and then we have lines connecting the tables  \\n which represent relationships,  \\n and then we have some kind of indicator  \\n for what kind of relationship it is.  \\n So for example, in many cases we have one table  \\n that's like a primary table,  \\n for example, like an order table.  \\n And then we have like a secondary cable  \\n that has a lot of detail, like an order's item.  \\n So if you have an order and there's maybe 10 different books  \\n on that order, we'd have one order in the order's table  \\n and 10 rows or 10 order items in the order items table  \\n that would be a one to many relationship or one to N.  \\n And sometimes the relationship can be one to one  \\n or one to zero or one or one to zero or many.  \\n And those all fit,  \\n these are all allowed under the rules of normalization.  \\n And we often see this kind of modeling  \\n when we're working with OLTP  \\n or online transaction processing systems.  \\n Now OLTP systems, for example,  \\n typically have many reads and writes  \\n so they're constantly being updated.  \\n Data is written once but then it may be updated  \\n again frequently by many different processes.  \\n So you can imagine like an e-commerce application  \\n with users updating many orders at the same time.  \\n Now, oftentimes these kind of OLTB systems  \\n are normalized to third normal form.  \\n Sometimes per performance they're de-normalized slightly  \\n but typically they're still considered normalized.  \\n Now contrast that with analytical databases,  \\n these are used typically for data analysis.  \\n And here we have many reads by many processes,  \\n but typically with analytic databases,  \\n we're not updating say a single customer's address  \\n or a single customer's order,  \\n we might be reading a single order  \\n or a single customer record,  \\n but in an analytical database we might look  \\n at thousands of orders or thousands of customers.  \\n So our reads tend to span many rows, but have fewer columns  \\n that we actually include in our query.  \\n We have many writes and with batch processing,  \\n many of those writes are done all at once  \\n like bulk updates where a job will run  \\n and it'll start from end to finish  \\n until the large number of rows are loaded in.  \\n We may also have streaming data  \\n where a process is ingesting data in near real time  \\n and writing it to a database.  \\n That's different from this idea of having maybe thousands  \\n of different users doing small updates or small writes,  \\n here we're doing fairly large amounts of writes  \\n when we have batch processes  \\n or we're consistently doing writes  \\n if we have a streaming process.  \\n These analytical databases are often de-normalized.  \\n And we'll take a look in the next video  \\n as to why that's the case.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2423375\",\"duration\":430,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Denormalization\",\"fileName\":\"2874221_01_02_XR30_denormalization\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover when to denormalize a data model to improve query performance. Normalized data models are not always as performant as denormalized data models so denormalization is often used to improve performance.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12796851,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Presenter] Now sometimes we choose  \\n to denormalize our data because denormalization  \\n can give us significantly better performance.  \\n So let's take a look at some of the characteristics  \\n of denormalized tables.  \\n Well, data is often redundant.  \\n So for example here in this example,  \\n looking at a book's table,  \\n we have author information that's duplicated  \\n as well as publisher information.  \\n We may have non-atomic values.  \\n So for example, some analytical databases allow  \\n and actually encourage the use of strokes or structures  \\n to have more complex values within a column  \\n than simple atomic values  \\n and then we also tolerate transitive dependencies.  \\n So if we have say a value that's dependent solely  \\n on a non-key attribute in the table  \\n and that non-key attribute is itself dependent  \\n on the primary key.  \\n That's totally fine.  \\n Again, it violates the third normal form  \\n but we're willing to tolerate that  \\n because in exchange for not being normalized,  \\n we're going to get significantly better performance.  \\n And the really the critical thing is  \\n in analytical databases in analytical applications  \\n we are typically at a reduced risk level  \\n with regards to anomalies.  \\n And that's because of how we work with the data.  \\n We have relatively few updates  \\n so while customers might constantly  \\n be updating their orders and their baskets  \\n and things like that.  \\n In an analytical database like a data warehouse  \\n or a data mart or say just a data science database,  \\n we typically load the table once  \\n and then maybe update it or add data  \\n but rarely do we go back and correct data  \\n that say for example we loaded a week ago.  \\n Now, if there are mistakes in the data,  \\n we may correct the batch or delete a batch  \\n or reload with corrected versions but outside of errors  \\n in data loading like that, we typically do few updates.  \\n Really we're more likely to be writing new data  \\n and reading data.  \\n Now, oftentimes we do batch inserts  \\n and those batch inserts go through  \\n an extraction load transform  \\n or extraction transform load process.  \\n So the data transformation is already kind of cleaning  \\n up the data massaging it and getting it into  \\n sort of our preferred form.  \\n So in that case, again,  \\n there's less risk of say an update anomaly.  \\n Again, unless say our transformation process broke  \\n but that would be a significant event  \\n and we would hopefully catch that  \\n with our monitoring system.  \\n Now sometimes we also have streaming inserts  \\n and those are relatively simple data structures.  \\n So you might think of like an IoT sensor  \\n that's measuring temperature and humidity  \\n and maybe some other environmental measures  \\n and then transmitting those say every minute.  \\n And so it might be a relatively simple structure  \\n that has a sensor ID, a timestamp  \\n and then whatever the measures are.  \\n So again, it's fairly simple  \\n with IoT data and streaming data.  \\n Again, you typically don't update it.  \\n And another thing about like IoT data  \\n which is typically used in the aggregate  \\n even if you did have a mistake even if there was an error  \\n in one of the readings and one of the minutes,  \\n if you're getting 60 readings per hour  \\n and you're aggregating those,  \\n it almost doesn't matter in terms of the aggregate value.  \\n So if you're looking at statistical measures  \\n across these data structures, very small errors  \\n just kind of get washed out and it actually  \\n doesn't make a material difference the way we report on it.  \\n So again, a data error may occur  \\n but it doesn't have as much of an impact  \\n as it would in an OLTP system.  \\n And another thing that happens,  \\n we eliminate the need for really complex joins.  \\n So that's particularly useful with regards  \\n to simplifying and speeding up performance.  \\n Now there's a couple of different ways  \\n that a denormalized database may look.  \\n A common way of building denormalized databases  \\n that's been popular for over a couple of decades  \\n is the star schema.  \\n And here are the idea is we have this fact table  \\n with a bunch of measures.  \\n For example, they might be all the measures  \\n that an IoT sensor is taking.  \\n And then we have different dimension tables  \\n and the dimension tables may be things like,  \\n oh, the sensor information and geography information  \\n and a time information.  \\n So we can keep track of like hours within days,  \\n within weeks, within years  \\n and be able to easily roll up data.  \\n So that's one way of denormalizing.  \\n And this is commonly done in databases  \\n where you have a row level orientation  \\n and row level orientation means  \\n when you go to read a data block,  \\n you read an entire row at a time.  \\n And that makes perfect sense when you're dealing  \\n with OLTP systems because if you're going to update something  \\n in a customer record you're likely  \\n maybe want to update multiple things.  \\n So it's helpful to have  \\n the entire customer data block available to you.  \\n Now in analytics systems when we're reading,  \\n we typically reading a few columns across many rows  \\n so a more performant way to work with it  \\n is using columnar orientation or using a columnar format.  \\n And in that way, when we go and we read data blocks  \\n we're actually reading many different attribute values  \\n from different rows but all from the same column.  \\n So if you want to do an average  \\n say of the temperature measure  \\n of a particular sensor over a 24-hour period,  \\n it makes a lot of sense just to grab the temperature data,  \\n grab the temperature column and not bring  \\n in the other data that you're not interested in  \\n and that's what columnar data stores do for us.  \\n Now, another common denormalization technique  \\n is to have what's called a white column table.  \\n And here you can imagine what some data modelers  \\n in the relational world might think of  \\n as their worst nightmare which is that we denormalize,  \\n we put everything into a single table  \\n and we make it very wide.  \\n Now this would be incredibly problematic for an OLTP system.  \\n It's actually quite advantageous  \\n for many analytical systems and the really high volume,  \\n large scale data warehouse systems.  \\n The kinds that scale to petabytes scale  \\n like Google big query or Google big table  \\n we'll use a wide column model.  \\n Now big table is a NoSQL database  \\n but big query is an analytical database,  \\n and it uses SQL for querying  \\n and it uses this kind of wide column model as well.  \\n Now the advantages of denormalizing  \\n that can be easier to query again,  \\n we're getting rid of complex joins.  \\n So we don't have the join table, A to B to C to D to E  \\n and introduce the risk of making mistakes  \\n in our complex queries.  \\n Queries can also be more efficient.  \\n Again, there are a lot of reasons for that,  \\n we discuss columnar versus row that's just one example  \\n and it also simplifies load procedures.  \\n Oftentimes with ETL processes, we can do joins sort of  \\n in stream and then write to a single table.  \\n It also makes it easier if you're dealing  \\n with something called slowly changing dimensions  \\n and you want to keep track of history.  \\n Well, by actually tracking the data  \\n in a road by denormalizing, you don't have to worry  \\n about tracking multiple records and say a dimension table  \\n and then pointing to the correct version of the data.  \\n So again, denormalizing  \\n can somewhat simplify load procedures.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2423376\",\"duration\":488,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Partitioning data\",\"fileName\":\"2874221_01_03_XR30_partitioning\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover when to partition data using range, list, and hash partitioning. Partitioning is an important strategy for improving the query performance of large data sets.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15040418,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] One of the most effective ways  \\n to deal with large data sets is to use partitioning.  \\n Now, what can happen  \\n when we're dealing with really large datasets  \\n is that large tables can be difficult to query effectively  \\n because they have so much data,  \\n and especially if you're scanning  \\n or you have to maintain very large indexes.  \\n So what partitioning does is it splits tables  \\n by either rows or columns into these subsections,  \\n which we call partitions.  \\n Now, horizontal partitioning  \\n is a way of limiting the amount of data we have to scan  \\n to a subset of a set of columns.  \\n We can have local indexes for each different partition,  \\n so in this example, we have a large table  \\n which is broken down into three different partitions,  \\n we could have three distinct sets of local indexes,  \\n and so our indexes would be smaller,  \\n scanning the indexes would be smaller,  \\n or if we needed to scan the entire partition,  \\n the amount of data would be smaller.  \\n And also partitioning makes adding  \\n and deleting data efficient,  \\n especially if you have like a time-based partition.  \\n It's very easy to say drop off the oldest partition,  \\n just drop that partition  \\n once that data set ages beyond whatever period of time  \\n you think it's useful for.  \\n Now, vertical partitioning  \\n is a little bit different from horizontal partitioning.  \\n With vertical partitioning we're separating out columns,  \\n and this is, in a way, what columnar storage does,  \\n it's sort of extreme vertical partitioning  \\n in that we increase the number of rows  \\n that can fit in a data block  \\n because we have fewer columns in each row.  \\n So when we're using a row oriented storage system,  \\n this is one way to start to get some of those advantages  \\n of columnar storage.  \\n Now we can have global indexes for each partition,  \\n so we're still covering the entire table  \\n like all global indexes,  \\n but we can still reduce I/O because when we fetch data,  \\n we're going to fetch a single block,  \\n or actually, you know, some number of data blocks  \\n but those data blocks will have the equivalent of more rows  \\n than if we had additional columns in the data block,  \\n and again, columnar storage has similar benefits.  \\n Now range partitioning is a type of horizontal partitioning,  \\n and basically what it allows us to do  \\n is to partition on non-overlapping keys.  \\n So we'll identify a column or set of columns  \\n that is a partition key,  \\n and those will be distinct,  \\n and we will be able to sort of designate which partition  \\n a particular row of data goes into based on that key.  \\n Now working with date is quite common,  \\n also numeric ranges are often used for range partitioning,  \\n and we could also use alphabetic ranges.  \\n This partitioning is a little bit different,  \\n we basically have a set of particular attributes.  \\n So for example, if we wanted to work with tables  \\n with data from global sensors,  \\n and we wanted to just partition them by continent,  \\n we could do that.  \\n Again, it's not overlapping keys,  \\n but there's some fixed list of values  \\n that we're going to be worked with,  \\n so that's the thing that's distinctive  \\n about list partitioning.  \\n Now another option is hash partitioning.  \\n In here, basically what we do,  \\n we still identify a partition key,  \\n but then we apply a hash to that,  \\n and then we take the modulus of that hash to identify,  \\n for example, which partition to go to.  \\n So with hash partitioning, we might decide,  \\n oh we want 10 or 20 or 30 partitions,  \\n and then we'll pick a partition key,  \\n that partition key will be used  \\n when it's time to determine  \\n where a particular row should go,  \\n and we'll just do the mod operator  \\n on the hash of that partition,  \\n and then based on whether the mod result is zero through 19,  \\n depending on where that lands,  \\n that'll determine which partition it should go into.  \\n Now, hash partitioning is useful  \\n when you don't have like a set of values  \\n that would make sense for list partitioning,  \\n or other numeric or alphanumeric,  \\n doesn't really make a lot of sense for range partitioning.  \\n It's also good if you want to make sure your data  \\n is evenly distributed,  \\n hash partitioning will keep it balanced.  \\n Okay, so now I've switched over to PG Admin  \\n which is a GUI tool we use for working with Postgres,  \\n and what I'd like to do now is to just do a quick example  \\n of creating a table with a range partition.  \\n I want to point out I'm using a Postgres database  \\n and I'm in a database named postgres,  \\n and right now I'm using a schema called iot,  \\n or internet of things,  \\n and so what I'm going to do is create a table,  \\n and I'm going to create it in the iot schema,  \\n and I'm going to call it sensor measurement,  \\n or msmt for short,  \\n and let's see, we will have a sensor ID  \\n which will be an integer, and it should be not null,  \\n and we'll have a measurement date,  \\n which will be a date and not null,  \\n and let's measure temperature,  \\n and for simplicity, we're going to keep this as an int,  \\n we will assume there's no decimal places on temperature,  \\n similarly with humidity, we'll call that an int,  \\n and that wraps up our list of attributes,  \\n but now I want to specify that I want to partition by,  \\n and I want to use range partitioning,  \\n so I'm going to petition by range over my measurement date,  \\n and I'll just execute that to create that table.  \\n And so basically I've created that table,  \\n and now what I need to do is actually create the partitions,  \\n and I do that by using actually the create table statement  \\n with the partition of option,  \\n so let me show what that looks like.  \\n So here we're going to to create a partition,  \\n one for each month.  \\n And so I'm going to create a table  \\n and I'm going to call it iot sensor measurement,  \\n and I'm going to call it year 2021, month 01,  \\n and I'm going to specify that this table  \\n is a partition of my iot sensor measurement table,  \\n and now I need to specify what values,  \\n so I'm going to say for values from,  \\n and I'll specify a date, I'll say 2021-01-01 to 2021-01-31,  \\n click on there, and then I will do the same,  \\n so let's say I want to do it for February as well.  \\n I won't do this for all 12 months,  \\n but I'll just do a couple partitions here,  \\n so you can just see, get a sense  \\n of what the syntax looks like.  \\n So let's see, we'll want to change that to 20,  \\n oh, and I made a mistake here,  \\n this should be 2021,  \\n this should be 2021,  \\n this should be 2021,  \\n 2021, and this should be actually 02,  \\n we'll change to February to 28,  \\n and this to 02.  \\n So we have our main table, which is iot sensor measurement,  \\n and then we specify that we're going to use a date column,  \\n in this case, it's called measurement date,  \\n and now we're going to be creating  \\n the partitions themselves.  \\n Now, if we wanted to have  \\n a whole year's worth of partitions,  \\n we'd have 12 create table partition of statements like this,  \\n but instead of doing that let's just run this,  \\n and it returns, and basically now we have a partition table.  \\n So that's the syntax, and the syntax,  \\n again, is similar for like list partitioning,  \\n hash partitioning,  \\n but again, the core idea here is  \\n you create like a a main primary table  \\n which specifies what the structure looks like,  \\n and then you create whatever partitions you need for that.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2428253\",\"duration\":496,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Materialized views\",\"fileName\":\"2874221_01_04_XR30_materialized_view\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore the benefits of materialized views and requirements to use them. Materialized views are a convenient and easy to maintain way to denormalize data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15707805,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now, another technique  \\n in our data modeling toolbox  \\n that we can use to officially work  \\n with really large datasets  \\n is something called materialized views.  \\n Now, a materialized view  \\n basically persists the results of a query.  \\n So rather than executing a query, say, multiple times  \\n and actually going through all the steps  \\n of the particular execution plan  \\n and then getting our results back,  \\n we can actually save the results  \\n after executing the query once  \\n and then we can go back  \\n and read that saved result sets many times.  \\n The value here is that reading from a single table  \\n can be much faster  \\n than executing a complex query over and over again  \\n to get the same data.  \\n So we can think of materialized views as a form of caching.  \\n It's also a way of getting some of the value  \\n of WideTable denormalization  \\n while still having on the side a more normalized model.  \\n And, ultimately, we're trading space for time,  \\n so you want to consider that in way the benefits  \\n of saving on time while adding more to your storage costs.  \\n Now, there are some kind of general rules of thumb  \\n about when is a good time to use materialized views.  \\n If you have a long-running query,  \\n that's a great candidate right there.  \\n Now, complex queries,  \\n especially when there are a lot of joins,  \\n or maybe you have subqueries  \\n or some common table expressions  \\n or things that might be complicated  \\n for people who aren't necessarily well-versed in SQL,  \\n it would make their life a lot easier  \\n if they could simply query a single table.  \\n So that's another good indicator  \\n that you might have a good candidate for materialized views.  \\n If you're doing a lot of aggregate computation  \\n and other derived data,  \\n that can be a form of long-running query,  \\n but that, again, is a good candidate for materialized view.  \\n Also, when there's really a distinction  \\n in the terms of class  \\n of like read operations and write operations,  \\n which we have, we have batch uploads or streaming processes,  \\n and then we have, you know, users  \\n building these queries and querying data,  \\n when you have that kind of clear partitioning  \\n between reading and writing,  \\n that's another sort of feature  \\n that lends itself to using materialized views.  \\n Now, there are some other considerations  \\n you want to keep in mind  \\n about when not to use materialized view.  \\n Materialized views use a form of consistency  \\n known as eventual consistency.  \\n And, basically, what can happen  \\n is the materialized view can get out of sync  \\n with the underlying tables  \\n that were used to build the query.  \\n So, for example, with our IoT data example,  \\n we might build a materialized view every hour.  \\n Now, since it's an IoT system,  \\n we're probably streaming the data in continually.  \\n So 15 minutes after we build a materialized view,  \\n we're going to have even more data in the database.  \\n Now, that 15 minutes of data won't be incorporated  \\n into the materialized view until the next hour,  \\n assuming we have a one-hour refresh.  \\n So if you can live with the fact  \\n that when you query, say, the materialized view,  \\n you might not have the latest data,  \\n and it may be up to maybe like 59 minutes of missing data,  \\n if that's not a problem, then that's fine.  \\n If it is a problem, then you need to either reconsider  \\n either refreshing more frequently  \\n to get it to something that's really in your tolerance range  \\n or not using materialized views  \\n if you really can't tolerate eventual consistency,  \\n if you need a stronger form of consistency.  \\n You also want to think of the cost of the the update process.  \\n So you're going to be executing an example,  \\n in our IoT example,  \\n going to be updating the materialized view every hour.  \\n Now, is it worth to run a very expensive query every hour?  \\n That's something you want to take into account.  \\n Also, you want to understand  \\n if you can concurrently read a materialized view  \\n while it's being updated.  \\n Now, in Postgres, that's the default,  \\n but it may not be in other databases.  \\n So if you're using  \\n other relational database management systems,  \\n you want to check into that.  \\n And, again, the size versus time trade-off,  \\n you want to consider the size of the materialized view data.  \\n And also, as we mentioned  \\n with regards to eventual consistency,  \\n you want to think about the refresh frequency.  \\n You know, what's the right balance,  \\n and can you strike that balance between eventual consistency  \\n and the benefits of having materialized view?  \\n Now, let's take a look at an example  \\n of creating a materialized view.  \\n I've jumped to a different schema here.  \\n I'm in the landon schema.  \\n And this is information about hotels,  \\n so we have a table about customers,  \\n and we have a table of expenses and locations,  \\n and there are some other data as well.  \\n But I would like to create a materialized view  \\n that will help me understand the expenses  \\n for each of my hotels.  \\n So what I'm going to do is start with a SELECT query.  \\n And so let's say I want to join  \\n my locations and expense tables.  \\n Let's fill the SELECT statement for that.  \\n And let's say I want to select from the locations table,  \\n which I'm going to alias with an l, so I'll use that here.  \\n I want to select hotel id  \\n and, let's see, the city, state or province,  \\n and let's get the country as well,  \\n and I think that's enough from location.  \\n From expenses, we want to get the year,  \\n let's get annual payroll and health insurance costs  \\n and finally the cost of supplies.  \\n And we're going to join or query from landon schema  \\n using the locations table, aliased with an l,  \\n and I'm going to LEFT JOIN,  \\n again, another table from the landon schema,  \\n this time it will be expenses, which we'll alias with an e,  \\n and we're going to join on the locations hotel_id,  \\n where that is equal to the expenses hotel_id, okay?  \\n So I'm just going to execute that query.  \\n And so, as we could see, we have some information  \\n from the locations table and from the expenses table.  \\n Now, something this simple,  \\n you probably wouldn't put into a materialized view,  \\n but I'm just keeping it simple  \\n purely for demonstration purposes.  \\n So now the next step now that I have my SELECT statement,  \\n to create a materialized view,  \\n I can specify CREATE MATERIALIZED VIEW,  \\n and I want to create this in the landon schema.  \\n I'm going to call this mv for materialized views,  \\n and then locations_expenses AS,  \\n and then I'm just going to wrap  \\n the SELECT statement in parentheses  \\n and execute.  \\n So now I have created a materialized view.  \\n Now, one thing I can do  \\n now that I have this materialized view  \\n is, of course, I can actually select from it.  \\n So if I do a SELECT * FROM landon_mv_locations_expenses,  \\n and I execute just that statement,  \\n I'm going to get the results,  \\n the same results I got from the query, of course.  \\n So that's how you create  \\n and basically work with materialized views.  \\n It's no different in a sense in terms of querying  \\n from working with other views.  \\n Now, if you want to refresh the materialized view,  \\n that's pretty straightforward too.  \\n The command is REFRESH MATERIALIZED VIEW,  \\n and then we specify the name,  \\n in this case it will be in the landon schema  \\n mv_locations_expenses.  \\n Now, when we execute this statement, what's going to happen  \\n is it will re-execute that SELECT statement  \\n that we specified in the CREATE MATERIALIZED VIEW statement.  \\n And so, it runs really quickly, of course,  \\n 'cause this is a small table.  \\n But that's basically the steps  \\n of working with a materialized view.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2427262\",\"duration\":225,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Read replicas\",\"fileName\":\"2874221_01_05_XR30_read_replicas\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover when to use read replicas to improve query performance. Read replicas can help improve the performance of extraction, transformation, and load processes.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6316664,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now in some cases, data scientists work  \\n with systems that may also be used  \\n by other people for different reasons.  \\n For example, you might want to use  \\n an online transaction processing system as a source of data,  \\n and you want to grab data continually.  \\n Well, you don't want to put too much stress  \\n on the OLTP system,  \\n which is being used by interactive users.  \\n One way to deal with a situation like that is  \\n for data scientists to work with database administrators  \\n to create something called a read replica.  \\n Now, when we think about database servers,  \\n it makes sense oftentimes we use just a single server.  \\n So for example, if we're in the cloud,  \\n we can decide how much memory we need,  \\n how much storage we need, and pick a virtual machine,  \\n or use a hosted database service  \\n and size the instance that we're interested in.  \\n And let's think of that as our primary.  \\n Now in that scenario, all the reads and all the writes  \\n go to the primary database operation.  \\n So we send data in, we also query from it.  \\n So the primary server is responsible for handling  \\n both read and write operations at the same time.  \\n An alternative option is to create a read replica.  \\n And here the idea is any time  \\n data is written to the primary,  \\n it's also written to another instance of a database server,  \\n full-blown Postgres instance that's on the network  \\n and can receive data from the primary  \\n and essentially keep a copy of all the data.  \\n When we have a read replica,  \\n then we can direct queries to the read replica.  \\n The data's kept up to date pretty quickly  \\n using the write ahead log in the primary.  \\n And so what happens is writes go to the primary,  \\n the primary sends data or replicates data  \\n to the read replica and read operations  \\n are routed to the read replica.  \\n So this allows the primary to do most of the work  \\n with regarding writing, but also offloading  \\n a lot of the read work.  \\n So the reason we use replicas is again  \\n so the primary can really focus on the writes.  \\n And we could even have multiple replicas if we needed that.  \\n So if we had so much read load  \\n that a single replica wouldn't satisfy our needs,  \\n we could add multiple replicas.  \\n And again, this is especially useful  \\n when you have really heavy read loads.  \\n Now, like with materialized views,  \\n we have to think about eventual consistency.  \\n Now, however, unlike materialized views,  \\n when we're working with a read replica,  \\n it's the update operations  \\n on the read replica are much faster.  \\n In Postgres, there's a parameter you can use  \\n to determine the level of consistency before a transaction.  \\n For example, a write operation is considered complete.  \\n The fastest way, but also the least reliable,  \\n is basically what's called the fire and forget method,  \\n which is the primary would just send the data  \\n to the read replica and just assume that it's written there.  \\n Now, there's a potential for the data to be lost  \\n on the read replica because of some error,  \\n but the primary's not waiting around for an acknowledgment.  \\n The slowest method enables strong consistency  \\n and basically the primary waits  \\n until it's guaranteed to have saved the data  \\n to the primary data storage,  \\n as well as the replica storage  \\n and get an acknowledgement  \\n that it has been stored like that.  \\n Now, that's the safest in terms of data integrity,  \\n but it's also the slowest  \\n in terms of when a transaction completes.  \\n So that's a factor you need to consider  \\n when using read replicas.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2425339\",\"duration\":86,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Design a data model for analytics\",\"fileName\":\"2874221_01_06_XR30_CH30_challenge\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Given a set of data analysis requirements, create a data model to support the requirements. A data scientist must know how to translate business requirements into data models that support analysis.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2734033,\"solution\":false,\"welcomeContent\":null,\"challenge\":true,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (bright music)  \\n - [Instructor] Here is a data modeling challenge.  \\n Let's assume you're a consultant  \\n and you're working with an internet of things,  \\n or an IoT company,  \\n that is collecting streaming data from thousands of sensors  \\n and they collect this data every minute.  \\n For them, low write latency is essential.  \\n So as soon as the data comes in,  \\n it needs to be written because we don't want to  \\n basically have a backlog of data or anything like that,  \\n and it's also important that the write latency  \\n be fairly consistent so we don't want,  \\n you know, some periods where it's very bursty  \\n and we have very low latency,  \\n and then other times it's more prolonged, it's more delayed.  \\n Now, at the same time,  \\n this IoT company has a team of data scientists  \\n and they're going to be performing different kinds  \\n of time series analysis  \\n including roll-ups of aggregate data.  \\n So, say for example, by each sensor, what's the,  \\n what are some aggregate measures over, you know,  \\n hours and over days?  \\n And with regards to that analysis,  \\n those data scientists need access to any data  \\n that's older than one hour.  \\n Anything newer than that isn't necessarily required  \\n for this aggregate analysis.  \\n So the challenge is to design a model  \\n that would support these requirements.  \\n Now this is a high-level model,  \\n just outline what kind of structures or design patterns  \\n would you use to address  \\n the business requirements identified here?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2427263\",\"duration\":148,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Design a data model for analytics\",\"fileName\":\"2874221_01_07_XR30_SO30_solution\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore the solution to the data modeling challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4749178,\"solution\":true,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Here is a solution  \\n to the data model challenge.  \\n So, first of all, the sensor data should be written  \\n to a table that models the data sent from the sensors.  \\n So, for example, if a sensor is sending a sensor ID,  \\n a timestamp, and then measure one, measure two,  \\n measure three, we should have a table  \\n with those five attributes: sensor ID,  \\n timestamp, and then the three measures.  \\n Now, likely the table will be partitioned by time.  \\n Now, we didn't explicitly state in the requirements  \\n the need for partitioning and we didn't discuss,  \\n for example, how long we wanted data  \\n to be kept in this table and if data past  \\n a certain age should be rolled off.  \\n Now, this is something you need to keep  \\n in mind when you're doing data science,  \\n and you're working with data modeling at the same time.  \\n Not all of the requirements are going  \\n to be outlined for you, the people that are,  \\n say, domain experts may not be aware of the kinds  \\n of factors that we think about when there's designing.  \\n So, any time you're presented with sort of a challenge,  \\n like we have here, it is important  \\n to not just take what's given, but also interrogate  \\n and probe other questions so, for example,  \\n how long should data be persisted?  \\n Now, this is a good use case for materialized views  \\n because materialized views can be used  \\n to generate a persistent view, or a materialized view  \\n of the aggregated data and whatever that aggregation is.  \\n For example, you might have one materialized view  \\n that is used for hourly aggregates,  \\n and another materialized view  \\n that's used for daily aggregates.  \\n And, of course, you want to refresh  \\n these, at least, once per hour.  \\n Now, with regards to the daily materialized views,  \\n we may not need to refresh at all  \\n because once the day has passed,  \\n and we build a materialized view with that day's roll-ups  \\n we shouldn't need to change those.  \\n Will data scientists be using that daily materialized views  \\n for partial results throughout the day?  \\n If that's the case, then you want  \\n at least that materialized view refreshed more frequently.  \\n Now, also, another factor we didn't delve down  \\n into too deeply with regards to the requirements  \\n is do the data scientists need access  \\n to the low level detail as well as the aggregates?  \\n Because if they need access to low level data  \\n like the raw data that comes from the IoT sensor,  \\n well, because of the low right latency requirement  \\n we don't want to bog down the primary server  \\n with a lot of ad hoc query, that would be  \\n a good use case for a read replica.  \\n \\n\\n\"}],\"name\":\"1. Data Modeling: Tables\",\"size\":68533912,\"urn\":\"urn:li:learningContentChapter:2422419\"},{\"duration\":981,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2424380\",\"duration\":232,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"B-tree indexes\",\"fileName\":\"2874221_02_01_XR30_b_tree_indexes\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Learn how b-tree indexes work and when to use them. B-trees are widely used and can significantly improve read performance.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6716368,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In addition to designing tables,  \\n part of data modeling is developing an indexing strategy.  \\n So let's take a look at indexes,  \\n different types of indexes,  \\n and how they apply when we're working  \\n with analytical queries.  \\n We'll start with B-Tree indexes.  \\n So, indexing is used primarily  \\n to reduce the amount of work we need to do  \\n when we have to go and fetch data.  \\n Now, typically this means we don't want to be  \\n scanning a lot of data blocks,  \\n so indexes are used to help us reduce that.  \\n Now, there is a cost associated with this  \\n because we have to maintain these indexes.  \\n So what that means is we're going to take up  \\n additional space, but we're also going to be doing  \\n additional rights, so anytime we're loading data  \\n or deleting data, we're going to need to update the index.  \\n Now, some things to keep in mind  \\n is that when we index a column, the higher the cardinality  \\n which means, you know, just the number  \\n of distinct values, really influences how well  \\n the index helps improve query performance.  \\n So for example, if you had a table of codes  \\n and the codes were numbers, one through 10,  \\n the codes are evenly distributed  \\n you can expect about 10% of the table to be returned  \\n if you look up a particular code.  \\n So 10% is not bad but it doesn't reduce a lot  \\n say compared to if the cardinality was say a 1000,  \\n and if you looked up a particular code  \\n you could reduce the amount of work that you need to do  \\n to maybe 1000th of the size of the table.  \\n Now, we're going to be talking about indexing,  \\n but again I want to point out in terms of things  \\n that are different in analytical databases,  \\n indexing is not used in analytical databases  \\n like Google BigQuery or AWS Redshift,  \\n again, they have different strategies.  \\n Now, there are several different types of indexes,  \\n we're going to pay attention to three broad categories,  \\n the B-tree, the bitmap, and the hash index,  \\n and then we're also going to look at special purpose index.  \\n So first, let's take a look at B-trees indexes.  \\n Now the B in B-tree stands for balanced.  \\n And what this means is that  \\n what we're trying to do is keep a record  \\n or information about rows of data  \\n by capturing a small amount of information,  \\n that's the attribute or attributes that we're indexing.  \\n And one of the things we want to be able to do with an index  \\n is to be able to look up a value very quickly.  \\n Well, B-trees are sort of the workhorse of indexing,  \\n they work really well in many different cases  \\n and what they give us is basically the ability  \\n to look up a value in logarithmic time.  \\n So let's see how that works.  \\n Say, we are indexing say a column that has one to 100,  \\n and the first one we index  \\n or the item right in the middle is a 50,  \\n and that makes sense because we want to be able to split  \\n sort of divide and conquer and with a balanced tree  \\n the way it works is the next time we insert a value,  \\n if the value is less than 50  \\n then we're going to put it in toward the left node  \\n and if it's greater than 50  \\n we're going to put it toward the right note.  \\n So for example, 25 would go to the left  \\n and 75 would go to the right.  \\n Now, let's say you want to insert a value  \\n say between 25 and 50,  \\n in that case, it's going to be less than 50,  \\n so we're going to go to the left, we'll go toward 25  \\n but then if it's greater than 25  \\n we're going to go to the right.  \\n So in this example, you know,  \\n 37 is placed first to the left  \\n and then to the right.  \\n And we can continue this example,  \\n and what we can see here is even though  \\n there could be a 100 different nodes in this tree,  \\n we're never going to have to go more than log n or log 100  \\n steps to actually find the value.  \\n So that's one of the big values  \\n of B-tree indexes is it gives us that order  \\n and look up time.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2424381\",\"duration\":183,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Bitmap indexes\",\"fileName\":\"2874221_02_02_XR30_bitmap_indexes\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover why bitmap indexes are used in analytical queries. Bitmap indexes can significantly improve queries over large data sets that require filtering.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5642354,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's take a look at bitmap indexes.  \\n Now, a bitmap index is relatively simple.  \\n It's quite different from a B-tree index  \\n and the basic idea is when we have a column  \\n that has a small number of possible values,  \\n we might be able to map those values  \\n or encode them in a bit string.  \\n So for example, here we have a table  \\n and there's an ID column  \\n and another column which is_union_member  \\n and you can see we list yes or no or null.  \\n And we could use literally those strings.  \\n Alternatively, we could map a yes to a one,  \\n a no to a zero and a null to a zero zero.  \\n And that's what we've done  \\n in the two columns on the right.  \\n Now, it doesn't have to be just two values.  \\n You can have a larger number of possible values.  \\n For example, in this new table here on the right  \\n where we have an ID and pay_type,  \\n we have three different kinds of pay type:  \\n salary, hourly, and contractor.  \\n Now, you'll notice the salary column has a one in it  \\n when the pay type is salary  \\n and a zero and a zero in hourly and contractor.  \\n Similarly, when the pay type is hourly,  \\n there's a one under the hourly column  \\n and a zero in the other two columns.  \\n And for a contractor,  \\n there's a one under the contractor column  \\n and a zero in the salary and hourly.  \\n So you can see, we can map these, for example,  \\n the is_union_member a simple yes/no  \\n or a list of value kind of pay_type into a bit string.  \\n And one of the reasons we would do that  \\n is because it makes it very easy to do  \\n kind of Boolean operations with bit operators.  \\n So, for example, looking for someone who is a union member  \\n and paid hourly would be a matter of doing  \\n some bit operations on the bitmap index.  \\n So we use bitmap indexes typically  \\n when there's a small number of possible volumes in a column,  \\n so a low cardinality.  \\n When we're going to be filtering by bitwise operations,  \\n such as and, or, and not,  \\n so if you're going to apply those operators,  \\n and the thing to keep in mind is that the time to access  \\n the selected rows using this index is based on the time  \\n that it takes to perform bitwise operations,  \\n which of course are very fast.  \\n Now, typically, bitmap index  \\n is when they are explicitly declared,  \\n like if you're working with a database  \\n that supports the explicit declaration of bitmap indexes.  \\n You typically save that for read-intensive use cases,  \\n like data warehousing or data science operations  \\n where there's relatively few writes.  \\n And that's because it can be expensive  \\n to build a bitmap index in some cases.  \\n So in terms of bitmap index availability, as I mentioned,  \\n some databases allow you to create them explicitly,  \\n but Postgres does not.  \\n However, Postgres does sometimes build bitmap indexes  \\n on the fly as is needed.  \\n So the decision to use bitmap indexes or not in Postgres  \\n is made by the query plan builder.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2428254\",\"duration\":80,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Hash indexes\",\"fileName\":\"2874221_02_03_XR30_hash_indexexes\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to use hash indexes for equality filter conditions. Hash indexes are used with large data sets to improve the performance of equality operations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2425332,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now a third type of index is the hash index.  \\n Now hash indexes use hash functions.  \\n Now hash functions are mappings of arbitrary linked data  \\n into a fixed-size string.  \\n Now hash values are virtually unique  \\n and the hash value is a function of the input, of course.  \\n So even slight changes in inputs will produce a new hash.  \\n So for example, if you're hashing a string  \\n and you add a space at the end,  \\n that would give you a totally different hash  \\n than the one you had originally.  \\n And here's some examples.  \\n So some things to consider.  \\n The size of the hash value depends on the algorithm used.  \\n So that's typically determined  \\n by the database management system designers.  \\n There's no ordering or preserving with hash functions  \\n so they could appear in just any random order,  \\n and similar inputs, again,  \\n can have vastly different outputs  \\n so you can't make any assumptions about the value  \\n that a hash function generates  \\n other than it's highly likely to be unique.  \\n What this means is hash indexes  \\n are only useful for equality operations.  \\n So if you're looking up, say,  \\n almost like key value kind of things,  \\n hash indexes can work very well in those cases.  \\n In the case of Postgres,  \\n hash indexes can be smaller than B-tree indexes,  \\n and they're built with at about the same speed  \\n or about the same pace as one builds B-tree indexes.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2427264\",\"duration\":113,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"GiST and SP-GiST indexes\",\"fileName\":\"2874221_02_04_XR30_gist_spgist\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how lossy indexes and space-partitioned indexes work. These specialized indexes are useful for geometric and geolocation queries.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3353274,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] GiST and SP-GiST indexes  \\n are specialized index provided by Postgres.  \\n Now, GiST stands for Generalized Search Tree,  \\n and it's basically, it's a balanced tree structure method.  \\n And GiST is used as a template  \\n to actually implement other indexing schemes.  \\n So depending on our data structure, we can use a GiST index.  \\n So for example, one could build a B-tree index,  \\n which is a self-balancing tree, or an R-trees  \\n which are used with multi-dimensional data.  \\n So these, for example, with geographic coordinates.  \\n Now, that GiST is used in Postgres for indexing  \\n things like hstore and ltree.  \\n These are two data types that we're going to delve into  \\n much more deeply, later in the course.  \\n But I just wanted to point out  \\n that GiST indexes are used for these specialized data types.  \\n Now, when we talk about indexes and data types  \\n we also have to talk about operators  \\n and what kind of objects can the operators apply to.  \\n So in the case of these indexes,  \\n we're talking about things like;  \\n operations have geometric shapes like boxes, circles,  \\n and also internet address points  \\n if we're talking about distances and dimensional planes,  \\n as well as polygons and ranges.  \\n And it also works with text queries,  \\n and that includes a data type known as text vectors as well.  \\n Now SP-GiST is a space-partitioned GiST  \\n and it supports partitioned search trees.  \\n And it's typically used for non balanced data structures.  \\n So we might use these  \\n with things like quadtrees or k-d trees,  \\n which again are used in multidimensional space.  \\n And SP-GiST can also be used to develop custom indexes.  \\n Now, the operators here include things like  \\n things for operations  \\n like K dimensional and quad operations, range operations.  \\n Again, geometric things like boxes and polygons  \\n as well as internet address data types.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2421481\",\"duration\":248,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"GIN and BRIN indexes\",\"fileName\":\"2874221_02_05_XR30_gin_brin\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover when to use generalized inverted indexes and block range indexes. These indexes are used with composite indexes and queries search for patterns within that composite value.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7452551,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] One of the reasons I like working  \\n with Postgres is that it has specialized indexes  \\n for data types that are more complex  \\n than we typically work with.  \\n So for example, there's something called a GIN index  \\n which stands for generalized inverted index.  \\n And that's used when we have to index data  \\n that are inside other elements or other pieces of data.  \\n And we call these things composite values  \\n and we use a GIN index when we need to index the values  \\n which are in that composite item.  \\n So for example, words in a document,  \\n the composite data structure is the document,  \\n but in that document, each of those words  \\n is a individual element,  \\n which we are interested in indexing.  \\n Now the index stores the data and key value pairs  \\n and the key is the element value  \\n and the posting list is a set of row IDs  \\n where that key occurs.  \\n So it's very fast to go from key to actual data location.  \\n Now GIN index has a large number  \\n of built-in operator classes  \\n that allow you to do things like operate on arrays,  \\n JSON data structures, and in particular  \\n JSONB data structures as well as text vectors.  \\n Now some tips when you're using GIN indexes,  \\n keep in mind that the insertion can be slow  \\n and this is because many keys may be inserted for each item.  \\n So if you have a large number of documents  \\n and you are indexing all of them, then each of the words  \\n and each of the documents need to be indexed.  \\n So that's why insertions can be slow.  \\n So for a very large bulk operation  \\n so if you're doing like an initial data load,  \\n it's likely faster to just drop the index  \\n and then recreate the index  \\n after you've loaded the entire dataset.  \\n An alternative way to do this is to  \\n let Postgres postpone a lot of the indexing work  \\n using something called temporary lists.  \\n Now temporary lists are eventually inserted  \\n into the index using kind of  \\n a optimized bulk insertion technique.  \\n So basically the temporary list we're keeping track of  \\n but we are not actually committing it to the index just yet.  \\n Now the disadvantage here  \\n is that the temporary list has to be searched  \\n in addition to the regular index  \\n anytime there's a lookup that uses the index.  \\n So in that case,  \\n large temporary lists will slow searches significantly  \\n so you're trading off,  \\n are your searches going to be slow  \\n or your load is going to be slow.  \\n Now, when you create index,  \\n you can disable the fast update parameter  \\n to disabled temporary lists.  \\n So if you really find that during the loads,  \\n the temporary lists are becoming problematic  \\n with respect to your query performance,  \\n try disabling the fast update perimeter.  \\n Now another index type that Postgres support  \\n is called a BRIN index, which stands for block range index.  \\n And we typically use BRIN indexes with very large tables.  \\n And within these tables, the column data  \\n has some kind of correlation with physical location  \\n like a postal code or dates where dates close  \\n to each other are going to be typically stored  \\n in the same data block near data blocks.  \\n And block ranges are pages  \\n that are physically adjacent in a table.  \\n So when we talk about a block range index,  \\n we're talking about ordering these data blocks  \\n and what the BRIN index does  \\n is it stores summary information about the block ranges.  \\n So you might have things like the max postal code  \\n and min postal code in a particular block  \\n or max statement data block.  \\n So you can end up storing very small amounts of data  \\n in a BRIN index, but still get really good performance.  \\n Some things to keep in mind with BRIN indexes,  \\n the entries are per entire block ranges,  \\n select the max and min,  \\n so it's not for individual elements.  \\n But the trade-off here is we don't have details  \\n about individual elements, but it allows us to quickly scan  \\n and skip large segments of a table when we're searching.  \\n And this is really important if you have ordered data  \\n like if you're working with time series data  \\n or you have other data where you can make use  \\n of kind of this min and max idea.  \\n Now BRIN operators have a whole bunch  \\n of operators like date min and max,  \\n also works with characters, floating point numbers,  \\n timestamps as well as UUIDs.  \\n And there are even more,  \\n which you can find in the Postgres documentation.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2421482\",\"duration\":37,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Choosing an optimal indexing strategy\",\"fileName\":\"2874221_02_06_XR30_CH30_challenge\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Given a set of data analytics requirements and a data set, define an optimal indexing strategy. This challenge assesses how well you understand how to uses different index types.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1119321,\"solution\":false,\"welcomeContent\":null,\"challenge\":true,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (bright music)  \\n - [Instructor] Alright, here's a challenge around indexing.  \\n Let's assume you have received a large dataset  \\n with insurance claims details.  \\n Now you want to be able to upload  \\n or ingest that data into an existing database  \\n that you're already using for your data science analytics.  \\n Now each claim has a unique claim identifier  \\n along with about 12 columns of data.  \\n The existing database has a table  \\n of all claim numbers ever generated.  \\n How would you index the new claims detailed  \\n to optimize a join operation on the claim ID?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2425340\",\"duration\":88,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Choosing an optimal indexing strategy\",\"fileName\":\"2874221_02_07_XR30_SO30_solution\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore the solution to the indexing challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2498580,\"solution\":true,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (bright music)  \\n - [Instructor] We have really two options we can think of.  \\n Now, by default, when we build an index in Postgres,  \\n we use a B-tree index.  \\n Now B-tree indexes are generally good choices  \\n because they're relatively fast to work with  \\n because, on average, you're going to get a time  \\n basically relative to the logarithm  \\n of the size of the table,  \\n so even a table with a large number of rows  \\n is going to be able to find  \\n the index value relatively quickly.  \\n Now because we're working with claim IDs  \\n and claim IDs are unique,  \\n that means that at most,  \\n one row in each table will have that particular claim ID.  \\n Well, in that case, since we're not going to be doing things  \\n like range scans or things like that,  \\n which work really well with B-tree indexes,  \\n we can use a hash index.  \\n A hash index uses a function that takes a column value,  \\n like a claim ID, and then converts it into a 32 bit integer,  \\n and we can use that 32 bit integer  \\n as basically like an address or an index into a table,  \\n and so, rather than kind of walking through a tree  \\n along multiple steps,  \\n we can immediately go to the storage area  \\n that has the location of the data block we're looking for  \\n or actually points to the data block.  \\n So definitely in this case, because claim ID is unique,  \\n and we're not doing any kind of range scanning,  \\n we're just doing a one-to-one lookup,  \\n that's a good use case for a hash index.  \\n \\n\\n\"}],\"name\":\"2. Data Modeling: Indexes\",\"size\":29207780,\"urn\":\"urn:li:learningContentChapter:2424385\"},{\"duration\":1767,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2425341\",\"duration\":461,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"EXPLAIN and ANALYZE commands\",\"fileName\":\"2874221_03_01_XR30_query_plan\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Discover what the ANALYZE and EXPLAIN commands do and why to use them. These two commands are used to generate execution plans and are widely used as part of the query tuning process.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14119558,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now as data scientists working with SQL,  \\n it's sometimes helpful to understand a little bit  \\n about how the database engine  \\n actually executes SQL statements.  \\n And that's particularly important  \\n if we're trying to optimize the performance of our queries.  \\n So let's start from first principles.  \\n SQL is basically a declarative language.  \\n So we specify what we want.  \\n So for example, I want these certain set of column  \\n returned from this particular table  \\n subject to this filter criteria.  \\n Now that again is stating what we want.  \\n I'm not saying in any way, how the database should go  \\n about doing that.  \\n Limiting the data we have to scan to a subset of rows.  \\n So many languages like Python  \\n or Java can be used procedurally.  \\n And when we're doing that  \\n we're specifying how to do something.  \\n So we're actually like directly manipulating data structures  \\n and determining like the sequence of steps.  \\n So one of the things that database engine does for us  \\n is it figures out those sort of procedural steps  \\n and then execute some for us.  \\n So one of the simplest kind of procedural steps  \\n is scanning a table.  \\n And basically what happens is we say,  \\n start at the beginning of table, fetch a row  \\n and then move on to the next row.  \\n And maybe we're doing a comparison.  \\n For example, we're looking at a status level  \\n and we want to filter, show me all the customers  \\n who are status level gold.  \\n Well, that's, what something like this kind of simple scan  \\n could do.  \\n We'll simply walk through each row in a table  \\n and check the particular column that's part of the filter.  \\n And then if it passes the filter, we return it.  \\n So again, the procedure is fairly straightforward.  \\n So in a lot of ways, scanning is simple.  \\n We just look at each row,  \\n fetch the data block that contains the row  \\n and apply the filter or the condition.  \\n So if we're thinking in terms of, well what's the order  \\n what's the time complexity of this?  \\n Well, the cost is basically based on the number of rows  \\n in the table.  \\n Now I should say that scanning isn't always as simple.  \\n I'm making the assumption that we're working  \\n with a database that uses a row storage  \\n or a row orientation.  \\n Now, some databases like AWS Redshift or Google Big Query  \\n use column in their storage.  \\n And again, we typically see columnar storage  \\n with data warehouses.  \\n So some of the things that I say here about scanning  \\n and row fetching certainly apply when we're using Postgres  \\n but maybe not something  \\n if you're working with say a petabyte scale data warehouse.  \\n So cost is based on the number of rows.  \\n Now that doesn't mean scanning is always bad.  \\n Some scanning can be really efficient when tables are small.  \\n So for example, rather than use an index  \\n and look up something in the index  \\n and then go fetch something from the table,  \\n it may be faster just to scan a row  \\n and not maintain an index.  \\n Now, scanning large tables can be efficient  \\n if we're only scanning them very few times.  \\n So for example, say you have a very large table  \\n and you have to scan it once.  \\n It's probably not worth building an index on that  \\n because of the cost of building the index,  \\n the storage that's required as well as the time.  \\n So even with very large tables,  \\n there may be times when scanning is efficient.  \\n But in general, with large tables that we query repeatedly,  \\n that's not efficient.  \\n So we want to find some other way  \\n to work with those tables  \\n rather than doing sequence scans or full table scans.  \\n One way to do that is to use indexes.  \\n And of course, we've already talked about this  \\n but just to recap, indexes are ordered,  \\n and they're faster to search for an attribute value  \\n than say scanning the whole table.  \\n And since the index is basically a set of attributes  \\n and an a pointer to a particular row  \\n that we can use the index say for our filter  \\n and use the index and do the filtering based on the index  \\n rather than doing the filtering  \\n based on the data in the rows.  \\n In the case of using 20 partitions.  \\n Now, another thing we want to watch  \\n when we're thinking about performance is joining tables.  \\n So of course the basic idea with joining tables  \\n is that we have data in say two tables  \\n and we want to somehow select a row from one table  \\n and link it or join it to a row in another table  \\n and basically produced a projection  \\n or a new view over that data.  \\n So the question is, how do we go about matching those rows?  \\n Well, one table is going to have a foreign key in it,  \\n and the other table is going to have of course the primary key  \\n in the other table.  \\n And so the question of how to match rows  \\n becomes how do we match keys?  \\n So there are a few ways to do this.  \\n There is a technique known as the nested loop join.  \\n And that's whether the database  \\n and Jim will compare all rows in both tables to each other.  \\n There is the hash join, in which we calculate a hash value  \\n of a key and then join based on matching hash values.  \\n And then finally, there's something called sort merge join  \\n in which we sort both tables and then join rows  \\n while taking advantage of the order.  \\n So for a nested loop, the basic sequence of steps  \\n is we loop through one table and then for each row,  \\n we loop through the other table  \\n and at each step we compare the keys.  \\n So the nice thing about nested loop joins  \\n is it simple to implement but it can be expensive  \\n if the tables are large.  \\n Nested loop join is often a good choice  \\n when the tables are small.  \\n Now a hash join computes the hash value of keys  \\n in a smaller table.  \\n It then stores those hashes in a hash table  \\n and the hash table has the hash value  \\n in some row attributes.  \\n And then we scan the larger table.  \\n And we find rows from the smaller hash table  \\n that matched with the hash value of the hash and key  \\n on the larger table.  \\n With sort merged join, we sort both tables,  \\n we compare the rows like we do with nested loop join  \\n but because both tables are ordered,  \\n we can stop when it's not possible to find a match  \\n later in the table because of sort order.  \\n And this is nice because we scan the driving table  \\n only once.  \\n So we've looked at indexes and we've looked at joins  \\n with regards to kind of things we look at  \\n with regards to query optimization.  \\n One of the things that the database engine  \\n or the the plan builder looks at  \\n is information about the tables themselves.  \\n And so the plan builder relies on statistics about the data  \\n in the tables that we're working with.  \\n Now, usually statistics are kept up to date.  \\n And in Postgres there is a process called autovacuum,  \\n which does that and also kind of cleans up  \\n after some deletes and compresses data  \\n and does some other sort of background,  \\n housekeeping kinds of things.  \\n Now, sometimes statistics can get out of date.  \\n So there is a command called ANALYZE  \\n which you can run, which will update statistics.  \\n So if for some reason you're looking at a query plan  \\n and you can't quite figure out  \\n why would the query plan builder  \\n choose this really inefficient way if it looks,  \\n like for example, it's doing a nested loop join  \\n when maybe a hash join would be more appropriate.  \\n A possible reason for that and outside chances  \\n for some reason, the statistics are out of date,  \\n so just run the ANALYZE command.  \\n And there are different parameters  \\n you can analyze whole schemas or tables.  \\n So you can definitely get more detail  \\n on the ANALYZE command.  \\n That's not something you necessarily need to use a lot,  \\n but if you do, there are some options  \\n with the ANALYZE command  \\n so you can either scan an entire schema  \\n or really target a particular table for that.  \\n And some things to keep in mind.  \\n When a table has a relatively small amount of data,  \\n the ANALYZE will look at all of the data.  \\n When it's a very large table, it will only sample the data.  \\n So it'll build statistics out of a sampling.  \\n So if you were to run ANALYZE say two times  \\n over a very large table  \\n you might get slightly different statistics  \\n but that's because of sampling.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2424382\",\"duration\":256,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Generating data with generate_sequence\",\"fileName\":\"2874221_03_02_XR30_generate_series\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to use generate_series to create a test date for an IoT time series use case.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8726836,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Okay, before we get into using explained plan  \\n let's create some data for us to work with.  \\n Now, the first thing I want to do is just paste in some code  \\n that we have seen already earlier.  \\n Basically what I want to do is recreate  \\n an IOT sensor measurement table,  \\n and create a couple of partitions.  \\n I won't go over all the details  \\n 'cause I've covered this in an earlier video  \\n but I do want to point out a couple of things.  \\n In this case, we're going to use measurement date.  \\n We're actually going to make that a timestamp  \\n so we can include hours and minutes as well.  \\n And I just want to point out in terms of creating partitions  \\n we want to keep in mind that this last value  \\n that at the end of the range is not inclusive.  \\n I always forget this.  \\n So for example, if I set this date to be 2021 01-31,  \\n that's a perfectly valid range to go from January 1st  \\n to January 31st, but it's not inclusive.  \\n I always forget that.  \\n So I just want to point that out.  \\n You want to have overlaps here.  \\n So for example, we'll go from January 1st  \\n to February 1st and then in our next petition  \\n we'll go February 1st to March 1st.  \\n Okay, and I also have a drop table statement here  \\n in case that's left around and okay,  \\n so I have created my table and it is partitioned.  \\n Now, the next thing I want to do is create some data.  \\n Now we're working with time series data.  \\n So there's a really useful set function  \\n in Postgres called generate series.  \\n So let's take a look at that.  \\n Now, generate series works with a select statement.  \\n So I'm going to say select star from generate series.  \\n We're just going to call the function.  \\n Now the arguments I pass in are integers or dates.  \\n And the reason is because generate series  \\n can work with either integers  \\n and create a series of integers  \\n say like one to a hundred  \\n or it can create a series of dates or timestamps.  \\n So we'll start by creating,  \\n let's say a hundred rows for (mumbles).  \\n And we'll say that is as T1 for table one.  \\n And what you'll notice here is it's simply generates a list  \\n of a hundred numbers.  \\n So that's perfect, that's great.  \\n That's one of the things we'll need.  \\n Those integers would make very good sensor IDs  \\n for our table.  \\n Now, the other thing we can do with generate series  \\n is to pass in dates or timestamps.  \\n So I'm going to pass in, let's see, 2021-01-01  \\n and we're going to work with hours and minutes.  \\n So I'll say zero hours and zero minutes.  \\n This is the start.  \\n Now I'm going to be explicit with Postgres  \\n and tell it to convert this string to a timestamp.  \\n And I can do that using the syntax, colon, colon  \\n and its specifying a data type, in this case timestamp.  \\n And that'll do the casting  \\n or the implicit conversion that we need.  \\n And I'm going to put as my N date 2021,  \\n let's go into February  \\n but let's only go up to say the 15th  \\n and we'll use 00 and 00, the stopping hour and minute.  \\n And again, let's cast that at a timestamp.  \\n And then, the one of the thing we need to put  \\n when we working with generate series  \\n and we're working with time series  \\n is put in some kind of interval specification.  \\n So I'm going to generate a timestamp for every minute.  \\n So I would say one minutes  \\n and let's call this a stable two, the series,  \\n let's see, kind of correct.  \\n And what you'll notice here  \\n as I'm now generating a series of timestamps  \\n that are increment in one minute steps.  \\n So as you go along, we'll see  \\n that we're still in 01, 01 and we're going down  \\n so we'll have about 60 timestamps  \\n for the first hour and so on.  \\n The reason this is going to help us  \\n is we're going to be able to use this generate series  \\n to actually create our generated data.  \\n So let's take a look at that in the next video.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2427265\",\"duration\":395,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Generating time series data\",\"fileName\":\"2874221_03_03_XR30_generate_data\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to analyze the performance of a basic query. WHERE clauses are common and SQL developers should understand how they are executed.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13997080,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now I'm starting off  \\n from where I left off in the previous video.  \\n And here we have a select statement that uses  \\n generate series to create some timestamps for us.  \\n Now, what I'd like to do at this point is actually use this  \\n and another generate series to generate a series  \\n of sensor IDs, and then a set of timestamps.  \\n But what I want is I want each sensor ID  \\n to have all of the timestamps.  \\n So let's see how we can do that.  \\n So first of all  \\n I'm going to be turning this select statement  \\n into a subquery.  \\n And just to make it a little less spread out  \\n I'll just do a little bit of formatting here, okay.  \\n Now I also want to be able to generate the sensor ID.  \\n So I'm going to put in a select star from generate series  \\n and let's go from one to 100  \\n and this is going to be a subquery.  \\n So I'm going to say, let's call that as T1  \\n and here this will be a sub query as well.  \\n And that'll be as T2.  \\n So by saying a subquery  \\n I mean a subquery in the from clause.  \\n So I'm going to select star from  \\n these two subqueries.  \\n Oh, sorry.  \\n I didn't mean to execute the query.  \\n I meant to indent these.  \\n There we go.  \\n Now, first thing I need to do is get my syntax correct  \\n by putting a comma there  \\n but you'll notice here we have two tables  \\n or two projections actually in the, from clause  \\n and there's no joint statement  \\n and that's intentional  \\n because when we don't put a joint explicit joint clause in  \\n and we have two tables,  \\n then Postgres is going to generate the Cartesian product  \\n or basically a one to all mapping  \\n for the rows in each table.  \\n So here in our first subquery  \\n we're going to generate 100 rows.  \\n Each one of those rows is going to be joined  \\n to each of the time series that are generated  \\n by the second select statement.  \\n So let's run that and see what we get.  \\n Now, what we'll notice here is  \\n we have the generated series begins with one,  \\n which is as expected.  \\n And for the one, we also have each  \\n of the time series that we saw before  \\n that's generated, you know, 60 per hour  \\n each one of those is joined to the integer that we generate.  \\n So we're going to have a large number of timestamps  \\n and generated sensor ID numbers.  \\n So this is great.  \\n Now let's take a look at our table actually  \\n and let's do a refresh to make sure it's up to date.  \\n And there's sensor management.  \\n Let's look at the columns.  \\n So we have our sensor ID  \\n and our measurement date or a timestamp.  \\n So the next thing we need to do is generate some random data  \\n for temperature and humidity.  \\n So random of course, is a good choice of random function.  \\n So let's think about this.  \\n How can we use these generated sensor IDs and timestamps  \\n but then also easily get data for temperature and humidity?  \\n Well, one way to do that is to use a common table expression  \\n and create a common table expression  \\n for this query right here.  \\n So to do that, we'll say with  \\n and let's call this sensors date times as  \\n and let's indent this a little bit  \\n and we'll close up that.  \\n So now we have something called sensor datetimes  \\n which we can use in another select statement.  \\n So we'll select something from sensors datetimes, okay.  \\n Have that spelled right.  \\n And let's say here set with ST.  \\n So the first thing I want to do  \\n is I definitely want to select everything from  \\n that's in sensor datetime.  \\n So I'll just use SD star for that.  \\n And now I want to generate some random numbers  \\n for let's start with temperature.  \\n So we're going to use the random function  \\n and random returns, a decimal between zero and one.  \\n And let's say, I want my temperatures to be  \\n say between zero and 30 degrees Celsius.  \\n So I'll multiply by 30.  \\n Now I specified temperature and humidity to be integers.  \\n So I'm going to take the floor function  \\n which just essentially drops off the decimal part.  \\n The counterpart is ceiling.  \\n If you want to, you know, have a decimal number  \\n and then go up to the next highest number  \\n that would be ceiling, but we'll just use floor here.  \\n And I'm going to alias that as temperature  \\n and we'll follow a similar pattern for humidity.  \\n We'll use a random number  \\n and let's say humidity  \\n and the up to 80%, and we'll say that as humidity.  \\n And so now what we're going to do is we're going to  \\n select everything from our sensor datetimes  \\n and then we're going to generate a random number  \\n for temperature and humidity for each row.  \\n So let's run this I'll double check commas,  \\n syntax looks correct.  \\n Let's give that a try.  \\n Okay, so let's take a look  \\n and as expected.  \\n So now what we have is we have our sensor IDs.  \\n We have all of our time series  \\n and now we have a temperature  \\n and humidity value for each row.  \\n So now we have our data, we've created our data.  \\n So the next thing we have to do is just simply save this  \\n or basically store it into our sensor measurement table.  \\n So for that, we can just use an insert statement  \\n and we can say insert into IOT schema.  \\n And the name of our table is sensor measurement, or MSMT.  \\n And then we wrap this in parentheses  \\n and wrap that in parentheses.  \\n And just for readability we'll indent this  \\n and we can just stop checking.  \\n That looks good and we'll execute this.  \\n Okay, so we have created our data.  \\n What you'll notice here is  \\n that we have about six and a half million rows inserted.  \\n So we have our data and we have it inserted into our table.  \\n So now we're ready to work with explain plan.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2427266\",\"duration\":166,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Analyzing a query with WHERE clauses and indexes\",\"fileName\":\"2874221_03_04_XR30_explain_where\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to analyze the performance of a basic query. WHERE clauses are common and SQL developers should understand how they are executed. Understand the impact of indexing on query performance.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5542777,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now let's take a look at using explained plan  \\n with a select statement that has a where clause.  \\n So to do that, we're going to explain select,  \\n let's just select star  \\n and we'll work with our time series data.  \\n So we'll say from, the IOT schema  \\n and we're going to use sensor measurement  \\n and let's say where sensor ID  \\n between 10 and 20, and let's execute that,  \\n and let's see what come up with.  \\n A couple of things to point out here.  \\n Here this top level operation, the append  \\n is sort of the outermost operation  \\n and then we can kind of drill down  \\n almost like in a modular way  \\n to the next set of operations.  \\n So the first operation under a pen, is a sequence scan  \\n and you'll notice it's scanning one of the partitions.  \\n Now this is the month one partition  \\n and below that there's another sequence  \\n scan scanning the month two partition.  \\n So these are two operations that can happen in parallel  \\n and Postgres can optimize and do things in parallel  \\n sometimes depending on configurations.  \\n But we want to notice here is we're doing a sequence scan  \\n and basically a sequence scan is basically,  \\n I'm going to look at everything in the table  \\n and see when I hit a row  \\n that satisfies a particular filter condition.  \\n Now at the top, we have a cost here  \\n which goes up to about 142, 000  \\n and it's an arbitrary like measure  \\n of computational load that Postgres uses.  \\n So the number itself  \\n or the units of measure, isn't all that important,  \\n it's more like from a relative perspective  \\n if you're comparing query plans  \\n you want to look at that number.  \\n Now we're doing a full scan  \\n or basically because we don't have any indexes  \\n so let's create an index and then see what happens.  \\n Now I'm using sensor ID in the where clause  \\n so I would create an index on that  \\n and let's call it IDX  \\n and this is on sensor measurement  \\n and it's on the sensor ID  \\n so we'll call it that on  \\n and then the table is IOT sensor measurement.  \\n And it's sensor ID that we're interested in.  \\n So let's execute the create index statement  \\n and now let's look at the explain plan.  \\n And what you'll notice is  \\n a significantly smaller number here.  \\n Let's see what's going on.  \\n Instead of a sequence scan  \\n we're actually scanning just the indexes.  \\n So this is how we can see whether  \\n or not indexes are helping us in particular queries  \\n is by just doing comparison,  \\n do an explain plan before and after creating the index  \\n and you can get a sense  \\n of how useful that particular index is.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2421483\",\"duration\":357,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Analyzing a query with a join\",\"fileName\":\"2874221_03_05_XR30_explain_join\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to analyze the performance of a query plan that uses a join. Discover when nest loop, hash, and sort-merge joins are optimally used.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12126952,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now Explain Plan is also useful  \\n for understanding how Postgres implements joins.  \\n So let's take a look at that.  \\n Now, the first step of course  \\n is we need another table to join  \\n to our sensor management table.  \\n So let's just create a simple little reference table  \\n for our sensor IDs.  \\n Let's make a table that gives a name for each ID  \\n and we'll keep it simple.  \\n We'll just call our sensors, we use the word sensor  \\n and then append the sensor ID and that'll be the name.  \\n So for that, I'm going to use the generate series again.  \\n So we'll do a select,  \\n select i from generate_series  \\n and we have 100 sensors so we'll go from one to 100  \\n and we'll alias that as i.  \\n Okay, so that gives us our list of 100.  \\n And we're going to use this  \\n as part of a common table expression.  \\n So I'll say with,  \\n let's call the sensor IDs as,  \\n and we'll say select as i,  \\n and we will select i  \\n as ID and then we'll create Sensor  \\n and we'll actually use Sensor with a space after it,  \\n we'll concatenate that sensor ID number.  \\n So concatenate i and cast it as a text.  \\n And let's alias that as sensor_name.  \\n And of course, we're going to be selecting this  \\n from our common table expression which has sensor_ids.  \\n So let's double check.  \\n Looks like the comma is in the right place.  \\n And so now what we have is a list of 100 IDs and 100 names.  \\n So that's great.  \\n So basically this just gives a table  \\n we can join to and we'll look up a name on it.  \\n So let's finish by now creating a table with this data.  \\n So we'll create table iot_sensors  \\n as  \\n with in parentheses, format that.  \\n Okay, so now we have a table called iot_sensors  \\n and I'm just going to jump over here real quickly to refresh.  \\n And let's see, we have our sensor measurements  \\n and then we have sensors.  \\n And let's just check our column names to make sure  \\n ID and sensor_name.  \\n So that looks good.  \\n All right, so we have a table now that we can join to.  \\n So now we're going to be doing an explain  \\n on a select statement.  \\n And what I want to do is I want to get a sensor name  \\n from the sensors table and then some measurements  \\n in the date from the sensor measurement table.  \\n So let's see, I'm going to select  \\n from iot.sensor measurement and let's alias that as sm  \\n and we're going to be doing a left join on iot.sensors  \\n and that's plural.  \\n And let's alias that as s,  \\n and we're going to be joining on the sensor measurement.  \\n We're going to use sensor ID and in our sensor table,  \\n and the corresponding column is ID.  \\n And so that's what our join looks like.  \\n And now let's put some columns in here.  \\n Let's get sensor name from that new table we just created.  \\n And from the measurement table  \\n let's get the measurement date time  \\n and temperature  \\n and the humidity.  \\n See if everything looks correct.  \\n So we have an explain,  \\n are from left join, okay?  \\n Check that.  \\n So what we have here, I'm going to scroll up over the text  \\n of the query so I can see what's going on here.  \\n Now, what you'll notice at the top,  \\n so the top level thing that's going on is a Hash join.  \\n So again, there are three different kinds  \\n of ways of doing join.  \\n There's a nested loop which works really well  \\n with small amounts of data, small tables.  \\n Hash join which works well when you have like large tables  \\n or you're scanning all the tables.  \\n And so we have a Hash condition here  \\n and I won't go into all of the details  \\n but we'll notice we're doing sequence scans  \\n and sequence scans tend to be expensive  \\n but we're joining on the whole table  \\n and we don't have any filtering down.  \\n So it's not unusual to see a full table scan.  \\n So here we have a join and it shows us  \\n how we're going to be able to join over an entire table.  \\n So let's make this a little bit smaller in terms  \\n of the amount of data that we're going to work with.  \\n And let's add a where clause  \\n and let's say where the sensor ID  \\n is equal to, let's pick 30.  \\n Now here, you will notice we've switched now.  \\n We're no longer doing a Hash join.  \\n And that's because we're working  \\n with a small amount of data.  \\n In fact, just one row from the sensors table.  \\n So we can switch over and use a nested loop.  \\n And so again,  \\n so we don't have to do something as complicated  \\n as a Hash join.  \\n So again, we can get a sense  \\n of which of the three techniques  \\n for implementing joins are being used by Postgres.  \\n So again, there's nested loop.  \\n That works well when you have small amounts of data,  \\n Hash joins when you're working with a lot of data  \\n and sort merge if you're working with a lot of data  \\n and you have a sort, you need to order the results.  \\n If you see yourself doing nested loop joins  \\n when you have a large amount of data  \\n that's a good indication that you might want to go back  \\n and look at your join criteria  \\n and also any use of indexes if possible.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2423377\",\"duration\":33,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Optimize a query using an explain plan\",\"fileName\":\"2874221_03_06_XR30_CH30_challenge\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Given a suboptimal explain plan, change the data model to improve query performance. This challenge tests your ability to analyze and tune a query.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":974071,\"solution\":false,\"welcomeContent\":null,\"challenge\":true,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (bright music)  \\n - [Instructor] Let's consider a challenge  \\n around query performance optimization.  \\n So imagine that a colleague has asked you  \\n to help them analyze a long-running query  \\n that they're working with.  \\n Now you take a quick look and you realize,  \\n okay, the query has three tables and two left joins.  \\n Two of the tables have over 500,000 rows  \\n and one table has 200 rows.  \\n So given that,  \\n how would you proceed to improve the query performance?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2424383\",\"duration\":99,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Optimize a query using an explain plan\",\"fileName\":\"2874221_03_07_XR30_SO30_solution\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore the solution to the query optimization challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2771851,\"solution\":true,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Now the first thing  \\n we'd want to do if we're trying  \\n to understand query performance is to run  \\n an explain plan on the query.  \\n That'll give us a breakdown of the steps  \\n that are actually being executed.  \\n Now, as you're looking at those steps,  \\n some things you might want to keep an eye out for are things  \\n like a full table scans or sequence scans,  \\n where we're scanning large amounts of data.  \\n That's often an opportunity for creating indexes.  \\n Also, you want to assess how the indexes  \\n are used with joins.  \\n So in particular, if you think that an index  \\n should be used on a particular join,  \\n and if it's not being used,  \\n then that may mean the index was dropped,  \\n or there's some other reason the index isn't being used,  \\n but it's something worth investigating.  \\n And we also want to look for opportunities  \\n to filter the dataset side.  \\n You know, a fast running query  \\n is a query that doesn't have to look at a lot of data.  \\n So if there are opportunities  \\n to reduce the amount of data  \\n that are pulled back from the large tables,  \\n that could be a help as well.  \\n And then finally, just in the off case  \\n the statistics are off,  \\n you could run the analyze command to make sure statistics  \\n are up to date.  \\n So those are just some of the solutions you can take  \\n to this challenge.  \\n Query optimization is a really in-depth area  \\n that we can dig into,  \\n and it's more than we can cover in this course.  \\n However, there is a course in the catalog  \\n on SQL query optimization and tuning.  \\n So I highly suggest you look into that course  \\n if you are interested in learning more  \\n about how to use explain plan,  \\n and how to how to really get better performance  \\n out of your queries.  \\n \\n\\n\"}],\"name\":\"3. Query Optimization\",\"size\":58259125,\"urn\":\"urn:li:learningContentChapter:2424386\"},{\"duration\":1642,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2423378\",\"duration\":186,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Extending SQL with user-defined functions\",\"fileName\":\"2874221_04_01_XR30_extending_sql\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn when to extend SQL with user-defined functions. User-defined functions can be used to implement specialized business logic in SQL.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5771646,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now we're going to shift our attention  \\n to extending SQL.  \\n And this is actually one of the more interesting topics  \\n with regards to advanced SQL,  \\n which is how do we get beyond what is given to us  \\n essentially in the box in the Postgres  \\n or whatever database we're using.  \\n How can we make it even more functional  \\n from a data science perspective?  \\n Well, the first thing to note is that of course,  \\n SQL provides many types of functions for operating on data  \\n and we use them all the time.  \\n But in terms of the universe of all possible functions  \\n we might need to work with from a data science perspective,  \\n what SQL provides is a small fraction.  \\n So there are many more functions  \\n we might want to be working with.  \\n So for example, SQL will provide aggregate functions for us,  \\n string manipulation functions,  \\n pattern matching, date/time, and geometric functions.  \\n And these are just a small number of examples.  \\n But we sometimes need custom functions.  \\n So let's think about say a retailer  \\n might have a customer loyalty program  \\n where they calculate different levels,  \\n like a one-two-three or a gold-silver-bronze  \\n based on how much a customer has purchased in the last year  \\n or the value or something like that.  \\n We also might have say some customers  \\n who are behind on payments  \\n and we want to sort of prioritize who do we focus on first.  \\n So we might want to have a priority score.  \\n In other cases, marketers or salespeople might want to predict  \\n how valuable is this customer to us  \\n over the lifetime of the relationship with the company?  \\n How much are they going to spend?  \\n Also, we might want to be able to say predict  \\n if we were a bank or a lender of some kind  \\n making predictions about what's the probability  \\n that this person will default on a loan?  \\n So when we're working with Postgres,  \\n there are five types of user-defined functions.  \\n There are query language functions,  \\n which are written in SQL.  \\n There are procedural functions,  \\n which are written in a procedural language called PL/pgSQL  \\n which is a combination of both SQL  \\n and something like a Pascal-like  \\n structured programming language.  \\n So if you're familiar with some of the older languages,  \\n ALGOL, Pascal, PL/pgSQL is similar to that.  \\n Now, another type of user-defined function  \\n is an internal function.  \\n Now, these are written in C  \\n and statically linked to the Postgres platform,  \\n the main engine.  \\n So this is something you only use  \\n if you are really working on the Postgres platform  \\n and you're trying to extend it.  \\n There are also C language functions  \\n that are available through shared libraries,  \\n so they're not necessarily linked in post compile time,  \\n but they're shared at run time.  \\n So that's another way to work with user-defined functions.  \\n But again, these are C-level functions  \\n and not something we typically use  \\n when we're say working with data science functions.  \\n There's also an extension called PL/Python  \\n which allows you to write user-defined functions in Python.  \\n And there are a lot of advantages to that,  \\n but I just want to note right off the bat  \\n that PL/Python is not a trusted language,  \\n which basically means you could really do some damage  \\n to the database if something's not quite right  \\n in your Python, so use PL/Python with caution.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2421484\",\"duration\":520,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"SQL query functions\",\"fileName\":\"2874221_04_02_XR30_sql_functions\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to write a user-defined function using SQL data manipulation language commands. These commands are used to implement specialized logic.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15730955,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now let's take a look at SQL functions.  \\n Now SQL functions are basically query language functions.  \\n So these are different from, say, writing a procedural code  \\n either in PL/pgSQL  \\n or in C  \\n or in Python.  \\n And so we're really writing in declarative language.  \\n So basically,  \\n we can execute an arbitrary list of SQL statements  \\n all at once in a single function.  \\n Now the thing to keep in mind about query language functions  \\n is that the last statement either has to be a select  \\n or the function has to be declared void.  \\n So if you're familiar with some procedural languages,  \\n you know, Java, C,  \\n sometimes, you can have a function  \\n which doesn't return anything,  \\n you declare it as void,  \\n which is basically means it's,  \\n you know, it's just a procedure,  \\n it has side effects,  \\n Similar for query language functions,  \\n you can declare it as void.  \\n Also important to note  \\n that although functions are typically functional,  \\n which means they return one value,  \\n that one value can be a set,  \\n so you can actually have multiple rows,  \\n and that would be by declaring that you are returning a set.  \\n Now there are the types of SQL statements  \\n that you can work with inside a procedure  \\n include the select, insert, update, delete.  \\n So your basic, you know,  \\n create, read, update, delete or CRUD functions  \\n are available to you.  \\n Now what's not available  \\n is anything involving things like transactions,  \\n where you'd like specifying a begin  \\n and then a commit or a rollback  \\n or a savepoint where you're doing like a checkpoint.  \\n You can't do any of those things in a function.  \\n Those are definitely side effects,  \\n but, like, from a procedural perspective.  \\n But they're not allowed.  \\n They're too big of a change within a database  \\n to allow them within a function.  \\n Now the way we create functions  \\n is with the create function statement.  \\n And no surprise here,  \\n it just creates a new function.  \\n I want to point out  \\n that Postgres has another way of creating a function,  \\n which is called create or replace function.  \\n And that's really nice,  \\n because if the function exists,  \\n it'll update the definition.  \\n And if it doesn't exist,  \\n it will create it for you.  \\n But I want to distinguish this  \\n from the drop and create function.  \\n Now if you drop a function and then create it,  \\n it sounds a lot like,  \\n well, that's the same as create or replace,  \\n but it's not.  \\n When you drop a function,  \\n you actually delete an existing function.  \\n And then you create a new function.  \\n Well, that new function essentially has a new identifier.  \\n And so if you referred to that function  \\n in rules, views, triggers,  \\n you need to drop those and recreate them,  \\n because now you need the new identifier  \\n that goes along with the new function.  \\n So again,  \\n if you are,  \\n just want to replace a definition,  \\n then use create or replace.  \\n If you really want to wipe it out  \\n and absolutely start from scratch,  \\n then drop function plus create is the way to go.  \\n Okay. Let's look at an example of a function  \\n where you might want to create.  \\n Now harmonic mean is a kind of average.  \\n It's one of the Pythagorean means.  \\n And basically, it's a reciprocal of the arithmetic mean.  \\n And we often see it used like with rates.  \\n So if you have a problem where you're,  \\n say, driving at 50 kilometers an hour for part of a trip,  \\n and then, you know, you're going back,  \\n and you're driving at 25 kilometers per hour,  \\n you know, what's the average rate?  \\n Harmonic mean is what will give you the, you know,  \\n sort of the correct average mile per hour  \\n across that whole time span.  \\n Now from a data science perspective,  \\n it's also used a lot with information retrieval  \\n in machine learning,  \\n particularly, when we combine precision and recall numbers.  \\n So we have two measures  \\n that we typically use in machine learning,  \\n precision and recall,  \\n in terms of classification problems.  \\n But sometimes, it's nice to work with just a single number  \\n that combines them both.  \\n Harmonic mean works really well for that.  \\n And in terms of information retrieval,  \\n it's often called the F-score or F1-score.  \\n Now let's imagine we are managing a hotel,  \\n and we have these two rates  \\n that we keep track of the bed occupancy rate  \\n and the room occupancy rate.  \\n Now these are really two different things,  \\n just like, you know, precision and recall,  \\n they're different things.  \\n We want to compute a harmonic mean of those.  \\n Well, we could simply spell it out  \\n by doing as we do here.  \\n We're going to round at the product of, you know,  \\n this division of a two times the bed occupancy rate  \\n times the room occupancy rate  \\n divided by bed occupancy rate plus room occupancy rate.  \\n And oh, by the way, let's cast that as a numeric.  \\n And let's name it as composite occupancy rate.  \\n So we could do that.  \\n We could write all that code in a SQL statement  \\n every time we need it to compute the harmonic mean.  \\n Alternatively,  \\n we could just create a function.  \\n So what we're doing here  \\n is we're specifying the create function statement.  \\n We give it a name called harmonic mean.  \\n It's a function,  \\n so we can have parameters.  \\n And in this case, we have two parameters, X and Y,  \\n both of which are numeric data types.  \\n We're returning a numeric type to type.  \\n And our definition is then captured  \\n in between the dollar sign dollar sign  \\n and the second dollar sign dollar sign.  \\n Basically, that dollar sign dollar sign is a way  \\n within Postgres that we can specify a literal string.  \\n So there's other ways to do it with quotes and things,  \\n but definitely,  \\n the convention is to use dollar sign dollar sign  \\n as the start of this string  \\n when working with create function.  \\n In essentially the body of the function,  \\n I'm saying select the round of the two times the product  \\n of the two parameters  \\n divided by the sum of the two parameters  \\n and cast that as a numeric  \\n and pass it back.  \\n The last statement  \\n after the closing dollar sign dollar sign,  \\n so I've closed off the string,  \\n I'm just specifying language SQL.  \\n So I could also specify language PL/SQL,  \\n or if I was working with some other language,  \\n I could specify that.  \\n But that is the, essentially, an example  \\n of how to create a function.  \\n So now let's jump over into Postgres  \\n and actually see in action.  \\n So basically,  \\n here I'm working in our iotschema.  \\n So let's say I've got a couple of measures  \\n in my central measurement table  \\n that I want to use harmonic mean for for some reason.  \\n So the first thing I'm going to do is say create function.  \\n And I'm going to give it a name,  \\n and I'll call this harmonic mean.  \\n And I'm going to use two parameters.  \\n I'm going to use X which is a numeric  \\n and Y which is also a numeric.  \\n And I want to tell it  \\n that this function is going to return a numeric value.  \\n And now I want to specify this as,  \\n so this is kind of like create views something as,  \\n this is create function something as.  \\n Now because we're working with strings here,  \\n I've got to put my dollar sign dollar sign in  \\n to say, okay, this is,  \\n I'm starting a quoted string.  \\n And I'm going to say select.  \\n And I know I need to round my number.  \\n And I'm going to use  \\n try two  \\n times my parameter X,  \\n times my perimeter Y.  \\n And I'm going to divide that  \\n by  \\n X plus Y.  \\n And I want to cast this as a numeric.  \\n And I want to round it to two decimal places.  \\n Now let me just make sure I got my parentheses correct.  \\n That closes correctly,  \\n and that closes correctly.  \\n Okay, great.  \\n Now I just need to close off my quoted string  \\n and tell Postgres I'm working with SQL  \\n and give it a run.  \\n Ah, what's happened here is  \\n that I have already defined this function in this schema,  \\n so what I should have done is written create or replace.  \\n Because essentially,  \\n what I want to do is update the function.  \\n Also, I want to point out in this error message.  \\n Note that it says the function harmonic mean already exists  \\n with same argument types.  \\n So it's not saying it simply it already exists,  \\n but it already exists with same argument types.  \\n That is a reference to a concept called overloading  \\n which we will discuss shortly.  \\n So I'm going to change this to create or replace function  \\n and execute it again.  \\n And now I succeeded.  \\n So now the next thing I want to do is,  \\n I just want to execute a select statement.  \\n I want to use this function.  \\n So let's select harmonic.  \\n And let's pass in the parameters of two and seven.  \\n And this is the only thing I want to execute,  \\n so I'll highlight that and execute.  \\n And here I get 3.11.  \\n Now this, of course, is different from the average.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2422414\",\"duration\":341,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Function overloading\",\"fileName\":\"2874221_04_03_XR30_function_overloading\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover how to use function overloading to expand the capabilities of a user-defined function. Overloading allows developers to apply functions to a wider range of use cases.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11032164,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Postgres supports the concept  \\n of function overloading.  \\n Now basically what this means is that a single function  \\n can actually have multiple definitions.  \\n Now, the way we do this is we  \\n use multiple create function statements,  \\n but those create function statements  \\n have different signatures or a different set of parameters.  \\n So for example, we could create a harmonic mean  \\n like the one we have created earlier,  \\n which takes in two arguments, both of which are numeric.  \\n We could also create a harmonic mean  \\n that takes in text values and then converts them.  \\n Function overloading is useful when the same function  \\n can apply or be applied to different data types.  \\n Now, one thing you want to keep in mind  \\n is you want to avoid anything that might be ambiguous  \\n in terms of how Postgres will interpret  \\n the particular signatures.  \\n So for example, if we called a function,  \\n let's call it ambiguous,  \\n and we passed in the value eight,  \\n well eight could be either an integer or a small integer.  \\n So if we had two create function statements  \\n which took in one parameter, one was an integer  \\n and one was a small integer,  \\n Postgres wouldn't necessarily know at the time  \\n it's processing that which of those signatures to use.  \\n So we want to make sure  \\n that we avoid those kinds of ambiguities.  \\n So let's take a look at an example.  \\n Now we've already defined a function called harmonic mean  \\n and now what I want to do is create another version  \\n or overload this function harmonic mean by specifying  \\n that someone can pass in the two parameters as text  \\n and we would simply change the definition accordingly.  \\n So in this case using text inputs,  \\n we'd want to convert the text into numeric types  \\n so we explicitly cast them as numeric.  \\n Now let's take a look at how we define overloaded functions.  \\n So I'm going to say create or replace  \\n function harmonic mean  \\n and we're going to specify the first perimeter as numeric,  \\n second as numeric.  \\n And we're going to specify  \\n that it returns a numeric string start indicator.  \\n And we'll say that we are going to select.  \\n And if you recall, the definition of harmonic mean  \\n is two times X times Y divided by X plus Y.  \\n And we want to round this to two decimal places.  \\n And just to be explicit, we can say  \\n we want to cast the results as numeric.  \\n And we'll just specify the end of the definition  \\n and of the actual function code  \\n and we have to tell Postgres which language we are using.  \\n We'll specify SQL.  \\n And let's just run that just to make sure  \\n I got that correct.  \\n So everything's correct there.  \\n Now what I can do is let's specify it with text.  \\n So here I'm going to just say create  \\n or replace function harmonic mean.  \\n So it's exactly the same name.  \\n There's no need to change that,  \\n what's changing is the parameter signature  \\n or what we put in the parameter list.  \\n I'm still going to call it X,  \\n but now I'm going to specify the data type as text.  \\n Y is a text.  \\n Now, I still want this to return a numeric  \\n so I'll specify numeric as the result.  \\n As dollar sign, dollar sign, and here I'm going to select.  \\n Now I'm going to do two times X, but X is a text  \\n so I need to cast that explicitly as numeric.  \\n And I'm also going to multiply by Y which again  \\n I have to specify explicitly as numeric.  \\n That's slightly easier to read.  \\n So that is the first part, that's our numerator  \\n so let's build our denominator here,  \\n which is simply X plus Y.  \\n And again, we'll specify a casting plus Y numeric.  \\n We'll round this up, round up to two.  \\n And we'll explicitly say numeric again.  \\n This is overkill,  \\n Postgres wouldn't have a problem figuring that out,  \\n but I'm putting it in in language is SQL.  \\n And let's see if I got that right.  \\n So this closes that,  \\n that closes correctly, that closes with the round.  \\n That looks correct so let's run that.  \\n So now we have both functions created.  \\n So now let's try select harmonic mean of 2.2 and 7.1.  \\n And we'll run that and we get an answer.  \\n Now let's add harmonic mean of quote 2.2 and quote 7.1.  \\n And let's run that.  \\n Now what you'll see is we're getting the same answer  \\n which obviously we would expect to do.  \\n So this is a little bit of a contrived case  \\n where we're passing in text values as numeric,  \\n but actually this can occur if you're doing a lot of like  \\n extraction load transform kind of work.  \\n So if you're working with say imported data  \\n or something like that,  \\n oftentimes you might have issues where you have to do  \\n either conversions or in this case,  \\n you can actually do function overloading  \\n to help you avoid some of the overhead  \\n of constantly converting different data types.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2425342\",\"duration\":242,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Function volatility\",\"fileName\":\"2874221_04_04_XR30_function_volatility\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore volatility categories and when to use each. Volatility specifications provide information to the optimizer which is used to implement the execution plan so function developers need to understand how to correctly specify volatility. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7542030,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Function volatility is something  \\n we need to consider with regards to optimizing  \\n the performance of our functions.  \\n Now, functions have a volatility classification  \\n and basically this classification is essentially  \\n like a promise or an indication  \\n of what kind of behaviors the function performs,  \\n or more practically, the kinds of things it won't do.  \\n This allows the optimizer to make certain assumptions  \\n about what optimizations are safe  \\n for that particular function.  \\n Now, there are three classifications:  \\n volatile, stable, and immutable.  \\n If you don't specify a classification,  \\n volatile is the default classification.  \\n A volatile function can perform any operation,  \\n including making changes to the database.  \\n So you can insert, you can update, you can delete.  \\n And this is because the optimizer doesn't make  \\n any assumptions about the function in terms  \\n of optimizing the way the code might run.  \\n And this is something to keep in mind,  \\n the function is reevaluated at every row  \\n the function is needed.  \\n So if you're applying this function  \\n to every row in a very large table  \\n and you haven't specified a volatility classification,  \\n it will be reevaluated at every row in that table.  \\n So, definitely use volatile functions  \\n and specify them as volatile when you have to,  \\n but use one of the other classifications if you can.  \\n Now, stable is the second classification.  \\n You cannot modify the database with a stable classification,  \\n so no deleting, updating, inserting.  \\n Another thing is it's guaranteed to return  \\n the same results given the same arguments  \\n for all rows within a single statement.  \\n So for example, if you have a select statement,  \\n and you're calling a function and that function  \\n is applied to many rows across the table,  \\n anytime a particular row has  \\n the same parameters as another row,  \\n they will always get the same result from that function.  \\n That's one of the things that we guarantee  \\n or promise to the optimizer  \\n when we specify a function is stable.  \\n And with stable, we get some optimizations.  \\n So it's definitely, if you can specify it as stable  \\n that's better than volatile in terms of performance  \\n when you're working with especially large data sets.  \\n Now immutable is the most restricted,  \\n and of course like stable, it cannot modify the database.  \\n We're guaranteed to return the same results  \\n given the same argument in all cases.  \\n it's not just restricted to a single statement.  \\n So anytime you call this function  \\n with say parameters two and three,  \\n you're always going to get the same answer.  \\n A function labeled as immutable basically tells  \\n the optimizer that it can just do as many optimizations  \\n as available for a function.  \\n So let's take a look at an example here.  \\n This is our harmonic mean function, which we've seen before.  \\n And you'll notice here in the last line,  \\n after we specify the language, which in this case is SQL.  \\n We're specifying the volatility classification,  \\n which in this case is immutable  \\n because the function is immutable.  \\n Under any circumstances if we have two parameters  \\n which are the same,  \\n and they are called at different times,  \\n they will always have the same result.  \\n Now there's some things you want  \\n to keep in mind about volatility.  \\n For performance purposes, you want to use  \\n the most strict volatility classification that's possible.  \\n That's going to give you the best performance.  \\n Now, any function with any kind of side effects  \\n that's altering something,  \\n that's changing the state of the database,  \\n that has to be volatile.  \\n Also, if a function has no side effects,  \\n but it uses another function.  \\n So for example, in our harmonic mean,  \\n if we made a call to another function,  \\n say like the random function or the time of day function,  \\n well those can change during the course  \\n of the execution of say a long statement,  \\n or random, it can change at any time  \\n and will change across rows,  \\n those functions need to be labeled as volatile.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2428255\",\"duration\":224,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"PL/Python functions\",\"fileName\":\"2874221_04_05_XR30_plpython\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to write a PostgreSQL user-defined function in Python. Python is widely used in data science and it is an option for developing user-defined functions in PostgreSQL.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6596738,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now, it is possible to write  \\n user-defined functions in Python when using Postgres.  \\n And we write Python by using the PL/Python language.  \\n So the PL/Python language is a Postgres extension.  \\n And as I mentioned,  \\n And what we would do  \\n is basically call the create extension command.  \\n And in this case,  \\n we could call the create extension command  \\n and then specify plpythonu  \\n and then either version two or version three.  \\n Now, let's look at some pros and cons.  \\n Now, there are definitely reasons to use Python.  \\n For many people who work in data science,  \\n we already know Python.  \\n It's a well-developed language.  \\n We have a lot of tools in it.  \\n We have ecosystems for developing around Python.  \\n We can also reuse code we've already written.  \\n We might be able to just copy and paste some stuff  \\n into our user deploying functions.  \\n And one of the other nice things is  \\n if we have to write procedural code for the database,  \\n you can avoid having to learn something like PL/pgSQL,  \\n which is an older procedural language.  \\n Now, there are also some cons or some reasons not to use it.  \\n So first of all, Python is not a trusted language  \\n within the Postgres database.  \\n And what that means is that the code in your functions  \\n could damage the database.  \\n There's no sort of guarantees.  \\n There's no guardrails protecting what you write  \\n from damaging the database.  \\n So that's what we mean by not trusted.  \\n Also, the installation can vary slightly  \\n by Postgres package.  \\n So yes, you always call create extensions,  \\n but depending on which package you use  \\n or which installation method you use,  \\n you may have to kind of hunt around  \\n and look for different libraries  \\n that might not be found by the create extension.  \\n So you definitely want to look at the documentation  \\n for your particular SQL package when installing this.  \\n Now, with installation,  \\n you need to use a user that has superuser privilege.  \\n Now, when you're creating PL/Python,  \\n you can specify the create extension  \\n and specify plpython3u if you want to use Python 3,  \\n or you can specify create extension pythonu  \\n and that'll use Python 2.  \\n Now, that's not recommended.  \\n Python 2 is no longer supported.  \\n So going forward,  \\n it's better to use Python 3 just in general.  \\n So here's an example of a function.  \\n Now, notice we used the create function syntax  \\n that we saw earlier.  \\n And here, we're going to specify a function called pymax,  \\n which is simply going to take the maximum of two integers.  \\n And you'll notice we have the typical function signature  \\n with create function, a function name,  \\n a list of parameters, a return statement,  \\n and then a data type as and then we have our string.  \\n The string part is what is written in Python.  \\n So here we just have a simple Python statement,  \\n so if x is greater than y, return x.  \\n Otherwise, return y.  \\n And then at the end, we specify this language  \\n and the language that you would specify  \\n is what you specified when you created the extension.  \\n So in this case,  \\n we're assuming we're working with Python 3.  \\n Now, some key points to remember about PL/Python.  \\n You need superuser privilege to install.  \\n So you might have to issue a command  \\n like alter user Postgres with superuser  \\n or you could substitute for Postgres some other user  \\n that you want to be installing this extension.  \\n And then once you have it installed,  \\n then others can write the PL/Python functions.  \\n They don't need to be superusers.  \\n And also you want to remember with PL/Python,  \\n arguments that you pass in,  \\n those are treated as global variables,  \\n and there are ways to modify them  \\n if you declare them as global within Python,  \\n but it's best to treat those arguments  \\n as read-only variables.  \\n And so if you do need to change those values or something,  \\n just copy them to a local variable,  \\n then make the changes there.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2422415\",\"duration\":34,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Write a user-defined function\",\"fileName\":\"2874221_04_06_XR30_CH30challenge\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Write a SQL user-defined function to implement a statistical function. This exercise tests your ability to develop user-defined functions in PostgreSQL.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1069595,\"solution\":false,\"welcomeContent\":null,\"challenge\":true,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] In this challenge, I'd like you  \\n to write a SQL function that returns a Boolean  \\n and the function will return true,  \\n if an input string is a palindrome.  \\n That is the string is the same  \\n in reverse as it is going forward.  \\n So, for example, the string abccba is the same  \\n in going forward and going in reverse.  \\n So, again, just write a SQL function  \\n that we will call isPalindrome.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2422416\",\"duration\":95,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Write a user-defined function\",\"fileName\":\"2874221_04_07_XR30_SO30_solution\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore the solution to the user-defined function challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3453713,\"solution\":true,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Narrator] This is our solution  \\n to the challenge to create a function called is_palindrome,  \\n which returns True when a string that's passed in  \\n is the same in reverse as it is going forward.  \\n You'll notice, we specify  \\n our create or replace function.  \\n Then we have our signature, followed by  \\n our function name, which in this case is, is_palindrome.  \\n We have a single parameter called STR,  \\n which is a type text and the function returns a Boolean.  \\n Then we specify the body of the function.  \\n Here, it's pretty straightforward.  \\n We're just going to use the reverse function.  \\n We're going to take the string, reverse it  \\n and see if it's equal to the string that was passed in.  \\n The other thing I want to point out  \\n is that because this function will always  \\n return the same value under any conditions,  \\n what if the parameter is the same at any time in the future?  \\n The results would always be the same.  \\n That means we can specify that this function is immutable.  \\n Finally, look, we can just run a test  \\n by selecting is_palindrome with an actual palindrome value.  \\n That should be True, and this value should be False.  \\n We can just execute that.  \\n As we see, the first argument actually is a palindrome,  \\n so we have a value of True returned.  \\n The second example, is_palindrome with foobarbaz, is not,  \\n so we have False return.  \\n That's basically the solution.  \\n \\n\\n\"}],\"name\":\"4. User-Defined Functions\",\"size\":51196841,\"urn\":\"urn:li:learningContentChapter:2422420\"},{\"duration\":2238,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2424384\",\"duration\":253,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Federated queries\",\"fileName\":\"2874221_05_01_XR30_federated_data\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover what federated queries are, when to use them, and their limits. Federated queries allow users to query data from multiple databases without copying or migrating data to a single database.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7677699,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now, there may be times we want  \\n to work with data that's not in the database,  \\n and we don't actually want to load the data either,  \\n but we'd still like to have access to it through SQL.  \\n In that case,  \\n we could use something known as federated queries.  \\n Now, federated data is data that's outside a database,  \\n but is still accessible by SQL.  \\n This is made possible by a SQL standard called SQL/MED,  \\n or management of external data.  \\n And it defines a number of different abstractions,  \\n but the sort of key ones  \\n are foreign data wrappers and datalinks.  \\n Now, a foreign data wrapper is a specification  \\n and a module or a package that allows us to view data  \\n that's outside the database.  \\n Datalinks compliment foreign data wrappers.  \\n They provide functionality we expect to have in a database  \\n like integrity, functions, recovery mechanisms,  \\n and authorization mechanisms.  \\n So, the way we create a federated data source is first  \\n we create an extension called postgres_fdw.  \\n Now, an extension is a module which is part of Postgres,  \\n but it is not installed by default.  \\n So when we want to install an extension,  \\n we use the CREATE EXTENSION command,  \\n and then we specify the name of the extension  \\n we want to install, which in this case is postgres_fdw.  \\n Next, if we want to create a foreign data wrapper  \\n for another database,  \\n we can use the command CREATE SERVER.  \\n And here I'm specifying a name called external_db_server.  \\n I need to specify what kind  \\n of foreign data wrapper I want to use.  \\n In this case, I'm going to use the postgres_fdw,  \\n and then I want to specify my options.  \\n Not surprisingly, the kinds of things we specify  \\n in the option are the kinds of things  \\n you would expect to see in a connect string,  \\n like name of a host and a database name.  \\n Now we also want to be able to map users.  \\n So if we're going to get data from another database,  \\n we're going to be getting that data using the roles  \\n or the permissions assigned to some user there.  \\n So the way we do that is we're using  \\n a CREATE USER MAPPING command,  \\n and we're going to map the current user  \\n to the server we just created, external_db_server.  \\n And we're going to specify in the options  \\n a username and a password.  \\n That's how we will access the data on the database.  \\n So, it's through this user, in this case analyst,  \\n which is a user on the external database,  \\n and we're going to be working through the analyst user  \\n to actually get the data out of the database  \\n when we're querying that external data.  \\n Now, another thing we can do is we can map  \\n a schema from an external database.  \\n So for example, let's say we have  \\n an e-commerce application that has some sales information  \\n and we want to reference that data,  \\n we can create a schema on our database.  \\n Let's call it external_sales.  \\n And then we import the foreign schema sales  \\n from the server that we created.  \\n Now again, when I say from the server we created,  \\n what I'm referring to is the data structure  \\n that we have on our Postgres database  \\n that references the external database.  \\n And I'm going to import that data into my schema  \\n that I just created, external_sales.  \\n Now, I can also create a foreign table.  \\n So let's say I have a log file that's CSV formatted.  \\n I can use the CREATE FOREIGN TABLE command  \\n and give the table a name.  \\n I'm going to call it page_visit_log,  \\n and it's a very simple table.  \\n It just has a timestamp, a username, and a webpage.  \\n And I can specify that as SERVER log_data  \\n and specify OPTIONS.  \\n So here, because I am working with a file,  \\n the options I'm going to specify are related  \\n to how I would access a file.  \\n So for example I need to specify  \\n the file name and the format.  \\n So, the CREATE FOREIGN TABLE command is a way  \\n of easily working with say structured data  \\n that's in a log file or some other data source,  \\n but is outside the database,  \\n and you would rather not, for whatever reason,  \\n bring that data into the database.  \\n This is how you can work with that data using SQL.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2422417\",\"duration\":278,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Bloom filters\",\"fileName\":\"2874221_05_02_XR30_bloom_filters\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover how to use the probabilistic filter on large data sets. Probabilistic filters can significantly reduce the time required to execute a SELECT query.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8421567,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now often when we work  \\n with very large data sets  \\n we often will trade accuracy for speed.  \\n So for example, we might sample a dataset  \\n and then do calculations over that sample  \\n and assume that the overall population  \\n has roughly the same measures.  \\n Well that's a probabilistic method and they're approximate.  \\n Another probabilistic method is known as a bloom filter  \\n and we can use those to create indexes  \\n which can be highly efficient in some cases.  \\n So a bloom filter is a space efficient method  \\n for determining set membership.  \\n So for example, if we need to find out  \\n which data block contains a particular piece of data  \\n or we have a complex query with multiple conditions  \\n in a where clause such as  \\n finding customers who live in a particular state or city  \\n and have been customers for less than a  \\n certain period of time and are delinquent on payments.  \\n Now, a blue filter is a lossy representation of data.  \\n So some compression methods are lossy.  \\n That means you can take the compressed version  \\n and then recreate it in an exact way the original source.  \\n Now bloom filters are similar  \\n in the sense that they're a probabilistic data structure.  \\n Now, with regards to the lossiness  \\n what that means is it may produce false positives.  \\n So a bloom filter is basically a boolean.  \\n Does this element exist in this set?  \\n And by false positives,  \\n we mean that the bloom filter may indicate,  \\n yes this element is in the set, when in fact it's not.  \\n So that can occur with bloom filters.  \\n However, we never get a false negative.  \\n If a bloom filter indicates that an element  \\n is not in a set, you know for certain it's not in that set.  \\n Now, the relative accuracy of bloom filters  \\n is a function of how many bits are used in the filter.  \\n And what people have found is that oftentimes  \\n about 10 bits per element will yield about  \\n a 1% false positive rate.  \\n Now a blue filter index is a Postgres extension.  \\n So that means it's part of Postgres  \\n but it's not installed by default.  \\n And bloom filters are particularly useful  \\n when a table has many attributes  \\n and we filter in our queries based on  \\n many different combinations or attributes.  \\n Now it's important to know  \\n that B-tree indexes are faster typically  \\n but they may require more indexes.  \\n So for example, if you have a table with 20 columns  \\n you can create a bloom filter,  \\n a single bloom filter with all of those attributes  \\n and get similar behavior that you would  \\n if you had created 20 individual B-tree indexes  \\n one for each of the columns.  \\n So the B-tree index may be faster  \\n but you're using up much more space.  \\n Now there are some limitations to keep in mind.  \\n Two in particular first is that the bloom filter indexes  \\n support only the equality operator  \\n and it only works with two data types,  \\n integers, four byte integers and text data types.  \\n If you need inequality or ranges or conditions  \\n in addition to equality  \\n then you'd probably want to use B-tree indexes instead.  \\n Now let's take a look at how we create a bloom filter index.  \\n First thing we need to do is create the extension.  \\n So basically install the bloom filter module  \\n and we use the create extension command for that.  \\n And then we just use the create index command  \\n but we specify a using clause.  \\n So for example, we might want to create  \\n an index on a table that has some locations  \\n and we might call the index IDX location bloom.  \\n And that index is being created on a table called locations.  \\n And we're using bloom, which is specification.  \\n We're using a bloom index  \\n and we're going to have three columns  \\n in that index, city, state, province and country.  \\n Now we can also specify a with clause  \\n and specify the length of the bloom filter  \\n and the longer the length  \\n the higher the accuracy of the bloom filter  \\n in the sense of reducing the number of false positives.  \\n Now because bloom filters have false positives,  \\n you can't assume that all of the data returned  \\n will actually satisfy the rule.  \\n So for example, you might have a statement such as,  \\n where attribute one equals A,  \\n attribute two equals B and attribute three equals C.  \\n It is possible to have rows returned and a result set  \\n that actually don't meet that criteria.  \\n So you want to always double-check and make sure  \\n that the results that are returned by a bloom filter  \\n actually meet the criteria that you're interested in.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2428256\",\"duration\":383,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Hstore for key-value pairs\",\"fileName\":\"2874221_05_03_XR30_hstore\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to model key-value pairs in PostgreSQL. Hstore provides a more efficient way to store key values in tables than using a normalized model.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12794479,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now, Postgres has features  \\n that may be more properly considered in the realm of NoSQL,  \\n including key-value stores  \\n and support for document structures using JSON.  \\n So let's first take a look at hstore.  \\n hstore is a data type for storing sets of key-value pairs.  \\n Now, we use hstore by first creating an extension.  \\n And an extension is a package or module  \\n that's part of Postgres but not installed by default.  \\n So we essentially install it  \\n by using the CREATE EXTENSION hstore command,  \\n and then we can create cables as we would normally.  \\n And for one of the comp source, several of the comps,  \\n we could specify the data type as hstore.  \\n Then when we insert data, we basically insert a string  \\n which is a list of key-value pairs.  \\n Now, why would you use hstore?  \\n Well, it's useful  \\n when it helps when keys are not predefined.  \\n So that is, the keys in the list can change arbitrarily.  \\n So if you have a large number of attributes  \\n and many of them are not used,  \\n so if you have sparse data, hstore may be a good option.  \\n Another advantage  \\n is you don't have to change the column definition  \\n to accomodate new columns  \\n because your column can be represented as a key-value pair  \\n in that single hstore column.  \\n Another advantage is that GIN and GiST indexes can be used  \\n to index all the keys  \\n so you can still have performant operations.  \\n So, for example, one example would be a catalog,  \\n so items in the catalog might have different attributes.  \\n So, for example, an appliance may have  \\n a power usage, a weight, height, depth dimensions,  \\n whereas a piece of clothing might have  \\n a size, color, material as attributes,  \\n and all of those can be represented as key-value pairs.  \\n Now, there are some limitations with hstore.  \\n First of all, all keys and values are stored as strings.  \\n You get no other data types.  \\n It does not support hierarchical data structures.  \\n Now, if you need hierarchical data structures,  \\n JSONB or XML data types are better options.  \\n Let's take a look at how we would use hstore.  \\n So I'm going to create a table called books,  \\n and let's have an id which is a serial  \\n and for a primary key,  \\n and we'll have the title of the book,  \\n and that will be of type text,  \\n and then we'll have a list of attributes  \\n which is of type hstore.  \\n Now, if you haven't installed hstore,  \\n you'll want to use the CREATE EXTENSION command hstore,  \\n and I'll just run this.  \\n I already have it installed  \\n so I'll get a little error message  \\n saying it's already exists.  \\n But if you haven't installed it,  \\n it would just give you a success message.  \\n So we'll get rid of it.  \\n We don't need that, so get rid of that.  \\n And your CREATE books, id serial primary key, looks good.  \\n We'll create the table.  \\n So now let's see how we would insert data into this table.  \\n So let's INSERT INTO books,  \\n and I just want to insert title and attributes  \\n because id is a serial.  \\n It will be assigned automatically.  \\n And we'll have our VALUES,  \\n and our VALUES will be the title of the book.  \\n Let's call the title SQL for Data Science.  \\n That's one value.  \\n And then our other value for attributes,  \\n let's have some attributes such as language,  \\n and we'll say that's in English.  \\n We use the equal and the greater than sign,  \\n and we'll specify English as the value.  \\n And let's say we have a page_count,  \\n and let's say that's 500.  \\n Can't forget that, okay.  \\n page_count 500.  \\n And let's say the publication year is equal to 2021.  \\n And, okay, so that inserted a row.  \\n Let's start another one.  \\n Let's just copy this and kind of mix it up a little bit.  \\n We'll change things slightly.  \\n Let's say we have SQL for Data Science 2,  \\n and the page_count is 600, the language is still English,  \\n and the pub_year is, let's say, 2022.  \\n So let's insert that.  \\n That's also inserted.  \\n And now let's do a SELECT statement,  \\n and let's simply SELECT * FROM books  \\n and see what it looks like.  \\n And what you'll see  \\n in the attributes column, the values appear as a string,  \\n but they don't behave like strings.  \\n So let's see, for example, how to work with the attributes.  \\n For example, if we wanted to,  \\n say, list the title and page count,  \\n we could do that by saying SELECT title.  \\n Now, we want page_count.  \\n Well, page_count is in the attribute's hstore  \\n so we need to specify attributes.  \\n And then here we don't use an equal sign,  \\n we use the minus sign or dash,  \\n and then the greater than sign,  \\n and then we specify what we're interested in,  \\n and we're interested in the page_count attribute,  \\n and we want to collect that from books.  \\n Now, notice I had to put these in quotes,  \\n page_count in quotes.  \\n So if I execute this,  \\n I should see title and just the page_count,  \\n which is what I have.  \\n And I could also alias this  \\n for something like pc for page_count  \\n and get something like that.  \\n I can also use attributes in a WHERE clause.  \\n For example, I could say WHERE attribute,  \\n WHERE attributes,  \\n let's see, page_count equals 500.  \\n Notice, I can't treat that as a numeric.  \\n I'm using that, making that a text value,  \\n and then I can use SELECT there.  \\n Now, if I had just put 500 here,  \\n I would get an error because it's not converted.  \\n So when you're working with values  \\n that are stored in hstore,  \\n you want to make sure to always treat them as strings.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2426335\",\"duration\":514,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"JSON for semi-structured data\",\"fileName\":\"2874221_05_04_XR30_json\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn to model semi-structured data in PostgreSQL. JSON allows for the use of document model storage in addition to relational storage.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17485347,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now PostgreSQL provides data types  \\n for supporting JSON,  \\n which is ideal for semi-structured data.  \\n So with PostgreSQL, we have options to use both relational  \\n and NoSQL type features.  \\n Now JSON is particularly good  \\n at modeling document databases.  \\n Now, document databases are used  \\n when a use case demands a flexible schema,  \\n nested structures,  \\n and the ability to query an index keys  \\n throughout the structure.  \\n Since PostgreSQL 9.2, we can have both relational  \\n and document structures in a single database.  \\n So here's an example of a document structure.  \\n Let's imagine we have a table called Customer Summary  \\n and we have the name of a customer, their address,  \\n and then some information about purchase history.  \\n Well, each of those could itself be a complex structure.  \\n So for example, name could be a pair,  \\n first name and last name.  \\n Address could be made up of a street, city,  \\n province, postal code.  \\n And finally, the purchase history could say,  \\n be an array of annual purchase values,  \\n so how much did that customer purchase each year,  \\n and also a lifetime value estimate,  \\n which might be just a scale or a single numeric.  \\n Well, both JSON and JSONB would work well for that.  \\n Both data types validate that a string is valid JSON,  \\n both allow querying and JSONB supports indexing  \\n of elements in a JSON structure.  \\n Now since 9.2, JSON represents the actual JSON structure  \\n in plain text.  \\n But starting in 9.4, we have the option of using JSONB.  \\n Now the B stands for better,  \\n and part of the better is that it's a binary representation  \\n and it's more efficient than the JSON data type.  \\n So JSONB supports nested JSON structures.  \\n We can use GIN indexes with them  \\n which index all the key value pairs in a structure.  \\n Now in general, if you're trying to choose  \\n between a key value hstore  \\n or JSON, which stores JSON and plain text, or JSONB,  \\n almost always the best option is JSONB.  \\n Okay, let's work a little bit with that JSON structure.  \\n First thing I want to do is create a table  \\n and let's call this customer_summary.  \\n And let's use id which is a serial  \\n as our primary key.  \\n And let's call our JSON column customer_doc,  \\n and that will be a JSON.  \\n And we're going to use JSONB since that's more efficient.  \\n And we will create that table.  \\n So now we have a table called customer_summary.  \\n It has an id as a primary key.  \\n And it has a single other attribute, which is customer_doc.  \\n So let's insert some data into that table.  \\n So I'm going to insert into customer_summary,  \\n and I'm just going to insert the customer_doc  \\n because the serial value will be set automatically.  \\n And we'll specify the values.  \\n And the values we're going to be specifying,  \\n we are going to specify a JSON structure.  \\n So that will be a string  \\n which is delimited using the curly brackets.  \\n And within that,  \\n we're going to have our three high-level structures,  \\n which is customer_name and address  \\n and purchase_history.  \\n Now, each of these, these are keys,  \\n so it's going to be followed by a value.  \\n And in JSON,  \\n we use the colon to separate the key from the value.  \\n Now the customer name, you'll remember,  \\n is also a structure  \\n and it includes a first_name  \\n and a last_name.  \\n And again, we're going to specify a value for each key.  \\n So let's say the first_name of this customer is Alice  \\n and the last_name is Johnson.  \\n And we need to put our commas in,  \\n put a comma there, put a comma there.  \\n And so now we've specified customer_name.  \\n So now let's specify address.  \\n So again, we're going to build another JSON structure.  \\n And an address has a street address,  \\n we'll just call it street though.  \\n It has a street,  \\n a city  \\n and let's say state, typo there, correct that.  \\n So those were our three keys.  \\n Now we specify the values,  \\n the street, let's say it's 5432 Port Ave,  \\n and the city is Boston  \\n and the state is Massachusetts,  \\n which we can use a two letter abbreviation MA for that.  \\n And so now we've specified our address as a key,  \\n and this is the value of that key,  \\n so I'll put a comma after that.  \\n And now the last thing we want to specify is  \\n purchase_history.  \\n And purchase_history, as you recall, has two elements.  \\n And that is annual_purchase_value  \\n and the other key is lifetime_value.  \\n And so we have our keys, we'll specify a value for each.  \\n Now the annual_purchase_value,  \\n let's assume it's an array  \\n and the first element of the array is the total value  \\n of the customer's purchase their first year as a customer.  \\n So let's say that's $100.  \\n And the second is the second year,  \\n let's say that's 200.  \\n And let's say at year three, it went up to 350, okay.  \\n So that completes our array of values.  \\n So in addition to the nested structures  \\n we can have with JSON,  \\n we can also have a RASON.  \\n And for lifetime_value, we wanted just a scalar.  \\n Let's say we estimate the lifetime value will be 1500.  \\n Okay, so that closes off purchase history.  \\n And then we need to close off customer_doc, which we do,  \\n and we'll close that.  \\n Actually, we need to put it right there and now close off.  \\n And now let's run this.  \\n Let's just do a quick check for missing commas.  \\n Nope, looks good,  \\n so we will execute that insert statement.  \\n That's an extra in there.  \\n Let's execute again.  \\n There we go.  \\n So we now have one row in that table.  \\n And let's clear this out  \\n and work with some select statements now.  \\n Now first, let's see what the data looks like.  \\n So I'm simply going to do a select star,  \\n customer_summary.  \\n And as we expect, we see the JSON structure  \\n and it contains what is essentially text values  \\n or strings of attribute value pairs  \\n and some of them are hierarchical.  \\n Also notice that the way  \\n that the key value pairs are ordered in the database  \\n is not necessarily the same way  \\n that we entered them using the insert statement.  \\n Now, if I want to just look at the customer_doc column,  \\n I could specify customer_doc  \\n and that gives me the entire JSONB structure.  \\n Now I could also essentially look inside  \\n by using the dash arrow.  \\n And what that will do is  \\n it will return the value of some key.  \\n So let's look at the customer_name and execute that.  \\n What you'll notice, it returns just the customer_name.  \\n Now the operator here I'm using is a single dash  \\n and then an arrow or greater than sign.  \\n Now, when you use that,  \\n that returns the results as a JSONB type.  \\n You'll notice there it's a JSONB type.  \\n And one reason we might want to do that  \\n is because we want to treat that JSONB structure  \\n as a hierarchical thing  \\n and we want to extract something from that.  \\n So for example, if we wanted to extract a first_name,  \\n we could do that.  \\n Now the first name is going to be a text.  \\n So I'm going to specify first_name  \\n and let's run that.  \\n And what you'll see here, we get back a text item.  \\n So the result is text and it's first_name.  \\n So that's how we reference items  \\n within the particular structure.  \\n And of course you can use the same syntax  \\n in the where clause as well.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2422418\",\"duration\":719,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Hierarchical data and ltrees\",\"fileName\":\"2874221_05_05_XR30_ltree\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to use the ltree extension to efficiently store hierarchical tree data and use specialty operators to perform matches over hierarchies.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":22993358,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Occasionally, we need to work  \\n with hierarchical data.  \\n There are different ways of working with hierarchical data  \\n and one is to use an extension in Postgres called ltrees.  \\n So, here's an example of a simple tree  \\n and oftentimes one of the things  \\n we need to do with these trees is work with paths.  \\n So for example, if I were to start at the root  \\n and go from node A, which is the root, to node B,  \\n and then to node D, and then to node H,  \\n we'd have a path from the root to one of the leaves.  \\n And we can depict that as a string of characters,  \\n such as A.B.D.H.  \\n Now, we can model this in different ways.  \\n A common way to do this is to use a table  \\n that has one row per node.  \\n So for example, in this table, node A is the root,  \\n so it's parent node is null.  \\n A is the parent of B and C,  \\n so what we see in the parent node column is the foreign key  \\n or the ID of the parent.  \\n So in the case of B and C, that would be ID one  \\n because node A is the parent.  \\n In the case of D and E, both of those are below B,  \\n and so B is the foreign key in the parent node table.  \\n Now, when you're querying hierarchical data,  \\n we often use recursive common table expressions  \\n but if you're working with Oracle,  \\n you can use a Connect By clause in your SQL query.  \\n Now, another way to represent paths  \\n is to use a pattern called materialized paths.  \\n In this case, each node in the tree is in its own row  \\n and each row representing a node has a path column.  \\n So we're no longer have a parent column,  \\n that's essentially been changed to a path column,  \\n and the path column contains a string  \\n which represents the path from the current node  \\n back through all of the ancestors to the root.  \\n And the way we query this and work with the path  \\n is by using string function.  \\n So for example,  \\n we might use regular expression, pattern matching,  \\n or use sub strings to match something  \\n in that particular path.  \\n Now, as you can imagine,  \\n if we're working with very large data sets  \\n and doing a lot of string operations  \\n across all many different rows,  \\n it can be rather inefficient,  \\n because for example, without indexes,  \\n we have to apply operations repeatedly over each row.  \\n So, wouldn't it be better if we had a mechanism  \\n that has allowed us to work with materialized paths,  \\n but gave us the advantages of indexes.  \\n And in Postgres, we can do that  \\n and that's by using the ltree extension.  \\n So, the ltree extension is part of Postgres,  \\n and it's used for working with trees and paths.  \\n It comes with many operators  \\n which we can use for operating on and comparing paths.  \\n It supports both B-tree and GiST indexes.  \\n Although GiST indexes are typically favored  \\n because you can use more operators,  \\n or more operators are available with the GiST index.  \\n So, some of the common ltree operators  \\n that we'll often work with are the ancestor  \\n and descendant pattern matching,  \\n the concatenate for concatenating tree paths,  \\n and the Tilda which basically answers the question,  \\n does an ltree match an ltree text query?  \\n So this is a basic pattern matching operation.  \\n And as I mentioned, there are quite a few operators.  \\n There are about 20 more in addition to these four.  \\n So I suggest if you'd like to learn more  \\n about ltree operators to consult the Postgres documentation  \\n on the ltree extension.  \\n Okay, so let's work with ltrees.  \\n So the first thing I'm going to do is create a table  \\n and let's call it something like paths_to_nodes,  \\n and  \\n we'll follow a pattern of having an ID  \\n which is a serial as our primary key,  \\n and we will have a node,  \\n which is a text type,  \\n and we will have a path which is an ltree.  \\n Now, I have already installed the ltree extension.  \\n So, I don't need to specify, create extension ltree.  \\n So I'm just going to execute that.  \\n Now, if you haven't installed the ltree extension  \\n then you can simply add the command, create  \\n extension  \\n ltree,  \\n and you can execute that.  \\n Now I'll execute it  \\n and Postgres will just tell me it's already installed.  \\n So it already exists so I don't need to do that.  \\n But first time you're using it,  \\n you want to put that create extension in.  \\n So we have our table,  \\n let's create an index on this table.  \\n And we'll call it  \\n idx_paths_to_nodes,  \\n and we're going to create this on paths_to_nodes,  \\n and we're going to specify a using clause  \\n because we want to use a GiST index here and not a B-tree,  \\n which is the default if we don't specify a using clause,  \\n and we want that GiST to be applied to the path column.  \\n So I will just execute that.  \\n Okay, so now we have our table.  \\n We have a GiST index on our path column.  \\n So, I've previously saved these insert statements  \\n so I'm just going to paste them in to save time.  \\n And so now we have data, so just take a look at it.  \\n Paths_to_nodes,  \\n and we'll execute that,  \\n and let's take a look.  \\n Okay, what would you expect?  \\n 11 rows, each with paths from the root  \\n down to the various leaves.  \\n So the first thing I want to do,  \\n I want to use the ancestor operator.  \\n So I'm going to select paths_to_nodes.  \\n I'm going to add a where clause here.  \\n And let's say, I want to find all of the descendants  \\n that have  \\n A.B at the beginning of their path.  \\n And to do that, I'm going to specify the path,  \\n which in this case has just A and B,  \\n and then I'll use the at greater than sign operator,  \\n which is the ancestor,  \\n and I want to apply that operator to the path column.  \\n So I will execute that,  \\n and what we'll see here is everything that is returned  \\n has the pattern A.B in the path  \\n at the beginning of the path.  \\n So that's pretty simple.  \\n And once we do this, we can do things like count the nodes.  \\n So things that you would normally do  \\n with other aggregation patterns or other things like that,  \\n you can of course do here as well.  \\n So now, let's say I want to kind of loosen up the criteria  \\n and I want to search for anything where B is an ancestor.  \\n Well, one way to do that  \\n is instead of using the ancestor path,  \\n we'll use a pattern matching operator, which is the Tilda,  \\n and now I can specify, for example,  \\n I want everything with a B in it.  \\n I can  \\n specify star  \\n dot B dot star.  \\n Now this should return anything that has a B in the pattern.  \\n We'll get rid of the count, we'll go back here,  \\n and here again, we have anything which has B in the pattern,  \\n but there needs to be one B.  \\n We can add a specification using a curly bracket  \\n and then specify one.  \\n So that means any number of ancestors can proceed B,  \\n we want to have one node following the B in the pattern.  \\n Now let's see what happens there.  \\n Well notice, we lost A.B itself  \\n but now we have A.B.D and A.B.E, which is what we'd expect.  \\n Now, we can also use a zero-based operators,  \\n and we could, for example say,  \\n we want to have at least zero and at most two.  \\n So we can specify both a min and a max here.  \\n So let's do that.  \\n So we should see this first row shows just A.B.  \\n Now, so there are zero nodes after the B.  \\n The other rows in the table show at least one  \\n and up to two nodes in the path.  \\n All right, let's do a little work  \\n with concatenation operator.  \\n Let's clear this out.  \\n And  \\n first of all, let's just start,  \\n we'll build this query incrementally.  \\n So we'll select, let's see,  \\n we're going to select from paths_to_nodes,  \\n and I'm going to alias set as p1.  \\n And,  \\n let's select from p1,  \\n the ID, the p1 node  \\n and the p1 path.  \\n And let's add a where clause.  \\n And let's say where  \\n p1 path  \\n matches  \\n the ltree query  \\n star dot B dot star.  \\n So the one we just used.  \\n So we should get results  \\n similar to what we've seen before  \\n when we're querying for a star B star.  \\n Okay, so that's working as expected.  \\n Now, what I want to do is I want to concatenate  \\n some other sub trees to those rows.  \\n So let's build another query,  \\n and in this case, I'm going to use select,  \\n and I'm going to be selecting from same table,  \\n paths_to_nodes, but this time I'm going to alias it as p2,  \\n and so I want to select, we'll select star.  \\n Here, we'll keep it simple from p2  \\n and I want to say  \\n where  \\n path  \\n matches a query,  \\n such that there is a C in the path.  \\n Now, let's just execute this select statement,  \\n and I'll do that by highlighting it.  \\n Okay, as we expect,  \\n everything that is returned has a C in it.  \\n Okay.  \\n So now let's build these two together.  \\n So we've got our two building blocks.  \\n I'm going to copy this select statement  \\n and move it up here into a with statement.  \\n So I'm going to say with, we'll call this paths_to_concatenate,  \\n or simply to concat as.  \\n So I'm creating a common table expression,  \\n and I'm going to paste in what I had below,  \\n and I'll just indent this a little bit for me to build them.  \\n Okay, so what I've done here  \\n is I've created a common table expression,  \\n and I'm calling paths to concatenate the table  \\n or that's the relation where there's are the Cs in the path.  \\n And what I'm wanting to do now is I want to join p1  \\n to  \\n paths_to_concat  \\n p2.  \\n Now, I want to do a Cartesian product  \\n so I'm not going to specify a joint clause.  \\n So each row in p1 is going to get matched up  \\n with each row in p2.  \\n And we'll leave the paths the same.  \\n So again, let's just review this one more time.  \\n We have a common table expression  \\n which allows us to build this query in a modular way.  \\n So all of the paths that have C in the path  \\n are going to be in the relation called paths to concatenate.  \\n I'm going to use paths to concatenate  \\n to do a Cartesian product with paths_to_nodes  \\n and I'm going to  \\n concatenate  \\n p1  \\n path  \\n to  \\n p2  \\n path.  \\n So this is how I'm going to combine the B paths  \\n with the C paths.  \\n So let's execute that query, and see what we have.  \\n What we'll notice here is that all of the rows,  \\n starting with the A.B paths,  \\n and then afterwards we have C paths.  \\n So for example, we have A.C, A.C.F, A.C.G.  \\n So this is how we can build ltrees through concatenation.  \\n Now, you'll notice  \\n that the concatenation operator is a double pipe,  \\n which is the same as what we use with strings,  \\n but this column that is being returned  \\n is actually ltree type, it's not a string.  \\n So we have all the advantages  \\n of having the ltree entry indexed.  \\n So, ltrees look a lot like strings,  \\n and even the operator, like the concatenation operator,  \\n looks familiar and the Tilda operator looks familiar,  \\n if you're familiar with working with strings, but again,  \\n there's potentially a significant performance difference  \\n by using ltrees instead of strings.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2427267\",\"duration\":33,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Design a table to support unstructured data\",\"fileName\":\"2874221_05_06_XR30_CH30_challenge\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Given a set of business requirements, determine the best data types to use for semi-structured data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":962234,\"solution\":false,\"welcomeContent\":null,\"challenge\":true,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (light techno music)  \\n - [Instructor] Okay, here's a challenge  \\n around data modeling and design choices.  \\n Let's imagine you're working with an ecommerce company  \\n who wants to create a new catalog of products.  \\n Now different kinds of products  \\n are going to have different attributes  \\n and it's critical  \\n that users be able to query and filter on any attribute.  \\n And of course, performance is a key consideration.  \\n So how would you design a table  \\n to support this type of product model?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2423379\",\"duration\":58,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Design a table to support unstructured data\",\"fileName\":\"2874221_05_07_XR30_SO30_solution\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Explore the solution to the special-purpose functionality challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1622291,\"solution\":true,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Now, in this solution,  \\n we need to consider a number of factors.  \\n Clearly we're working with semi-structured data  \\n and we need indexes to support query performance.  \\n Now, we could use a column for each attribute,  \\n but that's really not practical,  \\n particularly in a relational database.  \\n Now, that's not the case with another type of database,  \\n in particular the type of NoSQL database  \\n known as a wide-column database.  \\n Now, Apache Cassandra, Query Google Bigtable  \\n are both examples of wide-column tables,  \\n where having a column for each attribute  \\n is an appropriate way to design it.  \\n But when you're working with Postgres  \\n or other relational tables, that's really not a good option.  \\n A better option is to use a JSONB column,  \\n because with JSONB, we have the semi-structured document  \\n data structure to work with  \\n and JSONB supports indexing.  \\n \\n\\n\"}],\"name\":\"5. Special-Purpose Functionality\",\"size\":71956975,\"urn\":\"urn:li:learningContentChapter:2428257\"},{\"duration\":63,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2427268\",\"duration\":63,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"2874221_06_01_XR30_next_steps\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2192568,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Dan] Now that we've completed  \\n the Advanced SQL for Data Science course,  \\n there's some other courses you may be interested  \\n in taking a look at.  \\n SQL query tuning and performance optimization  \\n is a critical skill anytime you're working  \\n with large data sets.  \\n I'd highly recommend studying query tuning  \\n and performance optimization.  \\n I'd also suggest learning more about data modeling,  \\n including modeling NoSQL databases,  \\n because the lines have begun to blur  \\n between relational or SQL and NoSQL  \\n and so some of the techniques that you might learn  \\n in NoSQL data modeling may be applicable  \\n when you're working, for example, with JSONB data.  \\n Now, depending on the type of data you're working with,  \\n you may want to look into some specialized courses,  \\n like data science for time series data.  \\n And also, as you have seen,  \\n SQL for data science is much more  \\n than just writing queries.  \\n There's a lot of thought that needs to go in  \\n around how you structure your data,  \\n how you structure systems.  \\n So any work you can do with data architecture  \\n or learning data architecture patterns  \\n will almost certainly be helpful  \\n in your data science career.  \\n \\n\\n\"}],\"name\":\"Conclusion\",\"size\":2192568,\"urn\":\"urn:li:learningContentChapter:2421485\"}],\"size\":285263903,\"duration\":9038,\"zeroBased\":false}]}"