"{\"title\":\"\",\"courses\":[{\"course_title\":\"PyTorch Essential Training: Deep Learning\",\"course_admin_id\":2706322,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":2706322,\"Project ID\":null,\"Course Name\":\"PyTorch Essential Training: Deep Learning\",\"Course Name EN\":\"PyTorch Essential Training: Deep Learning\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"&lt;p&gt;PyTorch is the most flexible and expressive library for deep learning, and offers simple Python API, GPU support, and flexibility. It\u00e2\u20ac\u2122s designed to load data, apply transforms, and build deep learning models with just a few lines of code. Many machine learning developers and researchers use PyTorch to accelerate deep learning research, experimentation, and prototyping. In this course, software developer Terezija Semenski teaches you the important features of PyTorch with a hands-on approach to help you develop the skills you need to dive into your deep learning projects.&lt;/p&gt;&lt;p&gt;This course includes Code Challenges powered by CoderPad. Code Challenges are interactive coding exercises with real-time feedback, so you can get hands-on coding practice alongside the course content to advance your programming skills.&lt;/p&gt;\",\"Course Short Description\":\"Explore the basics of deep learning using PyTorch and test your knowledge with hands-on challenges.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":20533014,\"Instructor Name\":\"Terezija  Semenski\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Software Developer, Mathematician, Writer, and Learner\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2024-04-15T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"No\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/pytorch-essential-training-deep-learning-23753149,https://www.linkedin.com/learning/pytorch-essential-training-deep-learning-revision-q1-2024-coderpad\",\"Series\":\"Essential Training\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Artificial Intelligence for Technology\",\"Primary Software\":\"PyTorch\",\"Media Type\":\"Interactive\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":4902.0,\"Visible Video Count\":29.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":296,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3890261\",\"duration\":31,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Deep learning with PyTorch\",\"fileName\":\"2706322_en_US_00_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Storyboard + WSC\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":false,\"rawDurationSeconds\":129,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Take a closer look at the advantages of PyTorch and explore the goals of this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1981176,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- PyTorch is one of the most popular deep learning\\nframeworks that allows us\\nto implement neural networks more efficiently.\\nIt is designed to load data, apply transforms,\\nand build deep learning models\\nwith just a few lines of code.\\nJoin me to learn the foundations of PyTorch\\nwith the hands-on approach for the skills you need\\nto dive into Deep Learning Project.\\nHi, I'm Terezija Semenski.\\nI'm a software developer, mathematician,\\nand a teacher with a passion for AI and machine learning.\\nLet's get started.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3886261\",\"duration\":77,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"2706322_en_US_00_02_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Overlay - colab.research.google.com Please mask sign in to Google account\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":86,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Before starting this course, explore what skills and knowledge you need to have to be successful in this course. This video also lists all installation requirements you need to set up before you proceed.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2591273,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Before starting this course,\\nlet's explore what skills and knowledge you will need.\\nTo truly be successful in this course,\\nit will be helpful\\nto have a basic Python programming knowledge.\\nI assume you're familiar\\nwith the concepts and technologies behind deep learning,\\nbut you don't need any experience building models\\nusing neural networks, or NNs.\\nIn order to run our code,\\nwe will be using the Google Colaboratory environment,\\ncommonly known as Google Colab,\\nwhich is available at colab.research.google.com.\\n\\nGoogle Colab, or short, Colab, is a free computing service\\nthat provides Jupyter Notebook instances\\nthat run on the cloud.\\nThe great thing is that notebooks can be saved\\non Google Drive or GitHub.\\nConveniently, all you need to start writing code\\nis a Google account.\\nGoogle provides free access to GPUs,\\nor graphical processing units,\\nwhich we will use for training our models.\\nThere are some limits on GPU usage,\\nbut it will be sufficient for our course.\\n\\nAnd that's about it.\\nSo let's get ready to jump into PyTorch in depth.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3888262\",\"duration\":188,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Tour of CoderPad\",\"fileName\":\"2706322_en_US_00_03_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Use callouts to highlight areas covered by the [in]structor.\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":236,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5703994,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] This course includes\\nautomated code challenges\\nthat appear when you click on the Challenge links\\nin the course's Table of Contents.\\nEach challenge includes instructions\\nand a couple of code editors\\nyou can use to create and test\\nyour own solutions to the challenge.\\nThese challenges are hosted by CoderPad,\\nand they appear in the same area of the course page\\nwhere you watch the course's videos.\\nWe recommend using a desktop browser\\nfor the best experience with the code challenges,\\nbut you can use the LinkedIn Learning mobile app\\nif you prefer.\\n\\nThe Code Challenges has four areas:\\ninstructions in the top left,\\na code editor for your answer in the top right,\\nanother code editor where you can see how your code is used\\nin the bottom right,\\nand a console for output in the bottom left.\\nYou can use these rectangles to allocate space as you like.\\nTo get even more horizontal space for the code editors,\\nyou can collapse the course's Table of Contents on the left.\\n\\nEach challenge has instructions\\nthat include a description of the challenge\\nand the challenge's parameters and desired result.\\nParameters are values that are passed into your code,\\nand they have to be of a particular data type.\\nThe return value also has to be of a particular type,\\nand you'll also see that noted in instructions.\\nThe Constraints section has useful information\\nabout the parameters that will be passed in.\\nThe examples show different parameter values\\nand what results would be returned\\nfor each of those test cases.\\n\\nCreate your answer in the top right code editor.\\nThere are comments in the starting code\\nshowing where to put your solution.\\nWhen you click Test My Code,\\nyou'll see a message\\nindicating whether your code returned a correct result.\\nCreate your answer in the top right code editor.\\nThere are comments in the starting code\\nshowing where to put your solution.\\nWhen you click Test My Code,\\nyou'll see a message\\nindicating whether your code returned a correct result.\\n\\nIf your code isn't successful, you can ask for help.\\nThe show_expected_result and show_hints variables\\ndetermine whether you see the expected outputs\\nand any hints.\\nChange them to a value of True to control the output.\\nThe code editor in the lower right\\nshows you how your solution is used.\\nYou can change that code\\nto experiment with different test cases.\\nRegardless of whether your answer is successful,\\nyou'll see messages in the console output in the lower left.\\n\\nIf any messages are too long to fit in that area,\\nyou can scroll sideways to see all of the text.\\nWhen you finish each code challenge,\\nreturn to the course's Table of Contents\\nand click the next window to see my solution.\\n\"}],\"name\":\"Introduction\",\"size\":10230942,\"urn\":\"urn:li:learningContentChapter:3891288\"},{\"duration\":588,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3885248\",\"duration\":260,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to deep learning\",\"fileName\":\"2706322_en_US_01_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"01:03 - audio only pickup to replace the missing word \\\"the\\\"\\nB-roll (seen on screen starting the video off is in Collected Assets\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":348,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":true},\"description\":\"Learn what deep learning is, and how it is different from ML and AI, and explore examples of its applications in real-life scenarios.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9170693,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Neural networks used to be\\na cool theoretical concept just a few decades ago,\\nand now they're part of our everyday lives\\ncarried around in our smartphones.\\nWe can use them to enhance their pictures,\\nreceive contact-sensitive email replies,\\nand have our voices recognized.\\nSmart speakers are a common household item\\nand self-driving cars are reality,\\nbut what is behind them?\\nIn the last few years,\\nartificial intelligence, or AI,\\nmachine learning, or ML,\\nand deep learning, or DL, have been buzzwords.\\n\\nEverybody talks about them,\\nbut only a limited number of people understand the meaning\\nand difference between them.\\nSo, is ML the same as AI, or is it different from it?\\nWhat about the DL?\\nWhere does it fit into picture?\\nFirst, artificial intelligence, or short, AI,\\nis challenging to define,\\nand there is no clear agreement among AI professionals.\\nSo, how can we define what AI is?\\nIn AI, the goal is to simulate human learning\\nso that application can adapt\\nto uncertain or unexpected conditions.\\n\\nArtificial intelligence is a concept of creating machines\\nthat can perform tasks requiring human cognitive functions\\nsuch as discovering, finding new information,\\nlearning, reasoning, problem solving, perception,\\nand language Understanding.\\nAI has many subsets,\\nsuch as machine learning, expert systems,\\nnatural language processing, or short, NLP, and robotics.\\n\\nMachine learning, or ML,\\nis just one of several subsets of AI.\\nTo build any learning system,\\nyou need three essential components, input data,\\nprocessing and output layer.\\nWhen we have a learning system\\nwith improved performance over time\\nby learning from a new examples or data,\\nwe call it machine learning, or ML.\\nML is a set of multiple techniques\\nthat enable a computer to learn from data\\nand use what it learns to provide an answer,\\noften in form of a prediction.\\n\\nWhen they have a huge data sets,\\nit is time consuming for ML algorithms\\nto process large scale matrix computations.\\nIn this case, deep learning or DL is more applicable.\\nWe can represent the relationship\\nbetween the AI, ML and DL\\nwith a simple Venn diagram like this.\\nSo, ML is a subset of AI,\\nand deep learning is a subset of ML.\\n\\nWait, but what is the difference\\nbetween machine learning and deep learning?\\nDeep learning is a subset of machine learning\\nthat uses multiple and numerous layers\\nof non-linear transforms\\nto progressively extract features from row input.\\nWait, what?\\nDeep learning is called deep\\nbecause it uses multiple layers of neural networks.\\nEvery layer is trained by algorithms.\\n\\nIt takes its inputs from previous layers\\nand progressively refines them in order\\nto minimize their errors and improve accuracy.\\nSo, machine learning is usually used\\nfor all dimensional data and for small volumes of data.\\nOn the other side,\\ndeep learning is used when the data dimension is huge\\nand the training data is also high in volume.\\nFinally, you may wonder where we can apply deep learning.\\n\\nThere are numerous use cases,\\nsome of them are speech recognition,\\nanomaly detection from videos,\\nmachine translation,\\nspeech to text conversion.\\nThis introduction leaves us with a good starting point\\nto discover why we should choose PyTorch for deep learning,\\nso let's discover that next.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3885247\",\"duration\":121,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Why should you use PyTorch\",\"fileName\":\"2706322_en_US_01_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Overlay -  https://github.com/pytorch/pytorch/blob/main/LICENSE\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":130,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"PyTorch provides Python with easily run array-based calculations, allows the building of dynamic neural networks, and performs auto differentiation with strong graphics processing unit (GPU) acceleration. All the above are the most important features required for deep learning development.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3693987,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] PyTorch is a Python library\\nthat facilitates building deep learning projects.\\nPyTorch provides Python\\nwith easily run array-based calculations,\\nallowing it to build dynamic neural networks\\nand perform auto differentiation\\nwith a strong graphics processing unit or GPU acceleration.\\nThese are the most important features required\\nfor deep learning development.\\nPyTorch was developed by researchers and engineers\\nfrom the Facebook AI Research, FAIR division,\\nto process large-scale image analysis,\\nincluding object detection,\\nsegmentation, and classification.\\n\\nIt was written using Python and C++ languages.\\nPyTorch was initially released in September, 2016,\\nand its free and open-source under the modified BSD license,\\nso its development involves many contributions\\nfrom the community.\\nBut what makes PyTorch useful to us?\\nPyTorch is the world's fastest growing deep learning library\\nfor three main reasons,\\nits simplicity, flexibility, and Python interface.\\n\\nThis is what makes PyTorch powerful.\\nIt is supported by all major cloud platforms,\\nsuch as Amazon Web Services, Google Cloud Platform,\\nand Microsoft Azure.\\nIt is mature and stable since it's regularly maintained.\\nIt supports CPU, GPU, TPU, and parallel processing.\\nIt supports distributed training,\\nmeaning you can train neural networks or multiple GPUs\\non multiple machines.\\n\\nNow that you're familiar with PyTorch,\\nlet's jump in and discover Google Colab.\\nIt is super easy for beginners\\nas you can run PyTorch code in a browser\\nwithout installation or configuration.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3890260\",\"duration\":207,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Google Colaboratory basics\",\"fileName\":\"2706322_en_US_01_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":287,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, explore Google Colab which allows you to write and execute Python and PyTorch code in your browser.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5374245,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's explore Google Collaboratory,\\nknown as Google Colab,\\nthat allows us to write and execute\\nPython and PyTorch code in our browser.\\nLet's take a look at where we'll be writing\\nand running our code.\\nTo get started, let's head on to colab.research.google.com.\\nTo use Colab, you only need to have a Google account.\\nIf you haven't signed into your Google account,\\nyou'll be prompted to sign in or to create one.\\n\\nIn case you already signed into your Google account,\\nyou'll see a default screen,\\nand it gives you a good set\\nof beginner-friendly instructions on how to use it.\\nThe very first thing you're going to do\\nis to create a new notebook.\\nClick on File, New notebook,\\nand it creates an empty blank template.\\nAnd we can rename it to 0103.\\nColab is great for beginners\\nor anyone who wants to start playing\\nwith machine learning and deep learning models,\\nbecause it allows us to have a GPU for absolutely free.\\n\\nWell, there are limits in GPU usage per week,\\nbut it will be sufficient for our needs.\\nTo verify our configuration,\\nyou're going to import the PyTorch library,\\nprint the installed version,\\nand lastly, check to see if you're using a GPU.\\nWe got false, as by default,\\nour Colab notebook does not use the GPU.\\n\\nWe are going to change the runtime type\\nso that we use a hardware accelerator.\\nWe want to run this code using GPUs,\\nbecause if you just use CPUs, it will consume too much time.\\nNow, let's get our environment set up.\\nTo change CPU to GPU, Let's select Runtime.\\nAnd under it change runtime type.\\nThen select T4 GPU\\nfrom the hardware accelerator dropdown menu, and click Save.\\n\\nLet's run the cell again by selecting the cell\\nand pressing Shift + Enter.\\nNow we have a true as the output. Great.\\nWe have verified that PyTorch is installed\\nand we have a GPU available.\\nOne more thing I want to set up\\nis line numbers for each code\\nso you can easily follow along.\\nClick on Tools, click on Settings,\\nand select the Editor tab.\\nThere is an option, right, that says Show line numbers.\\n\\nLet's select that option and click on Save.\\nThere are some other options like font and indentation\\nthat you can customize later if you want.\\nSome may wonder why we don't run PyTorch locally\\nor in the cloud.\\nSetting up your local machine or your cloud environment\\nis beyond the scope of this course,\\nand it would require much more time and even money\\nas GPU instances or cloud platforms like AWS, GCP,\\nor Azure have additional costs.\\n\\nNow that we have our playground ready,\\nlet's get to know tensors.\\n\"}],\"name\":\"1. PyTorch Overview and Introduction to Google Colaboratory\",\"size\":16664192,\"urn\":\"urn:li:learningContentChapter:3888263\"},{\"duration\":548,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3890259\",\"duration\":204,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to tensors\",\"fileName\":\"2706322_en_US_02_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":245,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn the fundamental data structure used in PyTorch called the tensors. It allows you to understand how PyTorch handles and stores the data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6158463,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] A neural network knows how to deal with data\\nstored as floating-point numbers.\\nInputs are real-world data in many forms.\\nLet's take for example, image recognition.\\nA deep neural network takes images\\nand codes them in a digestible way\\nand then decodes them back to some output,\\nfor example, text.\\nIt happens in multiple stages,\\nand partially transformed data between stages\\nis a sequence of intermediate representations.\\n\\nIn our case, for images,\\nlet's think of a picture of a person.\\nEarly representation can be edge detection.\\nIntermediate representation can capture ears, nose, or eyes.\\nEach intermediate representation\\nis a collection of a floating-point numbers\\nresulting from multiplying the input\\nwith the previous layer's weights.\\nTo handle and store the data\\nin all stages of deep learning,\\nPyTorch uses an essential data structure called a tensor.\\n\\nSo inputs, intermediate representations, and outputs\\nare all stored as tensors.\\nBut what is a tensor?\\nFrom mathematics, we can look at tensors\\nas generalizations of scalars, vectors, and matrices\\nto any dimension.\\nSo we can define the scalar as a zero dimensional,\\nor 0-d tensor,\\na vector as one dimensional, or 1-d tensor,\\na matrix as a 2-d tensor,\\nand a matrix stack in a third dimension,\\nfor example, when you need to process data\\nthat has a time element as 3-d tensor.\\n\\nGenerally, tensors can have an arbitrary number\\nof dimensions.\\nIn PyTorch, a tensor is a multi-dimensional array\\ncontaining elements of a single data type.\\nIf you have worked with a NumPy library,\\nthen tensor reminds you of the fundamental object in NumPy\\ncalled ndarray, which is defined as an n-dimensional array,\\nhomogenous array of fixed-size items.\\nYou may wonder,\\nwhat is the difference between tensor and ndarray?\\nTensors in PyTorch are similar to NumPy's ndarrays.\\n\\nBut tensors have additional advantages\\nthat make them more suitable for deep learning calculations.\\nSome of those advantages are\\ntensor operations are performed significantly faster\\nusing graphical processing units, or GPUs.\\nTensors can be stored and manipulated at scale\\nusing distributed processing on multiple CPUs and GPUs\\nand across multiple servers.\\n\\nAnd tensors keep track of the graph of computation\\nthat created them.\\nWith all those mentioned,\\nyou can see that tensors are much more than a special sort\\nof multidimensional arrays.\\nTensors interact with each other\\nsuch that transforming tensors as a whole\\nmeans that each tensor\\nfollows a particular transformation rule.\\nWith that in mind, let's head on to practical examples\\nand learn how to create tensors.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3887273\",\"duration\":108,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating a tensor CPU example\",\"fileName\":\"2706322_en_US_02_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"01:18 - Pause 4 action mogrt\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":144,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn a CPU example of how to create a tensor and perform a tensor operation so you can use a built-in method on the tensor itself.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2925364,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's create our first tensors\\nusing the PyTorch library.\\nWe're going to open a Google Colab notebook named 02_02.\\nFirst, we're going to import the PyTorch library.\\nNext, we're going to create two tensors\\nfrom two-dimensional lists,\\nand let's call them first_tens and second_tens.\\nTensor data type will by default be derived\\nfrom the input data type,\\nand the tensor is allocated to the CPU device.\\n\\nNow, let's apply a simple arithmetic operation.\\nIn our case, addition.\\nWe can do that using the plus operator\\nand store the result in the third tensor,\\nwhich we'll call add_tens.\\nLet's print the new tensor, add_tens, and print its size.\\nAs you can see,\\nit's the sum of tensors first_tens and second_tens,\\nand you can see its dimensions are two by four.\\nNow, let's try subtraction.\\n\\nHow would you calculate it?\\n(upbeat music)\\n(buzzer buzzing)\\nThat's right, by using the minus operator.\\nNow, let's do that and store the result in sub_tens.\\nFinally, let's print the latest tensor, sub_tens,\\nwhich we can see is the difference\\nof the first_tens and second_tens,\\nand we print the size of sub_tens.\\nAs you can see,\\nboth add_tens and sub_tens are tensor objects themselves,\\nand we can use size method to return its matrix dimensions,\\nnamely two by four.\\n\\nNow that we have seen how to create a tensor\\nthat is allocated to the CPU device,\\nwe'll explore the GPU example next.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3889244\",\"duration\":106,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating a tensor GPU example\",\"fileName\":\"2706322_en_US_02_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":113,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn a GPU example of how to create a tensor and perform a tensor operation so you can use a built-in method on the tensor itself.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2738242,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] GPUs were originally developed\\nfor rendering computer graphics.\\nSo when I was a kid, every gamer talked about them.\\nMeanwhile, with the need for speed\\nof computational processing involving neural networks,\\nthey now play a crucial role in deep learning.\\nIn PyTorch, we have the CUDA library\\nthat is instrumental in detecting, activating,\\nand harnessing the power of GPUs.\\nJust as for the CPU, we'll explore a simple example.\\n\\nFirst, before using CPUs,\\nwe are going to check if they are configured\\nand ready to use.\\nWe are going to import PyTorch\\nand print the PyTorch version.\\nNext, by calling the torch.cuda.is_available function,\\nwe'll move the tensors to the GPU device\\nif one is available.\\nSo, our device has a GPU support.\\nWe are going to create two tensors\\nand let's just call them tens_a and tens_b.\\n\\nNow let's apply a simple arithmetic operation,\\nin our case, multiplication.\\nWe can do that using the asterisk operator\\nand store the result in the third tensor,\\nwhich we'll call multi_tens.\\nNotice that the output tensor is also allocated to the GPU.\\nWhen we run the code,\\nwe get the output, in our case, device='cuda:0'\\nwhich indicates that the first GPU is being used.\\n\\nIn the case our device contains multiple GPUs,\\nthis way, we can control which GPU is being used.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3885246\",\"duration\":130,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Moving tensors between CPUs and GPUs\",\"fileName\":\"2706322_en_US_02_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":212,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Explore the torch.to() method that allows you to move an existing tensor from CPU to GPU device.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3628251,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] A few important reasons exist\\nfor moving the tenors between CPUs and GPUs.\\nLet's explore them and see how to transfer\\nthe data from the CPU to GPU.\\nBy default, in PyTorch, all the data are in the CPU.\\nIn case we are training neural network, which is huge,\\nwe prefer to use GPU for faster training.\\nFor example, if we have high dimensional tensors\\nthat represent images, their computation intents,\\nand take too much time if run over the CPU.\\n\\nSo we need to transfer the data from the CPU to the GPU.\\nAdditionally, after the training,\\nthe output tensors are produced in GPU.\\nSometimes the output data requires pre-processing.\\nSome pre-processing libraries don't support tensors,\\nand expect an NumPy array.\\nIn that case, NumPy supports only data in the CPU,\\nso there is a need to move the data from the CPU to the GPU.\\n\\nLuckily, tensors can be moved easily from the CPU\\nto GPU device with the torch to method.\\nWe can call this method in one of the three ways.\\nFirst way, tensor.cuda,\\nor second way, tensor to cuda,\\nor the third way, tensor to cuda 0.\\nWhen we need to move the tensors in the opposite direction\\nfrom the GPU to CPU, there are two possible cases.\\n\\nFirst one, tensor with required grad = false.\\nIn the first case, when required grad = false,\\nthen we use the methods tensor.cpu.\\nIn the second case, when required grad = true,\\nthen we use method, tensor.detach.cpu.\\nNow, that we have mastered methods to move the tensors,\\nlet's head on to explore how to create tensors.\\n\"}],\"name\":\"2. Tensors\",\"size\":15450320,\"urn\":\"urn:li:learningContentChapter:3887274\"},{\"duration\":941,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3884263\",\"duration\":219,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Different ways to create tensors\",\"fileName\":\"2706322_en_US_03_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Overlay - https://pytorch.org/docs/stable/torch.html\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":250,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Sometimes you may want to construct tensors from preexisting data stored in array-like structures such as lists, tuples, scalars, serialized data files, or NumPy arrays. Learn how to achieve that.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7708833,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Sometimes we want\\nto construct tensors directly from Python objects\\nlike lists, tuples or NumPy arrays.\\nWe can also create tensors\\nby using different functions in case we want\\nto generate a particular type of tensor.\\nLet's first learn how\\nto create a tensor from a Python list and a tuple.\\nTo do that, open notebook 0301,\\nwe have already imported PyTorch and numpy as np.\\nI will create a tensor from a list\\nand call it tensor from list by using a torch.tensor method,\\nand this list will contain integers from one to five.\\n\\nThis method is used\\nto create a tensor from an existing data structure.\\nNow to create a tensor from a tuple.\\nTuples are lists that are immutable,\\nmeaning that once defined,\\nthe individual elements of a tuple cannot be changed.\\nYou can easily spot a difference between a tuple and a list\\nbecause a tuple is written in a sequence of numbers\\nand closed in the round parenthesis.\\nLet's call this tensor tensor_from_tuple.\\n\\nWe will call the same torch.tensor method,\\nbut this time we'll pass a tuple instead of a list.\\nGo ahead and display these two tensors.\\nNow to create a tensor from a NumPy array,\\nwe will call torch.tensor\\nand pass a NumPy array instead of a list or a tuple.\\nLet's display our tensor.\\nPyTorch also has different functions that we can use\\nto create an initialized tensors.\\n\\nThe most useful ones are torch.empty(), torch.ones()\\nand torch.zeros().\\nAs you can see, each one of them is used\\nwith a torch namespace.\\nThese functions take integers as the first two arguments,\\nwhich specified the size of a tensor.\\nYou can easily guess what they do from their name.\\nAs you can see in our example using the empty function,\\nwe create a tensor from uninitialized elements.\\nIf you use a torch.zero function, it creates a tensor\\nwith elements initialized with zeros\\nand torch.ones function creates a tensor\\nwith all elements initialized to ones.\\n\\nSometimes you want to initialize the tensor\\nwith random values,\\nand PyTorch has a few useful functions called torch.rand,\\ntorch.randn and torch.randint.\\nAs you can see in our code, the difference between them\\nis that the first one returns a tensor filled\\nwith random numbers from uniform distribution.\\nThe second one used normal distribution,\\nand the third one returns the tensor filled\\nwith random integers using uniform distributions.\\n\\nThese functions take integers as first two arguments,\\nwhich specify the size of a tensor.\\nWe can also pass in an optional arguments data type\\nor dtype and device.\\nOften we want to create tensors that have similar properties\\nto another tensor, including the dtype device\\nand layout properties to facilitate calculations.\\nFor example, torch.ones_like will create a tensor\\nwith all ones with a dtype device\\nand layout properties of tensor X.\\n\\nYou can learn more about additional functions used\\nfor tensor creation\\nat the official PyTorch documentation page.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3891286\",\"duration\":131,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Tensor attributes\",\"fileName\":\"2706322_en_US_03_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":233,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to effortlessly find information about tensors by accessing their attributes.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3786168,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Knowing attributes\\nsuch as device location, data type, dimension, and rank\\nis very important\\nwhen you're doing computations with tensors.\\nAs you have learned, a tensor is a container\\nwith a fixed size that has elements of the same type.\\nYou can think of a tensor as a multidimensional array.\\nWe have already imported PyTorch,\\nand now let's create our one-dimensional tensor\\ncalled first_tensor.\\nWe'll call the device function\\nto find out the tensor's device location,\\nmeaning the device on which the tensor is stored.\\n\\nHere we run the cell\\nand see that our tensor is on the CPU.\\nTo check its data type, we'll call the dtype function.\\nAnd when we ran the cell we got int64,\\nmeaning 64-bit integer.\\nWhen we want to find out the shape of the tensor,\\nwe can always call its shape function\\nand it gives us back the dimensions.\\nAnother useful function is ndim,\\nwhich gives us the number of tensors, dimension, or rank.\\n\\nLet's go ahead and run the code.\\nAnd we get one, as we have expected,\\nsince we have a one-dimensional tensor.\\nWhat if we create a two-dimensional tensor?\\nLet's create one and name it second_tensor.\\nNow we can check its attributes by running the cells.\\nAs you can see, when accessing object attributes,\\nwe don't include parenthesis as we do with the class method.\\nThere are many more variable functions to access attributes,\\nsuch as requires_grad,\\nwhich is used in automatic differentiation\\nor short autograd.\\n\\nYou can find out more\\nby heading onto the documentation page\\non the following link.\\nNow that we have learned the tensor attributes,\\nit's time to move to the next section\\nand explore tensor data types.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3885245\",\"duration\":196,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Tensor data types\",\"fileName\":\"2706322_en_US_03_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"01:48 - Pause4Action Mogrt\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":298,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"When creating tensors, you can specify the data type. It is also possible to cast a tensor to a new data type. In this video, explore how to achieve that.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6074139,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] What sets PyTorch apart from other libraries\\nis the usage of tensors, multidimensional arrays\\nthat can seamlessly perform fast operations on GPUs\\nand distribute operations on multiple machines\\nwhile keeping track of the graphic computations\\nthat created them.\\nLet's look at the various tensor data types\\nthat are used in computations happening in neural networks.\\nWe'll start by importing PyTorch\\nand creating a couple of tensor objects.\\n\\nWe are going to create our first one-dimensional tensor\\nand call it int_tensor,\\nand specify the data type by using dtype as an argument\\nto the constructor.\\nWe want all tensor elements to be integer type\\nof eight-bit size,\\nso we'll type dtype=torch.int8.\\nLet's displace data type using dtype function.\\n\\nNow let's create a second one-dimensional tensor\\nand call it float_tensor.\\nWe want all tensor elements to be 32 floating points,\\nso we'll type dtype=torch.float32\\nand just display the data type.\\nWhat if we want to create a one-dimensional tensor\\nwith an integer type of 16 bits?\\nHow would you do it?\\n(light music)\\nThat's right, by specifying the data type\\nas dtype=torch.int16.\\n\\nAnd let's go ahead and create one.\\nSometimes we need to cast the tensor to new data type.\\nIt can be done in two different ways\\nby using the corresponding casting function\\nwhich is named after the data type you want to convert to.\\nFor example, in case you want to convert\\nint_tensor to float,\\nwe can do that by calling the float function.\\n\\nAnd when we display the data type,\\nwe can see that our short tensor\\nis the same type as the float tensor.\\nAnother way to cast a tensor to a new data type\\nis using two function; for example,\\nto cast short tensor from int 16 to int 8\\nand assign it to a new tensor called last_tensor,\\nwe'll type last_tensor = short_tensor.to(dtype=torch.int8).\\n\\nThe important thing to remember\\nis that when we are doing operations\\non and between tensors that have different data types,\\nPyTorch will automatically cast tensors\\nto a larger data type.\\nIt would be best if you always were mindful\\non which data type to choose.\\nThis can have a huge effect\\non memory consumption and precision,\\nwhich is especially relevant in deep learning algorithms.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3888261\",\"duration\":161,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating tensors from random samples\",\"fileName\":\"2706322_en_US_03_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":193,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Often, you need to initialize weights to random values or create random inputs with specified distributions. Learn functions that you can apply to create tensors from random data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5108856,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In deep learning,\\nwe often need to initialize weights to random values\\nor create random inputs with specified distributions.\\nPyTorch provides valuable functions\\nthat we can use for random sampling.\\nWith the power of these functions,\\nwe can generate values by drawing randomly\\nfor probability distributions, like normal distribution\\nand uniform distribution.\\nLet's explore the most useful ones.\\nWe have already imported PyTorch.\\n\\nLet's go ahead and create the two-dimensional tensors\\nusing the torch.rand function.\\nIt will create a tensor filled with random values\\nfrom a uniform distribution on the interval zero to one.\\nIt's going to be 3 by 3 shape,\\nwhich we define by passing it as an argument\\nto rand function and displaying it.\\nNow, if we rerun our code, you can see that, as expected,\\nwe get a different result each time.\\n\\nHowever, sometimes we want the same result.\\nTo achieve this, we can just call manual_seed function\\nbefore creating our tensor.\\nLet's add the line before.\\nGreat, now when we rerun the code,\\nwe obtain the same result.\\nIf we want to select random values\\nfrom a standard normal distribution\\nwith zero mean unit variance,\\nwe can use the torch.rand function.\\n\\nLet's create a 4 by 4 shape tensor using rand function.\\nWe use torch.randint function to create a tensor\\nfilled with random integers generated uniformly\\nbetween specified low and high values.\\nIt accepts three values as parameters.\\nThe first parameter is the lowest inclusive value,\\nthe second parameter is the highest exclusive value,\\nand the third parameter will pass the shape.\\nLet's provide the parameters minus 1, 10,\\nand it's going to be 3 by 3 shape.\\n\\nwhen creating tensors using sampling functions,\\nyou can also specify dtype and device and other parameters.\\nWhen creating tensors using sampling functions,\\nyou can also specify dtype and device and other parameters\\nin the case of using the more advanced distributions\\nlike multinomial or exponential distributions.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3887272\",\"duration\":125,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating tensors like other tensors\",\"fileName\":\"2706322_en_US_03_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":144,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn similarity functions so you can create\u2009and\u2009initialize\u2009a\u2009tensor\u2009that has similar properties to the existing tensor.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3751344,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Sometimes there is a need to create a tensor\\nthat has the same device, size, data type,\\nand layout properties as another preference tensor.\\nPyTorch has special functions that allow to achieve this.\\nYou can easily spot and memorize those functions\\nas they have the postfix like\\nas a part of the function name.\\nWe have explored the tensor creation functions,\\nempty, zero, ones, rand and rand int.\\nAnd all these functions have similar function,\\nwhich we call similarity functions with postfix like.\\n\\nSo empty has its pair empty like,\\nzeroes have a pair zeroes like and so on.\\nWe have already imported PyTorch.\\nWe'll start by creating a tensor using zeroes function,\\ncall it starting tensor and display it.\\nWe'll create a tensor with the same size\\nfilled with random values using torch.rand like function.\\n\\nNotice we are using manual seed\\nto have the same numbers generated\\neach time we rerun our code.\\nLet's create another two dimensional tensor.\\nThis time we want to have the same shape as starting tensor,\\nbut filled with ones.\\nWe will use ones like function.\\nAnd we get a tensor filled with ones.\\nNow, imagine you want to create a tensor\\nwith the same shape,\\nbut this time you want all elements\\nto be of a specific value.\\n\\nThis time we'll use a full like function.\\nProvide an input tensor\\nand pass number seven as a fill value.\\nGreat, now that we know what tensors are\\nand have seen different ways to create tensors,\\nlet's jump right into some practice\\nin a coder pad environment.\\nGive the challenge a go\\nand then come back to see how I solved it.\\nGood luck.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:6605e3623450c8a8f9db62b3\",\"duration\":600,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code challenge: Create tensors\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1006782\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:3890262\",\"duration\":109,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Create tensors\",\"fileName\":\"2706322_en_US_03_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":124,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn a solution for creating tensors from different data structures.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4100885,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(bright music)\\n- [Instructor] All right, so for this challenge,\\nwe needed to create four functions that return four tensors,\\nthe first two using the already provided Python list\\nand NumPy array,\\nand the next two by using PyTorch functions.\\nSo, here is how to solve this challenge.\\nSince Torch is not supported by CoderPad environment,\\nI have created a mock tensor class\\nthat has all required functions to solve this challenge.\\n\\nSo we have our first function,\\nwhich takes input list as parameter.\\nTo convert a Python list to a tensor,\\nwe'll use a built-in function\\nfrom the Torch Library called torch.tensor.\\nThis function creates a tensor\\nwith elements in the same data type as the Python list.\\nIn our case, integers.\\nNext, for a second function,\\nwhich takes an input array\\nand needs to return a two-dimensional tensor,\\nwe'll again use the torch.tensor function\\nand pass in an input array as a parameter.\\n\\nTo create a tensor of a shape three by four,\\nwith all its elements being one,\\nwe'll use torch.once function.\\nWe'll pass a tuple {3, 4} as a parameter.\\nFor the last task\\nwhere we need to create a shape four by five\\nwith all elements being five,\\nwe'll call full function.\\nWe use it by passing a tuple {4, 5} as a parameter\\nand 5 as a default value.\\nNow, go ahead and click on test my code.\\n\\nWe got the right answer,\\nand you can see a printout of all of our four tensors.\\n\"}],\"name\":\"3. Creating Tensors\",\"size\":30530225,\"urn\":\"urn:li:learningContentChapter:3886262\"},{\"duration\":1205,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3885244\",\"duration\":275,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Tensor operations\",\"fileName\":\"2706322_en_US_04_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Start with WSC, PUWSC - video and audio are on separate files. PU replaces content at 02:45 and start with WSC, separate audiofile\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":false,\"rawDurationSeconds\":939,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video explore a set of tensor operations that allow you to access and transform your tensor data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9202498,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Managing and transforming large data sets in deep learning\\ncan be frustrating.\\nYou're likely wasting time and computational power\\nif your tensor operations aren't optimized,\\nmaking the task of handling data\\ncumbersome and less effective.\\nLet's learn to efficiently split, combine,\\nand manipulate tensors,\\nwhich lead to more robust and scalable deep learning models.\\nLet's perform indexing and slicing on tensors.\\nIf you have used NumPy, I have great news.\\n\\nIndexing and slicing of tensors\\nis done the same way as per NumPy arrays.\\nIf you're new to this, don't worry.\\nIndexing means accessing a single element of a tensor,\\nand slicing means accessing a range of elements.\\nNow let's create a one-dimensional tensor,\\ncall it 1-dim-tensor.\\nIf we want to access and display\\na third element of our tensor,\\nwe just have to type print 1-dim-tensor two.\\nWhen we run the cell, notice we got tensor three.\\n\\nIn PyTorch, when we are passing the tensor element\\nto a function like print, we first have to convert it\\nto a Python value by calling the item function.\\nLet's do that and rerun our code.\\nAnd you can see, we got the value as a number.\\nNext, let's see how slicing is done.\\nThere is a format that we use start column and column step.\\nWe need to specify a start index\\nand the end index separated by column.\\n\\nThe step is optional,\\nand it defines the number of indexes to move forward\\nwhile slicing an object.\\nIf the step is not indicated,\\nit means moving without skipping any index.\\nThe output tensor will contain all the elements\\nfrom the starting index including,\\nto the ending index excluding.\\nSo, let's say we want to get elements\\nbetween the first and the fourth element.\\nWe will type 1-dim-tensor, one column four.\\n\\nNext, let's see how to do indexing and slicing\\non a two-dimensional tensor.\\nWe'll create a four by six tensor called 2-dim tensor.\\nTo access the fourth element of the second array,\\nwe'll type:\\n2-dim tensor one three.\\nIn the case when we want to access two or more elements\\nat the same time, we'll perform slicing.\\nLet's say we want to access\\nthe first three elements of the first row,\\nand the first four elements of the second row.\\n\\nWe'll type print first three elements on the first row,\\n2-dim tensor, zero zero three.\\nAnd we'll print first four elements of the second row,\\n2-dim tensor one zero four.\\nSometimes we want to use indexing\\nto extract the data that meets some criteria.\\nFor example, if we would like to keep only elements\\nthat are less than 10, we would type 2-dim tensor,\\n2-dim tensor less than 11, and display it.\\n\\nPyTorch has many different built-in functions\\nthat we can use to access, combine, and split tensors.\\nLet's use the two most common ones in our example.\\nIf we want to combine tensors,\\nthere is a function called torch stack.\\nIt concatenates a sequence of tensors along a new dimension.\\nLet's create a new tensor called combined tensor.\\nLet's combine our two-dimensional tensor\\nusing torch stack function.\\n\\nThe second useful function is for splitting tensors,\\nand it is called torch and bind.\\nIf we want to split our two-dimensional tensor\\ninto four tensors, let's call them first tensor,\\nsecond tensor, and so on.\\nWe will type: first tensor, second tensor, third tensor,\\nfourth tensor equals torch unbind, 2-dim tensor.\\nAnd display it.\\nAs you can see, torch and bind function splits the tensor\\naccordingly to the number of rows,\\nbut we can select along which dimension we want it to work.\\n\\nNow, if we put dim equals one,\\nwhich corresponds to the column dimension,\\nthe tensor will split into six smaller tensors,\\nwith the corresponding values of each column.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3891287\",\"duration\":216,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Mathematical functions\",\"fileName\":\"2706322_en_US_04_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":259,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn a set of built-in math functions. It is extremely important to know them as deep learning is based on mathematical computations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6007871,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Mathematical functions\\nare one of the most useful functions in PyTorch.\\nLet's explore five types of built-in math functions,\\npointwise operations, reduction functions,\\ncomparison functions, linear algebra operations,\\nand spectral and other math computations.\\nDon't be overwhelmed with these fancy names.\\nLet's figure them out together.\\nPointwise operations are named after their function\\nas they perform an operation\\non each point in the tensor individually\\nand return a new tensor.\\n\\nSome commonly used pointwise operation\\ninclude some basic math functions,\\nadd(), mul(), div(), neg(), and true_divide(),\\nfunctions for truncation,\\nceil(), clamp(), floor(), et cetera,\\nlogical functions, trigonometry functions, and so on.\\nThe second type of mathematical functions\\nare reduction operations.\\nThey're also named after their role\\nas they reduce numbers down to a single number\\nor a smaller set of numbers\\nthat results in reducing the dimensionality\\nor rank of the tensor.\\n\\nReduction operations include statistical functions\\nsuch as mean, median, mode.\\nComparison functions, as the name suggests,\\ncompare all the values between the tensor\\nor compare values of two different tensors.\\nThey also include functions\\nto find the minimum or maximum value, sort tensor values,\\ntest sensor status or condition, or similar.\\nLinear algebra functions enable matrix operations\\nand are essential for deep-learning computations.\\n\\nThese include functions for matrix computations\\nand tensor computations.\\nThe last type of mathematical function\\nis a spectral and other math operations.\\nThese functions are useful\\nfor data transformations or analysis.\\nAs there are so many math functions for different purposes,\\nwe won't be covering all of them with code examples.\\nLet's explore some of them\\nby heading on to the Colab notebook.\\nWe have already imported PyTorch.\\n\\nLet's create two one-dimensional tensors\\nand call them a and b.\\nWe are going to add, multiply,\\nand divide tensor a with tensor b\\nby using add(), mul(), div() functions,\\nand display the results.\\nNext, let's see the most useful reduction functions.\\nThese are torch.mean(), torch.median(),\\ntorch.mode(), and torch.std().\\nAs their name suggests,\\nthey're used for calculating the mean, the median, the mode,\\nand the standard deviations\\nof all elements between the tensor.\\n\\nLet's create a tensor c that contains floating points.\\nWe are using floating points\\nas torch.mean() accepts only tensors\\nwith floating points data type.\\nNext, let's calculate these reduction functions\\nand display them.\\nAs you can see when we run the code,\\ntorch.mode() function returns a namedtuple\\ncontaining mode values and indices,\\nrepresenting the index occurrence of mode in each row.\\n\\nKeep in mind,\\nwe have only scratched the surface of math functions\\nas there are many more valuable functions.\\nYou can discover them later on your own.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3887271\",\"duration\":236,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Linear algebra operations\",\"fileName\":\"2706322_en_US_04_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"00:35 - Overlay - https://pytorch.org/docs/stable/linalg.html\\n01:04 - overlay - NumPy Essential Training\\n02:39 - PAUSE4ACTION\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":288,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Many computations, including optimization algorithms and gradient descent, use linear algebra to implement the calculations. Explore the most important built-in linear algebra operations which you can use.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7828922,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Linear algebra is used heavily\\nin deep learning computations,\\nfrom different optimization algorithms to gradient descent.\\nLet's explore the most important ones.\\nPyTorch has a module called torch lineal that contains a set\\nof built-in linear algebra functions\\nthat are mostly based on the basic linear algebra,\\nsubprograms or Blast and Linear algebra package\\nor APEC standardized libraries.\\nYou can find a complete list\\nof functions on the following PyTorch linear algebra\\ndocumentation page.\\n\\nIf you have already mastered Numpy,\\nyou will quickly learn these functions as most\\nof the functions from Numpy's linear algebra module are\\nthe same, but they're extended with accelerator\\nand L2 grad support.\\nOnly a few of the functions will be completely new.\\nIf you haven't explored the linear algebra capabilities\\nof Numpy, you can take my course, Numpy Essential Training.\\nWe have already imported PyTorch.\\n\\nThe first useful function performs matrix products\\nof two tensors, and it's called Torch Matmul.\\nWe'll create two tensors,\\nand then compute the dot product\\nof them using torch matmul and display it.\\nWhen we run our code, we see\\nthat the result is a zero dimensional tensor or scaler.\\nFor the next example, let's compute a product of two,\\ntwo dimensional tensors or matrix matrix products.\\n\\nLet's create a first tensor that has a shape, two by three\\nand a second tensor that has a shape three by two,\\nand after we perform a matrix product\\nand display the output, you can see we got a two\\nby two two dimensional tensor or matrix.\\nWhen performing matrix multiplication, it is important\\nthat matrices have compatible dimensions.\\nThis means that the number\\nof columns in the first matrix is the same as the number\\nof rows in the second matrix, so in our case,\\nwe multiplied two by three with three by two matrix\\nand got two by two matrix.\\n\\nWe could have also multiplied the two by three matrix\\nwith a three by five matrix.\\nWhat would be the dimension\\nof the result matrix in this case?\\n(upbeat music)\\nYou are right.\\nThe result matrix would be two by five matrix.\\nThere is a second function\\nfor calculating the matrix product of two tensors,\\ncalled torch mm.\\nUnlike torch matmul, it doesn't support broadcasting.\\n\\nIn deep learning, we often have\\nto calculate the matrix product\\nof in two dimensional tensors.\\nTo do that, we can use torch.linalg.multi_dot function.\\nLet's create five two dimensional tensors using torch render\\nand then call torch linalg.multi_dot function.\\nWhen we run our code, we see we got two by seven matrix\\nas a result.\\nOne of the most useful types\\nof matrix decompositions in deep learning is the eigen\\ncomposition, which decomposes a square matrix into eigen\\nvectors and eigen values.\\n\\nComputing eigen values\\nand eigen vectors is simply done\\nby calling the torch ike function.\\nLet's create a four by four square matrix A,\\ndisplayed and compute the Eigen values\\nand eigen vectors of the matrix.\\nNow that you have a basic foundation on tensors,\\nlet's head on to explore how to use them\\nto perform deep learning development.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3890258\",\"duration\":365,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Automatic differentiation (Autograd)\",\"fileName\":\"2706322_en_US_04_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"01:55. - Overlay - Machine Learning Foundations: Calculus\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":472,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn the backward() function. It is important because it is used to differentiate and compute gradients of tensors based on the Chain rule.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10276293,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] When we train a neural network,\\nwe follow two simple steps: forward propagation\\nand backward propagation.\\nIn forward propagation, we input data into the network.\\nIt runs the input data through each of these functions\\nand makes the best guess about the correct output.\\nThen we compare the predicted output with the actual output.\\nWe calculate the lowest function to determine the difference\\nbetween the predicted output and actual output.\\nIn the next step called backward propagation,\\nthe neural network adjusts its parameters\\nproportionate to the error in its guess.\\n\\nSo after we find the loss function,\\nwe take the derivative of the loss function\\nin terms of the parameters of our neural network.\\nLastly, we iteratively update\\nthe weight parameters accordingly\\nso that the loss function\\nreturns the smallest possible loss.\\nWe call this step iterative optimization\\nas we use an optimizer to perform the update\\nof the parameters.\\nWe call this process gradient-based optimization.\\nAutomatic differentiation, or auto diff,\\nis a set of techniques that allows to compute gradients\\nfor arbitrary complex loss functions efficiently.\\n\\nIf this sounds terrifying,\\nremember, we are just calculating partial derivatives.\\nIf you still need help with the concept of derivatives,\\nI highly recommend\\nthe Machine Learning Foundation calculus course.\\nWe can calculate derivatives in a few different ways.\\nThe first one is numerical differentiation,\\nwhich follows the definition of a derivative.\\nA derivative of Y with respect to X\\ndefines the rate of change of Y with respect to X.\\n\\nWe can write it down using the following formula.\\nThe cons are the computation costs,\\nwhich increase as we increase the number of parameters\\nin the loss function, the truncation errors,\\nand the round-off errors.\\nAnother method is symbolic differentiation\\nwhich is done in calculus.\\nBasically, we are using a set of rules,\\nmeaning a set of formulas that we can apply\\nto the loss function to get the gradients.\\n\\nFor example, if we want to calculate\\nthe derivative of a function, f of x equals 3x squared\\nminus 4x plus 5.\\nWhen we apply the symbolic rules,\\nwe get derivative of f equals 6x minus 4.\\nThe cons are it is limited\\nto the already defined symbolic differentiation rules,\\nso it cannot be used\\nfor differentiating a given computational procedure\\nand the computation costs,\\nas it can lead to an explosion of symbolic terms.\\n\\nThe way to avoid the above issues\\nis to use automatic differentiation.\\nEvery complex function that we want to differentiate\\ncan be expressed as a composition of elementary functions.\\nFor those elementary functions,\\nwe could apply symbolic differentiation,\\nwhich would mean storing and manipulating\\nsymbolic forms of derivatives.\\nBy using automatic differentiation,\\nwe don't have to go through the tedious process\\nof simplifying the expressions.\\n\\nInstead, we can just evaluate a given set of values.\\nAnother benefit of automatic differentiation\\nis that our function can contain if-else statements,\\nfor loops, or recursion.\\nTo understand automatic differentiation,\\nwe can represent the flow using a computational graph.\\nIt's a directed graph,\\nand nodes represent mathematical operations.\\nFor example, if you have a function f of x, y, z\\nequals x minus y multiplied by z,\\nand the variables x, y, and z are x equals 2,\\ny equals 1, and z equals 5.\\n\\nOn our graph, we can label an intermediate variable 'a'\\nthat will store the computed value of x minus y,\\nand final variable f which will store the final value:\\nx minus y multiply by z.\\nAutograd package provides automatic differentiation\\nfor all operations on tensors.\\nWhen we want to train or optimize the neural network,\\nthis requires computing the gradient.\\nIn the forward pass step, we substitute the variables\\nand get the final value f:\\n2 minus 1 multiplied by 5 equals 5.\\n\\nIn the next step, using automatic differentiation,\\nwe find gradients of f with regards to the input variables.\\nWe'll calculate gradients of the loss function\\nwith respect to the weights by using chain rule.\\nNow, enough math.\\nLet's see this simple implementation\\nof automatic differentiation using autograd package.\\nPyTorch autograd package can automatically differentiate\\nall tensor operations.\\nIt's super helpful for backpropagation computations\\nbetween our neural networks.\\n\\nPlus, we can easily access individual gradients\\nthrough a variables grad attribute.\\nLet's head on to Colab notebook.\\nWe'll define tensors and set the required gradient\\nequal to true to enable gradients to be computed.\\nWe can do that by assigning the keyword\\nfor requires grad as true.\\nNow we are computing the intermediate variable 'a'\\nthat is equal to x minus y,\\nand then computing to function f.\\n\\nNext, let's go ahead and call backward function,\\nand then the module computes\\nall the backpropagation gradients automatically.\\nAnd finally, let's print gradients\\nby accessing them using a variables grad attribute.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:6605e382498e6fccaed114c3\",\"duration\":600,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code challenge: Split tensors to form new tensors\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1006784\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:3888259\",\"duration\":113,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Split tensors to form new tensors\",\"fileName\":\"2706322_en_US_04_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":145,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn one solution for splitting tensors to form new tensors.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4212748,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Lecturer] For this challenge,\\nwe need to implement four functions\\nusing PyTorch building functions.\\nHere is how we solve this challenge.\\nOur task involves splitting two tensors, x and y,\\nusing PyTorch chunk and split functions.\\nWe use the chunk function when we want to divide the tensor\\ninto specified number of smaller tensors.\\nIt attempts to divide them as evenly as possible.\\n\\nFor the first implementation, I have called chunk function\\nwith parameter four and a dimension value equal to zero.\\nThis means we are dividing tensor x\\ninto four smaller tensors along the first dimension or rows.\\nYou can think of it as splitting the matrix\\ninto four smaller matrices,\\neach with roughly the same number of rows.\\nFor the second task, I've used chunk four,\\nwhich splits one dimensional tensor y\\ninto four smaller 1d tensors.\\n\\nSince y has 16 elements, each chunk will have four elements.\\nFor third task, we use split\\nsince we need more control over the sizes\\nof the resulting tensors.\\nIn this case, we specified the exact size of each chunk.\\nWe split x into two parts, the first with five rows\\nand second with three rows along first dimension.\\nLastly, we split y into three chunks\\nwith the specified lengths.\\n\\nThe first chunk has four elements\\nand the next two have six elements each.\\nFinally, if you go ahead and click on Test My Code,\\nyou can see that that is the right answer,\\nand we can scroll through\\nand see all of our modified tensors.\\n\"}],\"name\":\"4. Manipulate Tensors\",\"size\":37528332,\"urn\":\"urn:li:learningContentChapter:3885249\"},{\"duration\":1280,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3887270\",\"duration\":151,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to the DL training process\",\"fileName\":\"2706322_en_US_05_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Append root with part 2\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":178,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, explore the basic pipeline used to train, test, and deploy your deep learning models.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4345518,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] There are different ways\\nto build your deep learning model.\\nYou can achieve it using supervised learning,\\nunsupervised learning, or semi-supervised learning.\\nEither way, you decide you're going to use the same pipeline\\nto train, test, and deploy your deep learning model.\\nThe process begins with the data preparation stage.\\nAs it's name suggests, we load the generic data,\\nwhich can be in many different formats,\\nsuch as text, images, videos, audio files, et cetera,\\nfrom an external source,\\nand we convert it to numeric values\\nsuitable for model training.\\n\\nThese numeric values are in form of tensors.\\nThen tensors need to be pre-processed during transforms,\\nand we group them with batches\\nthat can be passed into the model.\\nThe second stage is the model development stage\\nthat consists of three parts,\\ndesigning the model,\\ntraining the model using training data,\\nand testing its performance.\\nWe take the data set and split it into three data sets,\\ntraining data, validation data, and testing data.\\n\\nWhen we design the model,\\nwe use training data to train its parameters.\\nThe next step is the testing step\\nwhen we perform back propagation\\nand validate the model by passing invalidation data,\\nmeaning we measure model's performance against unseen data\\nand tune in hyperparameters.\\nBut what is the difference\\nbetween training data and validation data?\\nOne of the common problems in deep learning is overfitting.\\nBasically, model becomes really good\\nat recognizing what it has been trained on,\\nbut cannot recognize examples it hasn't seen.\\n\\nIn order to prevent that, we use the validation set.\\nValidation is a crucial step\\nin measuring your model's performance.\\nDuring this process,\\nyou can evaluate training data against validation data\\nthat has never been used.\\nIn the last step called model deployment,\\nyou can save the model to the file\\nor deploy the model to a product or service.\\nThe model is usually deployed to a production environment\\non a cloud server or to an edge device.\\n\\nNow that we are familiar with the deep learning process,\\nwe can expand on the data preparation step in the next video\\nand see how it's done on a simple network.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3889243\",\"duration\":97,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data preparation\",\"fileName\":\"2706322_en_US_05_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":111,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn the steps of data preparations and why they are important.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2701886,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Data preparation is the first step\\nin developing a deep learning model.\\nThis step consists of loading the data, applying transforms,\\nand batching the data using PyTorch built-in capabilities.\\nWe won't carry how to generate a good dataset,\\nas we use an existing popular academic dataset\\ncalled CIFAR-10, developed by researchers from the Canadian\\nInstitute for Advanced Research, or short, CIFAR.\\n\\nCIFAR-10 dataset is a subset\\nof a much larger dataset with 80 million images in it.\\nIt consists of 60,000 small color photographs of objects\\nfrom 10 classes divided into 50,000 training images\\nand 10,000 test images.\\nHere is the table with class labels\\nand their associated integer values.\\nWe use a Python library called Torchvision,\\nas it has classes that support computer vision.\\n\\nThe Torchvision datasets module provides several\\nsubclasses to load image data from standard data sets,\\nsuch as our CIFAR-10 dataset.\\nTo create a training dataset using the existing CIFAR-10\\ndataset we'll import the torch and then import the CIFAR-10\\ndataset by typing from Torchvision dataset import CIFAR-10.\\nI'll show you how to load, summarize,\\nand display the training and testing dataset.\\n\\nSo let's dive into a mini deep learning project.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3888260\",\"duration\":157,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data loading\",\"fileName\":\"2706322_en_US_05_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Append with PU\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":false,\"rawDurationSeconds\":201,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Explore built-in classes and utilities for loading various types of data and learn how to apply them to pull the data from its source and create dataset objects.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4792123,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We can load and convert different types\\nof data into format that are ready for training\\nand if it's often the most time consuming task.\\nLuckily, PyTorch has two important standard conventions\\nfor interacting with data called datasets and data loaders.\\nA dataset is a Python class that stores the samples\\nand their corresponding labels,\\nand the data loader feeds the data from the dataset\\ninto the network.\\nLet's take a look at our dataset\\ncalled the CIFAR-10 dataset.\\n\\nAs our first baby steps into deep learning,\\nwe'll run a deep neural network\\nthat was pre-trained on the object recognition task,\\nmeaning we'll be using it\\nto recognize the subject of an image.\\nWe have imported PyTorch as well as our dataset\\nin Matplotlib.\\nWe are going to use Matplotlib\\nto display some of the images.\\nHere is an example of how to load the training\\nand test data sets and print their shape.\\n\\nWhen we run the code, you can see\\nthat we have 50,000 images in the training set\\nand 10,000 images in the testing set,\\nand their dimensions are only 32 by 32 pixels,\\nso we have very small images.\\nNow, we also want to display the first few images,\\nso we'll do that by plotting the first 16 images.\\nHere we iterate over them and then call pyplot,\\nimshow and show functions.\\nFinally, let's go ahead and run our code.\\n\\nWe can see Matplotlib has created a plot\\nwith the first 16 images,\\nwhich are 32 by 32 pixel color images.\\nNumber three in our tensors output represents\\nthat the color image has three channels, RGB,\\nwhich represent red, green, and blue color.\\nOne more thing that we would like\\nis to examine the training data set.\\nWhen we print this shape,\\nwe see we have 50,000 images divided into 10 classes\\nand corresponding labels.\\n\\nThe label is an integer value\\nthat represents the class of the image.\\nFor example, airplane zero, automobile one, birds two, etc.\\nTo check the class labels,\\nwe literate over the pictures.\\nWhen we run the code,\\nwe see that on the first photo is a frog,\\non the second two is a truck, and on the fourth is a deer.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3886260\",\"duration\":160,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data transforms\",\"fileName\":\"2706322_en_US_05_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":262,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Data often needs to be adjusted before it is passed into the model for training and testing. Learn different ways to adjust the data by applying transforms.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5242735,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Before passing the data into the model,\\nwe usually need to adjust it.\\nFor instance, data values need to be converted\\nfrom one type of object to a tensor.\\nAnother example of adjustment is data values\\nthat may be augmented to create a larger dataset.\\nWe can achieve this adjustment by applying transforms.\\nPyTorch has a torchvision library\\nthat supports common computer vision transformations\\nin the torchvision.transforms\\nand torchvision.transforms.v2 models.\\n\\nTransforms are common image transformations\\nthat we can use to transform or augment data\\nfor training or inference of different tasks.\\nFor example, image classification, detection, segmentation,\\nand video classification.\\nHere we have important transforms from torchvision.\\nWe'll then define a list of transforms\\nusing transforms.Compose function,\\nwhich composes several transforms together.\\nWe'll scale image data to 64 by 64 resolution,\\nthen turn it into a tensor and normalize the tensor\\naround the specific set of mean\\nand standard deviation points.\\n\\nYou may wonder where do these values\\nfor mean and standard deviation come from?\\nYou can calculate these statistics yourself\\nby iterating through the training set\\nand computing the mean and standard deviation\\nof each channel across all images.\\nHowever, since these values are standard for CIFAR-10,\\nthey're often provided in tutorials, documentation,\\nor widely used libraries like torchvision\\nto simplify the pre-processing step for users.\\nWithout normalization, these values would typically range\\nfrom zero to 255 for each channel,\\nrepresenting the intensity of RGB component of each pixel.\\n\\nAs the input passes through the layers of the network,\\nthere is a lot of multiplication,\\nso maintaining values between zero and one\\nprevents these values from increasing\\nduring the testing phase,\\nwhich is called the exploding gradient problem.\\nFinally, we use transforms\\nto create a training data dataset.\\nTo take a look at the transforms,\\nwe can simply print the training dataset\\nfor the first image.\\nOne more thing to add\\nis that we can also define a different set of transforms\\nfor testing dataset and apply them to our test data.\\n\\nLet's do that.\\nFor example, we may not want to resize;\\nwe want to convert the image to tensors and normalize them,\\nas you can see here in our code.\\nLet's go ahead and display it.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3890256\",\"duration\":175,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data batching\",\"fileName\":\"2706322_en_US_05_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Append with PU\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":260,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In the process of model training data is passed in small batches at each iteration. That way training is more efficient training and accelerated.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5953046,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] A data loader feeds the data\\nfrom the dataset into the neural network.\\nAt the core of PyTorch data loading utility\\nis a torch.utils.data.DataLoader class.\\nIt represents a Python iterable over a dataset,\\nwith support for map-style and iterable-style datasets,\\ncustomizing data loading order, automatic batching,\\nand single and multi-process data loading.\\nYou can find more about it\\non the PyTorch documentation page here.\\n\\nThe neural network trains best with batches of data,\\nmeaning rather than using the complete dataset\\nin one training pass,\\nwe've used mini batches,\\nusually 64 or 128 samples.\\n(upbeat music)\\n(buzzer buzzing)\\nThat's right.\\nSmaller batches require less memory than the entire dataset,\\nand it results in more efficient and accelerated training.\\nDataLoader has, by default, a batch_size of one,\\nso we will want to change that.\\n\\nSo DataLoader has a parameter called batch_size\\nwhich represents a number of images\\nthat go through the network before we train and update it.\\nNow, let's head on to the Colab notebook.\\nWe are using the same example and introducing DataLoader.\\nOne more thing we have to do\\nis import DataLoader from torch.utils.data.\\nAs we can see in our code, we have two dataset.\\nThe training set,\\nwhich is used in the training pass to update the model,\\nand the testing set,\\nwhich is the final dataset that we use\\nto evaluate the model's performance.\\n\\nTo build our data loaders,\\nwe'll set the batch_size to 16\\nand create training_data_loader and test_data_loader\\nby calling DataLoader and passing two parameters,\\ntraining_data or test_data and batch_size.\\nThe most commonly used parameters\\nare the dataset, batch_size, shuffle, sample,\\nand num_workers parameters.\\nNow in our example,\\nwe could also pass in the third parameter called shuffle\\nand set it to true if we want to shuffle our dataset,\\nso the data loader returns a random sample of data.\\n\\nLet's go ahead and do that for the training_data dataset.\\nAnd for the test data_dataset,\\nwe'll set it to false\\nas we want to have repeatable test results.\\nNow in our example,\\nwe could also pass in the third parameter called shuffle\\nand set it to true if we want to shuffle our dataset\\nso the data loader returns a random sample of data.\\nLet's go ahead and do that for the training_data dataset\\nand for the test_data dataset, we'll set it to false\\nas we want to have repeatable test results.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3884260\",\"duration\":286,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Model development and training\",\"fileName\":\"2706322_en_US_05_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Append with PU\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":false,\"rawDurationSeconds\":382,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn different steps of model development and importance of the each step. Explore fundamental processes used for training models.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9549213,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] After taking care of preparing data sets,\\nwe can finally explore model development.\\nIt consists of a few steps,\\nmodel design, training, validation, and testing.\\nIn the model design step,\\nwe can design one or more model architectures\\nand initialize weights and biases.\\nUsually we take an existing design and modify it.\\nWe won't be covering this step in more detail\\nas we are taking care to understand the basics.\\nIn model training, we feed the training data into the model,\\ncalculate the error, and then adjust the parameters\\nto improve the model's performance.\\n\\nAfter the training, in the validation step,\\nwe measure the model's performance\\nagainst the data that wasn't used in training.\\nLet's explore how to build our first neural network.\\nWe have already imported libraries and data set.\\nHere we define the neural network and we call it net.\\nThen we fill out the init and forward functions.\\nIn init, we are adding layers to our network.\\nYou can think of them as filters and lenses\\nthat help our model to focus\\non important parts of the image.\\n\\nFor example, the first layer called self.conv1\\nis the first convolutional layer.\\nIt takes three input channels for free channel color images,\\noutputs six channels and uses five by five kernel.\\nSelf pool is the max pooling layer\\nwith two by two window and a stride of two.\\nIt is used to reduce the spatial dimensions of the output\\nfrom the convolutional layers.\\nSelf.conv2 is the second convolutional layer.\\n\\nIt takes the six output channels from conv1 as input\\nand output 16 channels.\\nSelf.fc1 is the first fully connected layer.\\nThe input size is 16 by 5 by 5,\\nwhich is derived from the output dimensions\\nof the last convolutional layer.\\nSelf.fc2, as the name suggests,\\nis the second fully connected layer\\nand self.fc3 is the final fully connected layer\\nthat outputs 10 channels,\\nwhich correspond to the 10 classes in our cipher 10 dataset.\\n\\nThe forward function defines\\nhow data flows through the network\\nin both training and making predictions.\\nIt takes two parameters, self and x.\\nX is passed through these layers sequentially.\\nHere we have a view function which reshapes the tensor\\nbefore feeding it to fully connected layers,\\nand F.relu,\\nwhich for example, when applied to the output of conv1\\nresults in the tensor where all negative values\\nare set to zero and positive values are left unchanged.\\n\\nNext, we instantiate the model\\nby setting up the necessary tools\\nfor training the network, loss function and optimizer,\\nas you can see in the code.\\nThen we load and transform the data.\\nAnd finally, in the training step,\\nour model learns by adjusting its weights\\nbased on the computed loss and gradients.\\nIt involves iterating over a data set multiple times,\\nwhich we call epochs.\\nIn our case, we are looping 10 times.\\nNow we initialize a variable\\nto accumulate the loss over each batch.\\n\\nThis inner loop iterates over the training data set\\nand the train loader provides batches of data,\\nmeaning images and labels.\\nThen we unpack the data into images\\nand their corresponding labels.\\nHere we use the optimizer to zero all the gradients\\nfrom weights and biases.\\nThis prevents the accumulation of gradients\\nfrom multiple passes.\\nAfter we pass the input data through the network\\nto get outputs, we compute the loss\\nby comparing the model's outputs with the actual labels.\\n\\nBy calling loss.backward function,\\nwe perform a backward pass\\nto compute the gradient of the loss\\nwith respect to the network parameters.\\nFinally, we update the weights\\nbased on the computed gradients\\nand accumulate the loss over the batches for reporting.\\nIn the if statement, we use print inside it\\nto log the average loss every 2000 mini batches\\nso we can monitor the training.\\nI have already executed this upfront\\nas usually it takes around 10 minutes\\nfor this code to finish execution.\\n\\nOnce training is finished,\\nwe can see finished training message,\\nwhich we will use to indicate\\nthe end of the training process.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3884259\",\"duration\":250,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Validation and testing\",\"fileName\":\"2706322_en_US_05_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Append with PU\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":false,\"rawDurationSeconds\":295,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Validation and testing are important parts of model development as they take care that overfitting does not occur and that the model performs well against unseen data. Learn the key steps of each of them.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10043035,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Validation and testing are important parts\\nof model development as they take care\\nthat overfitting does not occur\\nand that the model performs well against unseen data.\\nLearn the key steps of each of them.\\nIn the validation step, we use a separate set of data,\\nwhich we haven't used previously in training\\nand call this set validation set.\\nThe main goal is tuning hip parameters such as learning rate\\nor number of epochs,\\nso we can provide an early indication\\nof how our model is performing.\\n\\nWe have already imported libraries and dataset.\\nNext, we define a neural network with init\\nand forward function.\\nAfter that, we instantiate the model\\nand define the loss function and optimizer.\\nTo load and transform the data,\\nwe will create transformations for the training data.\\nWe'll create validation set from our training data\\nby using torch.utils.data.randomsplit,\\nwhich splits the training data into the training set\\nof 40,000 images and validation set of 10,000 images.\\n\\nThen we set up loaders for the training set\\nand validation set with 16 images per batch\\nand enable shuffling.\\nA data loader for the test set is created\\nwith a batch size of four,\\nand we don't use shuffling.\\nAt the end of each epoch,\\nwe are going to evaluate our model on the validation set.\\nWe will usually check the loss and accuracy.\\nThe testing step is crucial\\nfor evaluating the model's performance on unseen data.\\nThe test set is a separate data set\\nthat the model has never seen during training.\\n\\nIt provides us with a final evaluation\\nof the model's performance.\\nWe are looping 10 times to train the model\\nas we have 10 epochs.\\nWe switch the model net to training mode.\\nThen we initialize the running loss to zero.\\nIn this inner loop,\\nwe iterate over our training data loader.\\nTrain loader batches the training data and enumerate,\\nwill give us the index and the data for each iteration.\\nThe data table is unpacked into inputs\\nthat represent features and labels.\\n\\nThen we zero out any gradients from the previous batch\\nbefore calculating the gradients.\\nThe input passes through the network\\nand it performs forward pass and returns the outputs.\\nLastly, we calculate the loss\\nusing the loss function criterion.\\nWe compare the network outputs with labels\\nand compute the gradient of the loss.\\nThen the optimizer updates the models parameters,\\nand we add the loss for this batch to running loss\\nso we can keep track of the total loss of epoch.\\n\\nAfter each training epoch, the model is switched\\nto evaluation mode using net.eval function,\\nwhich deactivates dropout\\nand normalizes batch normalization layers\\nfor consistent behavior during inference.\\nThen we iterate over the validation data set,\\nand for each batch of data, it computes the model's output\\nwithout updating the model's weights\\nas gradients are not needed for validation.\\n\\nWe calculate accuracy and loss on this validation data,\\nproviding insight into how well the model performs\\non data it hasn't been trained on.\\nAfter looping through all test data,\\nwe calculate the overall accuracy by dividing the number\\nof correct predictions by the total number\\nof samples in the test set.\\nAt the end of the epoch, we print the average training loss,\\nthe average validation loss,\\nand the validation accuracy percentage.\\n\\nThese averages are calculated by dividing the total loss\\nby the number of batches in the loader.\\nFor example, for average training loss,\\nwe divide the training loss by the number\\nof batches in the train loader.\\n\"}],\"name\":\"5. Developing a Deep Learning Model\",\"size\":42732224,\"urn\":\"urn:li:learningContentChapter:3890263\"},{\"duration\":48,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3890257\",\"duration\":48,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"2706322_en_US_06_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Twitter handle @Tsemenski\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":50,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"This video explains where to go from here.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1493818,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Terezija] Congratulations on completing this course.\\nNow that you have seen the basics of using PyTorch,\\nyou can try applying it to your own projects.\\nI encourage you to play around with the different datasets\\nthat you can find on Kaggle.\\nYou can look for more PyTorch courses\\ncoming from me in the future.\\nYou can also search our library\\nfor more deep learning courses to continue your journey.\\nI want to wrap up by saying thank you.\\nYou have devoted time to learning this material with me,\\nand I want you to know that I don't take that for granted.\\n\\nI really do appreciate your engagement\\nas well as your feedback on this course.\\nIf you have any questions, please feel free\\nto get in touch on LinkedIn and on Twitter.\\nIf you enjoyed this course, I'd love to know.\\n\"}],\"name\":\"Conclusion\",\"size\":1475670,\"urn\":\"urn:li:learningContentChapter:3885250\"}],\"size\":154611905,\"duration\":4906,\"zeroBased\":false},{\"course_title\":\"Hands-On PyTorch Machine Learning\",\"course_admin_id\":2884016,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":2884016,\"Project ID\":null,\"Course Name\":\"Hands-On PyTorch Machine Learning\",\"Course Name EN\":\"Hands-On PyTorch Machine Learning\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"Many of the world\u00e2\u20ac\u2122s most exciting and innovative new tech projects leverage the power of machine learning. But if you want to set yourself apart as a data scientist or machine learning engineer, you need to stay up to date with the current tools and best practices for creating effective, predictable models.&lt;br&gt;&lt;br&gt;In this course, instructor Helen Sun shows you how to get up and running with PyTorch, the open-source machine learning framework known for its simplicity, performance, and APIs. Explore the basic concepts of PyTorch, including tensors, operators, and conversion to and from NumPy, as well as how to utilize autograd, which tracks the history of every computation recorded by the framework. By the end of this course, you\u00e2\u20ac\u2122ll also be equipped with a new set of skills to get the most out of Torchvision, Torchaudio, and Torchtext.\",\"Course Short Description\":\"Discover the fundamentals of creating machine learning models with PyTorch, the open-source machine learning framework.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":1992748775,\"Instructor Name\":\"Helen Sun\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Technology Strategist and Thought Leader\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2022-12-13T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/hands-on-pytorch-machine-learning\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Artificial Intelligence for Technology\",\"Primary Software\":\"PyTorch\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":3412.0,\"Visible Video Count\":17.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":51,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2615032\",\"duration\":51,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Explore the capabilities of PyTorch\",\"fileName\":\"2884016_en_US_00_01_WX30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1591138,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Dr. Sun] PyTorch is one  \\n of the most widely adapted ML frameworks.  \\n If you know TansorFlow, learning PyTorch is essential  \\n to establish a foundation in AI and ML.  \\n And if you are just starting,  \\n PyTorch is an excellent place to start learning.  \\n Working the field of AI and ML,  \\n understanding the basics of PyTorch is imperative.  \\n In this course, I'll teach you the basics  \\n of computer vision, natural language processing,  \\n and audio processing through some hands-on experiences.  \\n Hello, I'm Dr. Helen Sun  \\n and I'm a senior engineering leader at Meta,  \\n working in the AI and ML space.  \\n Join me in this course to learn about the PyTorch platform  \\n that is used worldwide  \\n to support the work of AI research scientists  \\n and ML engineers.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":1591138,\"urn\":\"urn:li:learningContentChapter:2621017\"},{\"duration\":879,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2622022\",\"duration\":253,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"PyTorch overview\",\"fileName\":\"2884016_en_US_01_01_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"In this video, learn about PyTorch\u2019s tensor library and neural networks at a high level. This is a context-setting video and an overview of key concepts for the remainder of the videos.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7767736,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] PyTorch is one of the most popular  \\n deep learning frameworks,  \\n and it continues to gain adoption  \\n in both research communities and enterprises.  \\n It is an open source machine learning framework  \\n developed and maintained by Meta.  \\n The key benefits of PyTorch include  \\n quick iteration for research,  \\n seamless eager graph mode transition with TorchScript,  \\n and production ready with TorchServe.  \\n It has a backend called Torch.Distributed,  \\n which enables scalable distributed training  \\n and optimization.  \\n PyTorch also has a rich ecosystem of tools and libraries  \\n that extends development in computer vision,  \\n natural language processing, and more.  \\n Some featured projects in this ecosystem include Captum  \\n for model interpretability,  \\n geometric deep learning for irregular input data,  \\n such as graphs and Point Cloud,  \\n and Skorch that enables psychic learn compatibility.  \\n In this course, I'll be using PyTorch 1.10,  \\n and cover some of its top features,  \\n including tensors, autograd, APIs, and libraries.  \\n A PyTorch tensor is a multi-dimensional container of data.  \\n It is similar to a NumPy array.  \\n Torch defines 10 tensor types with CPU and GPU variants,  \\n including boolean, 8, 16, 32, 64 bit integer,  \\n 16, 32 and 64 bit floating point,  \\n 32, 64, and 128 bit complex.  \\n There are many ways to create a tensor,  \\n such as with pre-existing data.  \\n There are also many tensor operations in PyTorch,  \\n but they can be grouped into two categories,  \\n including slice and math.  \\n Autograd is what earns PyTorch it's popularity  \\n for fast and flexible iteration.  \\n PyTorch traces the computation dynamically at runtime  \\n to get correct gradients to drive learning.  \\n The key for model training is to minimize the loss  \\n through adjusting the model's learning weights.  \\n The gradients over the learning weights tells us  \\n what direction to change each weight  \\n to reduce the loss function.  \\n With deep learning, the number of derivatives  \\n goes up exponentially.  \\n Autograd tracks the history of every computation,  \\n and as a result, greatly speeds  \\n the local derivative computation.  \\n The primary API to PyTorch is Python.  \\n These APIs allow you to interact with PyTorch  \\n through tensors, views, CUDA, autograd,  \\n quantization, and storage.  \\n PyTorch also provides you with a C++ interface  \\n with access to PyTorch functionalities,  \\n including tensor and autograd, serializing PyTorch models,  \\n and building C++ extensions through TorchScript.  \\n The C++ APIs can also be used  \\n to write compact, performance-sensitive code  \\n with deep learning capabilities  \\n to perform ML inference on mobile platforms.  \\n There are five main PyTorch libraries.  \\n The first is Torchaudio for audio signal processing.  \\n Next, we have Torchtext containing data processing utilities  \\n and data sets for natural language processing.  \\n The third library, Torchvision  \\n with computer vision data sets, model architecture  \\n and common image transformation.  \\n Fourth is Torchserve, a highly performant and flexible  \\n serving tool for PyTorch eager modes and models.  \\n And finally, Torch_xla that runs PyTorch  \\n on xla devices such as TPU.  \\n Now that you have a high level overview,  \\n let's dig deep into the top features of PyTorch.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2617029\",\"duration\":230,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"PyTorch environment setup\",\"fileName\":\"2884016_en_US_01_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to install PyTorch and configure the environment. Get the environment set up so that you can start to work on the use cases.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9006947,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] To set up the PyTorch environment,  \\n we first download the PyTorch library  \\n from the PyTorch website.  \\n Click on the Install button here.  \\n And use the default tab here, Start Locally.  \\n Scroll down to go to the selection grid.  \\n You first select the build.  \\n You'll then choose the OS.  \\n The options are Mac, Windows, and Linux.  \\n And we'll choose Mac.  \\n For the package manager, we'll choose Pip.  \\n You can also select Conda, LibTorch,  \\n or build it using the source code from GitHub.  \\n There are two language options,  \\n Python and C++/Java.  \\n We'll choose Python.  \\n The compute platform is default here  \\n because we're using the Mac OS.  \\n Once you make all of the selections,  \\n the site will give you a command to install PyTorch:  \\n pip3 install torch torchvision torchaudio.  \\n And you just run this command to install PyTorch  \\n if you prefer to use the most recent stable version.  \\n The previous versions of PyTorch  \\n are available here on this website.  \\n Just click on this Previous PyTorch Versions tab.  \\n I'll be using the 1.10.1 for this course.  \\n And you copy and paste this command and run it in iTerm.  \\n So here is the command,  \\n I'll run it here,  \\n but I also added an optional parameter  \\n to suppress warnings here.  \\n So it says, \\\"no warning script location.\\\"  \\n If you receive a message saying \\\"Command Not Found,\\\"  \\n this is probably because you don't have  \\n Pip3 environment set up properly.  \\n You need to use a package manager on Mac OS called Homebrew  \\n to install Pip3.  \\n So here's the command.  \\n Now your first command to run PyTorch installation  \\n can be resumed right here.  \\n With this installation,  \\n it includes numpy, pillow, torch,  \\n torchaudio, torchvision, and typing-extensions.  \\n We'll use these packages in our later modules.  \\n Once you see your screen saying,  \\n \\\"Successfully installed those packages\\\"  \\n or \\\"These requirements already satisfied,\\\"  \\n then you're ready to go.  \\n A few other module installations  \\n that are useful for this course include:  \\n Matplotlib and Jupyter Notebook.  \\n To install Matplotlib,  \\n I'll use the command  \\n pip3  \\n install  \\n matplotlib.  \\n This command will install Matplotlib  \\n along with a number of packages, including:  \\n Pyparsing,  \\n python-dateutil,  \\n Packaging,  \\n Kiwisolver,  \\n Fonttools,  \\n and cycler.  \\n Once it is all and completed,  \\n you'll receive the following message here.  \\n To install the Jupyter Notebook,  \\n you can either use Anaconda or Pip3.  \\n Similar to installing PyTorch,  \\n we'll use Pip3 as follows.  \\n To verify the installation,  \\n try the following command:  \\n python3 -m notebook.  \\n You should see the console launching  \\n the Notebook environment in the browser here.  \\n There will be additional environmental setup  \\n in later modules,  \\n but this concludes the general setup.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2621016\",\"duration\":143,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"PyTorch use case description\",\"fileName\":\"2884016_en_US_01_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn about the use case, objectives, and problem space. This is an important context for model development and learning.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4163598,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] The use case I'll be using  \\n throughout this course is based on a project called Piraeus.  \\n The overall objective of Project Piraeus  \\n is to democratize knowledge.  \\n Piraeus is a portal that brings together learners and tutors  \\n in a digital portal.  \\n It then leverages machine learning and AI  \\n to find the best tutors for every learner  \\n and to help tutors and learners  \\n to devise a customized learning plan.  \\n Many students struggle in rigid linear learning systems.  \\n Education and learning is personal.  \\n Unfortunately, one-on-one learning, private tutoring,  \\n and custom-designed learning paths are just too expensive  \\n for many students and communities to access.  \\n AI may be able to help.  \\n Artificial intelligence, AI, can play a key role  \\n to achieve this objective and vision.  \\n AI provides the capability to mimic human judgment.  \\n In this case, it'll mimic the judgment  \\n of a tutor or a teacher with dedicated attention  \\n to a student's unique learning needs.  \\n The focus of our use case is to develop AI models  \\n to better match tutors with learners  \\n based on learning, characteristics, and learning objectives.  \\n We look to generate a guided learning path,  \\n select learning material,  \\n weave in checkpoints and assessment  \\n through feedback along the way,  \\n and continue to refine and adapt the next set of milestones.  \\n In this course, I'll demonstrate how to develop  \\n and train the machine-learning models,  \\n using as inputs the characteristics  \\n of the learners and tutors,  \\n the performance of the learner,  \\n along with the metrics associated with the tutoring sessions  \\n including number of sessions, duration of sessions,  \\n time between sessions.  \\n The inputs to the models  \\n and predictive methods must be consistently monitored  \\n to determine the most critical inputs  \\n that produce the best possible outcome  \\n for both learners and tutors.  \\n This type of rapid iteration of exploration,  \\n experimentation, and productization  \\n makes PyTorch the ideal choice for the AI and ML framework.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2615033\",\"duration\":253,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"PyTorch data exploration\",\"fileName\":\"2884016_en_US_01_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn about the dataset for this use case. Understanding the dimensionality, categories, and distributions of the data is fundamental to developing ML algorithms.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8855054,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - PyTorch can be used to explore a dataset  \\n for model development.  \\n PyTorch provides two data primitives  \\n DataLoader and Dataset.  \\n The Dataset class stores the samples  \\n and the corresponding annotation labels.  \\n The DataLoader class wraps in iterable around  \\n the Dataset class to enable easy access to the samples.  \\n These two classes allow you to use preloaded data sets  \\n and upload your own data.  \\n PyTorch comes with a set of domain libraries  \\n that provide many preloaded data sets.  \\n They are subclasses of torch utils: data, dataset.  \\n These libraries and subclasses implement  \\n specific functions to the particular data.  \\n One great example is Fashion MNIST  \\n which we will explore later.  \\n These preloaded data sets and functions  \\n enable ML engineers to prototype and benchmark their models.  \\n You can find these data sets and functions  \\n on the PyTorch website  \\n with the links included in this slide.  \\n For example, for image dataset  \\n go to pytorch.org/vision/stable/datasets.html.  \\n For text datasets  \\n go to pytorch.org/text/stable/datasets.html.  \\n And lastly, for audio data sets  \\n go to pytorch.org/audio/stable/datasets.html.  \\n Now let's take a look at the Fashion MNIST dataset.  \\n I'll first import the needed libraries here.  \\n I'll then create two data sets  \\n one for training  \\n and one for testing.  \\n The only difference is to set the parameter of train to true  \\n for training data set  \\n and false for testing data set.  \\n Next, I'll create a validation sample  \\n from the training data set.  \\n Next, I'll create three DataLoaders  \\n one for training, one for validation, and one for testing.  \\n Next, I'll create a data iterator  \\n to populate the image and label tensors.  \\n Lastly, I'll use the mat plot lib to plot the data.  \\n Next, let's upload and look at our own dataset.  \\n We'll go through four steps to load our data set.  \\n Step one, import all necessary libraries  \\n for loading our data.  \\n We will be uploading a CSV file that contains tutor's data.  \\n For that, we need to import the following libraries.  \\n Step two, find the file path.  \\n For this exercise  \\n I'll just upload our CSV file from a local drive.  \\n And here is the file path, here in the documents folder.  \\n And we'll put this path into a variable called data path.  \\n Step three, loading the data.  \\n We'll load the data using the pandas library  \\n we imported in step one.  \\n If you don't have pandas installed  \\n just run the following command: pip3 install pandas.  \\n Here we created a pandas dataframe called df  \\n to store the dataset.  \\n Now let's visualize the data  \\n using the pandas function display.  \\n And this is our dataset.  \\n Step four, convert a pandas dataframe into a PyTorch tensor.  \\n In order to use the data set and PyTorch for model training  \\n we'll need to convert it into a PyTorch tensor.  \\n Here's the code to do that.  \\n And we also printed out the tensor in this format.  \\n Now this wraps up our overview chapter.  \\n In the next chapters we'll dig into the details  \\n of the PyTorch libraries.  \\n \\n\\n\"}],\"name\":\"1. Preparation\",\"size\":29793335,\"urn\":\"urn:li:learningContentChapter:2615034\"},{\"duration\":891,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2622023\",\"duration\":213,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand PyTorch tensors\",\"fileName\":\"2884016_en_US_02_01_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"In this video, learn about the basic concepts of PyTorch tensors. Explore basic constructs that you are using for the rest of the use case implementation in PyTorch.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6395064,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Tensors are the building blocks  \\n of a neural network.  \\n A PyTorch tensor is the central data abstraction  \\n as a generalized form of arrays in n dimensions  \\n to run arbitrary computations.  \\n A tensor can be of any dimension.  \\n For example, zero dimensional tensors  \\n are a scale numbers like 0, 10, 100, et cetera.  \\n One dimensional tensors are array of numbers  \\n like an array of 9, 8, 7,  \\n or an array of 50, 30 10, et cetera.  \\n Tensors can also be two-dimensional.  \\n They are matrix of numbers like in 2x3 matrix  \\n with a two element array of an array.  \\n So for example, an array of 1, 2, 3 and a 7, 5, 3.  \\n So now let's create some tensors  \\n by importing torch and using the torch.Tensor methods.  \\n Here's our notebook, and here we are.  \\n We're creating some torch.Tensors,  \\n and here is the output of those tensors.  \\n The simplest way to set the underlying data type of a tensor  \\n is with an optional argument at creation time.  \\n Here is an example.  \\n We're setting the torch data type,  \\n the tensor data type, to be int16.  \\n There are 10 data types available.  \\n I have listed here boolean int8, uint8, int16, int32,  \\n int64, half, float, double, or bfloat.  \\n While training the model,  \\n you'll deal with higher dimensions.  \\n The neural network only accepts the dimension  \\n which is defined at the input layer  \\n while architecting the model.  \\n The dimension basically tells whether the tensor  \\n is zero-dimension, or one-dimension, or two-dimension,  \\n or even higher than that.  \\n Dimension of the tensor is also called  \\n the rank of the tensor.  \\n To get the size, you can use tensor.size method,  \\n or tensor.shape property,  \\n and to get the dimension of the tensor,  \\n use tensor.ndimension method, or tensor.ndim property.  \\n Let's take a look.  \\n There you have it.  \\n These are the examples.  \\n Sometimes working with a neural network,  \\n you need to change the dimension of the tensor.  \\n This can be done by using  \\n the tensor.view nrows, ncols method of the tensor.  \\n PyTorch tensor also provides flexibilities,  \\n such as in-place alterations, tensor copying,  \\n and changing the shape of the tensor,  \\n such as altering the dimensions of the tensor.  \\n One of the major advantages of PyTorch  \\n is its robust acceleration on CUDA-compatible NIVIDIA GPUs.  \\n CUDA stands for compute-unified device architecture,  \\n which is NIVIDIA's platform for parallel computing.  \\n By default, new tensors are created on the CPU,  \\n so we have to specify when we want to create our tensor  \\n on the GPU with the optional device argument.  \\n If you have an existing tensor living on one device,  \\n you can move it to another with a to method.  \\n It is important to know that,  \\n in order to do computation involving two or more tensors,  \\n all of the tensors must be on the same device.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2616025\",\"duration\":173,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand PyTorch basic operations\",\"fileName\":\"2884016_en_US_02_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn about the basic concepts of PyTorch operators. Explore basic constructs that you are using for the rest of the use case implementation in PyTorch.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5702499,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] PyTorch Tensors allow you to apply  \\n arithmetics on your tensors.  \\n PyTorch Tensors have over 300 operations  \\n that can be performed on them.  \\n You can apply these operations on Scala  \\n and multiple dimensions of tensors.  \\n The very basic operations on tensors are additions,  \\n subtractions, multiplications, and divisions.  \\n Here's an example for addition and subtraction.  \\n So we have two tensors  \\n and we are performing some additions and subtractions here,  \\n and you'll get those output.  \\n The major categories of operations  \\n include common functions such as abs, ceil, floor, clamp,  \\n trigonometric functions and their inverses  \\n such as pi, sin, asin, bitwise operations, comparisons,  \\n reductions such as max, min, standard, prod,  \\n unique, vector/matrices, and linear algebra operations.  \\n Now let's go over some more examples.  \\n First, here are some common functions  \\n like abs, ceil, floor, and clamp, and these are the output.  \\n Next, let's try a reduction function  \\n using the mean operator,  \\n and there you have the output.  \\n Finding maximum or minimum values in the Tensor  \\n can also be done by using tensor.max  \\n and tensor.min respectively.  \\n The expected output is right there.  \\n Now let's take a look at the trigonometric functions  \\n and their inverses.  \\n Supposed you have a tensor containing various values of pi  \\n and you want to apply the sine and cosine function on it.  \\n You can use the torch.sin tensor  \\n and torch.cos tensor as follows.  \\n And we import NumPy here as well.  \\n You can expect the output here.  \\n Sometimes you have to get an evenly spaced  \\n list of numbers between a range.  \\n You can use torch.linspace, start, end, and step to do that.  \\n Let's make it more interactive  \\n by plotting the sin minus pi to sin pi  \\n and cos minus pi to cos pi,  \\n making the dataset first here.  \\n Here we have the dataset.  \\n And last but not least,  \\n let's use Matplotlib to create the graph.  \\n Here you see the plotted output.  \\n The basic PyTorch operators offer a lot of functionalities  \\n and here is an example of that  \\n and hope you get the opportunity of exploring more.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2620024\",\"duration\":213,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand PyTorch NumPy Bridge\",\"fileName\":\"2884016_en_US_02_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to convert NumPy arrays to PyTorch tensors and vice versa. Explore the basic constructs that you are using for the rest of the use case implementation in PyTorch.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6786744,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Broadcasting is a way to perform an operation  \\n between tensors that have similarities in their shapes.  \\n This is an important operation in deep learning.  \\n The common example  \\n is multiplying a tensor of learning weights  \\n by a batch of input tensors,  \\n applying the operation to each instance  \\n in the batch separately  \\n and running a tester of identical shape.  \\n Here's an example with two by four  \\n multiplying one by four returns a tester of shape two,  \\n with dimension of one, with values of two and 16.  \\n If you're familiar with broadcasting semantics  \\n and NumPy ndarrays,  \\n you'll find the same rules apply with PyTorch.  \\n The exception to the same shape rule  \\n is tensor broadcasting.  \\n The rules for broadcastings are,  \\n one, each tensors must have at least one dimension,  \\n no empty tensors.  \\n Comparing the dimension sizes of two tensors,  \\n going from last to first, each dimension must be equal  \\n or one of the dimensions must be of size one  \\n or the dimension does not exist in one of the tensors.  \\n Tensors of identical shape,  \\n of course, are trivially broadcastable,  \\n as you saw earlier.  \\n Here are some examples of situations  \\n that honor the rules and allow broadcasting.  \\n We first have tensor A  \\n and tensor B, here, has the third and second dimensions  \\n identical to tensor A, with dimension one absent.  \\n Tensor C has third dimension equals to one,  \\n second dimension identical to A  \\n and first dimension empty.  \\n Tensor D has third dimension identical to A,  \\n second dimension equals to one  \\n and first dimension empty, and it is broadcastable as well.  \\n PyTorch's broadcast semantics are compatible with NumPy's,  \\n but the connection between PyTorch and NumPy  \\n goes even deeper than that.  \\n If you have existing ML or scientific code  \\n with data stored in NumPy ndarrays,  \\n you may wish to express the same data as PyTorch tensors.  \\n Whether to take advantage of PyTorch's GPU acceleration  \\n or its efficient abstractions for building ML models.  \\n It's easy to switch between ndarrays and PyTorch tensors.  \\n Let's take a look at some examples.  \\n Here's a conversion and here is the expected output.  \\n PyTorch creates a tensor of the same shape  \\n and containing the same data as NumPy array, going as far as  \\n to keep NumPy's default 64 bit float data type.  \\n The conversion can just as easily go the other way, here,  \\n as the example of a version.  \\n And the expected outcomes are here.  \\n It's also important to know that these converted objects  \\n are using the same underlying memory  \\n as their source objects,  \\n meaning that changes to one are reflected in the other.  \\n And here's an example,  \\n and the expected output is shown here.  \\n And this concludes the Intro to PyTorch NumPy Bridge.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2619025\",\"duration\":170,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand PyTorch autograd\",\"fileName\":\"2884016_en_US_02_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to use autograd to automate the computation of backward passes in neural networks. Explore the basic component within PyTorch and the important-to-implement neural networks for the rest of the use cases.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5241187,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Narrator] Automatic differentiation is a building block  \\n of every deep learning library out there.  \\n PyTorch's automatic differentiation engine called Autograd  \\n is a tool to understand how automatic differentiation works.  \\n Modern neural network architectures  \\n can have millions of learnable parameters.  \\n From a computational point of view,  \\n training a neural network consists of two phases.  \\n The forward pass computes the value of the loss function.  \\n In forward prop, the neural network makes its best guess  \\n about the correct output.  \\n It runs the input data through each of its functions  \\n to make this guess.  \\n The backward pass computes the gradients  \\n of the learnable parameters.  \\n In backdrop, the neural network adjusts its parameters  \\n proportionate to the error and its guesses.  \\n It does this through three steps.  \\n First, it traverses backwards from the output,  \\n then it collects the derivatives of the error  \\n with respect to the parameters of the functions,  \\n aka gradients.  \\n And lastly, it optimizes the parameters  \\n using gradient dissent.  \\n The forward pass is pretty straightforward.  \\n The output of one layer  \\n is the input to the next and so forth.  \\n Backward pass is a bit more complicated  \\n since it requires us to use the chain rule  \\n to compute the gradients of the weights  \\n to the loss function.  \\n It is impractical to calculate gradients  \\n of such large composite functions  \\n by solving mathematical equations,  \\n especially because these curves exist  \\n in a large number of dimensions  \\n and are impossible to fathom.  \\n This is where PyTorch's Autograd comes in.  \\n It abstracts complicated mathematics  \\n and helps us magically calculate gradients  \\n of high-dimensional curves with only a few lines of code.  \\n Let's run through an example.  \\n First, we need to import the libraries  \\n using Torch and Torch Autograd.  \\n Next, we'll use the Autograd variable function  \\n to create a variable.  \\n Let's now perform an operation of the variable.  \\n Y was created as a result of an operation,  \\n so it has a grad function.  \\n Next, let's perform more operations on y.  \\n Time to calculate the gradients now.  \\n Let's back prop now using out.backward  \\n which is equivalent to doing the following.  \\n And once we print, you should have got a matrix of 4.5.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2620025\",\"duration\":122,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Advanced PyTorch autograd\",\"fileName\":\"2884016_en_US_02_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to use autograd to automate the computation of backward passes in neural networks. Explore an advanced component within PyTorch an important-to-implement neural network for the rest of the use cases.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3771747,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] There's a lot more to AutoGrad and PyTorch.  \\n Let's try something more complex.  \\n In this first example,  \\n we first generate three random numbers,  \\n assign them to a variable called X,  \\n and we require, in this case, gradients for X.  \\n We then provide some operations on X  \\n to generate a new variable called Y,  \\n which will have a gradient function attached to it.  \\n Now, let's print out the gradient for X.  \\n So in your Neural Network,  \\n parameters that don't compute gradients  \\n are usually called frozen parameters.  \\n It is useful to freeze part of your model  \\n if you know in advance  \\n that you won't need the gradients of those parameters.  \\n This offers some performance benefits  \\n by reducing AutoGrad computations.  \\n Let's walk through a small example to demonstrate this.  \\n We'll first import some libraries,  \\n including torchvision, in this case.  \\n We load a pre-trained resnet18 model, in this case,  \\n and then freeze all of the parameters.  \\n Assume we want to fine tune the model  \\n on a new dataset with 10 labels.  \\n In ResNet, the classifier is the last linear layer,  \\n model.fc.  \\n We can simply replace it with a new linear layer,  \\n unfrozen by default,  \\n that acts as our classifier.  \\n Now, all parameters in this model,  \\n except for the parameters of model.fc, are frozen.  \\n The only parameters that compute gradients  \\n are the weights and bias of model.fc.  \\n Notice that although we register all the parameters  \\n in the optimizer,  \\n the only parameters that are computing gradients  \\n and hence, updated in the gradient descent,  \\n are the weights and the bias of the classifier.  \\n The same exclusionary functionality  \\n is available as a context manager in torch.no_grad.  \\n \\n\\n\"}],\"name\":\"2. PyTorch Basics\",\"size\":27897241,\"urn\":\"urn:li:learningContentChapter:2617031\"},{\"duration\":603,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2617030\",\"duration\":521,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Torchvision introduction\",\"fileName\":\"2884016_en_US_03_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to install Torchvision and understand the basic usage and implementation of the package. Understanding the basics of Torchvision helps you to further implement the use case in future sections.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15422991,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] TorchVision is a library within PyTorch  \\n for image and video processing,  \\n it contains a number of important and useful data sets,  \\n model architecture,  \\n as well as models and transformation operations  \\n that are commonly used for computer vision projects.  \\n There are seven main packages for the TorchVision library:  \\n dataset, IO, models, feature extraction,  \\n ops, transforms, and utils.  \\n Let's look at each one of them.  \\n TorchVision dataset,  \\n this package, as the name suggests,  \\n contains some of the most popular computer vision data sets.  \\n All data sets are subset of torch.util.data.Dataset,  \\n they have getitem and lamp methods implemented,  \\n as a result, you can pass any of them  \\n to torch.utils.data.DataLoader,  \\n which can load multiple samples in parallel  \\n using multiple processing workers.  \\n We covered the PyTorch DataLoader in previous chapters.  \\n One thing to note is that all data sets  \\n have identical implementations and signatures.  \\n All of them have these two common arguments:  \\n transform, and target transform.  \\n The former will transform the input,  \\n and latter, to transform the target.  \\n Some of the most popular data sets are CalTech,  \\n which refers to the CalTech 101 project  \\n that contains pictures of objects  \\n belonging to 101 categories,  \\n each category has about 40 to 800 images.  \\n CIFAR.  \\n The CIFAR 10 and CIFAR 100 are labeled data sets  \\n of the 80 million TinyImage data set.  \\n Cityscapes.  \\n The Cityscapes data set focuses on semantic understanding  \\n of urban street scenes,  \\n with 50 cities across different seasons,  \\n with 5,000 images finely annotated,  \\n and 20,000 coarsely annotated.  \\n ImageNet contains 14,197,122 annotated images  \\n according to the WordNet hierarchy.  \\n Since 2010, the dataset is used  \\n in the ImageNet Large Scale Visual Recognition Challenge,  \\n a benchmark in image classification and object detection.  \\n Many of you have heard the MNIST dataset,  \\n which was developed by none other, Yann LeCun,  \\n the chief scientist at Meta  \\n who was dubbed as one of the god fathers of deep learning.  \\n The MNIST dataset has become a standard benchmark  \\n for learning, classification, and computer vision systems.  \\n Torch Vision Dataset also has a number of extended dataset  \\n from MNIST, including EMNIST,  \\n Extend MNIST that constitutes  \\n more challenging classification tasks  \\n involving letters and digits,  \\n and that shares the same image structure and parameters  \\n as the original MNIST tasks,  \\n allowing for direct compatibility  \\n with all existing classifiers and systems.  \\n Fashion-MNIST, consisting of a training set  \\n of 60,000 examples, and a test set of 10,000 examples.  \\n Each example is a 28 by 28 gray scale image  \\n associated with a label from 10 classes.  \\n KMNIST, it's a data set which is a replacement  \\n for the MNIST data set, 28 by 28 gray scale,  \\n with 70,000 images provided in the original MNIST format,  \\n as well as NumPy format.  \\n Torchvision.io,  \\n this package provides functions  \\n for performing IO operations,  \\n including reading and writing images and videos.  \\n Images, the Torch Vision IO read_image  \\n reads a JPEG or PNG image  \\n into a three-dimensional RGB tensor,  \\n optionally converts the image to the desired format.  \\n The values of the output tensor  \\n are unit 8, between 0 and 255.  \\n decode_image detects whether an image is a JPEG or PNG,  \\n and performs the appropriate operations  \\n to decode the image into a three-dimensional RGB tensor.  \\n encode_jpeg takes an input tensor and CHW layout,  \\n and returns a buffer with the content  \\n of its corresponding JPEG file.  \\n decode_jpeg decodes a JPEG image  \\n into a three-dimensional RGB tensor,  \\n optionally converts the image to the desired format.  \\n The value of the output tensor are unit 8  \\n between 0 and 255.  \\n write_jpeg takes an input tensor and CHW layout  \\n and saves it to a JPEG file.  \\n For each of the JPEG method above,  \\n there is an equivalent for PNG format.  \\n For videos,  \\n read_video reads a video from a file,  \\n returning both the video frames as well as the audio frames.  \\n read_video_timestamps list the video frames' timestamps.  \\n write_video writes a 4D tensor in THWC format,  \\n and a video file.  \\n VideoReader is a high performance, low-level API  \\n for more fine grain control video reading API,  \\n it supports frame by frame reading of various streams  \\n from a single video container.  \\n Models.  \\n The Models sub package contains definitions of models  \\n for addressing different tasks,  \\n including image classification,  \\n pixel-wise semantic segmentation,  \\n object detection, instant segmentation,  \\n person key point detection,  \\n and video classification.  \\n There are four model sub-packages,  \\n including Classification,  \\n Semantic segmentation,  \\n Object/keypoint detection,  \\n and Video classification.  \\n Operations.  \\n Feature extraction utilities enable ML engineers  \\n to access intermediate transformations of the model inputs.  \\n TorchVision provides great feature extractors  \\n for this purpose.  \\n The torch.fx.documentation  \\n provides a more general and detailed explanation  \\n of the above procedure  \\n and the inner workings of the symbolic tracing.  \\n TorchVision Ops,  \\n it implements operators  \\n that are specific for computer vision.  \\n Here are the key operators.  \\n batched_nms,  \\n it performs non-maximum suppression in a batched fashion.  \\n box_area, it computes the area of a set of bounding boxes.  \\n clip_boxes_to_image,  \\n clip boxes so that they lie inside an image of a size.  \\n mask_to_boxes,  \\n it computes the bounding boxes around the provided masks.  \\n remove_small_boxes,  \\n remove boxes which contain at least one size smaller  \\n than min size.  \\n sigmoid_focal_loss,  \\n loss used in retina net for dense detection.  \\n stochastic_depth implements the stochastic depth  \\n from deep networks  \\n with stochastic depth used  \\n for randomly dropping residual branches  \\n of residual architecture.  \\n Transforms, this library allows ML engineers  \\n to perform various transformation steps on images,  \\n including rotation, cropping, composing,  \\n adjusting color jitter, converting to gray scale,  \\n adjusting padding, resize,  \\n amongst many others.  \\n For details, check out the documentation in the link below.  \\n utils, this library provides  \\n a number of processing utilities for computer vision tasks,  \\n including making a grid of images,  \\n saving a tensor into an image file,  \\n and drawing segmentation masks on object detection.  \\n And the details and examples can be found  \\n at the PyTorch documentation site as well.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2620026\",\"duration\":82,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Torchvision for video and image understanding\",\"fileName\":\"2884016_en_US_03_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to use Torchvision to extract entities from various training and learning videos. Torchvision is a very powerful package to accomplish various computer vision tasks for object detection, entity extraction, and classification so similar training videos can be recommended.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3050507,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now that you have a good understanding  \\n of the TorchVision library,  \\n let's walk through a lab,  \\n continuing with the use case of Piraeus.  \\n When presenting a set of tutors to a student for selection,  \\n it'll be helpful to include some photos  \\n of the instructions in the lectures.  \\n We'll use the TorchVision image library  \\n and try a few transformation actions.  \\n First, we have to import some libraries.  \\n We then define a function to display images, here,  \\n called show.  \\n Say this is a Padre class.  \\n We'll load the images and show them here.  \\n Now, we define a function to plot various images  \\n through transformation,  \\n and the function is called plot,  \\n right here.  \\n Most transforms natively support tensors  \\n on top of the PIL images.  \\n Let's add some paddings to the image.  \\n We then try a few image transformations  \\n such as random crop,  \\n color jitter,  \\n random perspectives.  \\n And finally, let's crop to the center  \\n and find the best centered image.  \\n \\n\\n\"}],\"name\":\"3. Torchvision\",\"size\":18473498,\"urn\":\"urn:li:learningContentChapter:2617032\"},{\"duration\":448,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2616026\",\"duration\":205,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Torchaudio introduction\",\"fileName\":\"2884016_en_US_04_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to set up the environment and implement a basic model for audio processing using Torchaudio. Understanding the basics of Torchaudio helps you further implement the use case in the next section.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6476643,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] TorchAudio is a library  \\n for audio and signal processing with PyTorch.  \\n It provides IO, signal, and data processing functions,  \\n datasets, model implementations, and application components.  \\n TorchAudio offers a set of APIs,  \\n including backend, functional, transforms,  \\n datasets, models, pipelines,  \\n sox_effects, compliance.kaldi, kaldi_io, and utils.  \\n Similar to TorchVision,  \\n TorchAudio also provides a number of popular datasets  \\n out of the box.  \\n Examples include CMUDict,  \\n CMU pronouncing dictionary,  \\n Common Voice,  \\n GTZAN, which is music genre classification of audio signals,  \\n speech commands,  \\n and VCTK, which is speech data uttered  \\n by 110 English speakers with various accents.  \\n Details of these datasets can be found  \\n at PyTorch dataset documentation.  \\n Audio I/O package allow you to query audio file metadata,  \\n loading audio data into a tensor,  \\n and saving audio to files.  \\n Audio resampling.  \\n To resample an audio wave form  \\n from one frequency to another,  \\n you can use transforms.Resample or functional.resample.  \\n transforms.Resample pre-computes  \\n and caches the kernel used for resampling,  \\n while functional.resample computes it on the fly.  \\n TorchAudio's resample function can be used  \\n to produce results similar to that of Librosa,  \\n which is resampy's kaiser window resampling,  \\n with some noise.  \\n TorchAudio provides a variety of ways  \\n to augment audio data.  \\n TorchAudio SoX effects allows for directly applying filters,  \\n similar to those variables in SoX to tensor objects,  \\n and file object audio sources.  \\n Convolution reverb is a technique  \\n that's used to make clean audio sound  \\n as though it has been produced in a different environment.  \\n Using room impulse response, RIR, for instance,  \\n we can make a clean speech sound  \\n as though it has been uttered in a conference room.  \\n To add background noise to audio data,  \\n you can simply add a noise tensor  \\n to the tensor representing the audio data.  \\n A common method to adjust the intensity of noise  \\n is changing the signal-to-noise ratio.  \\n torchaudio.functional.apply_codec can apply codecs  \\n to a tensor object.  \\n Combining all of the above techniques,  \\n TorchAudio can simulate audio.  \\n that sounds like a person talking over a phone  \\n in an echoey room with people talking in the background.  \\n Similar to TorchVision,  \\n TorchAudio implements feature extractions  \\n commonly used in the audio domain.  \\n They are available in torchaudio.functional  \\n and torchaudio.transforms.  \\n There are also augmentation techniques available  \\n in TorchAudio, including TimeStretch, TimeMasking,  \\n and FrequencyMasking.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2620027\",\"duration\":243,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Torchaudio for audio understanding\",\"fileName\":\"2884016_en_US_04_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to implement audio characteristics of the training tutorials in this use case. Knowing how to extract language, intonation, and other audio characteristics of the video files can be a useful technique to generate more metadata for training material recommendations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8727736,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Now let's continue with a pyrees use case.  \\n We would like to transcribe the audio  \\n and video lectures into scripts.  \\n We'll go through an example using Pie Torch's  \\n Torch Audio Library to transcribe some audio speech.  \\n This example is provided by Hiro Moto and can be found  \\n at the Pie Torch website will be using pre-trained models  \\n from web to back 2.0 through Torch Audio.  \\n The high level steps  \\n of this pipeline includes extract the acoustic features  \\n from audio waveform and then estimate the class  \\n of the acoustic features framed by frame  \\n and then generate hypothesis  \\n from the sequence of the class probabilities.  \\n Torch audio provides an easy access  \\n to the pre-trained weights and associated information such  \\n as the expected sample rate and class labels.  \\n They're bundled together and available  \\n under the Torch Audio dot pipelines module.  \\n First, we import map plot lib.  \\n Next, we'll create a Wave two VAC two model  \\n that performs the feature extraction and the classification.  \\n We will use the torch audio dot pipelines  \\n dot wave two VAC two, ASR Base nine 60 H here.  \\n The bundle object provides the interface  \\n to instantiate model and other information.  \\n Sampling rate and the class labels  \\n are also found as follows.  \\n We'll create a model  \\n through automatically fetching the pre-trained weights  \\n and loaded into the Model.  \\n Importing S S L module is important to set the context  \\n so that the model can be downloaded  \\n through HTTPS via the secure sake layer.  \\n Next, we load data.  \\n We use the speech data from Voices dataset  \\n which is licensed under Creative Commons by 4.0.  \\n Let's listen to the output here.  \\n - [Male Computer Voice] I had that curiosity beside me  \\n at this moment.  \\n - Next, let's load the data into the model  \\n and run the resample if necessary  \\n for efficiency and performance.  \\n Next, we extract acoustic features frame by frame.  \\n The returned features is a set of tensors.  \\n Each tensor is the output of a transformer layer.  \\n Now let's visualize these features.  \\n All right. The result is back.  \\n As you can see, there are a total of 12 layers.  \\n Next, we'll classify these features into categories.  \\n Wave two, VAC two model provides method  \\n to perform the feature extraction  \\n and classification in one step.  \\n Let's visualize the classification.  \\n It's in the form of logics instead of probabilities.  \\n Next, we'll generate the transcript  \\n using a greedy decoding algorithm,  \\n meaning we'll simply pick up the best hypothesis  \\n at each timestamp  \\n and do not require external resources such as a dictionary.  \\n Now we will create the decoder object  \\n and decode the transcript.  \\n Lastly, let's check the result, and this is the output.  \\n I had that curiosity beside me at this moment.  \\n Let's listen to the audio  \\n and see if the transcription is correct.  \\n - [Male Computer Voice] I had that curiosity beside me  \\n at this moment.  \\n - Yes, it is correct.  \\n That wraps up the Torch Audio Lab.  \\n \\n\\n\"}],\"name\":\"4. Torchaudio\",\"size\":15204379,\"urn\":\"urn:li:learningContentChapter:2622024\"},{\"duration\":467,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2619026\",\"duration\":219,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Torchtext introduction\",\"fileName\":\"2884016_en_US_05_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to set up the environment and implement a basic model for text processing using Torchtext. Understanding the basics of Torchtext helps you to further implement the use case in the next section.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7261880,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - TorchText is PyTorch's Library  \\n for Natural Language Processing.  \\n It consists of data processing, utilities,  \\n and useful data sets for natural language processing.  \\n It also comes with pre-trained word embedding, dataset API.  \\n Iterator API ought to be used  \\n for training models with PyTorch.  \\n TorchText comes equipped with some popular data sets  \\n for text classification, language modeling,  \\n machine translation, sequence tagging, question and answers,  \\n and also unsupervised learning.  \\n Please note that these are actually data pipes  \\n from the Torch Data Project, which is still in beta status.  \\n So, be aware that APIs might change.  \\n Couple of recommendations while using these data sets.  \\n Use data loader for shuffling the data pipe.  \\n Use the built-in worker n-net function  \\n for multi-processing.  \\n And use drop last equals to true, to ensure that  \\n all batch sizes are equal.  \\n In addition to popular data sets,  \\n TorchText package also provides utilities  \\n for text data processing for training.  \\n Functional. The TorchText data functional module  \\n contains the following set of utilities.  \\n Generate SP model for training a sentence piece tokenizer.  \\n Load SP model for loading a sentence piece model for file.  \\n Sentence piece numericalizer is a sentence piece model  \\n that numericalizes a text sentence into a generator  \\n over the IDs.  \\n Sentence piece tokenizer is a sentence piece model to  \\n tokenize a text sentence into a generator over the tokens.  \\n Custom replace converts text strings.  \\n Simple space split splits text strings by spaces.  \\n Mericalize tokens from literator yields a list  \\n of IDs from a token iterator with a vocab.  \\n Filter Wikipedia XML filters Wikipedia XML lines.  \\n To map style dataset converts iterable style dataset  \\n to map style dataset.  \\n The metrics modules with blue score computes the blue score  \\n between a candidate translation corpus  \\n and a reference translation corpus.  \\n The data utils module contains get tokenizer function  \\n that generates tokenizer function for a string sentence.  \\n TorchText offers some common text transforms,  \\n including a few tokenizers, such as  \\n a sentence piece tokenizer, clip tokenizer,  \\n bert tokenizer, vocab transform,  \\n ToTnesor, label index, truncate, add token,  \\n sequential, pad transform, and string to int transform.  \\n They can also be trained together using  \\n torch.nn, sequential,  \\n or using TorchText, transforms, sequential  \\n to support torch's credibility.  \\n TorchText also contains a number  \\n of pre-trained models, including Roberta,  \\n that iterates on bert's pre-training procedure,  \\n including training the model longer  \\n with bigger batches over more data,  \\n removing the next sentence, prediction objective,  \\n training on longer sequences, and dynamically changing  \\n the masking pattern applied to the training data.  \\n We'll go through an example next.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2619027\",\"duration\":248,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Torchtext for translation\",\"fileName\":\"2884016_en_US_05_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to implement text transcription and translation from the video files. You need to understand how to use Torchtext for translation to make more learning content available to users.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9220109,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lab,  \\n we continue with the Piraeus use case.  \\n I'll walk you through the tutorial  \\n called \\\"Classifying Names with a Character-Level RNN\\\"  \\n by Sean Robertson.  \\n We will be building a text classifier  \\n to identify the language used in the course by the tutor.  \\n The character-level RNN  \\n reads words as a series of characters,  \\n outputting a prediction and hidden state at each step,  \\n feeding its previous hidden state into each next step.  \\n We take the final prediction to be the output,  \\n which class the word belongs to.  \\n Specifically, we'll train on a few thousand surnames  \\n from 18 languages of origin  \\n and predict which language a name is from,  \\n based on the spelling.  \\n First, we import the matplotlib.  \\n We then load the data files from the directory  \\n and build our category lines.  \\n Now we have category lines,  \\n a dictionary mapping each category, which is the language,  \\n to a list of lines, which are names.  \\n We also kept track of all categories,  \\n just a list of languages,  \\n and categories for later references.  \\n Let's print the first five lines of the Italian category.  \\n Now we have all the names organized.  \\n We need to turn them into tensors  \\n to make any use of them in our model.  \\n Next, let's create the network.  \\n To run a step of this network, we need to pass an input,  \\n in our case, the tensor for the current letter,  \\n and a previous hidden state,  \\n which we initialize as zeros at first.  \\n We'll get back the output probability of each language  \\n and a next hidden state, which we keep for the next step.  \\n We will use the lineToTensor  \\n to avoid creating a new tensor every step.  \\n This is more efficient.  \\n Before doing our training,  \\n let's create a few helper functions.  \\n First one is to get category from output.  \\n The second helper function  \\n is to quickly get training example.  \\n We will train the network using NLLLoss function.  \\n We set the learning rate accordingly,  \\n and define a function, train the model.  \\n Now we'll run the train function  \\n that we defined in the previous step with some examples.  \\n This will take a while.  \\n So after the training is done,  \\n we will plot the learning results.  \\n We are almost halfway through.  \\n Okay, now let's plot the learning results.  \\n Here we go.  \\n Next step, we will evaluate the results.  \\n We create a confusion matrix in order to do that,  \\n which indicate for every actual language, which are rows,  \\n which language the network guesses and predicts,  \\n which are the columns.  \\n To calculate the confusion matrix,  \\n a bunch of samples are run through the network  \\n with the evaluate function,  \\n which is the same as the train function  \\n is set for minusing the backprop.  \\n This is the evaluate function.  \\n As you can see, the rows are the actual language  \\n and the columns are what the model predicts.  \\n Lastly, let's see how the model work  \\n by giving it some examples through a predict function.  \\n We're giving it four examples towards the end,  \\n and this is the prediction.  \\n \\n\\n\"}],\"name\":\"5. Torchtext\",\"size\":16481989,\"urn\":\"urn:li:learningContentChapter:2618025\"},{\"duration\":73,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2616027\",\"duration\":73,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Continuing your PyTorch learning process\",\"fileName\":\"2884016_en_US_06_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, get a summary of the key concepts covered in this training and get ready to select the next set of learning from the available resources. This is a very elementary introduction to PyTorch. There are greater details that you need to delve into in order to be more proficient with the framework.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2076513,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Thanks for watching.  \\n Understanding the basics of computer vision,  \\n natural language processing, and audio processing  \\n through PyTorch can be a lot to get your head around.  \\n So if you need to go back  \\n and rewatch videos until you get it.  \\n It's very common for new learners to take a couple of passes  \\n through this material before it really sinks in.  \\n If you want to keep learning about PyTorch  \\n or Machine Learning, the next step is to dive  \\n into the documentation and build something,  \\n create an implementation for an idea you have.  \\n And if you want to learn more about AI,  \\n there is a 10 course learning series  \\n on LinkedIn called Master The Fundamentals of AI  \\n and Machine Learning.  \\n Courses such as Artificial Intelligence Foundations  \\n Neural Networks with Doug Rose,  \\n or AI Algorithms for Gaming with Eduardo Corpeno  \\n will both be super informative and fun.  \\n If you want to keep in touch with me,  \\n please follow me on Twitter or LinkedIn  \\n or look me up on Facebook and Instagram.  \\n Thank you and bye.  \\n \\n\\n\"}],\"name\":\"Conclusion\",\"size\":2076513,\"urn\":\"urn:li:learningContentChapter:2617033\"}],\"size\":111518093,\"duration\":3412,\"zeroBased\":false},{\"course_title\":\"AI Workshop: Build a Neural Network with PyTorch Lightning\",\"course_admin_id\":3096406,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":3096406,\"Project ID\":null,\"Course Name\":\"AI Workshop: Build a Neural Network with PyTorch Lightning\",\"Course Name EN\":\"AI Workshop: Build a Neural Network with PyTorch Lightning\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"&lt;p&gt;If you\u00e2\u20ac\u2122re looking for hands-on AI practice, this workshop-style coding course was designed for you. Join instructor Janani Ravi as she shows you how to build a neural network with PyTorch Lightning, the open-source library from Python that provides an interface for the popular deep learning framework PyTorch. Explore the core components of building a neural network with PyTorch, including setting up the virtual environment, loading and exploring data, preprocessing data for training, creating and training a simple neural network, setting up the Dataset and DataLoader, visualizing losses, and much more. Along the way, Janani covers the basics of using modules in PyTorch Lightning to build, train, and evaluate both regression and classification models.&lt;/p&gt;&lt;p&gt;This course was created by Loonycorn. We are pleased to host this content in our library.&lt;/p&gt;\",\"Course Short Description\":\"Learn how to build a neural network with PyTorch Lightning in this interactive, workshop-style coding course.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":20161004,\"Instructor Name\":\"Janani  Ravi\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Certified Google Cloud Architect and Data Engineer\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2023-12-08T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/ai-workshop-build-a-neural-network-with-pytorch-lightning\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Artificial Intelligence for Technology\",\"Primary Software\":\"PyTorch\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":5449.0,\"Visible Video Count\":19.0,\"Contract Type\":\"INSTRUCTOR_PRODUCTION\"},\"sections\":[{\"duration\":719,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4552015\",\"duration\":339,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"AI workshop: Build a neural network with PyTorch Lightning\",\"fileName\":\"3096406_en_US_00_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":339,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be familiar with the features of PyTorch and PyTorch Lightning.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8486016,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Hi, and welcome to this course.\\nAI workshop: Build a neural network with PyTorch Lightning.\\nSince this course is an AI workshop, for most of this course\\nwe'll be performing hands-on coding.\\nWe'll build a neural network with PyTorch and we'll see how\\nwe can write cleaner, more modular reusable code\\nwith PyTorch Lightning.\\nNow, before we get to the demos, just a little bit of an overview\\nof PyTorch and PyTorch lightning.\\nFirst, what exactly is PyTorch?\\nHere is a definition from the PyTorch documentation.\\n\\nIt's an optimized tensor library for deep learning using\\nGPUs and CPUs.\\nThe tensors here refer to multi-dimensional arrays that can\\nbe trained in a distributed manner.\\nAnother way to look at PyTorch,\\nalso from the documentation.\\nIt's an open-source machine learning framework based on the\\nPython programming language.\\nIt has simple and intuitive APIs which accelerate the path from\\nresearch prototyping to production deployment.\\nThe PyTorch framework is primarily used to build deep learning\\nneural network models, and its APIs are so simple that\\nwith just basic knowledge of Python,\\nyou should be able to work in PyTorch right away.\\n\\nNow, hopefully, you've worked with neural networks before,\\nand this is not your first neural network course.\\nNeural network models, you know, are just directed acyclic graphs.\\nPyTorch uses something known as dynamic computation graphs.\\nThis means you can build the graph for the model and execute\\nit right away.\\nThis makes it easier and more flexible to build complex\\nneural networks.\\nNow, PyTorch is deeply integrated with NumPy.\\nYou can set up your data in the form of NumPy arrays and convert\\nthose to PyTorch tensors and vice-versa very easily.\\n\\nPyTorch has native support for training on GPUs.\\nYou can have your model parameters and data all moved to the GPU\\navailable on the machine that you're running\\ntraining and the entire training process will run there.\\nPyTorch also uses a powerful library called Autograd for\\nautomatic differentiation.\\nAutomatic differentiation is an important part of training\\na neural network model.\\nThis involves computing partial derivatives of the loss function\\nwith respect to every model parameters,\\nand then using that information to tweak model parameters to minimize\\nthe loss of a network.\\n\\nThe PyTorch library contains everything that you need to\\nbuild neural networks.\\nYou have classes for the layers of a neural network,\\ndifferent kinds of layers, optimizers that you use to\\ntrain neural networks,\\nloss functions for different kinds of models,\\nserializers to serialize the model out to disk.\\nThe PyTorch framework is flexible and easy to use.\\nWhen you use PyTorch directly to build and train neural networks,\\nyou get access to a low-level API for model training.\\n\\nAnd this is great when you really want to configure and customize\\nyour model and want very granular control over the training process.\\nBut if you want to be abstracted away from the details\\nof model training,\\nusing the PyTorch framework directly is not a great choice.\\nPyTorch has many repetitive tasks and a lot of boilerplate code,\\nso it's often very tedious to write code in PyTorch\\nto train your model.\\nIf you want to avoid working with all of these nitty-gritty details\\nand want your model code to be cleaner and more modular,\\nwell, you use PyTorch Lightning.\\n\\nPyTorch Lightning is an open-source\\nlightweight wrapper or framework built on top of PyTorch that\\nsimplifies the training and research process for deep\\nlearning models.\\nSo this is something important.\\nPyTorch Lightning is just a wrapper,\\nso you can't do anything in Lightning that you can't\\ndo with PyTorch.\\nThe fact is, it's just much easier to work with\\nPyTorch Lightning. PyTorch Lightning abstracts away all of\\nthe nitty-gritty details and really reduces the boilerplate\\ncode that you have to write for training models.\\n\\nWhatever you need to do to build and train neural networks,\\nPyTorch Lightning will give you a high-level interface for this.\\nYou can define data sets, define models,\\nset up training loops, and log your experiments all using\\nthis high-level interface.\\nWhat are some of the advantages of using PyTorch Lightning?\\nWell, your code is much cleaner because most of the repetitive\\ncode and training loops that you have to use in PyTorch\\nis abstracted away.\\nYour code is also more modular with Lightning.\\n\\nIt encourages a modular design by separating the different parts of\\nthe training process into well-defined components\\nsuch as the model DataLoaders and training logic. Cleaner\\nmodular code results in better reproducibility of your\\nmodel and data.\\nLightning provides built-in support for experiment logging\\nand tracking, as well.\\nLightning support for distributed training is more straightforward\\nbecause you do not need to\\nmove your model parameters and data to specific devices to\\nactually train on that device.\\n\\nLightning offers a callback system,\\nallowing you to add custom functionality at various points\\nduring the training process without modifying the\\ncore training loop.\\nInstead of writing complex for loops to train your model,\\nthe trainer class in Lightning abstracts away many training\\nloop details.\\nIf you feel that PyTorch Lightning does not offer you the flexibility\\nthat you need for model training, well, you can use Lightning along\\nwith the PyTorch API. So they are interoperable,\\ngiving you experiment flexibility.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4554027\",\"duration\":51,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Prerequisites\",\"fileName\":\"3096406_en_US_00_02_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":51,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be familiar with the prerequisites to working with PyTorch.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1217837,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Before you get hands-on and that will be very soon,\\nlet's quickly look at some of the prereqs\\nyou need to have to make the most of your learning.\\nThe first thing here is you need to have a basic understanding\\nof machine learning.\\nThis is not a beginner machine learning course.\\nInstead, it's an AI workshop, which means we'll get hands-on\\nwith demos right away.\\nA basic understanding of machine learning,\\nregression, and classification models will really help you.\\nAlso, you need to have a basic understanding of how neural\\nnetworks work.\\n\\nneural networks function, but that's more of revision\\nrather than explaining all of the nitty gritty of neural\\nnetwork training.\\nSo basic understanding of neural networks would really help.\\nAnd finally, because we are going to be coding a lot using Python,\\nyou should be comfortable programming in Python.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4551029\",\"duration\":329,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Quick overview of neural networks\",\"fileName\":\"3096406_en_US_00_03_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":329,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be familiar with how neural networks work.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8928669,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Let's do a quick revision of how neural networks work before\\nwe get down to the demos.\\nRemember, this is not a comprehensive overview or a look\\nat neural network training, but a quick overview to give you\\nthe main points to remember as you dive into the code.\\nNeural network models are made up of layers and how these layers are\\narranged and connected make up the architecture of the model.\\nYou can think of every layer in a neural network as being connected\\nto other layers in the neural network.\\n\\nThe way neural networks function is that every layer in the neural\\nnetwork is responsible for extracting a different detail\\nfrom the underlying data,\\nand all of the layers put together make predictions.\\nNow, the first layer here in our neural network,\\nthat's the input layer.\\nThis is where we feed in the input data,\\nwhether during the training process or for predictions.\\nThe final predictions of the neural network are available\\nfrom the last layer,\\nthat is the output layer. Between the input and output layers,\\nyou have one or more hidden layers and these hidden layers\\ntransform the data.\\n\\nThese transformations are applied as the data flow through the\\nlayers of the model.\\nThe operation of each hidden layer is to extract a different bit of\\ninformation from the data that passes through. In a\\nneural network,\\nevery layer is made up of active learning units called neurons.\\nThey are called active learning units because it's these neurons\\nthat are identifying patterns and making generalizations from the\\ndata that passes through the network.\\n\\nNeurons are fed inputs and they produce outputs.\\nAnd these inputs and outputs are essentially interconnections\\nin the model.\\nThe output of every neuron may be connected to one or more neurons\\nin the layer after it. And how these connections\\nare set up,\\nwell, that's a part of the neural network architecture.\\nNow, we've said that neurons are active learning units,\\nbut what exactly is a neuron?\\nEach neuron is nothing but a mathematical function.\\nEach neuron applies this function that you see here at the\\nbottom to its inputs.\\n\\nIt computes the weighted X values, X values are the input,\\nadds a bias, and applies an activation function on Wx + b\\nto compute the final output Y.\\nThe first of these mathematical functions that the neural network\\napplies is responsible for learning linear relationships\\nthat exist in the data.\\nA neuron receives a vector of inputs.\\nYou can think of these as X1 through Xn,\\nand it basically applies a weight value to each element\\nof the vector.\\n\\nThese weights are associated with the connections that flow into the\\nneuron. Wx + b is the first mathematical operation\\nof the neuron, and this operation is responsible\\nfor learning linear relationships that exist in data.\\nThe second mathematical function that a neuron applies to its\\ninputs is the activation function.\\nThe activation function is responsible for learning\\nnon-linear relationships that exist in data.\\nPopular activation functions include the rectified\\nlinear unit or ReLU,\\nthere is, the sigmoid activation function,\\nand many others.\\n\\nThe choice of activation function is a part of the neural network\\ndesign.\\nThe weights and biases of all of the neurons in your neural network\\nmake up the trainable parameters of the model.\\nThese weights and biases are what are found during the training\\nprocess. You can think of\\nthe objective of the training process of a neural network is to\\nfind the weights and biases for all of the interconnections that\\nminimizes the loss of the model.\\nThe loss here is essentially a measure of how far the predictions\\nof the model are from the actual values\\nin the training data. We know that model parameters are\\nfound during the training process of the neural network.\\n\\nBut how does training work?\\nHere is a very high level explanation.\\nDuring the training process, we feed training data in batches\\nthrough the network and get predictions using the current\\nparameters of the model.\\nThese predictions, at least to start off with,\\nwill not be very good ones.\\nWe'll measure how good the predictions of the model are\\nby computing the loss.\\nThe loss represents how far is the prediction of the model from the\\nactual labels in the training data.\\n\\nOnce we have the loss function, we'll compute gradients, where\\ngradients are just the partial derivatives of the loss with\\nrespect to each parameter\\nin model training. These gradients give us a sense of\\nhow to tweak the model parameters to minimize the loss of the model.\\nWe then make a backward pass through the model to update\\nparameters to minimize the loss.\\nAnd this forward pass to get predictions and then backward pass\\nto update the model parameters continues through the entire\\ntraining process until the model parameters converge.\\n\\nThe entire objective of the training process is to minimize\\nthe loss of the network, and thus improve the predictions\\nof the model.\\nThis minimization of the loss of the network is done using an\\noptimization algorithm called gradient descent.\\n\"}],\"name\":\"Introduction\",\"size\":18632522,\"urn\":\"urn:li:learningContentChapter:4550022\"},{\"duration\":2462,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4554028\",\"duration\":264,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Setting up the virtual environment\",\"fileName\":\"3096406_en_US_01_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":264,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to set up a virtual environment.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6648730,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"In this course, we'll first build a simple neural\\nnetwork model for regression using PyTorch.\\nAnd you'll see that there are a lot of granular details that you\\nneed to know about model building in order to work with\\nPyTorch directly.\\nThere will be a lot of boilerplate code.\\nThen we'll basically build the same neural network using\\nPyTorch Lightning.\\nWith PyTorch Lightning, we'll eliminate a lot of the boilerplate\\ncode and create reusable components.\\nYou'll see how much cleaner the code is with PyTorch Lightning.\\n\\nBefore we do any of that, let's set up a virtual environment\\nwithin which we'll install PyTorch and build and train our\\nneural network models.\\nHere I am on my local machine and I have Python installed.\\nMake sure you have a recent version of Python.\\nYou can see that I'm working with Python 3.10.9.\\nI'll now create a Python virtual environment.\\nA virtual environment is just an isolated environment for\\nPython projects, ensuring that each project can\\nhave its own dependencies regardless of what dependencies\\nother projects may have.\\n\\nThis command creates a virtual environment called\\npytorch_venv using the venv module.\\nWhen you create a virtual environment,\\nthis sets up a directory under your current working directory\\nwith the name of your virtual environment.\\nYou can see the pytorch_venv directory here.\\nThis is where all of the packages that we install in the virtual\\nenvironment will be set up and stored.\\nNow, to activate the virtual environment,\\nyou'll run the source command pytorch_venv/bin/activate.\\n\\nThis will activate the virtual environment.\\nNotice that my prompt has changed.\\nThe name of the virtual environment is now part\\nof the prompt.\\nThe same version of Python that I had installed on my local machine\\nshould now be available here\\nin my virtual environment. It's possible to create virtual\\nenvironments using different versions of Python,\\nbut I'm happy with the Python version that I have.\\nIn order to be able to work within this virtual environment\\non a Jupyter Notebook, let's install the ipykernel\\nmodule in Python.\\n\\nUse pip install to install the latest version of ipykernel.\\nThis is the Python package that provides the kernel for Jupyter\\nNotebook and Jupyter Lab.\\nThe kernel is just the computational engine that executes\\nthe code on the notebook.\\nNow, once we have this installed, run jupyter kernelspec list to\\nsee what Python kernels you have available.\\nYou can see I have just the one Python 3.\\nI'll now install the kernel associated with my virtual\\nenvironment using this command here, python -m ipykernel\\ninstall the kernel with name pytorch_venv.\\n\\nRemember, pytorch_venv is a folder in our current working directory,\\nand it is that folder that will be used to set up the kernel.\\nThus, we have a kernel with our virtual environment available when\\nwe use Jupyter Notebooks.\\nRun jupyter kernelspec list once again,\\nand you can see the pytorch_venv kernel.\\nThat's the kernel that we'll use to run our code.\\nWe are within our virtual environment.\\nWe've installed the pytorch_venv kernel.\\nLet's bring up the Jupyter Notebook server so that we can\\nwork within Jupyter Notebook to build and train models using\\nPyTorch and PyTorch Lightning.\\n\\nHere, let's open up a new notebook,\\nclick on \\\"New\\\" and make sure that you choose pytorch_venv\\nas the kernel that you want to run your code on.\\nIf you choose this kernel, we'll be running within our\\nvirtual environment. On the top right,\\nnotice the kernel.\\nIt's pytorch_venv.\\nIf you happen to be in some other kernel and you need to switch,\\nsimply select the kernel dropdown menu here on this page,\\nand there you'll find an option to switch your kernel.\\n\\nMeanwhile, let's rename this notebook and give it\\na meaningful name.\\nLet's call this TrainingRegressionModelUsingPyTorch\\nbecause first, we'll work with PyTorch,\\nwhich is much more low-level and involves much more boilerplate in\\norder to understand the different components that you'll use to\\nbuild a neural network model\\nand then we'll switch to PyTorch Lightning. In order to have more\\nscreen space for code, I'm going to toggle the header and\\ntoggle the toolbar here in this notebook.\\n\\nSo we are only left with the code cells.\\nWe don't have all this extra stuff here on top.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4552016\",\"duration\":320,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Loading and exploring regression data\",\"fileName\":\"3096406_en_US_01_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":320,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to load data for regression.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9886927,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"The first thing I'm going to do here is put in some code to ignore\\ncertain warnings in this notebook.\\nNow, generally, it's not a good practice to ignore\\nwarnings in your Python code, but at the time of this recording,\\nthe Seaborn Library throw some internal warnings\\nwhich we can't really eliminate.\\nThere is an issue being tracked for Seaborn right now.\\nThe next release should fix it, but since I did not want the demo\\nscreen to be cluttered with warnings,\\nI added these ignore statements at the very top of my code.\\n\\nThis is the issue that I refer to.\\nBy the time you are watching this course,\\nit's likely that it's fixed and you can just get rid\\nof these warnings.\\nWe'll be using the scikit-learn module in Python in order\\nto preprocess our data.\\nSo go ahead and pip install scikit-learn.\\nWe'll use functions such as train, test,\\nsplit, etc. to get training and test data to train our model.\\nSo scikit-learn is a package that you'll need.\\nAnd I install this in the virtual environment.\\nNext, of course, we are going to be using PyTorch to train our model.\\n\\nSo you'll need to pip install the PyTorch framework, as well.\\nYou can see the PyTorch framework version that I'm working\\non is 2.0.1.\\nAnother PyTorch-related module that you need is Torch metrics.\\nThis is the library that contains metrics to evaluate our model's\\nmean square error for regression,\\nno accuracy, precision, and recall for classification.\\nSo make sure you pip install and have this module ready within\\nyour virtual environment.\\nNow within this virtual environment,\\nI need to set up other commonly used libraries for data processing\\nand analysis\\nsuch as pandas.\\n\\nSo make sure you pip install pandas and have that available.\\nAnother library that we need is the Seaborn Library.\\nThis is what we'll use for visualization.\\nSeaborn depends on matplotlib, so the matplotlib library should\\nalso be installed.\\nNow that we have the libraries that we need,\\nlet's confirm the PyTorch version that we are using.\\nI'm running PyTorch version 2.0.1, so make sure you have a recent\\nversion of PyTorch greater than or equal to two\\nin order to be able to run these demos.\\n\\nI'll now set up a bunch of import statements for all of the\\nlibraries and classes that we need\\nfor this demo. I'll use NumPy, pandas,\\nSeaborn,\\nmatplotlib,\\nTorch, scikit-learn.\\nWe won't go through these import statements right now,\\nwe'll discuss each class or function as we use it.\\nThe dataset that we'll be using to train our regression model is\\nan insurance charges dataset and is present here within this\\ndatasets directory.\\nUnder my current working directory, you can see this insurance.csv\\nfile.\\n\\nThis is the file that I'm going to read into my notebook\\nusing pandas.\\nThe dataset is fairly simple.\\nThe records contain details of insurance,\\ncustomers, age, gender, bmi, number of children,\\nwhether they smoke or not, and the region in which they live.\\nThese are all the features of the data.\\nWe'll try to use this information to predict how much they've been\\ncharged for insurance.\\nThe label column is charges.\\nLike I said, this is a fairly simple and small dataset,\\nperfect for training a simple regression model using PyTorch.\\n\\nThe shape of the data shows us that there are a total\\nof 1338 records.\\nLet's make sure every column in this data is of the right type.\\nFor that, I run the info method on a pandas data frame and you can see\\nthat the types are correct. Age, bmi, number of children,\\nand charges are numeric columns,\\nthe remaining are string columns or categorical columns.\\nLet's just use a few visualization techniques to understand the data\\nthat we are working with.\\nLet's take a look at a histogram of the charges information.\\n\\nYou can see that for a vast majority of customers,\\nthe insurance charges tend to be under $15,000.\\nYou can see that on the x-axis.\\nHowever, there are a few customers for whom insurance charges tend to\\nbe very high in the order of $40,000 to $50,000.\\nIn this dataset, all of the feature variables are\\nrelevant to predict the insurance charges for customers,\\nbut one of the most significant features is this smoker feature.\\n\\nWhether you're a smoker or not, heavily influences your\\ninsurance charges.\\nYou can see that for non-smokers, the insurance charges tend to be\\nmuch lower than for smokers.\\nThis boxplot makes that very clear.\\nAlso, how old the customer is influences insurance charges.\\nYou can see a scatterplot of insurance charges versus age,\\nand there is a linear relationship.\\nYou can see that insurance charges tend to increase with age.\\nBut for each age, they seem to be different bands of charges.\\n\\nNow that we've understood the data that we're working with,\\nlet's quickly split the data into training and test data using\\ntrain_test_split. X features include all columns except\\ncharges,\\nthe Y values that we're trying to predict are the insurance charges.\\ntrain_test_split will split the features and labels so that we\\nhave 80% of the data to train our neural network model\\nand 20% of the data to validate the model. Thousand and seventy records\\nfor training and 268 records for validation.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4552017\",\"duration\":383,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Preprocessing data for training\",\"fileName\":\"3096406_en_US_01_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":383,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to preprocess data for model training.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10646982,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Now that we have training and validation data,\\nthe next step is to preprocess the data so that we can feed that into\\na machine learning model.\\nNow, neural network models and all machine learning models only\\nunderstand numeric values.\\nYou can't feed in strings to those models,\\nwhich means you need to numerically encode all of your\\ncategorical variables, which may be represented\\nas strings.\\nNow, there are different ways to numerically encode this data.\\nWe'll use one-hot encoding.\\n\\nThere are three categorical columns in our data; sex,\\nsmoker, and region.\\nAll of these are nominal categorical values.\\nThat means that there is no inherent ordering across\\ncategories for any of these columns.\\nAnd so one-hot encoding is perfect\\nin such a situation. We instantiate the one-hot encoder\\ncategorical transformer, handle unknown is ignored.\\nThat is, if we encounter unknown values,\\nthis transformer will simply ignore them.\\nDrop is equal to\\ncategories in the feature.\\n\\nLet's say the smoker column can have two possible values,\\nyes or no.\\nIn the final output, you'll have just one column, either\\nsmoker yes or smoker no\\nwith zero/one values indicating whether the customer\\nis a smoker or not.\\nSparse output set to false means that the resulting one-hot encoded\\nvalues will not be in the sparse representation.\\nIt will be a complete representation.\\nLet's instantiate a column transformer next to perform\\nthe preprocessing,\\nand this column transformer uses the one-hot encoder for\\ncategorical values.\\n\\nFor the remaining numeric columns, we'll just pass through,\\nmeaning the numeric columns won't be affected,\\nonly the categorical columns will be affected.\\nOnce we've instantiated the transformer,\\nlet's pre-process our data.\\nI call preprocessor.fit_transform on the training data, and\\nusing the computed values on the training data,\\nwe simply call transform on the validation data.\\nThis ensures that the properties computed on the training data are\\nwhat we use to transform the validation data,\\nand we are left with eight features in our data after\\none-hot encoding.\\n\\nLet's take a look at the training data and you'll find that\\nit's just a NumPy array.\\nWe don't have the corresponding columns.\\nIf you want to understand this data,\\nlet's convert it to a temporary data frame and see how the data\\nhas been transformed.\\nSo we have one column for gender,\\nmale,\\none column for smoker,\\nyes/no;\\nand then columns for northwest, southeast, and southwest.\\nNotice that the categorical feature southeast, that column\\nhas been dropped.\\nA value of all zeros for northwest,\\nsoutheast, and southwest essentially indicates that\\nthe region is southeast.\\n\\nThe numeric columns; age, bmi, and children,\\nthey have been passed through unaffected.\\nThe Y values are still in the data frame format.\\nLet's convert those to the NumPy format, as well.\\nAnd from NumPy arrays,\\nlater on, we'll convert these to Torch tensors.\\nWe haven't yet completed the preprocessing of our data.\\nWe've converted all of the columns to numeric values.\\nAnd I'm now going to standard scale these numeric values.\\nStandard scaling involves converting all numeric\\nvalues to z scores,\\nthat is, expressing every feature in terms of number of standard\\ndeviations from the mean.\\n\\nWhen you feed in numeric values to neural networks,\\nneural networks perform much better with smaller\\nnumeric values,\\nand also when the individual features do not have wildly\\ndifferent ranges.\\nAnd standard scaling is one way to preserve the information\\nin individual features, but also have them all centered\\naround zero and expressed using small numeric values.\\nSo I call fit_transform on the training data, and the mean and\\nstandard deviation computed on the training data will be used to\\ntransform the validation data.\\n\\nNotice I call just transform on X_val.\\nAnd now if you look at the training data, you can see that the\\nnumbers are very small, usually between -3\\nand +3.\\nStandard scaling or standardization computes the\\nmean for each feature,\\nsubtracts the mean from every value,\\nand then divides by the standard deviation so that the values are\\nexpressed in terms of z scores or number of standard deviations\\naway from the mean.\\nNow, the Y values that we need to train our model are currently in\\nthe form of a single-dimensional array or a vector.\\n\\nIn order to feed them into a neural network,\\nthey need to be in the form of a multi-dimensional array.\\nSo instead of a vector of length size,\\nwe'll have a multi-dimensional array of dimensions\\nsize comma one.\\nAnd that's what this reshape operation accomplishes.\\nIt's the same Y values, the charges,\\nin the form of a multi-dimensional array.\\nAs I mentioned before,\\nneural networks work better when you're dealing with small\\nnumeric values.\\nNow, our insurance charges vary from zero to about $50,000, $60,000.\\n\\nThese are not small numeric values.\\nAnd in order to make our neural network training more robust,\\nmore likely to converge, I'm going to use the min_max_scaler\\nto scale all insurance charges to be expressed\\nin the range zero to one.\\nThat's what the min_max_scaler does by default.\\nNow, all insurance charges are expressed as values\\nbetween zero and one.\\nWe now have our input features and labels that we'll use to train\\nour neural network.\\n\\nHowever, they are in the NumPy format.\\nI'm now going to convert them to Torch tensors.\\ntorch.from_numpy will convert NumPy arrays to Torch tensors.\\nNow, Torch tensors are the primary data structures used in PyTorch\\nfor building neural networks and other ML models.\\nTorch tensors are multi-dimensional arrays like NumPy,\\nbut they support distributed training using GPUs.\\nThey are used in neural network training because they support\\nautomatic differentiation for gradient computation\\nand essential part of the training process of a neural network.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4550020\",\"duration\":343,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating a simple neural network\",\"fileName\":\"3096406_en_US_01_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":343,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to create a simple neural network in PyTorch.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8858003,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"At this point, our training data is processed and ready to be used\\nto train a neural network model.\\nSo the first thing we'll do is set up a very simple neural network.\\nIn fact, the simplest possible neural network,\\none with just one neuron and no activation function.\\nSo all this neural network will be able to learn is linear\\nrelationships in the data.\\nSince the dataset that we're using is a simple\\nstraightforward one, you'll find that this very simple\\nneural network also does fairly well and can be used to build a\\nfairly decent regression model.\\n\\nLet's see how we set up this very simple neural network.\\nHere I've defined a class called SimpleNeuralNet that inherits\\nfrom the PyTorch nn.Module class.\\nnn.Module is a base class for all neural networks that\\nyou'll build using PyTorch.\\nIt gives you a wide range of functionality that makes the\\ndevelopment and management of neural networks easier\\nand more sustainable.\\nWithin the init method of the nn.Module class,\\nyou'll specify the layers of the neural network.\\n\\nHere are init method, takes in one input argument,\\nthe number of features in the training data,\\nand within that we have just one layer,\\nand that layer has just one neuron.\\nWe instantiate that layer using the nn.Linear object.\\nNotice this linear layer takes in num features as input,\\nthat is all of the features in our training data, and has\\njust one neuron.\\nYou override the forward function in the base nn.Module class to\\nspecify the forward pass-through the neural network.\\n\\nNotice the forward function here takes in the record as\\nan input argument,\\nthat is, a training data and essentially invokes the linear\\nlayer on the training data.\\nSo you're passing the input data through that linear layer.\\nAnd this transformed output x is what we return.\\nThe nn.Module class gives you other bits of functionality\\nthat will be useful, such as moving all of the\\nparameters of the neural network to a CPU or GPU device\\nso that it can be trained on that device.\\n\\nNow that we have our very first simple neural network,\\nlet's explore and understand it.\\nLet's instantiate the neural network.\\nThe number of features that we have are eight.\\nRemember that's the number of columns in the X train data.\\nPrinting out the neural network will give us a string\\nrepresentation of the layers in the net.\\nWe have just one linear layer with a single neuron and no\\nactivation function.\\nBefore a neural network is trained,\\nthese weights and biases are initialized to random values.\\n\\nYou can actually access the weights and biases of each layer\\nby running a for loop through the layers in your model.\\nHere, for every linear layer in the model,\\nwe have just one.\\nI print out the weights and biases,\\nand you can see that there are eight weights corresponding to the\\neight features that are going to be fed into the single neuron\\nin our linear layer\\nand one bias value.\\nThese weights and biases are the model parameters that will be\\nfound during the training process of the model.\\n\\nThese weights and biases will converge to some values,\\nallowing the model to make predictions.\\nYou can actually count the number of parameters that this model\\nis going to train.\\nHere is a count parameters function that will\\nhelp us do that.\\nYou can iterate over all of the model parameters by accessing the\\nparameters function on the model, and if the parameter\\nrequires gradients, that is, gradients\\nwill be computed,\\nyou know that it's a trainable parameter.\\nAnd by running this, you see that we have a total of\\nnine trainable parameters for this very simple neural network,\\neight weights and one bias.\\n\\nTraining a neural network involves using a loss function.\\nThis loss determines how good the neural network is at\\nany point in time.\\nYou can think of the loss as representing how far away the\\npredictions of the neural network are from the actual target values.\\nSince this is a regression model, the loss function that we'll use\\nis going to be the mean squared error loss available\\nin F.mse_loss.\\nLet's compute the loss on predictions made by our\\nuntrained model.\\n\\nNotice that we pass train inputs through the model and then compare\\nthose predictions with the train targets.\\nAnd then we compute the loss function.\\nAnd the loss here is 0.5406.\\nThe loss is not really meaningful unless we see how the loss falls\\nas we train the model.\\nBut this is just to show you how the loss function is computed.\\nLet's get and view the predictions from our model\\nat this point in time.\\nRemember we haven't trained the model at all.\\nThe model parameters all have random values.\\n\\nSo these predictions are actually just random predictions.\\nWhat we are seeing here is some of the steps that\\nwe'll be performing during the model training process.\\nOnce the model has been trained and we get the final predictions\\nfrom the model, we'll evaluate the model using the\\nr-square score and by computing the mean squared error.\\nThe R2Score and the MeanSquaredError\\nclasses are available as a part of Torch metrics regression.\\nWe instantiate those objects, MSE and r2score,\\nand we compute these values on Lines 8 and 9 by passing in\\nthe predictions from the model and the actual target values.\\n\\nAgain, remember the model has not been trained.\\nSo both the mean squared error, that's just the loss,\\nand the r squared will essentially have random values.\\nThe r squared is actually negative.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4551030\",\"duration\":146,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Setting up the dataset and DataLoader\",\"fileName\":\"3096406_en_US_01_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":146,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to create a PyTorch dataset and dataloader.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3933470,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"In order to feed our training data into our PyTorch neural\\nnetwork in batches, we are going to be using a tensor\\ndataset and a tensor DataLoader. In PyTorch,\\nthe dataset and DataLoader are foundational classes provided by\\nthe torch.utils.data module.\\nThese facilitate the loading, processing, and batching of data.\\nDataset is an abstract base class in PyTorch and we'll be using the\\nderived tensor dataset class.\\n\\nYou can think of a dataset as representing a collection\\nof data items.\\nAnd this tensor dataset that we've instantiated here holds\\nour training data.\\nWe instantiate a tensor dataset using the train inputs\\nand train targets,\\nand here are the first five records in the tensor dataset.\\nJust a heads up that here we've used one of the built-in dataset\\nclasses, the tensor dataset,\\nbut it's also possible for you to create your own custom dataset by\\nderiving from the dataset base class.\\n\\nOnce we've set up a dataset, when we actually access the\\ntraining data to train our model, we'll want to load the\\ndata in batches.\\nWe may want to shuffle the data or use multiple workers to\\nspeed up data loading.\\nAll of that is done via the DataLoader.\\nI've instantiated a DataLoader here and specified a batch size of\\n8. For the training data,\\nI've also set shuffle equal to true.\\nSo when we feed the data into our model for training,\\nit will be shuffled.\\nIt will not come in any predictable pattern.\\n\\nThe DataLoader is an iterable that allows us to iterate over the\\ndata in batches. On Line 5\\nyou can see I create an iterator and call next on it,\\nand this will allow us to see the first batch of training data.\\nNotice that we have eight records here because we specified\\nbatch Size 8.\\nWe've now created a dataset and DataLoader for our training data.\\nLet's do the same for validation data.\\nFirst, we convert X and Y value to the tensor format.\\nAnd once that's done, we'll instantiate a tensor dataset\\nfor our validation data.\\n\\nAnd then we'll instantiate a DataLoader using this tensor dataset.\\nNotice that when we instantiate a DataLoader for the\\nvalidation data,\\nI haven't specified shuffle equal to true.\\nValidation data is only used to evaluate the model and does\\nnot need shuffling.\\nHere is the first batch of records from the validation data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4552018\",\"duration\":465,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Training a neural network using PyTorch\",\"fileName\":\"3096406_en_US_01_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":465,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to train a neural network in PyTorch.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14452465,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"We are now ready to start training our neural network.\\nI'm going to set up a dictionary called loss_stats\\nwhich will hold the values of the training loss and validation\\nloss for each epoch.\\nWe'll run 100 epochs of training and epoch,\\nas you likely already know, is one pass through the\\nentire training data.\\nThe next thing we need to do is figure out on what device\\nwe'll run this training.\\nNow, because I'm running on my local machine,\\nI do not have access to a GPU, but it's possible that you are\\nrunning this on Google Colab or a machine where a GPU is available.\\n\\nWhat we are doing here is a check to see whether a GPU is available.\\nIf it is, we'll use the GPU.\\nOtherwise, we'll use the CPU.\\nSo if torch.cuda.is_available\\nreturns true, that is, we have a GPU, then the device will be CUDA.\\nIf torch.backends.mps.is_available is true,\\nthis means that the new metal performance shaders backend for\\nGPU training and acceleration is available on your machine,\\nso the device will be mps.\\n\\nThis is likely to be available if you're working on a\\nnew Mac device, or if neither of these\\noptions is true, we'll just go with CPU training\\nand device will be CPU.\\nYou can see here that we're using the CPU device because I do\\nnot have a GPU available.\\nWe'll now instantiate and train our neural network using PyTorch.\\nRemember PyTorch is a lower-level API\\nand it does not abstract us away from the details of neural\\nnetwork training.\\nAnd there will be a lot of boilerplate code involved.\\n\\nI instantiate the simple neural network num features\\nis equal to eight, and I call to device in order to\\nmove the model parameters of the neural network to whatever device\\nwe are using for training either CPU or GPU.\\nMoving to the device that you're using for training is a part\\nof PyTorch's boilerplate.\\nFor all of your training data and your model,\\nyou have to move them to the right device so that the training occurs\\non the right device.\\nThe objective of training a neural network is to update your model\\nparameters for every iteration of training,\\nso as to minimize the loss function.\\n\\nIn PyTorch, it's the optimizer that actually updates the model\\nparameters using gradient values.\\nSo I've instantiated an optimizer for that purpose.\\nThe optimizer I've used here is the SGD or the stochastic gradient\\ndescent optimizer.\\nThere are several different optimizers available as a part\\nof the PyTorch framework.\\nSGD is a commonly used straightforward optimizer.\\nThe optimizer takes in the model parameters that need\\nto be updated,\\nas well as the learning rate, which are set to 10^-2.\\n\\nThe learning rate determines the step size for how the model\\nparameters converge to their optimal values.\\nToo larger step size, your model may not converge,\\ntoo smaller learning rate, and your model may take\\ntoo long to converge.\\nThis 0.01 works well for this particular model,\\nand that's why I've selected it.\\nNext, let's set up the training loop for our model.\\nAnd here is where you'll really see the boilerplate code.\\nAnd you'll find that when we use PyTorch Lightning in\\nthe next demo,\\nmost of this code will be eliminated.\\n\\nFirst, I have a for loop to iterate over the number of epochs\\nof training that we'll run.\\nI initialize the training epoch loss to zero,\\nwe'll reset this for every epoch, and then we make sure that the\\nmodel is in training mode by calling model.train.\\nIn the training mode, gradients will be computed so that\\nmodel parameters can be updated using those gradients.\\nGradients are just partial derivatives of the loss function\\nwith respect to individual model parameters,\\nand these partial derivatives are used to determine how model\\nparameters should be tweaked to minimize the loss function.\\n\\nThis is all you need to understand conceptually about gradients.\\nFor every epoch, we run another for loop,\\niterating over each batch of the training data.\\nThis is on Line 7.\\nNow, for each batch of training, we zero out the optimizers\\ngradients so that gradients that were computed previously do not\\naffect this particular batch.\\nOn Lines 12 and 13, we move our training data to the\\ndevice that we are using for training,\\nwhether it's a GPU or a CPU.\\nNotice the boilerplate to device code here.\\n\\nNext, we make a forward pass through the model,\\nthis is on Line 16, for the first batch of training data\\nand get predictions.\\nThis forward pass uses the current value of the model parameters.\\nWe then compute the loss of the model at this stage by invoking\\nthe loss function, which takes in the prediction\\nand the Y values,\\nand then we perform a gradient descent.\\nWe make a backward pass-through the model by calling\\ntrain_loss.backward.\\nThis is where gradients are computed,\\nare partial derivatives, and optimizer.step\\nwill then use those gradients to tweak our model\\nparameter values.\\n\\nAnd then on Line 25, we add the current training loss\\nfor this batch to the training loss of the epoch as a whole.\\nAll of the steps you see from Line 9 through Line 25 is repeated\\nfor every batch of data in each epoch.\\nModel parameters will be updated for every batch in each epoch,\\nand the loss function will be minimized to improve the model.\\nAfter each epoch of training, we'll run the validation data\\nthrough the model and evaluate the model's performance.\\n\\nYou can see the with torch.no_grad with block on Line 28,\\nthat's within the outer for loop for the epoch,\\nbut outside of the for loop for the individual batches\\nof training.\\nThe torch.no_grad method turns off gradient computation\\nfor the model,\\nso gradients will not be computed when we pass through the\\nvalidation data.\\nI initialize the validation epoch loss to zero on Line\\n30, and on Line 32,\\nwe switch the model over to the evaluation state by calling\\nmodel.eval.\\n\\nThis is the state for evaluating the model.\\nThen once again I have a nested for loop where we iterate over the\\nbatches of validation data.\\nOn Lines 36 and 37, we move the validation data to the\\ndevice that we are using for training, CPU or GPU.\\nWe get the predictions on the validation data on Line 39,\\ncompute the loss on Line 41, and we add the loss of this batch\\nto the validation epoch loss.\\nOn Lines 45 and 46, we compute the training loss and\\nvalidation loss for the entire epoch,\\nand then append that information to our loss stats dictionary.\\n\\nThis is on Lines 48 and 49. On Line 51,\\nfor each epoch, we print out the training loss\\nand the validation loss.\\nAnd that's it.\\nThis is the training process.\\nIt's simple, but there is really a lot of boilerplate code involved.\\nLet's look at the output of training. Because this is a very\\nsimple neural network, training runs through very quickly.\\nYou can see that after the first epoch of training,\\ntraining loss was at 0.07 and validation at 0.01,\\nbut then training loss falls to 0.011,\\nvalidation loss falls to 0.008. Training loss further falls to\\n0.010 and stays there for the remaining epochs.\\n\\nThat is the lowest value of loss for this particular neural network\\nthat our model was able to achieve.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4553030\",\"duration\":167,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualizing losses and evaluating models\",\"fileName\":\"3096406_en_US_01_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":167,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to evaluate a regression model using PyTorch metric libraries.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4413310,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Now that we have a trained model, let's visualize how the training\\nloss and validation loss change over epochs of training.\\nAnd for that, I'm going to set up a data frame with the training\\nloss and validation loss along with the epochs.\\nThis information is available in the loss statistics that we've\\nmanually populated in the training process.\\nAnd we now have this in the form of a data frame.\\nWe have the epochs,\\nthen whether it's training or validation loss and the\\ncorresponding value. The head shows us all of\\nthe training losses,\\nand the tail of this data frame contains all of the\\nvalidation losses.\\n\\nNow with this information in the data frame format,\\nvisualizing this using a Seaborn lineplot is very straightforward.\\nWe'll have epochs on the x-axis and training and validation\\nlosses on the y-axis.\\nAnd here is what the line chart looks like.\\nThe training loss and the validation loss falls drastically\\nin the first two or three epochs, but remains constant after that.\\nTwo to three epochs of training was sufficient for this model.\\nThe model does not improve beyond that.\\n\\nLet's now compute the R-square score of the model on\\nthe validation data.\\nIdeally, we should have a separate test dataset for this purpose,\\nbut because the dataset was fairly small,\\nlet's just do it with the validation data.\\nWe turn off gradients with torch.no_grad,\\nswitch the model to eval mode, model.eval, and we iterate over\\nevery batch in the validation data.\\nMove the features to the device.\\nThis is on Line 9.\\nGet the predictions from the model. y_pred will hold the predictions\\nfrom the model, y_true will hold the labels from the actual data.\\n\\nLet's quickly look at the format of the actual data that is in\\ny_true.\\nWhat we have here is a list of tensors where each tensor contains\\na prediction for one batch of data.\\nYou can check this out on your own.\\nThe prediction data will also be in the same format.\\nI'll now perform a torch.stack operation that will give us the\\nactual values in the form of a single tensor,\\nrather than a list of tensors.\\n\\nThis is what torch.stack outputs.\\nI now have a single tensor with all actual values.\\nI'll use torch.stack on the predicted values\\nso that we now have a single tensor with all predicted\\nvalues, as well.\\nNow that the data is in this form, let's compute the mean squared\\nerror and R-square score for this model on the validation data.\\nI instantiate mean squared error and R-square score and move those\\nto the device as well and compute the two values.\\nYou can see that the R-square of this model is 0.797,\\nwhich is a fairly good score.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4552019\",\"duration\":374,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Building and training a more complex neural network\",\"fileName\":\"3096406_en_US_01_08_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":374,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to build a complex neural network in PyTorch.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15258281,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"What I've done here is scroll back up in our Jupyter Notebook to\\nwhere we had defined our simple neural network.\\nRemember we mentioned that this neural network contains just a\\nsingle linear neuron with no activation function.\\nWhen you use a neural network with a single neuron,\\nessentially what you're doing is performing linear regression that\\nyou would with a traditional machine learning library\\nsuch as scikit-learn.\\nI'm now going to replace this neural network to have a more\\ncomplex one with several layers and interconnections.\\n\\nI've still called it SimpleNeuralNet.\\nWe still inherit from the nn.Module base class,\\nbut you can see that overall the neural network has\\nmany more layers.\\nThe layers are defined in the init method.\\nWe take in the number of features as an input argument,\\nand Layer 1 comprises of 16 neurons.\\nNotice I instantiate nn.Linear num features,\\nthat is, the input and the number of neurons is 16.\\nLayer 1 is the first layer in our neural network.\\n\\nThe output of this linear layer will be fed to the second\\nlinear layer.\\nThis is Layer 2 instantiated once again using nn.Linear.\\nThe dimensionality of the inputs to the second layer is 16,\\nand this needs to match the number of neurons in the previous layer.\\nSo Layer 1 has 16 neurons and this matches the number of input\\nfeatures fed into Layer 2.\\nRemember this is because the output of Layer 1 is\\nfed into Layer 2.\\n\\nThe number of neurons in Layer 2 is 32.\\nAnd then we have Layer 3, the third linear layer in\\nour neural network.\\nThe input features to the third linear layer is 32,\\nand this should match the number of neurons in the previous\\nlayer which is 32.\\nLayer 3 has just 16 neurons.\\nAnd then finally, we have the last linear layer, that is the\\noutput layer, layer out.\\nIt accepts 16 input features as input and produces\\njust one output.\\n\\nThe 16 features as input corresponds to the 16 neurons\\nin the previous layer.\\nEach of the three layers;\\nLayer 1,\\nLayer 2, Layer 3 have an activation function,\\nand the activation function that I've chosen here is\\nReLU activation.\\nActivation functions are what allow us to learn nonlinear\\nrelationships that might exist in the data.\\nNow, the forward function defines the forward pass-through\\nthe neural network.\\nWe receive the inputs as an input argument\\nto this forward function, we apply Layer 1 and then the ReLU\\nactivation function.\\n\\nThen the outputs are then passed through to Layer 2,\\nand then ReLU activation again.\\nAnd the outputs are passed through to Layer 3,\\nand we have the ReLU activation\\nyet again. The transformed data after passing through three layers\\nis finally passed through the output layer.\\nAnd the output layer's prediction is what we return from\\nthe forward pass.\\nThe predict method is invoked when you use this model to\\nget predictions.\\nAnd the predict method makes the same forward pass-through\\nthe neural network.\\n\\nNow, let's quickly execute all of the remaining steps here in\\nthis Jupyter Notebook.\\nHere is a representation of the model, three linear layers and\\noutput layer and ReLU activation.\\nNow this model will have many more parameters\\nsince we have a large number of layers and many interconnections\\nbetween layers.\\nThe weights and biases of each layer have been initialized\\nto random values.\\nLet's count the number of parameters in this model,\\nand you can see that this model has 1233 parameters.\\n\\nYou are familiar with the remaining steps here.\\nI'll just quickly execute them and let's go directly to where we set\\nup the dataset and DataLoader and train our model.\\nHere is where we instantiate the dataset.\\nYou've already seen how that works before.\\nAnd once we have the dataset, we set up the DataLoader.\\nThis is for the training data.\\nWe do the exact same thing for the validation data, as well.\\nSet up the dataset and the DataLoader.\\nAgain, we initialize the loss_stats dictionary and we'll\\ntrain for 100 epochs.\\n\\nWe then see what device is available and that is stored\\nin the device variable.\\nNow that we've set all this up, instantiate the simple neural\\nnetwork and move the model parameters to the device that\\nwe're using for training.\\nOnce again, initialize the optimizer,\\nwhich will update the model parameters during the\\ntraining process.\\nNext, we actually train the model.\\nWe train for 100 epochs, and after each epoch, we pass the\\nvalidation dataset through the model and compute the\\nvalidation loss.\\n\\nI'm going to kickstart the training process and let's see how\\nthis more complex neural network performs.\\nYou can see that initially our training loss was 0.041,\\nthe validation loss is the same.\\nAnd as we run through epochs of training,\\nnotice how both the training loss and the validation loss drops.\\nUntil at the very end, after about 99 epochs,\\nour training loss is 0.007 and validation loss 0.005.\\nLet's set up the data frame with the training loss and\\nthe validation loss so that we can plot and\\nvisualize the values.\\n\\nExecute this visualization code and you'll be able to see how the\\ntraining loss and validation loss fall over epochs of training.\\nBoth losses fall until about 45 epochs of training,\\nafter which they remain fairly steady.\\nI'll hit \\\"Shift-Enter\\\" to get the predictions from the model and the\\nactual values from the data.\\nI'll use torch.stack to get the predictions and the actual\\nvalues in a single tensor.\\nAnd let's compute the R-square and the mean squared error.\\n\\nNotice the R-square score has gone up.\\nIt's now 0.881.\\nSo a more complex neural network that included an activation\\nfunction improve the performance of our model.\\n\"}],\"name\":\"1. Building a Neural Network with PyTorch\",\"size\":74098168,\"urn\":\"urn:li:learningContentChapter:4554030\"},{\"duration\":1121,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4549017\",\"duration\":412,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Encapsulating data using a LightningDataModule\",\"fileName\":\"3096406_en_US_02_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":412,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to encapsulate data with a LightningDataModule.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13924647,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"At this point, we've successfully built and trained a neural network\\nto perform regression, but we used PyTorch and\\nnot PyTorch Lightning.\\nIt was important that you see how the model is built using\\nPyTorch first,\\nso that you can see how much cleaner our code is\\nwhen we use PyTorch Lightning. We'll eliminate most of the\\nboilerplate code that you saw for training loops and feeding\\ndata in batches, iterating through number\\nof epochs,\\nAll of that code will just disappear,\\nand we'll build our model in a very clean manner with\\nPyTorch Lightning.\\n\\nPyTorch Lightning is a lightweight PyTorch wrapper that helps\\nresearchers and developers organize their PyTorch code and\\nstreamline the training process.\\nPyTorch Lightning eliminates boilerplate code,\\nsimplifies the training loop, and provides a more structured\\napproach to PyTorch programming without compromising flexibility.\\nPip install lightning to get PyTorch Lightning installed\\non your local machine. You need the lightning module\\nin addition to the PyTorch framework that we've\\nalready installed.\\n\\nLet's go ahead and set up the import statements for the various\\nlibraries that we'll be using.\\nThe imports here are the same as in the previous demo,\\nbut I have a few additional imports for PyTorch Lightning.\\nThe one on Line 9,\\nyou can see import lightning.pytorch as pl.\\nIn addition to abstracting away the training process of\\na neural network,\\nthe training loop, and other code associated with it,\\nPyTorch Lightning also makes available the lightning\\ndata module.\\n\\npl.LightningDataModule allows us to abstract and organize the\\ndata-related aspects of our deep learning model.\\nIt's a way to decouple the data processing steps,\\nthe loading, preprocessing, and splitting of data from\\nthe model training logic.\\nWhen we structure all of our data processing tasks to be\\nencapsulated within a lightning data module class,\\nwe are centralizing all data-related operations in one place\\nand encapsulating all of our code to make it more modular\\nand reusable.\\n\\nHere, I've specified the skeleton of the insurance data module class\\nthat inherits from pl.LightningDataModule.\\nAnd you can see that there are several functions of the base\\nclass that I'm about to override.\\nThe functions have all been named in very meaningful way.\\nSo you know exactly what goes in each of these functions.\\nLet's start by adding in the code for the init method here in this\\ninsurance data module.\\nThis is where we'll initialize various parameters we may want\\nto use with the data.\\n\\nThe only one we have is the batch size. In the prepare_data function\\nis where you access the data wherever it's stored,\\nmaybe you'll need to download the data,\\nand you can also perform a little bit of preprocessing if needed.\\nThe only thing I do here is to read in the insurance.csv file\\ninto a pandas data frame\\ninsurance data.\\nThis method is not invoked in a distributed manner and usually\\ncalled on a single GPU.\\nNext, we have setup.\\nThis is where you will split the dataset and apply whatever\\ntransformations and pre-processing that you need.\\n\\nThis will be called on every GPU separately.\\nAn input argument to the setup function is what stage the model\\nis currently running.\\nThis can be the fit stage, that is, training\\nor it can be the validation or test stages.\\nHere we are keeping things simple, so I'll apply the data\\ntransformations all in the fit stage or the training\\nstage of the model.\\nThe data transformation operations that you see here should all\\nbe very familiar to you because these are the steps we\\ncarried out in the previous demo.\\n\\nOn Lines 9 and 10, we extract the X features\\nand Y values.\\nOn Line 12, I check whether the stage is fit or stage is none.\\nSo in the training phase, we split the data into training\\nand validation.\\nThis is on Lines 13 and 14.\\nThe code on Lines 16 through 28 is where we one-hot encode the\\ncategorical features in our data.\\nOn Lines 30 and 31, we convert the Y values to NumPy\\narrays. On Line 33 through 36,\\nwe standard scale our features. On Lines 38 through 41,\\nwe minmax scale our targets.\\n\\nThat is the Y values. And on Lines 43 through 47, we\\nconvert all our NumPy arrays into tensors.\\nWe'll feed in data to our model in the form of tensors.\\nThe training and validation data are available as member variables\\nof this class;\\ntrain_inputs, train_targets, val_inputs, and val_targets.\\nThe feature and target tensors need to be instantiated\\nas DataLoaders.\\nThe train_dataloader method returns the DataLoader\\nfor the training data.\\n\\nThe steps here are again familiar.\\nWe instantiate a tensor dataset, and using that, we instantiate\\na DataLoader.\\nNotice that I've specified num workers equal to four\\nfor the DataLoader.\\nThis is because I have four cores on my machine,\\nand this will allow me to load data in parallel using\\nthose four cores.\\nThe train_dataloader function returns an instance of the DataLoader\\nfor the training data. In exactly the same way, I've\\noverridden the val_dataloader function.\\n\\nThis is just a DataLoader for the validation data.\\nWe instantiate a tensor dataset and use that to instantiate\\na validation DataLoader.\\nnum_workers, again, set to four, so that four workers running on\\nfour cores can be used to load this data.\\nWhen we actually train our model, this insurance data module will be\\npassed in as an input argument to the trainer object that we'll use.\\nAnd the individual methods of this data module will be invoked at the\\nright point in time to get access to the right bits of data needed\\nfor training and validation.\\n\\nLet's just make sure that the insurance data module\\nworks as expected.\\nI'm going to create an object of the insurance data module.\\nLet's now call prepare data and set up manually so that\\nthe data is available.\\nAnd now I'm going to invoke the train_dataloader,\\nand I'll print out one batch of the training data.\\nAnd you can see that there are eight records in this batch.\\nIn a similar way, let me access the val_dataloader\\nand I'll print out one batch of the validation\\ndata, as well.\\n\\nAnd we have eight records in this batch.\\nObserve how by using a data module to manage all of the data\\npreparation and processing operations,\\nwe've created a modular bit of code that can be reused anywhere.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4551031\",\"duration\":347,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Encapsulating a model using a LightningModule\",\"fileName\":\"3096406_en_US_02_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":347,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to encapsulate a model with a LightningModule.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10871150,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"In the previous demo, we built a simple regression\\nmodel using PyTorch.\\nNow in this demo, I'm going to build the same neural\\nnetwork that we used before, but this time I'm going to\\nuse PyTorch Lightning.\\nWhat I've defined here on screen is the skeleton of a class that\\nderives from pl.LightningModule.\\nJust like the lightning data module encapsulates all of the\\ndata-related operations,\\na lightning module is a fundamental class in PyTorch\\nLightning that encapsulates everything related to our\\ndeep learning model.\\n\\nIt extends the functionality of the nn.Module class that we used\\nin PyTorch to build up our neural network.\\nThe lightning module adds additional methods and structures\\nthat streamline the training, validation, testing,\\nand prediction processes of the model.\\nA lightning module organizes your PyTorch code into different\\nsections,\\nand each section is a different method that you override from the\\nlightning module base class.\\nYou can see that I have six different methods here.\\n\\nThese correspond to the six different sections into which\\nlightning module organizes your code.\\nLet's look at and understand each of these step by step,\\nstarting with init. The init method,\\nand in addition, there is also a setup method that I've\\nnot overridden,\\nin the nn.Module init method,\\nI've essentially moved that code in here to the init method\\nof lightning module.\\nThe init method takes in a number of parameters for the model, num\\nfeatures is the number of input features and the learning rate,\\nwhich I've set to 0.01.\\n\\nThis learning rate parameter will be used by the optimizer\\nthat we'll instantiate.\\nNow within the init method,\\nyou can see I've set up the layers of the neural network.\\nThere are three layers,\\nand then the final output layer. And the activation function that\\nwe'll use for the three layers is the ReLU activation.\\nThis network is exactly the same network that we've used\\nin our earlier demo.\\nAn interesting thing to note here is the code on Line 11 where I\\ncall self.save_hyperparameters.\\n\\nsave_hyperparameters is a method in the lightning module\\nbase class.\\nhere in the init method, all of the input arguments to init,\\nhere we have two, num features and learning rate, will be saved as\\nhyperparameters and will be accessible via the\\nself.hparams object.\\nLet me explain what I mean by adding the code here in the\\nconfigure optimizers method.\\nThis is the method where you will instantiate and set up any\\noptimizers and schedulers that you use to train your model.\\n\\nI once again use the stochastic gradient descent optimizer, SGD,\\nI pass in the parameters of the model available in\\nself.parameters,\\nand I pass in the learning rate of the optimizer using\\nself.hparams.learning rate.\\nThe learning rate that we passed in as an input argument to init on\\nLine 3 has been saved in this hparams object that we access\\nhere on Line 13 because we invoked\\nself.save_hyperparameters on line 11. self.save_hyperparameters\\nthus saves all input arguments that you pass into init.\\n\\nThe forward function is where you define the forward pass-through\\nthe neural network, and here we define the same\\nforward pass as we did before\\nin PyTorch. We pass the inputs through the three linear layers.\\nEach layer has the ReLU activation.\\nAnd then finally, we pass through the last output layer and\\nreturn the final value.\\nNow if you remember in PyTorch, we set up a training loop,\\nand within that, we define the training process of a model.\\nThis is what you'll define here\\nin training step.\\n\\nThis training step function will be invoked in a loop.\\nWhat you define here are the operations that need to be\\nperformed on a single batch of training data.\\nYou can see the input arguments a batch of data is passed in.\\nWe access the X features and Y values from this batch.\\nWe make a forward pass through the model by calling self.forward\\non the X features.\\nOn Lines 26 and 27, we instantiate the mean squared\\nerror loss function and compute the loss for this batch\\nof predictions.\\n\\nAnd then we simply call self.log and log the loss out.\\nActually, displaying the loss on screen and logging it out,\\nwell, PyTorch Lightning will take care of that automatically.\\nTo define the validation process of your model,\\nyou'll simply specify validation on one batch of data here in\\nthe validation step method.\\nWell, validation is straightforward.\\nOnce again, we make a forward pass with a batch of data using\\nself.forward,\\nwe compute the mean squared error loss on the validation data\\nusing the loss function, and we log out the\\nvalidation loss.\\n\\nAnd we have a predict step that you can override here.\\nIn order to make predictions on the data,\\nwe access the X variables and Y values from the batch and simply\\nmake a forward pass through the model to get predictions.\\nYou can see here that the entire model training code,\\nwithout the additional boilerplate of moving the model to a device\\nor the training loop, has been encapsulated\\nhere in one class.\\nLet me just instantiate and print out a string representation of our\\nmodel defined in this lightning module class.\\n\\nAnd in the next movie, we'll see how we will actually\\ntrain a model using PyTorch Lightning.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4554029\",\"duration\":362,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Training the model using the PyTorch Lightning Trainer\",\"fileName\":\"3096406_en_US_02_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":362,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to train a model with the PyTorch Lightning trainer.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11982679,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"We've defined our model, our training step, and\\nvalidation step\\nnicely encapsulated in a PyTorch Lightning module, and we are now\\nready to train our model.\\nIf you remember in PyTorch, you had to write a lot of code to\\nactually train your model and get validation metrics for your\\nmodel after each epoch.\\nIn addition, in order to ensure that your model trains on a GPU,\\nif a GPU is present, you had to also move the model\\nparameters as well as the X and Y values that you're using to train\\nyour model to the right device for training.\\n\\nNow, keep all of that in mind while we see how easy it is to\\ntrain a model using PyTorch Lightning.\\nYou see those five lines of code here on screen,\\nincluding the import statement,\\nwell, that's all the code you need to run a training loop,\\nrun validation at the end of every epoch,\\nand display all of that nicely to screen.\\nNo nested for loops, no moving the model to\\nthe right device,\\nno loss computation on training and validation data,\\nno optimizer step, no loss.backward.\\n\\nNothing.\\nFirst, let's take a look at the import statement,\\nwhere I import a CSVLogger that will log the details of the\\ntraining process out to screen in a CSV format.\\nNext, I instantiate the insurance data module class,\\nwhich encapsulates all of the data preparation and processing\\noperations for the data that I plan to use to train the model.\\nNext, on Line 5, I instantiate a CSVLogger class\\nto write the logs out to the logs subfolder under my current\\nworking directory.\\n\\nThe actual training process will be taken care of by the\\npl.Trainer class.\\nThis is a central class that manages and automates the\\nentire training process, and abstracts away a lot of the\\nboilerplate code typically associated with training loops,\\ndistributed training, and evaluation.\\nI instantiate the trainer, specify\\nI want to run a maximum of 50 epochs of training,\\npassing the CSVLogger so that logs are generated in my current\\nworking directory.\\nAnd that's it.\\n\\nI call trainer.fit.\\nTrainer.fit takes in an instance of a lightning module,\\nthat is the model that we want to train,\\nand a data module, that is a data set we should use for training.\\nTrainer.fit will invoke the right methods on the model class,\\nas well as the data class to get the right data for training\\nand validation, and it will run the training\\nprocess for 50 epochs.\\nOur training process has begun.\\nObserved that the trainer automatically checks to see\\nwhether a GPU or a TPU is available.\\n\\nIf not, it will just run training on the CPU.\\nThe trainer also shows you the number of trainable parameters\\nin the model.\\nWe had manually computed this earlier\\nin PyTorch. We have about 1.2K or 1200 parameters.\\nAs the training process continues, you'll see for every\\nepoch of training, the trainer prints out the epoch,\\nso you can see Epoch 16 here on screen,\\nthe training loss at this epoch, and the validation loss\\nat this epoch.\\nWe had set show progress bar to true when we logged out our\\ntraining loss and validation loss.\\n\\nAnd that's why you see these progress bars on screen,\\nas well. At Epoch 49\\nare 50th epoch, the training process is complete.\\nThe training loss at this point in time is 0.00598,\\nand the validation loss is 0.00477.\\nNow that we have a trained model, let's see how easy it is to get\\npredictions from the model. I call trainer.predict,\\npass the model in, and specify the validation DataLoader as the\\nDataLoader for the data for which I want predictions.\\n\\nAnd you can see the predictions from the model output\\nhere on screen.\\nThe output format of predictions gives us every batch of\\npredictions in a separate tensor.\\nLet's get all of these in a stacked format by using\\ntorch.cat to concatenate them.\\nThis will give us a single tensor with all the predictions from our\\nmodel for the validation data.\\nIn order to compute metrics for the model,\\nI need the actual values or the labels from the validation\\ndata as well,\\nand I extract this by using a for loop.\\n\\nNow that I have this, let's stack the labels as well\\nso we get a single tensor with all of the labels from our data.\\nNow that we have the predictions as well as the labels,\\nall that's left to do is to compute the mean squared error and\\nthe R-square score for this model.\\nAnd since this is the exact same neural network as the one\\nwe built in PyTorch, the R-square score is\\nalso the same 0.882.\\nIf you remember, we passed in a CSVLogger to the\\npl.Trainer object that we had instantiated to log out the\\nmetrics during the training process.\\n\\nWe can now access these metrics.\\nThey'll be in a metrics.csv file in your log directory.\\nThe log directory is accessible via the trainer instance,\\ntrainer.logger.log_dir/metrics.csv.\\nThis is a data frame that I read in,\\nwhich gives us the training loss, validation loss for every epoch.\\nWe do a little bit of pre-processing with this\\nmetrics data frame.\\nFor instance, I drop the step column and then I go ahead and\\ndisplay the metrics as a line plot.\\n\\nAlong the x-axis, we have the epochs of training.\\nAlong the y-axis, we have the training and validation losses.\\nThe blue line represents the training loss and the orange\\ndotted line is the validation loss.\\nNow, you've trained the same model using PyTorch Lightning,\\nand you can see how much simpler and more intuitive the code was.\\n\"}],\"name\":\"2. Using PyTorch Lightning to Build a Regression Model\",\"size\":36778476,\"urn\":\"urn:li:learningContentChapter:4550023\"},{\"duration\":1069,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4550021\",\"duration\":252,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Loading and exploring classification data\",\"fileName\":\"3096406_en_US_03_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":252,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to load data for classification.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6823426,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"We've seen how easy it is to build and train a neural network\\nusing PyTorch Lightning.\\nLet's get some more practice with this.\\nAnd this time, we'll build and train a classification model.\\nHere I am on a new Jupyter Notebook.\\nSince we'll be exploring and visualizing a new dataset\\ncomprising of classification data,\\nI'm going to ignore some warnings that will display when I use\\nSeaborn for visualization.\\nBy the time you're watching this course,\\nyou're unlikely to need this bit of code if you're using the\\nlatest version of Seaborn.\\n\\nNext, I set up the import statements for all of the\\nlibraries that I'll use.\\nThis involves data access, data processing libraries.\\nYou can see that I've also imported lightning.pytorch as pl\\non Line 10. Because this is a classification model,\\nwe'll evaluate this model with a different set of metrics.\\nOn Line 13 and 14,\\nyou can see I import accuracy and F1Score.\\nThese are the metrics we'll use to evaluate the model.\\nI have the data that we'll use to train the model in the\\ndatasets folder here.\\n\\nThis is a churn modeling dataset.\\nWe'll have information for a number of customers,\\nand we'll use that information to predict whether the customer\\nchurned or not.\\nLet's read in and take a look at the data before we actually set up\\na lightning data module to encapsulate all our\\ndata operations.\\nI'm going to read in from the Churn_Modelling.csv\\nfile under datasets.\\nHere in this dataset, we have information for bank customers,\\nrow number, customer ID, last name,\\ncredit score, gender, age, tenure, whether the customer has\\na credit card or not.\\n\\nAnd finally, the last column here is exited.\\nThat column contains the labels that we are trying to predict.\\nExited equal to one, meaning the customer churned.\\nZero means the customer did not churn.\\nWhen you take a look at the columns here in this dataset,\\nit's pretty clear that there are certain columns which are not\\nreally relevant in predicting whether the customer\\nchurned or not.\\nFor example, columns such as row number,\\ncustomer ID, or even the surname or last name of a customer.\\n\\nThose are bits of information not really relevant to figuring\\nout churn.\\nNext, let's take a look at the data types for the various columns\\nto make sure they are all of the right type.\\nA quick glance over the column shows me that numeric values\\nare numeric types, either integers or floats,\\nand categorical values are of type objects or strings.\\nThings look good so far.\\nThis dataset contains fields with missing values.\\n\\nWe'll clean up this data by using dropna to drop any records which\\nhave missing values,\\nthat's the code on Line 1,\\nand then this dataset also contains a few duplicate records.\\nAnd we'll eliminate duplicates by calling drop_duplicates\\non a pandas data frame.\\nAnd once both of these are done, the number of records we are left\\nwith for training as well as validation data is about\\n10,000 records.\\nThe classification model that we're trying to train is a binary\\nclassification model.\\n\\nWe are trying to predict whether a bank customer exited the\\nrelationship or not.\\nWe are trying to predict churn.\\nIf you look at the value counts for the exited field,\\nyou can see that this data set is very skewed.\\nEight thousand out of the 10,000 customers did not churn and only 2,000 did.\\nYou can actually view the same information using a nice Seaborn\\ncount plot. Based on the exited values,\\nyou can see that there are many more customers who haven't exited\\nas compared with customers who have exited or churned.\\n\\nOnce again, this is a skewed dataset,\\nand I'll give you a heads-up right now that we won't actually be\\nmitigating the skewness of this data.\\nSo I won't perform any sampling to balance out the customers who have\\nchurned and who've not churned.\\nSo there is a limit to how good our binary classification\\nmodel can be.\\nIf your training data is skewed, it's hard to build a great\\nclassification model.\\nBut since our focus is primarily on learning to use PyTorch\\nLightning to build a neural network model, we'll work with the\\nskewed data.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4549018\",\"duration\":268,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating a LightningDataModule\",\"fileName\":\"3096406_en_US_03_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":268,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to create a LightningDataModule for data preparation.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10302851,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"If you remember the previous regression demo where we\\nused PyTorch Lightning,\\nthe first thing we set up was a data module class.\\nThis data module encapsulated all of the data preparation,\\nprocessing, and other operations to work with our training\\nand validation data.\\nHere is our BankCustomerChurnDataModule which inherits from\\nthe pl.LightningDataModule base class.\\nNow, in the init method is where we initialize the parameters\\nof the data.\\n\\nto the init method.\\nIn prepare_data is where we load the data from wherever\\nit's available\\nand perform a few preprocessing operations to set up our data.\\nThe prepare_data method when we train our model is run\\non a single GPU.\\nHere I just read the CSV file from the datasets folder,\\nand then I drop all of the records with missing values using dropna\\nand I drop records that are duplicates by invoking\\ndrop_duplicates.\\n\\nThe setup function is where we perform data transformations and\\npreprocessing and data splitting.\\nIt takes in the stage as an input argument.\\nNow, the first thing I do in setup is drop those columns that are\\nnot relevant for prediction.\\nThe X features will not include exited.\\nThat's of course the target.\\nWe have to drop that.\\nBut we'll also not include row number,\\ncustomer ID, and surname because these fields do not have any\\npredictive power to figure out whether a customer churned or not.\\n\\nThe Y value or the target of the classification is the exited\\ncolumn that I assign on Line 18.\\nNow, I'll preprocess the data only if stage is equal to fit\\nor stage is equal to none.\\nThat is only in the training phase.\\nThe training phase will get the training as well as validation\\ndata and preprocess them both. In the training stage on\\nLines 21 and 22,\\nI first use train_test_split from scikit-learn to split the\\ndata, 80% for training, and the remaining 20% to\\nevaluate the model.\\n\\nOn Lines 24 through 37, we one-hot encode the categorical\\nvalues present in the data.\\nThe only categorical features include geography and gender,\\nand we instantiate the one-hot encoder and then use a column\\ntransformer to one-hot encode\\nthese values. On Lines 36 and 37 is where we actually perform the\\none-hot encoding by calling fit_transform on the training data and\\ntransform on the validation data.\\n\\nOn Line 39, we convert the Y values to NumPy arrays,\\nand on Lines 41 through 45, we use the standard scalar to\\nstandardize all numeric values.\\nAnd finally, on Lines 47 through 51,\\nwe take our training as well as validation data and convert them\\nall to PyTorch tensors.\\nAnd then, of course, we have the\\ntrain_dataloader function, which returns the DataLoader\\nfor the training data.\\nWe instantiate a tensor dataset and then we use that to\\ninstantiate a DataLoader.\\n\\nWe load the data using four workers\\nsince I have four cores on my machine.\\nAnd the code in the validation DataLoader is identical,\\nwe instantiate a tensor dataset with the validation data and use\\nthat to instantiate a DataLoader.\\nLet's quickly check that our data module works just fine.\\nI instantiate the bank_customer_churn data module, call\\nprepare_data, and then set up.\\nI'll now access the train_dataloader,\\nand let's take a look at one batch of training data.\\n\\nOnce again, we've used a batch size of eight.\\nAfter all of the pre-processing that we've performed,\\nlet's see the number of features that we have in the input data.\\nWe access one record in the training data,\\nuse the shape property, and get the second dimension,\\nwhich will give us a number of features.\\nIt's equal to 11.\\nRemember, we need this information to set up the layers of our\\nneural network model.\\nNext, let's look at one batch of the validation data to make sure\\nthat the validation DataLoader is also working fine.\\n\\nAnd you can see that indeed it is.\\nAll that's left for us is to set up a lightning module\\nwith our model,\\nand train the model using a lightning trainer object.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2708016\",\"duration\":301,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating a LightningModule\",\"fileName\":\"3096406_en_US_03_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":301,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to create a model with a LightningModule.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10514992,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"We are now ready to define our PyTorch Lightning module,\\nwhich will hold our model as well as have the steps required for\\ntraining and validating our model.\\nHere is the class that I have defined,\\nLitBinaryClassificationModule.\\nWe are going to be performing binary classification,\\npredicting whether customers churned or not.\\nWe inherit from the lightning module base class.\\nThe init method is where we set up our model.\\nThis is where we define the model architecture and its layers.\\n\\nNow, init takes in two input arguments, the number of features\\nin the input data and the learning rate that\\nI've set to 0.001.\\nI'm using a smaller learning rate here.\\nMy model comprises of three linear layers again,\\nLayer 1, Layer 2, and Layer 3.\\nAnd each of the three linear layers has a ReLU activation.\\nAnd these are initialized separately as Activation 1,\\nActivation 2, and Activation 3.\\nThe final output is also a linear layer.\\n\\nThis is the output layer on Line 12.\\nThe output of this layer will be a probability score,\\nand this is the probability score that we'll use to determine\\nwhether a particular customer churned or not.\\nOnce again, I call self.save_hyperparameters on Line 13.\\nThis will cause num features and learning rate both to be saved as\\nmembers of the hparams object that can then be accessed\\nby the optimizer.\\nThe forward function is where we define what a forward pass-through\\nthe model looks like.\\n\\nYou can see we pass the input through the three layers,\\neach layer has the ReLU activation, and finally, we pass the data\\nthrough the output layer.\\nThis will give us the final predictions that we return.\\nThe training step defines the operations for a forward\\npass through the model\\nfor a single batch of data. The batch is passed in\\nas an input argument,\\nwe access the X features and the Y variables,\\nand then we do a forward pass through the model by invoking\\nself on the input features.\\n\\nA forward pass through the model will output logits for\\nclassification.\\nThe term \\\"logits\\\" refers to the raw unnormalized prediction scores\\noutput by a model for each class or category.\\nSince it's binary classification, we'll have just one logits score.\\nNow, the loss function that we use for our binary classification\\nmodel is the BCE with logits loss.\\nBCE here stands for binary cross-entropy.\\nThe BCE with logits loss converts the raw logit scores\\ninto probabilities.\\n\\nIt squashes the output logits to be probability values\\nin the range zero to one.\\nIt then computes the binary cross-entropy loss,\\nwhich is essentially computing the difference between two probability\\ndistributions.\\nThe probability distributions of the actual value versus\\nthe predicted values.\\nOn Line 30, we compute the BCE with logits loss for\\none batch of data,\\nand we log that out as a training loss along with a progress bar.\\n\\nIn the validation step, we perform predictions on one\\nbatch of validation data, so you can see the code on Lines\\n37 through 42 is the same.\\nWe compute logits and then compute the loss.\\nHowever, here in the validation data, we want the actual\\npredictions from the model.\\nAnd we compute that by converting the raw logits scores to\\nprobability scores using the torch.sigmoid activation.\\nThis will give us probability scores in the range zero to one.\\ntorch.round of this probability score will give us the\\nmodel prediction, zero for not exited, one for exited.\\n\\nNext, on Lines 46 through 49, we compute the accuracy as well as\\nthe F1Score of the classification model.\\nThe accuracy tells us how many predictions the model got right,\\nbut because we have a skewed dataset,\\nthe accuracy is not a great measure of how good this model is.\\nThe F1Score here represents a trade-off between precision\\nand recall,\\nmetrics that are better suited to evaluate models trained\\non skewed data.\\nPrecision is the proportion of positive identifications\\nthe model got right, and recall measures of the\\npositive identifications in the dataset,\\nhow many was the model able to correctly identify.\\n\\nIn the validation step, we log out the validation loss,\\nthe validation accuracy, and the validation F1Score.\\nThe predict step just gives us the predictions from the model.\\nHere we just make a forward pass through the model.\\nIn configure_optimizers, you can see that we've set\\nup an Adam optimizer.\\nThe Adam optimizer is an adaptive learning rate optimization\\nalgorithm,\\nvery popular and widely used in the real world.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2708017\",\"duration\":248,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Training a classification model and evaluating metrics\",\"fileName\":\"3096406_en_US_03_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":248,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to evaluate a classification model using PyTorch metric libraries.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8966511,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Now that we've defined our lightning module,\\nlet's instantiate our classification module.\\nWe need to pass in the number of features.\\nHere is what the layers of the model look like.\\nWe know that the training process of this model is very\\nstraightforward using a PyTorch trainer.\\nOnce again, I use a CSVLogger to log out the training metrics\\nin a CSV file.\\nI instantiate the data module on Line 3,\\nthe logger on Line 5, and the trainer,\\nwhich will actually run the training and validation\\nprocess on Line 7.\\n\\nWe'll run for a maximum of 20 epochs of training and start the\\ntraining process by calling trainer.fit.\\nThe trainer will identify that we don't have a GPU here\\non this machine, and it will automatically train\\nthe model using the CPU.\\nNow, the training process took about 6 or 7 minutes to run.\\nHere at the end of one epoch of training,\\nvalidation accuracy is 0.819, so accuracy is high.\\nThe F1 score is just 0.268.\\n\\nI let this run through again for all 20 epochs.\\nAnd at Epoch 19, let's take a look at the various\\ncourse. Validation accuracy is 0.859, it has risen,\\nthe validation F1 score is 0.41, much better than before,\\nbut not great. Now\\nfor some reason, the validation F1 score doesn't display\\ncorrectly here\\nwhen we look at the output of the training of the model.\\nWe'll actually compute the accuracy,\\nprecision, recall, and F1 score for the validation data in just a bit.\\n\\nMeanwhile, let's access the metrics for the training process\\nof this model in the metrics.csv file under the training\\nlogs directory.\\nIn addition to the training and validation loss,\\nwe also logged out the validation accuracy and the validation\\nF1 scores.\\nThose additional metrics are also available here as part\\nof the logs.\\nvisualization, a Seaborn line plot.\\nAnd here you can see four different lines corresponding to\\nthe four metrics that were tracked during the training process.\\n\\nThe orange dotted line on top is the validation accuracy,\\nthe green dotted line is the validation F1 score,\\nand the remaining two lines represent the training and\\nvalidation losses, respectively.\\nNext, I'll call model.predict on the validation data.\\nSo we get predictions from the model and we get them\\nas a stacked tensor.\\nSo we have one tensor with all the predictions.\\nThe predictions here are in the form of raw unnormalized\\nlogits scores.\\n\\nIn order to get these scores in the form of probability values\\nbetween zero and one, you need to apply the\\ntorch.sigmoid function and then torch.round will give us the actual\\npredictions in the form of zero/one values.\\nLet's get the actual values from the validation data.\\nWe'll have to compare the predictions from the model against\\nthese actual values.\\nWe need to call torch.stack on these labels as well\\nin order to get them all in a single tensor.\\nNow that we have the predictions and the actual values,\\nlet's compute some evaluation metrics for the validation data.\\n\\nWe'll compute accuracy, precision, recall,\\nand the F1 score.\\nLet's go ahead and hit \\\"Shift-Enter\\\" and we'll see how the model is.\\nAccuracy is quite good at 0.859.\\nPrecision, which is the proportion of positive identifications\\nof exited customers\\nthat the model got right is quite high, 0.785.\\nThe recall score is on the lower side,\\n0.464.\\nOf the customers identified as exited or churned by the model,\\nhow many were actually right?\\nThis is what recall measures.\\n\\nAnd finally, we have the F1 score, which is the trade-off between\\nprecision and recall,\\nand that's the 0.58.\\nThat's great.\\nAt this point, you've successfully built and trained a classification\\nmodel using PyTorch Lightning.\\n\"}],\"name\":\"3. Using PyTorch Lightning to Build a Classification Model\",\"size\":36607780,\"urn\":\"urn:li:learningContentChapter:4549019\"},{\"duration\":78,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2708018\",\"duration\":78,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Summary and next steps\",\"fileName\":\"3096406_en_US_04_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":78,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1923056,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"And this demo brings us to the very end of this AI workshop on\\nbuilding a neural network with PyTorch Lightning.\\nNow, we started this course off with a quick overview of PyTorch\\nand PyTorch Lightning, and we discussed how PyTorch\\nLightning allows us to write cleaner and more modular code\\nfor model training.\\nWe then got hands-on and we trained a regression model\\nusing the PyTorch APIs.\\nWe identified that there was a lot of boilerplate code here in the\\nmodel building and training process,\\nand we eliminated a lot of this boilerplate repetitive code in the\\nnext demo where we train the same regression model,\\nbut this time we used PyTorch Lightning.\\n\\nAnd then in order to get some more practice with PyTorch Lightning\\nfor model training, we built and trained a\\nclassification model using PyTorch Lightning.\\nWell, this brings us to the very end of this AI workshop.\\nIf you're interested in neural networks and you want\\nto study further, here are some other courses\\non LinkedIn\\nlearning\\ntables and hands-on PyTorch machine learning are both great\\ncourses that might be a good fit for you.\\nWell, that's it from me here today.\\n\\nI hope you had fun in this AI workshop.\\nThank you for listening.\\n\"}],\"name\":\"Conclusion\",\"size\":1923056,\"urn\":\"urn:li:learningContentChapter:4549020\"}],\"size\":168040002,\"duration\":5449,\"zeroBased\":false},{\"course_title\":\"Natural Language Processing with PyTorch\",\"course_admin_id\":3004335,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":3004335,\"Project ID\":null,\"Course Name\":\"Natural Language Processing with PyTorch\",\"Course Name EN\":\"Natural Language Processing with PyTorch\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"Learn about natural language processing with PyTorch, the popular deep learning tool used by tech giants like OpenAI and Microsoft. In this course, Zhongyu Pan guides you through the basics of using PyTorch in natural language processing (NLP). She explains how to transform text into datasets that you can feed into deep learning models. Zhongyu walks you through a text classification project with two frequently used deep learning models for NLP: RNN and CNN. She also shows you how to tune hyperparameters and construct model layers to get more robust and accurate results, as well as the differences between the two models for NLP tasks.\",\"Course Short Description\":\"Learn the basics of using PyTorch, a powerful deep learning tool, for natural language processing.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":20160017,\"Instructor Name\":\"Zhongyu Pan\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Content Creator at LinkedIn\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2022-04-15T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/natural-language-processing-with-pytorch\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Artificial Intelligence for Technology\",\"Primary Software\":\"PyTorch\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":2464.0,\"Visible Video Count\":12.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":161,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3070068\",\"duration\":77,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Natural language processing\",\"fileName\":\"3004335_en_US_00_01_WX30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"In this video, explore the course overview and learn the prerequisites for the course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2854751,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Pan] We interact with machines  \\n most commonly through natural languages  \\n like English, Chinese, or Spanish, you name it.  \\n We ask our smartphones to open our favorite playlist  \\n on the music app.  \\n We type a sentence in Google Translate  \\n to be able to communicate with people  \\n from a different culture.  \\n Natural language processing, or NLP for short,  \\n gives machines the ability to understand human languages  \\n that we use in daily life accurately and efficiently.  \\n It has also undergone a massive transformation  \\n with the introduction of Deep Learning.  \\n PyTorch, which is one of the most popular  \\n Deep Learning libraries, developed by Meta,  \\n makes NLP even easier to learn and work with.  \\n Hi, my name is Pan Zhongyu.  \\n I work as a data scientist and software developer.  \\n I have built NLP-related systems  \\n for companies with knowledge of Deep Learning.  \\n Join me on my LinkedIn Learning course  \\n as we explore the exciting field  \\n of natural language processing with PyTorch.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3068059\",\"duration\":84,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"3004335_en_US_00_02_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"In this video, learn about the benefits of PyTorch and whether it is the best fit for you.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2810381,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this course,  \\n I will be giving you brief introductions  \\n to natural language processing, deep learning and PyTorch.  \\n You will learn how to pre-process  \\n and transform the text data into numeric datasets  \\n that can be fed into deep learning models.  \\n Then, you will be guided through  \\n a text classification project  \\n where you build and train a deep learning model  \\n for classification predictions.  \\n If you have never worked  \\n with natural language processing before, don't worry.  \\n This course is designed for anyone  \\n with a basic Python programming background  \\n who wants to dive into NLP  \\n with the powerful tool of PyTorch.  \\n You will need to have knowledge of key concepts in Python,  \\n like variables, functions, classes and so on.  \\n It will also help if you are familiar  \\n with the use of Python packages,  \\n like Numpy, pandas, NLTK.  \\n We assume that you have some deep learning exposure,  \\n you understand the basics of neural networks  \\n so that when we are introducing the PyTorch way  \\n of building neural nets,  \\n it will be easier for you to follow.  \\n We are going to use Google Colab throughout the coding part  \\n and I will help you set this tool up before we start coding.  \\n Let's dive right in.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":5665132,\"urn\":\"urn:li:learningContentChapter:3069059\"},{\"duration\":353,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3068060\",\"duration\":134,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Popular topics in NLP\",\"fileName\":\"3004335_en_US_01_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn the basic definition of NPL, its popular fields, and why you should learn it.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4490083,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] So, what is natural language processing?  \\n There can be multiple interpretations,  \\n but one way of defining it  \\n is that natural language processing  \\n is a subfield of artificial intelligence  \\n that helps machines better understand  \\n or generate human languages.  \\n NLP is therefore divided into two major components,  \\n natural language understanding  \\n and natural language generation.  \\n As the names imply, natural language understanding  \\n enables machines to understand the meaning of the text  \\n while natural language generation instead  \\n produces text responses based on specific text input.  \\n We will mainly focus on natural language understanding  \\n in this course.  \\n Let's take a look at some popular applications of NLP.  \\n One of the most important applications  \\n is sentiment analysis.  \\n It is a type of text classification task  \\n and it has been largely implemented by enterprises  \\n for understanding customer feedback,  \\n monitoring social media,  \\n conducting market research, et cetera.  \\n The second application is chatbot,  \\n which has been usually applied as a tool of customer support  \\n by various companies that need to interact with customers.  \\n It is also used for marketing or sales purposes.  \\n Speech recognition allows us to communicate  \\n with Apple's Siri or Amazon's Alexa  \\n just like the way we talk to a real assistant.  \\n Machine translation is also a common use case of NLP.  \\n One popular application is Google Translate,  \\n which translates one language to another  \\n in a very short period of time.  \\n One of the coolest applications is advertisement matching,  \\n which you can see probably every day on YouTube.  \\n The most relevant ads are displayed  \\n based on your personal preferences.  \\n The NLP application we're going to build in this course  \\n is text classification, stay tuned.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3073044\",\"duration\":219,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introducing deep learning\",\"fileName\":\"3004335_en_US_01_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to describe deep learning at a high level and intuitively explain how deep learning works under NLP concepts.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7773368,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Deep learning is a subset  \\n of machine learning,  \\n and machine learning is a subset of artificial intelligence.  \\n In this video, we are going to introduce three types  \\n of deep learning models:  \\n artificial neural networks,  \\n convolutional neural networks,  \\n and recurrent neural networks,  \\n about how they work at a high level  \\n and compare their differences.  \\n Note that CNN and RNN are both ANN models.  \\n Let's first look at artificial neural networks  \\n or ANN for short.  \\n It normally has one input layer,  \\n one or more hidden layers  \\n and one output layer.  \\n The data is fed into the input layer,  \\n then calculated and extracted by hidden layers.  \\n And finally, the output layer delivers the result  \\n based on the information simplified  \\n by previous layers.  \\n However, regular ANN does not work so well  \\n in natural language processing or image processing tasks.  \\n So let's look at convolutional neural networks, or CNN,  \\n which performs better in both tasks.  \\n But why does CNN work better?  \\n Because it works spatially along the data  \\n and it focuses more on the feature  \\n rather than on the position of the data.  \\n The unique layer structures of CNN  \\n are convolutional layers and pooling layers,  \\n compared with a regular ANN.  \\n The world convole refers to the filtering process.  \\n This filtering process simplifies the information included  \\n within each layer  \\n so that it is easier for the next layer  \\n to process the output as shown in the picture below.  \\n This way it is convenient  \\n to find important features and ignore the trivial ones  \\n so that we get more solid results.  \\n CNN is also the deep learning model we're going to build  \\n for solving a natural language processing task  \\n in this course.  \\n Another major type of deep learning model  \\n is called recurrent neural networks or RNN.  \\n The structure of RNN is similar to a regular ANN,  \\n which contains one input layer,  \\n one or more hidden layers  \\n and one output layer.  \\n The difference between RNN and CNN  \\n is that RNN not only passes data forward  \\n throughout the layers  \\n but also feeds the data back into itself  \\n while CNN passes data only one way forward.  \\n That means RNN can remember the context before  \\n and after the current word in a sentence.  \\n An RNN is therefore well-suited for sequential data,  \\n such as text or audio.  \\n Another thing to keep in mind  \\n is that CNN is on average faster than RNN  \\n in processing text data.  \\n I hope you now have a general understanding  \\n of how these deep learning models work differently.  \\n In the next chapter,  \\n we're going to get our hands dirty  \\n by writing code and we will introduce PyTorch,  \\n the essential framework  \\n for learning natural language processing.  \\n So you in the next chapter.  \\n \\n\\n\"}],\"name\":\"1. NLP with Deep Learning Introduction\",\"size\":12263451,\"urn\":\"urn:li:learningContentChapter:3071047\"},{\"duration\":390,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3067062\",\"duration\":102,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Why PyTorch?\",\"fileName\":\"3004335_en_US_02_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, explore which tech giants are using PyTorch for NLP and its most popular uses and functionalities within the tech sector.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3747172,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Speaking of deep learning,  \\n PyTorch, developed by Meta,  \\n and TensorFlow, developed by Google Brain,  \\n are two open source Python libraries  \\n that are extensively used nowadays  \\n and they do share some similar features.  \\n So why do we use PyTorch instead of TensorFlow  \\n in this course?  \\n PyTorch has a reputation for being more applied  \\n in research than in production  \\n and most of the top AI papers use PyTorch  \\n as their deep learning framework  \\n while TensorFlow is widely used in industry  \\n by tech companies for deploying AI products.  \\n But for people who are just getting started with NLP,  \\n it is recommended to learn it with PyTorch.  \\n The reasons are;  \\n first, PyTorch was created to make  \\n the models easier to write.  \\n The code in PyTorch is relatively more concise,  \\n which makes it beginner-friendly.  \\n Second, the statements in PyTorch are designed  \\n to mimic the ones in Python.  \\n So if you have previous working knowledge of Python,  \\n you will feel pretty natural coding in PyTorch.  \\n Third, although PyTorch is more geared toward research,  \\n it has been used by large enterprises  \\n for building NLP-related services.  \\n For instance, Airbnb used to build dialogue assistant  \\n using PyTorch for improving the customer service experience.  \\n PyTorch has also been used by Microsoft  \\n for its language modeling service.  \\n Without further ado, let's start coding.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3073045\",\"duration\":288,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"PyTorch tensor\",\"fileName\":\"3004335_en_US_02_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn what a PyTorch tensor is and how to implement the basic calculations and functions of PyTorch Tensor.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10976137,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this video,  \\n we will introduce the PyTorch tensor  \\n and the common methods of tensors  \\n but before that, let's first set up Google Colab.  \\n Make sure you have a Google account  \\n and you're able to access your Google Drive.  \\n Then visit colab.research.google.com.  \\n There is this documentation of welcome to Colab.  \\n You can read it briefly if you're interested  \\n but we will cover everything you need to know  \\n for learning this course.  \\n Now please open your exercise file  \\n of this video in Google Colab  \\n and code along with me.  \\n We can think of a tensor as a data container  \\n or data structure  \\n that carries arrays of numbers in PyTorch.  \\n Let's first look at how to create a PyTorch tensor.  \\n If you don't have Torch library yet,  \\n type pip install torch  \\n and run the cell to install the package.  \\n I have already installed the package,  \\n so it says requirement already satisfied.  \\n We then import Torch library and NumPy.  \\n Now, let's create a PyTorch tensor with an array.  \\n We already had an array here.  \\n So we type torch.tensor array  \\n and name it tensor0.  \\n And then we print out tensor0  \\n as well as its data structure type,  \\n which is torch.tensor,  \\n and data type, which is the type  \\n of the data carried by the tensor.  \\n In this case, it's integer.  \\n We also printed out the shape of the tensor  \\n where the first number in the case, the number of rows  \\n and second number in the case, the number of columns.  \\n Another way of constructing a tensor  \\n is from a NumPy array.  \\n This is basically the same as what we just did  \\n but instead of a regular array,  \\n we put a NumPy array  \\n inside of torch.tensor container, like this.  \\n Now let's take a look at some common methods in PyTorch.  \\n The first one is slicing.  \\n Slicing of tensors is basically the same  \\n as the slicing of NumPy arrays.  \\n We have tensorA and B here.  \\n And we can slice the first two rows of tensorA like this  \\n where index one is inclusive  \\n and index two exclusive.  \\n If we want to slice the first two columns,  \\n we will need to first take all the rows  \\n and then select from columns, just like before.  \\n The second common method is concatenation.  \\n Concatenation is extremely useful  \\n when we're building deep learning models.  \\n By using torch.cat method,  \\n we can concatenate two tensors vertically  \\n when both tensors have the same amount of columns.  \\n Don't forget to wrap the two tensors in one array.  \\n We can also concatenate them horizontally  \\n and specify dimension as one.  \\n Let's print out the results  \\n to see what the concatenated tensors look like.  \\n Okay, the first one is the vertical concatenation  \\n of the two tensors  \\n and the second one is the horizontal concatenation.  \\n Those are exactly what we want.  \\n One thing to point out is that torch.cat performs  \\n vertical concatenation by default.  \\n So if you want to apply horizontal concatenation,  \\n you will need to add an argument.  \\n Then equals one to get the desired tensor.  \\n Congratulations, you just finished learning the basics  \\n of PyTorch tensor.  \\n In the next chapter, we will start our guided project  \\n by building a convolutional neural network  \\n for solving a tax classification task.  \\n See you in the next chapter.  \\n \\n\\n\"}],\"name\":\"2. PyTorch Basics\",\"size\":14723309,\"urn\":\"urn:li:learningContentChapter:3068062\"},{\"duration\":1513,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3073046\",\"duration\":557,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Preprocessing text dataset\",\"fileName\":\"3004335_en_US_03_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to preprocess text data by learning how to load and transform text into a format that can be fed into a DL model. Explore different ways of preprocessing and learn the difference between them.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17603256,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] From now on,  \\n we will be working on the guided project  \\n which is text classification  \\n using convolutional neural networks.  \\n The first step of the project  \\n is to pre-process text dataset,  \\n which is the goal of this video.  \\n Please download the exercise file of this video  \\n and open it in Google CoLab.  \\n Before we start coding,  \\n let's change runtime type to GPU  \\n to ensure the code runs as fast as possible.  \\n Simply click on Runtime.  \\n Then change the runtime type to GPU.  \\n Please keep in mind that when you change runtime type,  \\n your Jupyter Notebook will be restarted,  \\n and you will have to rerun all the code.  \\n So make sure you change runtime type  \\n before you start typing any code.  \\n Let's first import the required libraries.  \\n Here we import torch from torchtext dot legacy,  \\n import data and datasets.  \\n Lastly, we import random.  \\n Then in the second cell at line number two,  \\n we call torch dot manual seed here  \\n to set the seed of the random number generator,  \\n so the random values generated by torch  \\n will be reproducible the next time we run the code.  \\n You can also set the seed value  \\n to any number you want.  \\n To check if you actually switched your runtime type  \\n to GPU successfully,  \\n you can run line number four and five  \\n to print out your current device.  \\n If it prints out cuda,  \\n then congratulations.  \\n That means you have your GPU ready.  \\n If not,  \\n it means you are still using CPU,  \\n which will make your model run slower.  \\n But don't worry.  \\n Even with CPU,  \\n you can still complete our project,  \\n since CNN is by itself a lot faster  \\n than most of the deep learning models.  \\n Now let's define the fields,  \\n text field and label field.  \\n We are using data dot Field and data dot LabelField here.  \\n And you can read more about them in this documentation.  \\n So field and label field are basically the same things,  \\n except that label field doesn't take sequential data  \\n like text,  \\n which is reasonable,  \\n since label field takes in classification labels  \\n rather than the text.  \\n And for field,  \\n it basically models our text data  \\n to numerical representations carried by tensors.  \\n We set tokenize method to spacy.  \\n And we will lowercase all the text.  \\n Now you must be wondering,  \\n what does the data actually look like?  \\n Let me show you.  \\n PyTorch has some built-in datasets  \\n that we can import directly.  \\n And the datasets we are using here  \\n is called Text Retrieval Conference or TREC dataset.  \\n It's question classification dataset.  \\n So each piece of text in this dataset is a question.  \\n And each question is labeled as one of the six categories.  \\n The table here gives us an idea  \\n of what the TREC dataset actually looks like.  \\n Each data example in a dataset  \\n contains a question and a label.  \\n Each question in the first column of the table  \\n is labeled as one of the six categories,  \\n which are in the second column.  \\n And the third column is just the full name of those labels.  \\n The six categories are abbreviation,  \\n numeric, human, entity, description and location.  \\n These names are pretty self-explanatory.  \\n Let's take a look at a few examples here.  \\n The answer to the question,  \\n what is the date of boxing day,  \\n is clearly a date,  \\n so this question is labeled as numeric.  \\n If the question is asking about the city's name,  \\n like what is California's capital,  \\n then it is labeled as location.  \\n As we mentioned,  \\n the text or the question  \\n might be labeled as any of the six categories,  \\n so our task is to build a CNN model  \\n to successfully predict the labels of input questions.  \\n For example, if I feed the question,  \\n who discovered electricity,  \\n into the model,  \\n since the answer to it must be a human name,  \\n we would expect the model to output the correct label,  \\n which is human.  \\n I hope the mission of this project is now clear to you.  \\n Let's split TREC dataset into train and test datasets.  \\n We are passing the text and label fields we just built  \\n here at line number one.  \\n Now we got our train and test data ready,  \\n but we also need a validation dataset  \\n for the sake of evaluation.  \\n So let's split a part of the training data  \\n as validation dataset.  \\n And don't forget to save a random seed here as well  \\n to save the randomness here at line number two.  \\n Now we can run this line of code  \\n to check a sample of the training data.  \\n The text or the question is  \\n how fast does the fastest car go?  \\n And the label is categorized as numeric.  \\n That makes sense.  \\n Since our data is ready,  \\n we can start building vocabulary for text and label fields.  \\n To reduce the number of unique words in our vocab object,  \\n we set the minimum frequency to two.  \\n That means a word has to appear at least twice  \\n in a train data  \\n to be included in a vocab object of the text field.  \\n Please also notice that we are building vocabularies  \\n based on only training data,  \\n so make sure you split train validation and test datasets  \\n before you build vocab.  \\n Let's print out the vocab object of label field.  \\n We can see the six labels  \\n as well as their numerical representations.  \\n Similarly, in a vocab object of text field,  \\n we also have words with their numerical representations.  \\n Now let's check how big those vocabs are.  \\n There are 2,679 words in text field,  \\n and of course, six words in label field,  \\n which represent the six categories.  \\n Amazing.  \\n We just got our train validation and test set  \\n and successfully built the vocabs  \\n with their numerical representations.  \\n We only have one thing left to do,  \\n which is to construct the iterators  \\n for each set in the last cell.  \\n But why do we need iterators?  \\n The bucket iterator transforms  \\n the train validation and test datasets into batches  \\n at line number two.  \\n The batch size is set to 64 at line number three,  \\n which means the number of training examples  \\n in one batch is 64.  \\n Then in a sort key argument at line number four,  \\n we are sorting based on the length of each sentence,  \\n which means it batches the text of length together.  \\n Finally, we set the device to GPU  \\n for an even faster training process at line number five.  \\n That's all for pre-proccessing the text dataset  \\n with PyTorch.  \\n See you in the next video  \\n where we are going to build a CNN model.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3067063\",\"duration\":386,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Building a simple CNN model\",\"fileName\":\"3004335_en_US_03_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to build a basic deep learning model using PyTorch, as well as how to fine-tune model parameters and make the model more solid.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14679394,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] I hope you are excited.  \\n In this video,  \\n we are building CNN model using PyTorch.  \\n What we are building is a very simple CNN  \\n that helps you consolidate your understanding  \\n of CNN architecture.  \\n We first import required libraries in the first cell.  \\n Then in a second cell,  \\n we define the class called CNN at line number one,  \\n and pass in nn.Module.  \\n All the PyTorch models  \\n inherit from the subclass of nn.Module.  \\n As you can see here, we are going to define  \\n init and forward functions,  \\n which are two of the most essential functions  \\n in a neural network model.  \\n Init is the constructor where we define the model layers.  \\n And when you instantiate a model object,  \\n init will be called.  \\n Forward method is where we actually add layers to the model  \\n and build the model architecture.  \\n In the init function,  \\n we are passing in the parameters we need  \\n for the constructor.  \\n There are vocabulary size, embedding size,  \\n number of kernels, kernel sizes,  \\n output size, and dropout rate.  \\n We use super.init to call the entire nn.Module class.  \\n Then we define the embedding layer  \\n with the required arguments.  \\n And then, the convolutional layers.  \\n Now, the dropout layer.  \\n And finally, the fully connected layer.  \\n Please notice there are actually three convolutional layers  \\n with three different kernel sizes we are going to define,  \\n namely, two, three, and four.  \\n The number of convolutional layers is the same  \\n as the number of different kernel sizes.  \\n I will leave you a little task here.  \\n Please try searching for the name of those nn. layers online  \\n and figure out what those layers do,  \\n as well as what their arguments mean.  \\n For example,  \\n when searching for nn.Embedding layer at line number five,  \\n take a look at the meanings of arguments,  \\n vocabulary size, and embedding size.  \\n Keep doing this until you understand all the layers  \\n and their arguments in the CNN model.  \\n I ensure this will help you a lot  \\n with understanding any CNN model  \\n you are going to work with in the future.  \\n Now, it's time to move on to the forward function.  \\n The forward function  \\n is where we build the model's architecture  \\n using the layers we defined in the init function.  \\n As we mentioned earlier in the course,  \\n the forward function should take in a piece of text  \\n and output the prediction of one of the six categories.  \\n We need to first permute the size of the text  \\n so that our embedding layer can properly carry the data.  \\n nn.Embedding will automatically build  \\n the embedding of the text.  \\n Then we pass the result to convolutional layers,  \\n max pooling layers, and dropout layer.  \\n By the way,  \\n the purpose of using dropout is to prevent overfitting.  \\n And finally, we return the result  \\n of the fully connected layer.  \\n The architecture looks the same  \\n as what we discussed in theory,  \\n but the shape of the output of each layer  \\n needs to be taken care of  \\n to be able to fit in the next layer.  \\n That is also why we are using  \\n functions permute, unsqueeze, and squeeze  \\n in a forward function.  \\n I encourage you to print out  \\n the shape of the output of each layer  \\n and think about why we use those functions  \\n to modify the shape of the data  \\n when we are passing them through different layers in CNN.  \\n After building the model,  \\n we need to instantiate it.  \\n And to do that,  \\n we will need to pass in all the arguments of the model  \\n from the init function.  \\n We give each argument a value,  \\n but you can play with those parameters  \\n to see if you can get a better result.  \\n This is actually one way of building a more solid model  \\n to achieve better accuracy.  \\n Now that the model is instantiated,  \\n we can print the model out  \\n to see if it has the architecture we wanted.  \\n We see an embedding layer,  \\n three convolution layers  \\n with kernel sizes two, three, and four,  \\n then a dropout layer, and a fully connected layer.  \\n Exactly what we wanted.  \\n Finally, in the last cell,  \\n let's move our model to GPU  \\n so we can train a lot faster.  \\n We say model.to(device)  \\n where our device is cuda, or cpu.  \\n That's it for building the CNN model.  \\n Our next step is to complete  \\n the train and evaluate functions for the training process.  \\n See you very soon.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3069057\",\"duration\":226,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Train and evaluate functions\",\"fileName\":\"3004335_en_US_03_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn about CNN at a high level, how CNN works in text classification, and how CNN and RNN perform different text classification tasks.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8297142,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] For a complete training process,  \\n we need to have a train function  \\n and evaluate function.  \\n The train function makes predictions based  \\n on training data  \\n and calculates how inaccurate the predictions are  \\n by comparing them with true labels  \\n of the training dataset.  \\n The train function then updates the parameters  \\n of the network to minimize the error  \\n or we call it minimizing the loss.  \\n So the model performs better.  \\n The evaluate function acts very similarly  \\n to the train function.  \\n It makes predictions based on validation data  \\n or test data and calculates  \\n how inaccurate the predictions are by comparing them  \\n to true labels of the dataset.  \\n But it doesn't update the parameters of the model.  \\n It simply evaluates the model based  \\n on the model's current state.  \\n Let's code the train function together.  \\n There are three essential parameters we need to define  \\n for the training process:  \\n the accuracy, the criterion and the optimizer.  \\n For criterion, we simply use a loss function  \\n call CrossEntropyLoss.  \\n We also move the criterion to GPU  \\n for a faster training process.  \\n You can check out PyTorch documentation  \\n and change the loss function as needed.  \\n For the optimizer,  \\n we are using Adam optimizer here  \\n and passing in the model's parameters.  \\n You can experiment with different optimizers as well.  \\n For accuracy, we prepared a function  \\n to calculate it in the second cell.  \\n As you can see here,  \\n it is basically calculating the percentage  \\n of correct predictions out of all the true labels.  \\n We also prepared the train function.  \\n As we mentioned before,  \\n we iterate through batches in the iterator.  \\n The optimizer helps us update the parameters  \\n of the model to minimize the loss.  \\n We use the model to make predictions  \\n of the text data  \\n and calculate the loss and accuracy.  \\n Then we add in total the loss  \\n and accuracy of each batch for the loss  \\n and accuracy of the entire epoch at line number 20 and 21.  \\n Finally, we return average loss and accuracy  \\n at line number 23.  \\n The length of the iterator is just the number  \\n of data examples in the iterator.  \\n In the evaluate function,  \\n we are doing something similar to the train function  \\n where we use the model to make predictions,  \\n then calculate and return the average loss and accuracy.  \\n The only thing we didn't do in evaluate function  \\n is that we didn't update model parameters  \\n based on the loss of each batch.  \\n We have pre-processed text data,  \\n built CNN model,  \\n completed train and evaluate functions,  \\n it is time to train the model.  \\n I am going to make it a challenge for you  \\n so you can work on it on your own  \\n but don't worry, in the next video,  \\n you will get enough hints  \\n on how to solve the challenge.  \\n Let's jump right into the challenge.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3068061\",\"duration\":108,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Training process\",\"fileName\":\"3004335_en_US_03_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to clean text data as the first essential step in building a model.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4570464,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Welcome to the challenge.  \\n This is also the last section of our guided project  \\n which is the model training process.  \\n When you open the exercise file of this video,  \\n you can see that we wrap up everything we did together  \\n in one file,  \\n including text preprocessing,  \\n model building,  \\n train and evaluate functions.  \\n You can take your time to review the above sections,  \\n but we will go to the training section right now.  \\n The challenge for you is to fill in all the code  \\n under the Python comments,  \\n where it asks you to write code.  \\n As you can see,  \\n there are only three lines of code you need to fill in  \\n at line number eight, number 10, and number 14.  \\n For line number eight and 10,  \\n you need to figure out how to calculate  \\n the training loss, training accuracy,  \\n validation loss, and validation accuracy  \\n using the functions we built in previous sections  \\n and passing the correct arguments.  \\n The purpose of the code at line number 14  \\n is to update the best accuracy  \\n which we initialized to negative infinity  \\n at the beginning of the training process  \\n here at line number three.  \\n Try to complete the code on your own  \\n and please do review the code in the previous sections  \\n as it does help with solving this challenge.  \\n See you in the solution video.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3069058\",\"duration\":236,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Training process\",\"fileName\":\"3004335_en_US_03_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, explore the solution to Challenge: Training process.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9207649,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Let's complete the training process together.  \\n First, we define the number of epochs we want to train  \\n at line number one.  \\n You can change 20 to a smaller or bigger number  \\n to get the desired accuracy and loss.  \\n Then we loop through each epoch.  \\n And for each epoch, we calculate accuracy and loss  \\n for the training and validation set  \\n using the train and evaluate functions  \\n at line number 8 and 10.  \\n Like this, we can easily pass in the model, train,  \\n or validation iterator, optimizer, and criterion.  \\n I hope you got the answer right.  \\n Now, let's look at how we can update the best accuracy.  \\n When we find that the validation accuracy  \\n is larger than the best accuracy,  \\n that is, of course, a good thing,  \\n because we just found a better model.  \\n This is the time when we should update the best accuracy.  \\n As it is so far the best model,  \\n we will save it at this epoch at line number 15.  \\n And finally, we print out the accuracies and losses  \\n for each epoch from line 17 to line 19.  \\n Let's start training right now.  \\n We can see that both training and validation accuracies  \\n are going up.  \\n Our training set might have overfitting issues  \\n since it has quite a bit of difference  \\n compared to the validation accuracies.  \\n But we are training a very small CNN model,  \\n so we will ignore that for now.  \\n After the training, we need to test the actual performance  \\n of the model when it takes in real data.  \\n We load the best model,  \\n which has the highest accuracy at line number one  \\n and evaluate the model using our test set  \\n at line number three.  \\n This is also the last step in the project,  \\n which is to get the final test accuracy.  \\n The accuracy is around 85%,  \\n which is pretty decent for a CNN model sample like this.  \\n As we mentioned before, we can play with the parameters  \\n that we passed into the model  \\n to get hopefully a better accuracy.  \\n Just make sure to change only one parameter at a time  \\n if you want to explore the impact  \\n of one particular parameter on model accuracy.  \\n Let me show you how to do it.  \\n Let's change the dropout_rate to 0.6.  \\n Rerun this cell as well as all the cells after it  \\n and wait for the test accuracy to be calculated  \\n at the end of the file.  \\n We can see that the test accuracy  \\n is lower than last time,  \\n which means playing with the parameters  \\n can actually influence the model accuracy.  \\n I encourage you to keep exploring  \\n using this Jupyter Notebook.  \\n Congratulations, you just completed the entire project  \\n on text classification with PyTorch.  \\n You have successfully preprocessed text data,  \\n built a CNN model,  \\n trained and evaluate the model.  \\n I hope you find this project informative,  \\n and see you in the last chapter.  \\n \\n\\n\"}],\"name\":\"3. Guided Project: CNN Text Classification with PyTorch\",\"size\":54357905,\"urn\":\"urn:li:learningContentChapter:3066067\"},{\"duration\":47,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3073047\",\"duration\":47,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Keep learning and connect\",\"fileName\":\"3004335_en_US_04_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"This video celebrates you for completing the course and encourages you to keep learning through more advanced courses on NLP using PyTorch.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1610716,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Zhongyu] We've reached the end of the course.  \\n At this point, you have successfully built  \\n and trained a deep learning model  \\n for solving a tax classification task  \\n using the popular framework, PyTorch,  \\n but this isn't the end.  \\n I encourage you to learn more about deep learning models  \\n and natural language processing related tasks  \\n and continue to expand your knowledge of NLP,  \\n as we were only able to scratch the surface of it.  \\n Don't forget to connect with me on LinkedIn  \\n and check back often, as we are adding more advanced courses  \\n on NLP with PyTorch soon.  \\n I hope you enjoyed learning my course.  \\n Until next time, my name is Pan Zhongyu  \\n and thank you for watching.  \\n \\n\\n\"}],\"name\":\"Conclusion\",\"size\":1610716,\"urn\":\"urn:li:learningContentChapter:3066068\"}],\"size\":88620513,\"duration\":2464,\"zeroBased\":false}]}"