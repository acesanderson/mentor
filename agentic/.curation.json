"{\"title\":\"\",\"courses\":[{\"course_title\":\"Skill Up with Python: Hands-On Data Science and Machine Learning Projects\",\"course_admin_id\":5979027,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":5979027,\"Project ID\":null,\"Course Name\":\"Skill Up with Python: Hands-On Data Science and Machine Learning Projects\",\"Course Name EN\":\"Skill Up with Python: Hands-On Data Science and Machine Learning Projects\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"Python has risen to popularity as one of the most versatile and beginner-friendly programming languages in the world. Its simplicity, readability, and extensive libraries make it a powerful language in a wide variety of different domains, including web development, data analysis, scientific computing, artificial intelligence, and automation. If you\u00e2\u20ac\u2122re looking to kickstart your Python programming journey, this hands-on course was designed for you. Join instructor Shaun Wassell as he offers an interactive approach to building your Python skills through a series of practical projects, all written from scratch. Test out your new know-how along the way in areas such as data analysis, machine learning, web scraping, and more.\",\"Course Short Description\":\"Explore Python's capabilities for data science and machine learning projects in this hands-on, skills development course.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":\"21169000, 9363054\",\"Instructor Name\":\"Pearson Licensor, Shaun Philip Wassell\",\"Instructor Transliterated Name\":\",\",\"Instructor Short Bio\":\"Learning and Education Provider|Senior Software Engineer, Educator at CBT Nuggets\",\"Author Payment Category\":\"LICENSED, NONE\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2024-10-22T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"No\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/skill-up-with-python-hands-on-data-science-and-machine-learning-projects,https://www.linkedin.com/learning/skill-up-with-python-hands-on-data-science-and-machine-learning-projects-licensed\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Programming Languages\",\"Primary Software\":\"Python\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":8481.0,\"Visible Video Count\":21.0,\"Contract Type\":\"LICENSED, NO_CONTRACT\"},\"sections\":[{\"duration\":173,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5991098\",\"duration\":173,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Skill up with Python: Introduction\",\"fileName\":\"5979027_en_US_00_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":173,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9733976,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Hi, I am Shaun Wassell and welcome to Skill Up with Python\\nData Science and Machine Learning Recipes.\\nI'm a lifelong programmer and problem solving addict,\\nand I currently work as a professional software development\\ninstructor at CBT Nuggets.\\nNow, it's no secret that machine learning\\nand data science have become both incredibly popular topics\\nfor people to want to learn, as well as critical skills\\nfor advancing your career.\\nAnd that's really regardless of what field\\nyou happen to be in and it's also no secret\\nthat as far as programming languages go,\\nPython is the language of choice\\nfor just about any data science\\nor machine learning project you may have in mind.\\n\\nSo in this course, I've pulled together a series of projects\\nthat will teach you everything that you'll need to know\\nabout creating and working with basic machine learning\\nand data science projects in Python\\nand in addition to being a fun,\\nhands-on way to learn these topics,\\nbuilding projects will also give you\\nsome great portfolio pieces that you can use\\nin order to impress interviewers\\nonce you start your job search.\\nNow, before we jump into all of these projects,\\nlet's go over the basic structure of the course\\nand take a brief high level look at the projects\\nthat we'll be developing just so that you know\\nwhat you're going to be learning about.\\n\\nSo the first thing that we'll be doing in this course\\nis taking a look at the basics of data manipulation\\nand visualization using Jupyter.\\nNow, in case you haven't heard of\\nor worked with Jupyter yet, you're really in for a treat\\nbecause Jupyter is an incredibly popular tool\\nfor pretty much all kinds of data science\\nand machine learning projects,\\nand it's just a lot of fun to work with.\\nNow, after we finish with the basics of machine learning\\nand data science in Jupyter, we'll dive into the basics\\nof performing basic sentiment analysis in Python.\\n\\nNow this will basically build on the data visualization\\nand manipulation skills from the previous chapter,\\nand it will also introduce you to some useful\\nsentiment analysis libraries and techniques in Python\\nthat you can use for your own projects.\\nNow our third undertaking\\nis going to be working with basic image recognition in Python\\nand this is pretty exciting\\nbecause the ability to turn images into other types of data\\nis an incredibly important one in modern machine learning\\nand data science, so this lesson will teach you\\nhow to do just that using a library called OpenCV\\nAnd in particular what we're going to be doing\\nis seeing how to detect human faces in pictures\\nthat will load into our programs.\\n\\nAnd finally, in order to round out our journey,\\nwe're going to shift our attention a little bit\\nand see the basics of scraping data from the web in Python,\\nwhich obviously has tremendous potential to help you\\ngather data for your future machine learning\\nand data science projects.\\nAnd that's the basic plan for the course.\\nSo thanks again for joining me\\nand I hope you enjoy learning data science\\nand machine learning with Python with all of these projects.\\n\"}],\"name\":\"Introduction\",\"size\":9733976,\"urn\":\"urn:li:learningContentChapter:5991099\"},{\"duration\":2901,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5987197\",\"duration\":50,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Learning objectives\",\"fileName\":\"5979027_en_US_01_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":50,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2801654,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Welcome to Lesson 1,\\nManipulate and Visualize Data in Jupyter.\\nIn this lesson, we start off by helping you get familiar\\nwith the ins and outs of setting up\\nand navigating Jupyter Notebooks,\\nwhich are an incredibly popular tool for doing data science\\nand machine learning with Python.\\nAnd really the goal here is to help you feel comfortable\\nwith creating and executing Jupyter cells,\\nand we're also going to be taking a look at how to work\\nwith things like the Pandas Library\\nand do some very fundamental data manipulation tasks,\\nas well as we're going to jump into\\nthe art of data visualization,\\nwhich will enable us to take datasets\\nand actually take a look at them visually\\nin much more detail.\\n\\nSo, that's our plan of attack for this lesson,\\nlet's get started.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5987196\",\"duration\":748,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Get started with Jupyter Notebooks\",\"fileName\":\"5979027_en_US_01_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":748,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":26673246,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Alright, so in order to get started here,\\nwe're going to need somewhere to write our code.\\nAnd unlike in some of the other courses that we've seen\\nwith Python, we're not actually going\\nto be using Visual Studio Code, at least not at first.\\nWe will use this for a few of our examples, you know,\\nonce we get into other things\\nlike image recognition, et cetera.\\nBut for now,\\nwhat we're going to be doing is actually using something called\\na Jupyter Notebook.\\nNow, you've probably heard of Jupyter Notebooks.\\nThey're a tremendously popular thing\\nin the machine learning data science world right now.\\n\\nAnd essentially what they are is just a really intricate\\nreevaluate print loop, right?\\nA really intricate console that you can write\\nand run Python code inside of.\\nNow saying that really doesn't do Jupyter Notebooks justice\\nbecause frankly, they just have a lot\\nof really amazing features that make machine learning\\nand data science much easier to perform than if you had\\nto write all your Python code and files\\nand continuously restart, run them and so on.\\n\\nSo the first thing that we're going to have\\nto do is get set up, get our local environment\\nthat is set up so that we can actually run\\nand use a Jupyter Notebook.\\nAll right?\\nSo there's really two main options available to you.\\nNow, in fact, there's many more options\\nthat you can actually use,\\nbut these are probably two of the representative options.\\nAnd those are either running Jupyter Notebooks locally\\nor using some sort of cloud-based Jupyter Notebooks service.\\n\\nNow, there's pros and cons to each of those things,\\nbut I'm just going to show you both of those ways\\nand you can really choose\\nwhat you want to do, which one you want to use.\\nSo first of all, what I'm going to do is I'm going to head over\\nto anaconda.cloud.\\nAll right, now what Anaconda is,\\nit's a provider that allows you\\nto either download\\nand run your own Jupyter Notebooks locally\\nor it also has a cloud-based service, which is\\nwhat you see here in my browser that allows you\\nto just get started\\nwith Jupyter Notebooks in a much easier way.\\n\\nSo I'm going to be using the cloud-based service throughout\\nmost of this, but if you want to actually download this\\nand run pretty much the same thing locally,\\nwhich again has benefits, for example,\\nyou can still do this when you don't have\\nreliable internet connections, something like that, usually\\nthat probably won't be a problem for most of you,\\nbut you never know.\\nSo if you go to anaconda.com/download,\\nthat's the current URL anyway, you should be able\\nto download the latest Anaconda distribution\\nand it should automatically just, you should be able\\nto download that for whatever operating system\\nyou you're working on.\\n\\nSo you can download that, I already have that downloaded.\\nIt is a pretty big download, which is another,\\nwhich is a potential downside to downloading this\\nand running it locally.\\nBut once you've done that, what that allows you to do,\\nlet me just pull that up here, is\\naccess what's called the Anaconda Navigator.\\nAnd this basically just allows you to get started with a lot\\nof different data science tools, right?\\nSo for example,\\nright here is just your basic Jupyter Notebook.\\n\\nI'll show you what that is in a minute here.\\nBut what we're actually going to be using\\nis something called Jupyter Lab.\\nAnd Jupyter Lab is,\\nit's basically just a more full featured IDE if you will,\\nwrapped around a Jupyter notebook.\\nYou'll see what that looks like as well.\\nBut anyway, this is what you'll be dealing with locally\\nand if you click on the launch button for any\\nof these tools, once you've downloaded and installed this,\\nand by the way, that does take a few minutes.\\nSo I'd recommend pausing this\\nand just waiting for that to complete, if\\nthat's the route you're going to go.\\n\\nBut you can click on launch for any of these\\nand what that'll actually do is open,\\nit'll start a server running this tool\\nand it will open that server up in your browser, right?\\nSo you'll see something very similar\\nto what I just had up over here.\\nThis is running on the anaconda cloud though.\\nAnd this over here is actually running locally.\\nYou can see local host 8,890 slash lab.\\nAnd again, whichever one you choose to use, it's going\\nto be a pretty similar process.\\n\\nBut Jupyter Lab, again, is just sort of\\na more full featured wrapper around Jupyter Notebooks.\\nIt makes it much easier.\\nGives you this little file tree thing\\nover here on the side and so on.\\nSo I'm actually going to close this here\\nbecause again, I'm going to be using the cloud-based service\\nand I'm going to close this window as well\\nand we'll get to this thing Kaggle a little bit later.\\nSo anyway, once you've got the Jupyter Lab tool up\\nand running, the next thing that you're going to want to do\\nis get started with a notebook.\\n\\nSo a notebook, first of all,\\nAnaconda does give you a few options.\\nIf you're running this locally,\\nthe options may be different,\\nbut just select whatever is there\\nand there really shouldn't be too much of a difference\\nfor the examples that we're going to go through,\\nI'm just going to click on the first option here.\\nAnd these are just different variations, different versions\\nof the notebook that you can run.\\nAnd what you're going to see is that this opens up a sort\\nof console where you can actually write and run Python code.\\nNow in a way, this is just like if you were\\nto type in Python\\nor Python three into a basic terminal, it allows you\\nto just write and run, as I've said, basic Python code.\\n\\nSo you can say something like print hello, for example.\\nAnd if you press shift\\nand enter, what that actually will do is run all of the code\\nthat you wrote inside of there\\nand print out the results down below, right?\\nSo you can see that all that we did here\\nwas write a simple print statement\\nand we see the output down below.\\nNow if you want to write more complex code,\\nyou can do that as well, right?\\nSo if we wanted to say something like four I in range 10,\\nand then say something like, print hello,\\nand here we'll just use a template string here,\\nwe'll say hello,\\nand then we will insert the value of I into there.\\n\\nWell, if you press shift\\nand enter now, you'll see that\\nthat will work as well, right?\\nAnd again,\\npressing enter in most cases just gives you a new line\\nbecause in many cases you will want\\nto have multiple lines in here.\\nSo that's another nice feature of working\\nwith Jupyter Notebooks over\\njust writing basic Python code in the terminal.\\nAlright, so in addition to just writing\\nand running basic Python code, there's a lot\\nof other interesting things\\nthat you can do in Jupyter Notebooks as well.\\n\\nNow, getting to the name Jupyter Notebook,\\nlet's talk a little bit about why it's called a notebook\\ninstead of just a basic Jupyter terminal\\nor something like that.\\nWell, the whole idea of Jupyter Notebooks,\\nand one of the most important use cases for a notebook\\nis that it gives you ways to share your data science\\nand machine learning projects with other people in a nice,\\nvisually-pleasing and readable way, right?\\nSo, well, first of all, you have the fact\\nthat on every piece of Python code you write is saved here,\\nand you can, these are all called cells, by the way,\\nand the Anaconda\\nor Jupyter Lab rather gives you options for how\\nto manage these so you can kind of move them\\naround using these arrows, right?\\nIf you want to reorganize things, you can add cells above\\nor below, you can delete cells obviously.\\n\\nSo if there's a cell you don't want\\naround anymore, you can delete it.\\nAnd you know, really this just,\\nit's a Python terminal with a much greater emphasis\\non being aesthetically pleasing, right?\\nSo it allows you to format things, format your code,\\nformat your graphs, right?\\nWe'll see how to actually do things like data visualizations\\na little bit later in a way\\nthat people are going to be able to read.\\nSo anyway, that's the basic idea\\nof Jupyter notebooks.\\n\\nSo just to show you a few more things that you can do here,\\nlet's just take a quick look\\nat how to do things like display a simple graph.\\nSo what we're going to do is we're actually going\\nto use something called mat plot lib.\\nAnd by the way, that's another thing that Jupyter notebooks\\nand Jupyter lab in this case do as well,\\nis they come prepackaged with a lot of important modules\\nthat will make data science\\nthat are commonly used basically in data science\\nand machine learning, right?\\nSo mat plot lib is one of those.\\n\\nSo here's what this is going to look like.\\nWe're going to start off\\nby saying import matplotlib.pyplot as PLT.\\nAnd this, by the way, is something you're going to want\\nto use a lot in Jupyter Notebooks, right?\\nThe renaming imports as something much shorter\\nbecause well, frankly, when you do that, it just makes it,\\nso that you have to do a lot less typing.\\nAll right?\\nthat looks like in just a minute.\\nAnd then what we're going to do is we're going to say plt.plot.\\n\\nOops, there we go. And then we're just going\\nto specify some basic data.\\nSo what we're going to do here is we're going\\nto display a simple line graph.\\nIf you're not familiar with what a line graph is,\\nyou'll see in just a minute.\\nAnd really all that we have to do here is specify some data\\nthat we want to display on the graph.\\nSo what we're going to do is we're going to come up\\nwith just two simple lists of numbers.\\nOne will be the X coordinates, that's this first thing here.\\nAnd the second one will be a list of the Y coordinates.\\nSo let's do something like\\ntwo, 10, five, and three.\\n\\nOkay?\\nAnd now that we've done that, what we're going\\nto do is we're going to add a label to the Y axis.\\nSo this is another thing\\nthat you can do in Jupyter notebooks, is it allows you\\nto not only plot things on different kinds of graphs,\\nbut it also allows you to manipulate those graphs.\\nSo add titles, add labels, change the colors.\\nThere's lots of stuff that you can do.\\nAnd really, we're only going to be able\\nto scratch the surface here today.\\nSo anyway, let's say, plt.ylabel.\\n\\nAnd basically all that this is doing is it's changing\\nthe number on the Y axis.\\nAnd we'll say something like profit, okay?\\nAnd then all we have to do is say, plt.show.\\nAnd if we hit shift\\nand enter, now what this is going to do\\nis it's going to display an automatically scaled line graph\\nwith all of those values on it, right?\\nSo you, we can see\\nthat the X axis goes from one to four, right?\\nWe have one, two, three, and four.\\nAnd for the Y values, we have two, 10, five,\\nand then three all displayed as values on there, right?\\nSo you could probably imagine that there's a lot\\nof doing this with real-world data would be,\\nwould enable you to really see that data\\nand what that data looks like, how that data behaves,\\net cetera, in a lot more detail.\\n\\nAll right?\\nSo we're going to get into much more detail\\non data visualization a little bit later on in this chapter,\\nbut I just wanted to show you\\nwhat this looks like in Jupyter Notebooks, so\\nthat you can better understand what these things look like.\\nNow, one last thing that I wanted to mention here\\nis that you can always save this notebook\\nand notebooks, by the way, you can see that these\\nare basically just files with an extension IPYN and B.\\nBasically that stands for IPI notebook or I Python notebook.\\n\\nAnd let's just call this something like notebookdemo.ipi\\nand B.\\nAll right, so we'll say rename.\\nAnd over here on the left-hand side, you can see\\nthat we have sort of a file tree in.\\nThis, as we'll see very shortly, allows us\\nto do things like add data files\\nthat we can then actually load into our Jupiter notebooks\\nand manipulate, visualize, perform machine learning on.\\nThere's lots of things that we can do.\\n\\nSo anyway, that's what we're going to take a look at next.\\nNow that we've seen the basics of working\\nwith Juphter notebooks.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5988144\",\"duration\":675,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Load data into Jupyter\",\"fileName\":\"5979027_en_US_01_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":675,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":27464881,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- All right, so now that we've got a Jupyter Notebook\\nup and running, and we've learned a little bit\\nabout what the main purpose and usage\\nof Jupyter Notebooks is, the next thing\\nthat we're going to need to do is obviously load data\\ninto Jupyter in the first place, right?\\nObviously, while it would definitely be possible\\nto type all of the data that we wanted to work within\\nas Python variables, for obvious reasons,\\nthat's not usually going to be how we do things.\\nSo, the next question is, where are you going\\nto get your data from, right?\\nYou know, where you can either obviously generate\\nyour data yourself, if you did some kind of study, right,\\nif you did some kind of research,\\nor if you're working for a company\\nthat regularly collects data about its users,\\nwell, then you already have your data at your fingertips.\\n\\nAll you would need to do is get that\\ninto some kind of file format and load that into Jupyter.\\nHowever, assuming that you don't already have\\nyour own dataset laying around,\\nwhat I've found to be a very good location\\nto find very interesting datasets too, by the way,\\nthat you can work with\\nand use to practice basic data science\\nand machine learning is a website called Kaggle.\\nKind of a strange name, but it's a very good site.\\nSo what we're actually going to do is we're going\\nto find a dataset on Kaggle here,\\nand we're going to then download that dataset\\nand load it into our Jupyter Notebook, all right?\\nThe basic process that we're going to be following here\\nis really going to be the process that you'll follow\\nfor most other datasets as well\\nwith obviously a few slight variations.\\n\\nFor example, if you're working with files\\nthat are in a different format.\\nBut anyway, what we're going to look for first,\\nthis one is a dataset that I found,\\nand it's pretty interesting.\\nWe're going to search for world educational data.\\nNow let's just search for that.\\nAnd here we go.\\nThat's this first result right here.\\nAnd as you can see, there's lots and lots\\nof other datasets as well.\\nThere's pages and pages of datasets\\nthat just match that query, but we're just going\\nto take a look at the first one here, all right?\\nAnd what we're going to do, first of all,\\nnotice that when you click on this dataset,\\nit tells you all about the dataset,\\nwhat the dataset contains, the different fields\\nthat the dataset contains as well, right?\\nSo as you can see, there's quite a few different aspects,\\nquite a few different properties\\nthat each of the rows in our dataset is going to have,\\nas well as in many cases, it'll have a list\\nof potential use cases for this dataset as well.\\n\\nNow, this particular dataset is a dataset\\nrepresenting world educational data.\\nIt contains just different things like,\\nit contains educational data for different countries\\nand regions, things like literacy rate,\\nunemployment rate, dropout rate.\\nJust feel free to take a look at all of these columns here.\\nWe're not going to be doing any kind\\nof in-depth machine learning with this ourselves,\\nbut you could, obviously,\\nthere's quite a few interesting things\\nthat you could learn from this dataset\\nif you know what you're doing.\\n\\nSo, let's start off by downloading this dataset.\\nAnd you should see that that just downloads right here.\\nAnd the next thing that we're going to do\\nis we're going to, that basically is going to be a zip file\\nthat will show up in your downloads.\\nAnd I'm going to basically just drag that over.\\nI have that on my other monitor here.\\nI'm going to drag that over to my file tree in my Jupyter lab.\\nSo, let's just rename this here.\\nThis is called Global_Education 3\\nbecause I've downloaded it a few times\\nas I've been trying to work out the kinks from this course.\\n\\nSo let's rename this to just plain old Global_Education.csv.\\nAnd let's actually just create a new Jupyter Notebook\\nwhere we can actually start to load this data\\nand take a look at it.\\nSo let's just open this up here.\\nWe'll just click the plus button next to the tabs.\\nThat will open up the launcher here.\\nWe'll just open up a new notebook\\nthat'll create another one called Untitled.\\nAnd let's just rename that\\nto something a little bit more descriptive.\\n\\nWe'll call this something like Load-Data.ipynb.\\nAnd well, really all that you need to do in order\\nto load data in Jupyter Notebook is use a module,\\nwhich is included in most cases by default called Pandas.\\nNow pandas, kind of a strange name,\\nbut basically Pandas is a library\\nthat allows you to really work with data\\nin a very fluent way in Python.\\n\\nSo it's much easier than if you were to just have all\\nof the data you're working with be\\nin regular old Python lists and dictionaries.\\nYou'll see what I mean in just a minute here.\\nBut first, let's import Pandas.\\nWe're going to say import pandas as pd.\\nAgain, we're going to rename this pandas module\\nso that it's much shorter to type out from now on.\\nAnd then what we can do is we can use a function\\ncalled pd.read.csv in order to load in data from a csv file.\\n\\nNow, again, if you were working with some other type\\nof data, if you were working with, let's say,\\nan Excel document or a JSON file,\\nthen there are corresponding read Excel\\nand read JSON functions as well,\\nthat in most cases will do a good job of loading your data.\\nSo in our case here, we just need to specify the name\\nof the file we want to load,\\nand that's going to be Global_Education.csv.\\nI believe that's what it's called.\\nLet me just double check that here. Yep.\\nGlobal_Education.csv.\\n\\nAnd well, let's just try and load this thing.\\nIf we hit shift enter,\\noops, it looks like we get an error here.\\nAnd this is something that you might run into a lot\\nwith data that you download from sources like Kaggle.\\nBasically, what happened here is the encoding\\nisn't quite right.\\nSo by default, this read csv function expects this file\\nto be in UTF-8 encoding.\\nIn this case, I believe,\\nand we might have to actually go back\\nand make this change manually on our computer to this file.\\n\\nLet's hope that's not the case.\\nI believe here that if we add,\\nyep, it's encoding equals,\\nand then I believe this one is latin1,\\nso that's just a slightly different encoding\\nthan the regular UTF-8.\\nSo let's try running this again.\\nAnd by the way,\\nyou can rerun cells in Jupyter Notebooks\\njust by basically going into the cell\\nand pressing shift and enter again.\\nSo let's try this again.\\nAnd sure enough, that actually does work.\\nSo down here, what you can actually see is part\\nof that dataset displayed with all of the columns\\nand all of the rows here, right?\\nSo you can see that it's got the names\\nof the countries and regions.\\n\\nIt's got the latitude and longitude\\nof the countries and regions.\\nAnd it's also got all of those other,\\nmany, many other properties here\\nthat you can use for your own purposes.\\nSo anyway, in this case, the dataset, part of it anyway,\\nhas been automatically printed out,\\nbut in many cases you'll want to just be able\\nto view basic datasets.\\nAnd you'll also, obviously,\\nwhen you load a dataset like we did here,\\nwant to assign it to a variable.\\n\\nSo what we're going to do here is we're going to say\\ndf equals, and I'll tell you about that name\\nin just a minute, equals pd.read_csv.\\nAnd you'll notice that if we press shift and enter now,\\nthat doesn't print out the dataset.\\nThat's just because we've taken the output from this thing,\\nwhich is what was getting printed out below,\\nand assigned it to a variable.\\nSo now if we want to view what data is in this variable,\\nwe can just say df.head with parentheses after it,\\nand if we press shift and enter,\\nsure enough, we'll see that basically what that does\\nis it just shows us the first few rows in that dataset\\nthat we loaded, right?\\nSo this is a great way to just get a basic sense\\nof what kind of data you're working with,\\nwhat properties it contains,\\nwhat some of the expected values are, and so on.\\n\\nNow, going back to this df thing.\\nBasically, when we read in data using Pandas,\\nthe main unit or the main structure that Pandas uses\\nto hold data when it's in this kind of format, right,\\nwhen it's kind of a, when it's almost a tabular format,\\nis something called a data frame.\\nNow, a data frame is kind of like a nested Python list,\\nbut it's much more powerful,\\nand you'll see that in just a minute here,\\nor in the next video, that is,\\nwhen we see how we can use data frames\\nto manipulate data in Python.\\n\\nBut before we get there,\\none last thing I want to show you is if you want to see\\njust basic statistics about this data\\nthat you've loaded, right,\\nthings like the averages, the minimum and maximum values\\nfor all of these attributes here,\\nwhat you can do is you can just say, df.describe,\\nand this describe function will basically calculate\\nall of that for you, right?\\nSo you can see that this gives you a number\\nof different pieces of statistical data\\nabout all of the columns here.\\n\\nSo you can see the count.\\nThis is basically how many non-null values are\\nin that column, right?\\nSo we can see that that's 202 for pretty much all of these.\\nI don't believe this dataset has any missing values.\\nAnd you can also see the mean, the standard deviation,\\nthe minimum, right?\\nStandard deviation, by the way, is just sort of the,\\nlet me describe it this way, in case you aren't familiar\\nwith any of these things yet, but the standard deviation\\nis sort of the average amount, so to speak,\\nthat we can expect any given value\\nto be different from the mean.\\n\\nDon't worry too much if that doesn't make sense.\\nYou can read about standard deviation there.\\nThere are plenty of good statistical textbooks out there,\\nand I'm sure you could read about it online as well.\\nAnd then obviously you have the minimum, the maximum,\\nand then you have different percentiles, right,\\nwhich basically just tells you how that data is distributed.\\nSo anyway, just feel free to take a look at that.\\nIt's kind of interesting to look at this\\nfor all of these properties.\\nFor example, you can see that the mean unemployment rate\\nis about 6%.\\n\\nThat's the average unemployment rate across all\\nof the countries and areas that we have data for.\\nAnd you know, you can take a look\\nat other things as well, obviously.\\nSo anyway, those are just some basic things that you can do\\nwith data once you've loaded it.\\nThe next thing that we're going to be taking a look at is\\nhow we can manipulate data\\nand how we can actually take a more in-depth look\\nat the data that we have using this data frame thing\\nthat's provided for us by the Pandas module.\\n\\nSo, that's what we have coming up next.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5990182\",\"duration\":628,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Manipulate data with pandas\",\"fileName\":\"5979027_en_US_01_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":628,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":26803054,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Alright, so now that we know how to load data\\nfrom things like a basic CSV file\\ninto our Jupyter Notebooks,\\nand we also know how to just view some basic characteristics\\nof the data set that we currently have,\\nthe next thing that we're going to take a look at is\\nhow we can actually view this data set in more depth, right?\\nAs well as how we can actually manipulate it.\\nSo, the first thing that you'll want to know is\\nhow to access things like individual columns\\nand rows from this data frame.\\n\\nNow this is where a data frame is a little bit more exciting\\nthan basic things like a Python dictionary or a Python list\\nbecause it provides you with, as I've said before,\\na very fluent way, a very easy, sort of intuitive way\\nof accessing different parts of your data.\\nSo, for example, if we wanted to access an individual column\\nof our global education data here, what we can do,\\nyou're really going to like this syntax here, I think,\\nis we can just say DF,\\nand then inside these square brackets,\\nwe can just put the name of the column that we want to access.\\n\\nSo we're going to say,\\nand let's just make sure we have\\nthe actual column name here.\\nWe'll say longitude, right?\\nI'm just going to copy that and paste it in here.\\nAnd if we hit enter, sure enough what you'll see\\nis that that gives us the entire column there, right?\\nAll of the values for all of our rows\\nfor that longitude characteristics.\\nSo, the next thing that we're going to take a look at is\\nhow do we rename columns when they're not quite the way,\\nwhen they're not quite in the format\\nthat we want them to be in,\\nor when there's just something that's a little bit off\\nwith the column name.\\n\\nNow to show you what I mean here,\\nlet's try and access the latitude column of our data frame.\\nWhat we're going to do is we're going to say DF and then latitude.\\nAnd if we hit shift and enter,\\nwhat you're going to see here is that we get an error.\\nNow ,this is kind of a confusing error at first\\nbecause, as you can see, this latitude string\\nlooks like it matches the name of this column.\\nHowever, it actually doesn't,\\nand that's because, unfortunately,\\nthis dataset, for some reason or other,\\nI'm not exactly sure why this happened,\\nbut this dataset actually has a space,\\nI believe, after latitude.\\n\\nAnd sure enough, if you put a space after the name latitude,\\nyou get all of those latitude values.\\nNow it's things like this that\\nwhere you really want to be able\\nto change the names of columns, right?\\nAnd really get this data into a format\\nthat you're used to working with instead of being stuck\\nwith whatever the person's choices were\\nwhen they put the data set together\\nin the first place, right?\\nSo, in our case here, instead of having to remember\\nthat this latitude column has just a single space\\nafter the name, we can rename that column\\nto something that makes a little bit more sense, right?\\nSuch as latitude without the space after it.\\n\\nNow, another thing that we might want to do as well,\\nand I certainly like to have things this way,\\nis rename all of these column names\\nso that they're just in basic Python snake case, right?\\nSo countries and areas, for example,\\nright now, has spaces in it,\\nand it's also got some weird capitalization, right?\\nSo if you try and say something like countries\\nand areas, like so, you can access all of them just fine,\\nbut it is a little bit strange to have\\nto put spaces in for a column name like that.\\n\\nSo, in my opinion, it would be a lot better\\nif we were able to just say something\\nlike countries and areas,\\nand, you know, be able to access things like that.\\nNow, obviously we can't do that\\nbecause that's not the column name right now,\\nbut as I've already said, data frames allow us\\nto rename the column.\\nSo let's take a look at how we can do that.\\nBasically, well, the first thing that we'll want to do\\nis be able to access the actual columns,\\nwhich you can do by saying df.columns.\\nAnd basically what this does,\\nit prints out this index thing.\\n\\nDon't worry too much about what that is.\\nBasically it's just a sort of wrapper around this list\\nthat contains all of the names of these columns, right?\\nSo we have countries and areas, latitude,\\nyou can see that little space in there, longitude,\\nand then all of these other very long names for the column.\\nSo, if we want to rename our columns,\\nwe could always just say something like df.,\\nand then, now let me just double check here\\nto make sure I have the right method name here.\\n\\nYes, we can say df.rename,\\nand then what we can do is we can basically say\\nthat we want to rename all of our columns\\nand make them, let's say all lowercase, right?\\nSo if we wanted to change all of our columns to lowercase,\\nwe could say df.rename,\\nand then basically we could pass the function\\nthat we want that contains the transformation\\nwe want to apply to all of those columns,\\ncolumn names, that is.\\nIn this case we could say string.lower,\\nand then we can say axis equals columns.\\n\\nAnd that will basically change\\nall of the column names to lowercase.\\nAnd, by the way, we do need to actually assign\\nthe result of this to a new variable.\\nWe could call it something like df clean equals df.rename.\\nAnd if we print out df clean now,\\nwe'll just say dfclean.head\\nto view the first few rows in the dataset\\nand we'll say shift + enter.\\nSure enough, we see that it's converted all\\nof those column names to lowercase.\\nSo, in addition to changing them all to lowercase\\nand, well, in addition to changing them all to lowercase,\\nI guess, that's the only thing we've done so far.\\n\\nWe probably will also want to replace any spaces in there\\nwith underscore characters\\nand remove any additional white space.\\nNow, as a matter of fact,\\nwe can actually do all of this in a much easier way\\nup where we load the data in the first place.\\nSo let's see what this looks like here.\\nWe're going to say df equals pd.readcsv.\\nAnd then under that we're going to say df.columns equals,\\nand we're actually going to assign a new value to columns.\\n\\nYou see, the nice thing about these data frames\\nis that in many cases we can actually use\\ncertain parts of them, as you know,\\njust in the same way that we can use\\nregular Python data structures.\\nSo in our case here,\\nwe can actually just map each column name\\nto some other value\\nand use something like a list comprehension.\\nSo, in our case here, we'll say df.columns equals,\\nand then in our case what we're going to do\\nis we're going to say column\\nand we're going to convert it to lowercase.\\n\\nSo we'll say .lower,\\nand then we're going to strip\\nany extra white space out of it.\\nSo we can say .strip like so,\\nand then we can say .replace.\\nAnd then we're going to replace any\\nextra white space here, right?\\nAny spaces in between words or characters in that name,\\nwe're going to replace that with an underscore character.\\nSo this should basically have the effect of converting\\nall of the column names to basic Python snake case.\\n\\nAnd then, of course, we'll say four column\\nin df.columns, right?\\nSo, basically all that we're doing here\\nis we're just modifying all of the columns in our data frame\\nand reassigning that to the columns property.\\nSo, let's just hit shift and enter here.\\nWe're going to rerun this cell,\\nand if we rerun this df.head thing underneath it,\\nwe'll see that sure enough,\\nall of the column names are now\\nin a much nicer format for us to work with\\nand they no longer will have those extra spaces\\non either side that we have to worry about.\\n\\nNow, what you'll notice here is that even though\\nwe've made this change up here in the top cell,\\nthe rest of the cells haven't necessarily been rerun\\nwith those changes, right?\\nSo in other words, df.describe here you can see\\nthat all of those things are still the old column names.\\nNow you can rerun this just by clicking in\\nand pressing shift and enter,\\nand that will now have the updated column names.\\nBut then if you get down to things like this with the code,\\nthe previous way that it was,\\nif we run this now we'll get a key error\\nbecause that's no longer the column name.\\n\\nSo, let's just change that here.\\nWe'll say df longitude.\\nAnd sure enough we get those values,\\nwe'll remove df.columns, we'll change that too,\\nand actually let's remove these other ones here.\\nWe'll just remove those cells altogether.\\nAnd we can now access things like df latitude.\\nThere we go, latitude,\\nwithout having to worry about\\nhaving an extra space on either side of that.\\nSo we can press enter and, or shift and enter that is,\\nand that gives us all of the latitude values\\nfor our countries.\\n\\nSo, anyway, a few more things\\nthat you might want to know how to do here is,\\nlet me just find some things here.\\nYou might want to do things like access rows.\\nObviously, we haven't actually taken a look at that yet,\\nso let's just take a look at that.\\nThe nice thing about data frames is that you can actually,\\njust like how you're able to access columns,\\nwhich is the default here, right?\\nWe can access rows and the syntax here\\nis a little bit strange, but just bear with me here.\\nWe can access rows by saying I-L-O-C\\nand then basically we can just use indices\\nin order to access those rows.\\n\\nSo this would be the first row of our data frame, all right?\\nWe can see that that's Afghanistan,\\nwe can see all of the corresponding values for that country\\nand we can, you know, take a look at other ones, as well.\\nSo if we take a look at six,\\nthat gives us an another country and, you know,\\nyou can basically just use that\\nin the same way that you would use a Python list\\nand it's not a Python list,\\nbut that you can access it in the same way\\nas what you would do with that.\\n\\nSo, anyway, that is how we access basic\\npieces of data in our data frames in a Jupyter Notebook.\\nThe next thing that we're going to take a look at here is\\nhow we can actually start to visualize this data\\nso that we can see things about the data.\\nSo, that's what we're going to take a look at next.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5984259\",\"duration\":800,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualize data\",\"fileName\":\"5979027_en_US_01_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":800,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":35235951,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- All right, so at this point, we know how to load our data\\nand we know how to do things, like, manipulate it, right?\\nWe saw how to change the column names,\\nwe saw how to view individual rows and columns, and so on.\\nBut obviously at some point, we're going to need some way\\nto actually visualize our data, right?\\nData visualization, whether that's displaying it on a graph,\\nwhether it's, you know, displaying it in some other kind\\nof visual way, really helps us to get a sense\\nof how the data behaves and gives us an idea\\nof what patterns there might be in the data.\\n\\nSo, let's take a look here at a few different built-in\\nvisualizations that Jupyter Notebooks\\nand the Pandas library in particular provide us with.\\nAnd the first thing that we're going to take a look at here\\nis how we can just take a look\\nat an individual column, right?\\nThe distribution that is of an individual column.\\nNow, the way that we can do this,\\nthere's actually a few different\\nvisualizations we can use here.\\nThe first is called a histogram,\\nand basically, the histogram shows us the basic distribution\\nof values in a column.\\n\\nSo what we can do here, let's say that we want\\nto take a look at the basic distribution\\nof the unemployment rate\\nin our global education data set here.\\nWhat we can do here is we can actually say df\\nand then we're going to select the unemployment_rate\\ncolumn like so, right?\\nWe already saw how to do that.\\nThat'll give us the unemployment rate.\\nAnd then what we can actually do,\\nthis is a really, really nice, easy and fluent,\\nas I've said several times before, way to make this happen,\\nwe can just add .hist on the end,\\nand that stands for histogram.\\n\\nAnd if we press Shift and Enter here,\\nwhat that'll actually do is, as I said,\\ndisplay the overall distribution of those values.\\nAll right, now the way to read this histogram here is,\\ndown here on the x-axis, you have the actual values\\nthat that unemployment column contains.\\nAnd on the y-axis here, you have how many records,\\nhow many rows in that dataset have the unemployment rate\\nbetween those values, right?\\nSo in other words, you can see\\nthat there's quite a few countries here, quite a few rows\\nwhere the unemployment rate is pretty low.\\n\\nAnd then you have a few rows up here\\nthat have a very high unemployment rate, right?\\nSo that's more or less what a histogram tells you.\\nAnd again, it can just be a really nice thing to be able\\nto take a look at the overall distribution, right?\\nIf you're wondering what the general lay of the land is\\nwith regards to unemployment rate in the countries\\nin your dataset, that gives you\\njust a really nice way to know that.\\nWhereas, you know, just printing out the values\\nlike we did before,\\nwe'll just say df unemployment_rate, like that, right?\\nIf we just take a look at the values,\\nit doesn't really tell us too much.\\n\\nAnd even if we take a look at the describe function, right?\\nWe did describe up above here,\\neven though we can see things like the mean,\\nthe standard deviation, the minimum, the percentiles,\\net cetera, for the unemployment rate,\\nunless you're really an experienced statistician,\\nthat doesn't really tell you very much, right?\\nIt's not very user-friendly to just have\\nto look at numbers like that.\\nAnd that's where these different visualizations\\nreally start to come in handy.\\nSo let's take a look at another visualization we could use\\nin order to, again, find what the lay of the land is\\nfor different values in our data.\\n\\nLet's take a look at the unemployment_rate again,\\nbut in a slightly different format.\\nWhat we're going to do is, we're going to use a box plot,\\nand you'll see what a box plot\\nlooks like in just a minute here.\\nWhat we're going to do is we're going to say df and oops,\\nactually, we don't do it on the column quite like that.\\nWe actually say df.boxplot, and then we pass a keyword\\narg called column=unemployment_rate.\\nAnd what this is going to do\\nif you press Shift and Enter again is,\\nit's going to display this little, well,\\nit's called a box plot, all right?\\nNow, just to go over the basics of how\\nto read this box plot here,\\nthis green line in the middle, that is called the median.\\n\\nBasically, that represents the midpoint of the data.\\nAnd then you have things like the 75th percentile,\\nas well as the 25th percentile.\\nAnd then the minimum and maximum as well\\nas these little circles up here above it\\nare referred to as outliers.\\nThose are generally data points\\nthat are well beyond the expected bounds.\\nSo anyway, that's the basic.\\nThose are just a few different ways\\nthat you can visualize the data in your data set.\\n\\nLet's take a look at one other one here,\\nand that is something called a correlation matrix\\nor a heat map in our case is what we're going\\nto be taking a look at.\\nAll right, now, in case you're not familiar\\nwith the idea of a correlation matrix\\nor a heat map, basically, what this does is it shows us\\nhow correlated any two columns,\\nthe values in any two columns tend\\nto be with each other, right?\\nIn other words, does the value in one column tend\\nto go in the same direction\\nas the value in another column, right?\\nSo in other words, as the unemployment_rate goes up and down\\ndoes, let's say, the lower_secondary_end_proficiency_reading\\nvalue tend to go up and down as well, right?\\nThis is where we start to get some very interesting\\nobservations about our dataset,\\nif of course we find anything interesting.\\n\\nSo, here's how this works,\\nand you're, I think, really going\\nto like this visualization here.\\nIt's definitely one of the more interesting ones.\\nThe first thing that we need to do\\nis import another module here called Seaborn.\\nSeaborn contains a lot\\nof different graphing utilities, stuff like that.\\nSo we're going to say import seaborn as,\\nand we'll just call it sns.\\nThat's usually what they abbreviate it as.\\nAnd then we're going to create something\\ncalled a correlation matrix\\nfor all of the values in our data set, right?\\nSo basically what this is going to do is,\\nit's going to sort of take all of our columns\\nand check them against all of the other columns\\nto find out how correlated those values tend to be, right?\\nAre the values in these columns sort of connected\\nin some way or another?\\nSo, here's what this is going to look like.\\n\\nWe're going to start off by saying correlation_matrix = df,\\nand then we're going to say .corr, all right?\\nSo as you can see, data frames just have\\nthis basic corr method that will automatically generate\\na correlation matrix for us.\\nSo basically just let's print out this correlation matrix\\nto see what happens or to see what it looks like.\\nAnd if we hit Shift and Enter, we're actually going\\nto see an error here, saying something like,\\n\\\"Could not convert string to float Afghanistan.\\\"\\nNow, that might seem like kind of a strange error,\\nbut basically, because correlation matrices\\ndepend on the values that they're comparing\\nbeing numerical, right?\\nIn general, it's not liking this country's and area's value,\\nbecause that's almost always a string, right?\\nIn other words, trying to see whether\\nthe name of a country is correlated\\nwith something like its unemployment rate.\\n\\nThat's just ridiculous.\\nAnd in order to actually generate a correlation matrix,\\nwe need to remove values like that.\\nWe need to remove columns that aren't numerical.\\nNow, the way that we do that is,\\nwe're going to say something like df_numerical.\\nAnd this is another thing that data frames allow you to do,\\nis they allow you to sort of filter out the columns\\nbased on some sort of criteria, right?\\nSo in our case here,\\nwe can actually say df.select_dtypes right?\\nSelect data types, that is.\\n\\nAnd then we're going to say include=['number'], all right?\\nSo basically what this is going to do is it's only going\\nto include numerical data in the data frame\\nthat it's returning,\\nwhich we're assigning to another variable here.\\nSo now, we can just say, df_numerical.corr(),\\nand this should actually give us a much more, well,\\nit should actually give us a result\\nthat's not an error.\\nAnd what you can see here is that,\\nbasically, all of these numbers that are in here\\nare somewhere between positive 1 and -1.\\n\\nAnd in many cases here, you can actually see that all\\nof the cells on the diagonal are exactly 1, right?\\nYou have 1.0, 1.0, 1.0, et cetera.\\nAnd that's because those are the intersections\\nof the same column with itself, right?\\nSo, really in general, you just to ignore these,\\n(chuckles) because we don't really care,\\nor we already know that latitude is correlated with latitude\\nbecause it's the same column, right?\\nSo, in most cases along the diagonal, you'll have,\\nlet's call it perfect correlation\\nwhere one value is directly influenced,\\nor not directly influenced.\\n\\nThat's a little bit misleading.\\nWhere one value follows directly another value.\\nSo anyway, we'll usually just want to ignore those,\\nbecause, again, we already know that latitude\\nis correlated with latitude,\\nlongitude is correlated with longitude, and so on.\\nBut then, you know, if you take a look at some\\nof the other things here, you can see that in many cases,\\ntwo columns aren't really very correlated at all, right?\\nSo the gross_primary_education_enrollment,\\nyou can see that's pretty close to 0\\nwhen we're comparing it with latitude and longitude, right?\\nAnd in general, any kind of correlation you see\\nbetween latitude and longitude\\nand some of these other educational statistics,\\nthey're not really going to be very important.\\n\\nAnd more often than not,\\nthey're going to be pretty close to 0, right?\\nYou know, I mean, you might see things\\nthat look like they're correlated, right?\\nLike this one here is -0.5, right?\\nAnd considering that -1 is the maximum value there,\\nor the -1 is the minimum value there that is,\\nit might seem like,\\nbetween birth rate and latitude?\\\"\\nWell, not necessarily, right?\\nYou got to use some common sense\\nwith things like that as well.\\nAnd in many cases, you'll just find\\nthat correlations like these are just something\\nthat happens purely by chance\\nor as the result of something else in your data set.\\n\\nAll right, so for example, if we take a look\\nat the youth literacy rate male\\nand youth literacy rate female,\\nwe'll see that if we scroll over\\nto the corresponding columns for those,\\nhere they are right here,\\nthose are pretty closely related.\\nThey're pretty closely correlated with each other,\\nindicating that there's a very strong correlation\\nbetween the male and female youth literacy rate, right?\\nSo, there's some very interesting things\\nthat you can learn about your data set\\njust by taking a look at the correlation.\\n\\nAnd, again, in case this format is making your eyes hurt,\\nthere is a much more, generally, much more user-friendly\\nand human-readable way of viewing this data,\\nand that is by using something called a heat map.\\nNow, a heat map is basically just a visual version\\nof this correlation matrix here,\\nwhere it translates each of these numbers into a color.\\nSo here's what this is going to look like.\\nWe're going to say,\\nand this is why we imported this Seaborn thing\\nup at the top,\\ninstead of printing out correlation matrix,\\nwe're going to say sns.heatmap.\\n\\nAnd oops, I spelled that wrong.\\nThere we go.\\nHeat map, and then we're going\\nto pass the correlation_matrix to that function.\\nAnd if we hit Shift and Enter,\\nwe're going to see that this is basically\\nthe same thing that we were looking at,\\nbut instead of numbers,\\nwe have all of these colors indicating the correlation.\\nSo you see that a perfect correlation up here\\nat the top, right?\\n1.0 goes all the way along the diagonal.\\nThat's because that's the correlation\\nof each column with itself, all right?\\nSo we can see in here,\\nthat there are some other correlations, right?\\nSuch as completion_rate_primary_male,\\ncompletion_rate_lower_secondary_male.\\n\\nYou can see that those are correlated\\nwith some of these columns down here.\\nSo anyway, this correlation matrix,\\nor this heat map, rather,\\nis just a generally considered to be a much easier way\\nto view and try and find correlations\\nbetween things in your dataset.\\nSo anyway, those are just some basic visualizations\\nthat you can use in Jupyter Notebooks.\\nThere are obviously lots of other ones,\\nincluding that line graph that we saw earlier.\\nSo, you know, feel free to just read in much more detail\\nabout different Jupyter visualizations that you can do\\nand see what you can come up with.\\n\\nSo anyway, this has just been a brief introduction\\nto working with Jupyter Notebooks.\\nAll right, so this has just been a brief introduction\\nto working with Jupyter Notebooks.\\nHopefully, this has helped you to understand not only\\nwhat Jupyter Notebooks are and how they're used,\\nbut how powerful they really are.\\n\"}],\"name\":\"1. Manipulate and Visualize Data in Jupyter\",\"size\":118978786,\"urn\":\"urn:li:learningContentChapter:5987198\"},{\"duration\":2431,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5989159\",\"duration\":53,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Learning objectives\",\"fileName\":\"5979027_en_US_02_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":53,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2802728,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Welcome to Lesson Two,\\n\\\"Perform Sentiment Analysis.\\\"\\nIn this lesson, we're going to explore sentiment analysis,\\nwhich is all about understanding emotions in text.\\nAnd this can be useful in a number of ways,\\nincluding for projects like automatically analyzing tweets.\\nWe'll talk about that in much more detail shortly.\\nNow, we kick things off here\\nby looking at some of the useful tools in Python\\nthat are designed for this purpose.\\nAnd in particular, we're going to dive into a package\\ncalled NLTK,\\nwhich is a widely used library\\nfor processing language data\\nand doing tasks like sentiment analysis.\\n\\nAnd with all of these tools in our toolkit,\\nwe'll then see how to incorporate basic sentiment analysis\\ninto a real app,\\nas well as just some different tasks\\nto put our skills to the test.\\nSo that's our plan of attack for this lesson.\\nLet's get started.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5984258\",\"duration\":210,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Learn about sentiment analysis tools in Python\",\"fileName\":\"5979027_en_US_02_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":210,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11510249,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- All right.\\nof working with a Jupyter Notebook,\\nand we saw that it allowed us to load in\\nsome data and do some pretty cool\\nmanipulation, visualization, et cetera\\nwith that data.\\nSo we're moving on now to Sentiment Analysis,\\nwhich is a really in-depth topic.\\nAnd it's sort of a subset of something called\\nNatural Language Processing,\\nNLP in the Data Science,\\nand Machine Learning world.\\n\\nSo the basic idea of Sentiment Analysis is this,\\nwhen you read something on the internet, right,\\nor when you read, let's say a sentence\\nor a review on a website, generally as humans,\\nwe're pretty good at\\ntelling what the intention is\\nor what the general feeling is\\nbehind that text.\\nAnd there are obviously exceptions\\nto this, right?\\nI'm sure that we've all had that experience\\nwhere we send an email or a text\\nor something like that that gets misinterpreted.\\n\\nBut in general, we're pretty good\\nat telling if we see something saying,\\n\\\"This product is not very good\\\", right.\\nWe know that that's a fairly negative sentiment.\\nWhereas if we read something saying,\\n\\\"I really, really love this product\\\",\\nwe know that that's a very positive sentiment.\\nSo the idea of Sentiment Analysis\\nis using basic data science,\\nand machine learning concepts\\nin order to discern,\\ngiven a piece of natural language, right.\\nLike a sentence that a person has typed,\\nwhat the overall sentiment is,\\npositive or negative,\\nor in many cases neutral of that text.\\n\\nAnd this has obviously\\na lot of different applications.\\nOne, the example that we're going to be\\ntaking a look at shortly is,\\nwe're going to be seeing how we can use\\nSentiment Analysis to analyze things\\nthat people are saying about different companies.\\nAnd you could maybe incorporate that\\nin some way into an automated\\nstock trading program.\\nWe're not going to do the automated\\nstock trading part,\\nbut we will at least be taking a look at\\nhow we can perform sentiment analysis\\non those types of things, such as tweets.\\n\\nAnd anyway, that's the basic idea of,\\nthat's the basic idea of sentiment analysis.\\nSo really there are lots of different tools\\nthat you can use.\\nSome of them are not very popular.\\nSome of them are very popular.\\nAnd in this chapter,\\nwe're going to be taking a look at\\na tool called NLTK,\\nthat is Natural Language Toolkit.\\nAnd that is automatically included\\nas part of your Jupyter Lab here.\\nSo what you should be able to do is just,\\nlet's create a different...\\n\\nLet's create a new notebook here.\\nWe'll just say New Notebook.\\nClick on that button there.\\nAnd you should be able to say,\\nhere, let me just pull up my notes here\\nso that I make sure I'm doing this all correctly.\\nYou should just be able to say something like,\\nimport NLTK.\\nAnd if you hit shift and enter,\\nyou should see, well,\\nyou shouldn't see an error, right.\\nIf you see an error,\\nyou just need to install that\\nby saying, PIP install NLTK,\\njust like you would install it into a regular\\nPython project.\\nSo anyway, that's the basic idea\\nof Sentiment Analysis.\\n\\nThe next thing that we're going to be\\ntaking a look at is,\\nhow we can get this NLTK module up and running,\\nand actually apply it to some simple text\\nthat we'll enter into it.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5986170\",\"duration\":550,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Learn the basics of NLTK\",\"fileName\":\"5979027_en_US_02_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":550,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":20336977,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- All right, so now that we're familiar\\nwith the basic idea of sentiment analysis,\\nlet's see how we can get things set up\\nand actually start to perform some sentiment analysis\\nin our Jupyter Notebook.\\nSo we already saw how to import the NLTK module,\\nand that's pretty easy, right?\\nImporting, we just say import NLTK,\\nbut what we're going to want to do next\\nis set up something called a sentiment intensity analyzer.\\nThat's just the class name that they used for this thing\\nthat we're going to create.\\n\\nAnd as you might expect by the name,\\nit analyzes the intensity of a sentiment,\\neither positive or negative,\\naround a given piece of text.\\nSo what we're going to want to do,\\nlet's just add this to the same cell as our import NLTK\\nsince this is all related code,\\nis we're going to say \\\"from nltk.sentiment import\\\",\\nand then we're going to import\\nthat \\\"SentimentIntensityAnalyzer\\\".\\nAnd then after that, we're going to say \\\"nltk.download\\\",\\nand we're going to download a few things based on strings.\\n\\nNow, don't worry too much\\nabout what we're doing here exactly, I'll explain it later.\\nThe first thing that we're going to download\\nis something called a vader lexicon, okay?\\nThen we'll say \\\"nltk.download\\\",\\nand we're going to say \\\"subjectivity\\\",\\nand then we're going to say \\\"nltk.download\\\",\\nand we're going to download something called \\\"movie_reviews\\\".\\nNow, these are just a few of the things\\nthat you can download using the NLTK module,\\nand basically what they contain are different strategies\\nfor figuring out the sentiment\\nbehind a given piece of text, right?\\nAs you'd probably expect,\\nthere are lots of different strategies,\\nlots of different ways that you can quantify the emotion\\nin a piece of text.\\n\\nAnd these are just a few of the most popular ones here.\\nSo the Vader Lexicon and Subjectivity,\\nthose are kind of the,\\nI don't know if I'd call 'em an industry standard,\\nbut those are very popular ways of figuring that out.\\nAnd movie reviews, this is sort of an extra thing\\nthat is based on a data set\\ncontaining a bunch of movie reviews from the site IMDB\\nas well as actual human-generated analysis,\\nor human-generated numbers,\\nfor the emotion behind that review, right?\\nSo if the movie review says, \\\"This movie is horrible\\\",\\nthat's going to be a very negative review.\\n\\nIf the review is something like,\\n\\\"This is the best movie I've ever seen,\\\"\\nthat's going to be a very positive review.\\nSo anyway, these things that we're downloading\\nare just going to be used whenever we try and analyze\\na piece of text using this sentiment intensity analyzer\\nand by calling nltk.download, we're basically\\njust making those things available behind the scenes\\nfor our sentiment intensity analyzer, right?\\nIt's sort of equivalent to installing modules\\nusing PIP, right?\\nBy saying, \\\"pip_install\\\", blah, blah blah,\\nthat's making that library available behind the scenes.\\n\\nThat's sort of what's going on here\\nwhen we say \\\"nltk.download\\\".\\nAll right, so anyway, after this, what we're going to do\\nis we're going to say, \\\"sentiment intensity analyzer\\\",\\nwe're just going to abbreviate that SIA,\\nwhich will make things much easier\\nwhenever we need to use this thing later on.\\nAnd then we're going to say \\\"=\\\",\\nand we're just going to create that sentiment.\\nAnd oops, I misspelled intensity. It needs another N there.\\nIntensity Analyzer.\\nAnd now that we've created the Sentiment Intensity Analyzer,\\nwe're going to be able to analyze any piece of text\\nby saying \\\"sia.polarity_scores\\\",\\nand then we're going to pass some kind of string to it.\\n\\nI'll show you that in a minute, but first,\\nlet's make sure this is all working\\nby pressing shift and enter.\\nAnd basically what you'll see down here,\\nthis actually isn't an error,\\ndespite the pink background color there,\\nit's just telling you\\nthat it's downloading all of these things.\\nAnd in my case, it says those packages are up to date,\\nthat's just because I've already run this code\\nin this project before when I was figuring out the course.\\nSo anyway, now that we have all of those things, right,\\neverything is up to date in my case,\\nyou might have to wait for it to finish downloading,\\nwe should be able to do what I said before\\nand say, \\\"sia.polarity_scores\\\",\\nand let's try just passing into something very neutral,\\nlike, \\\"I\\\", let's see, \\\"I am a human\\\",\\nsomething like that, okay?\\nIf we hit shift and enter here, what we're going to see\\nis this little Python dictionary that gets printed out.\\n\\nAnd you can see that the properties in here\\nare negative, neutral, and positive,\\nall abbreviated, obviously,\\nand then you have this thing called compound.\\nNow, the way that each of these work is,\\nthese are sort of the different components of the feeling\\nbehind whatever piece of text we're analyzing.\\nNow, in our case here, \\\"I am a human\\\",\\nthat's not necessarily either a positive or a negative,\\nthere's not really a positive or negative emotion in there\\naccording to these packages that we've downloaded\\nup here anyway, right?\\nSo what you can see there\\nis that the neutral score is 1, right?\\nSo these are all going to be between 0 and 1.\\n\\nAnd then what we're going to do,\\nor what the sentiment intensity analyzer's going to do,\\nis it's going to combine these three scores,\\nthe negative, neutral, and positive components\\nof that feeling, and figure out a compound score,\\nwhich is going to be a score between -1 and 1,\\n- 1 being very negative,\\nthe positive 1 being a very positive emotion.\\nSo let's try something a little bit different.\\nWe'll try \\\"sia.polarity_scores\\\",\\nand we'll try calling this on \\\"horrible\\\", all right?\\nAll right, and if we press shift and enter here,\\nsure enough, we'll see that negative there is 1.0\\nand the compound score is negative 0.6, right?\\nSo that's a fairly negative sentiment.\\n\\nNow, why exactly is this 0.6229 and not -1?\\nWell, that just has to do with the algorithms\\nthat go on behind the scenes\\nto figure out a sentiment behind the scenes.\\nSo let's try this again.\\nWe're going to say, \\\"sia.polarity_scores\\\",\\nand let's pass a more positive string to this,\\nlet's use the example\\nof creating some sort of stock trading program.\\nWe'll say something like,\\n\\\"This company has done incredibly\\nwell over the past year\\\", okay?\\nSo we're going to press shift and enter here.\\n\\nAnd what you can see is that the compound score is 0.33.\\nSo it is positive, and you know,\\nit might not be as positive as what you would expect, right?\\nThis is actually a very positive sentiment, I would say,\\nyou know, the idea of a company doing incredibly well,\\nbut it's only 0.3.\\nAnd that just, again,\\nthat just has to do with the algorithms\\nthat we're using behind the scenes,\\nso depending on your application, right,\\ndepending on what exactly you're trying to do,\\nyou might want to use different packages up here\\nin order to make sure you're getting\\nthe most accurate result possible.\\n\\nSo let's try doing the same kind of thing with-\\nLet's do another positive one.\\nWe'll say something like \\\"polarity_scores\\\",\\nand then we'll say something like,\\n\\\"This movie is incredible\\\", all right?\\nAnd if we press enter, that one is even stranger,\\nbecause that one has a neutral sentiment, right?\\nSo apparently the word incredible doesn't register\\naccording to our packages here\\nand this just goes to show how difficult a task\\na sentiment analysis is,\\nand just to give you an idea of that,\\nlet's think about the two sentences, the first one,\\n\\\"The economy has never been so good,\\\" all right?\\nAnd let's compare that to,\\n\\\"The economy has never been good.\\\"\\nWell, what you can see\\nis that even though these two sentences\\nonly differ by one word,\\nand that word is kind of an ambiguous one at that,\\nthe word \\\"so\\\",\\nthey express completely different sentiments, right?\\n\\\"The economy has never been so good\\\" is like,\\nyou know, everything's going great,\\n\\\"The economy has never been good\\\" is like,\\nwell, things have always been horrible.\\n\\nSo you can kind of hopefully\\nstart to see the difficulty here.\\nAnd again, you're really just going to need\\nto figure out what works for you and your project\\nthrough trial and error.\\nSo now that we've seen just the basics\\nof creating a sentiment intensity analyzer\\nand using it on some text that we've typed in,\\nthe next thing that we're going to take a look at\\nis how we can actually start to apply this\\nin more of a real world context.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5990181\",\"duration\":719,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Incorporate sentiment analysis into an application\",\"fileName\":\"5979027_en_US_02_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":719,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":29361740,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Great, so at this point we've seen\\nhow to just create a basic sentiment intensity analyzer\\nusing the NLTK library.\\nAnd by the way, this is just one of many things you can do\\nwith natural language processing.\\nNatural language processing is not at all limited\\nto sentiment analysis.\\nThere's a lot of other things you can do with it,\\nsuch as, you know, doing things like getting information out\\nof pieces of text, things like that.\\nBut obviously that's well beyond the scope of this course.\\nSo what we're going to do next is we're actually going\\nto take a look at how\\nto incorporate this sentiment intensity analyzer thing\\ninto what you might think of\\nas a more real world application.\\n\\nNow, there might be a lot of things\\nthat you can think of here.\\nThe stock trading program might be one of them,\\nand we'll see how we can actually apply something like this\\nto real world data shortly.\\nBut first, let's actually just imagine a Python program\\noutside of a Jupyter Notebook, right?\\nSo let's just say\\nthat you're developing a simple Python application\\nin Visual Studio Code.\\nI'm just going to use Visual Studio Code as my IDE here\\nas I have in some\\nof the other Python courses you may have seen.\\n\\nAnd we'll start off\\nby installing the NLTK package into here.\\nSo first of all, obviously we'll, actually, here,\\nI need to say conda deactivate.\\nThat's just from when I was running Anaconda\\nin the background.\\nSo let's actually create a new virtual environment\\ninside this folder here.\\nI've just created a new folder called ml-python.\\nIt's completely empty.\\nWe'll say python3 -m venv venv.\\nAll right?\\nAnd now we need to activate our environment\\nso that we can actually install some packages into it.\\n\\nSo we'll just say source venv/bin/activate.\\nAnd you should see this little venv thing appear,\\nif you're using Visual Studio Code, that is.\\nAnd now we can just install the NLTK package,\\nbecause unlike in Jupyter Notebook,\\nthis isn't included by default\\nwhen you're just creating a basic Python project.\\nSo we'll say pip install nltk,\\nand that should install that package for us.\\nSo just give it a second.\\n\\nAnd there we go.\\nSo now let's just create a simple program.\\nWhat we're going to do is we're just going\\nto create a new file.\\nAnd let's say that we're creating something\\nlike a customer service program, right?\\nThat's a potential place\\nwhere you might want to use\\nthis sentiment intensity analyzer thing,\\nbecause what this might allow you to do,\\nif you have a limited number\\nof customer service agents on staff,\\nyou might want to have a program\\nthat helps out the most angry customer first, right?\\nSo if a customer, you know, sends you a message\\nthrough your website saying, \\\"This is horrible,\\nblah, blah, blah, blah, blah,\\\"\\nyou might want to handle that one first, right?\\nOr I guess you could always handle that last as well,\\ndepending of course on what your objectives are\\nwith your customer service team.\\n\\nBut assuming of course that you want to handle those kinds\\nof intense complaints first\\nand leave more of the neutral messages\\nfor a little bit later,\\nwhat you could do is,\\nyou could actually use\\nthis sentiment intensity analyzer thing,\\nwhich of course you'd want to perform a lot of tuning on\\nto make sure you're actually getting the appropriate scores.\\nYou'd want to use the sentiment intensity analyzer\\nin order to score all of the incoming messages\\nand sort of rate them in order of their intensity.\\nSo here's what this is going to look like.\\n\\nWe're just going to create a new program,\\nwe'll call it something like customer-service,py,\\nand then inside here we're going to start off\\nby using the same code\\nfor our NLTK sentiment intensity analyzer thing\\nthat we were using before.\\nSo here's what that's going to look like.\\nLet me just open my notes up here again.\\nSo we're going to say import nltk,\\nand then we'll say from nltk.sentiment,\\nwe're going to say import SentimentIntensityAnalyzer.\\n\\nAnd then what we'll do is,\\nwe'll download those same things.\\nWe'll say nltk.download vader_lexicon.\\nAll right? And then we'll do some of those other ones.\\nWe'll do the subjectivity\\nand then we'll do the movie_reviews as well.\\nOkay?\\nAnd now let's imagine\\nthat we have some incoming messages.\\nAnd in order to simulate this, we'll just have a for loop\\nthat gets input from the user, right?\\nSo we'll say something like for,\\nwell, let's just do while True, okay?\\nSo this will just continuously run.\\n\\nAnd what we'll do is we'll say something like, next_message,\\nthis will be the next message\\nthat you receive from a customer.\\nAnd we'll just in order to test this out,\\nwe'll just allow ourselves to enter some input\\nthrough the terminal.\\nSo we'll say input, we'll just say message like so.\\nAll right?\\nAnd then what we'll do is,\\nwe'll use this SentimentIntensityAnalyzer,\\nwhich we actually still have to create.\\nSo we'll just say sia.\\nAnd since this is an actual more static Python program,\\ninstead of just something we're running\\nin a Jupyter Notebook,\\nyou might want to call this something a little bit\\nmore descriptive, like analyzer,\\nwe'll say analyzer = SentimentIntensityAnalyzer.\\n\\nAnd then we'll just say something like scores =\\nand then call analyzer.score,\\nor polarity_scores, that's what it was.\\nAnd we'll call that on the next message\\nso that we can actually figure out what it is.\\nAll right, so in our case here,\\nwhat we're going to care about\\non this scores thing is, not all of it,\\nbut more the compound score.\\nAnd actually you could take a look\\nat the negative as well, right?\\nSo if you wanted horrible,\\nor in our case here, let's see, incredible.\\n\\nNah, I guess we'll look at the compound score, right?\\nAnd, oops, I didn't mean to do that.\\nLet's go back to our IDE.\\nWe're going to look at the compound score\\nby saying something like, well,\\nwe'll just call it compound = scores('compound'), like so.\\nAnd basically what we'll do is we will just sort\\nof sort these in order of their compound scores.\\nWell, for now, I guess we'll just print something out.\\nWe'll say something like,\\nif compound > 0,\\nwell, in that case, that would be a positive comment.\\n\\nWe'll say something like print('Positive comment!')\\njust so that we can see\\nwhat this is going to do.\\nOtherwise we'll say something like Negative Comment!.\\nAnd actually here, let's change this\\nto else if compound,\\nand you can set some kind of threshold if you want, right?\\nYou could say if it's less than negative 0.1\\nor greater than 0.1,\\nin that case you would label it as positive or negative,\\notherwise you might just want to call it a neutral comment.\\n\\nOkay?\\nAll right, so let's test this thing out.\\nAnd then what you're going to need to do is actually type\\nout a few lines of code.\\nAnd this code is a little bit ugly.\\nSo what I'm actually going to do is create a new file\\nthat I can just copy and paste it from\\nto avoid accidentally running code, et cetera.\\nSo what we're going to do is we're going to import NLTK here\\nas well as something called SSL.\\nAnd this SSL thing,\\ndon't worry too much about what I'm doing here,\\njust know that this avoids an error\\nthat for some reason I always get on my computer\\nwhen I'm doing this kind of thing, working with NLTK.\\n\\nAnd then we're going to just have a try-accept structure.\\nAnd inside here, I'm just going to say,\\nand I'm going to copy and paste this actually\\nto make sure I type it correctly,\\n_create_unverified_https_context =\\nssl._create_unverified_context.\\nDon't worry what this is doing,\\njust make sure you type it all correctly.\\nAnd then we're going to say except AttributeError.\\nAnd then we're just going to say pass inside of here.\\nAnd then we're going to say else.\\n\\nAnd then we're going\\nto say ssl._create_default_https_context =,\\nand then we're going\\nto say _create_unverified_https_context.\\nSo again, don't worry too much about what's going on there.\\nWe're just going to be copying\\nand pasting this into our console.\\nWe're going to say nltk download, like so,\\nand then we're just going to take this code, paste it in here.\\nYour Visual Studio Code might ask you\\nif you really want to paste 11 lines.\\nI'm going to say paste.\\n\\nAnd if we run this now,\\nwe should see this little window show up.\\nAnd this basically is where we can manage all\\nof those packages that our NLTK module can use, right?\\nSo things like,\\nwhere do we have it here back here?\\nThings like the vader_lexicon, subjectivity,\\nmovie_reviews, et cetera.\\nThat's where we get those.\\nSo in order to find those things inside this window,\\nwhat you're going to want to do is go to the All Packages tab.\\n\\nAnd ignore that some of these are green.\\nThose are just those started installing a little while ago\\nwhen I was figuring this out.\\nAnd you're going to want to look for vader_lexicon in here,\\njust the same three that we had there before.\\nSo let's just double click on this\\nand that should start installing that for you.\\nActually just select that and click Download, rather.\\nAnd actually, you're going to need\\nto make sure you highlight the entire thing\\nby clicking and sort of highlighting it.\\nKind of strange,\\nnot the most user-friendly interface around.\\n\\nBut anyway, it should work if you do that.\\nAnd then let's find the other ones here.\\nWe're going to want to find the subjectivity,\\nso let's just find that here.\\nThere it is, right there.\\nSame thing.\\nHighlight that thing, click the Download button,\\nthat'll download that for you locally.\\nAnd then let's just go back\\nand we're going to get the movie reviews thing as well.\\nSo scroll up\\nand find movie_reviews, highlight that, click Download.\\nAnd there we go.\\nSo let's go back to our NLTK now.\\n\\nWe're just going to close this program,\\nwhich you can actually do by closing that like so,\\nand then I'm going to close the Python terminal there,\\nand we can actually just get rid of this file.\\nI'm going to say Don't Save.\\nAnd now we can remove nltk.download\\nand just create the sentiment intensity analyzer knowing\\nthat NLTK is going to have that available.\\nCool. So anyway, let's try running our program again.\\nWe're going to say python3, customer-service.py,\\nand let's try just entering some messages.\\n\\nWe'll say something like,\\n\\\"Your products are amazing!\\\"\\nAll right?\\nAnd sure enough, we see\\nthat that says positive comment,\\nso we might want to put that toward the bottom of the queue.\\nWhereas if there's a negative comment\\nlike, \\\"This is horrible!\\\"\\nand hit Enter, we see that that's a negative comment.\\nAnd if we type something in fairly neutral\\nlike \\\"I am eating lunch,\\\"\\nwe see that that is a neutral comment.\\nSo again, you could take this basic core functionality\\nthat we have here\\nand kind of, you know, wrap a program around it, right?\\nAgain, you could use this\\nas part of your customer service scheduling\\nor prioritizing flow, something like that.\\n\\nAnd there's obviously lots\\nof other situations you could use this in as well.\\nThis is just one example.\\nSo anyway, that's just an example\\nof how we can use this NLTK thing as part\\nof a larger Python application.\\nThe next thing that we're going to take a look at is,\\nwe're going to go back to our Jupyter Notebook\\nand see how we can apply this to some real world stock data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5989157\",\"duration\":899,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Analyze with real-world data\",\"fileName\":\"5979027_en_US_02_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":899,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":36200795,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Alright, so now that we've seen how to use NLTK\\nto analyze some fictional messages\\nthat we might receive in a customer service program,\\nthe last thing that we're going to take a look at here\\nis how we can apply this package,\\nand just some of the basic sentiment analysis functionality\\nto some real-world data.\\nSo what we're going to do is we're going to go back to Kaggle here\\nso that we can look for another real-world dataset.\\nAnd we're going to look for a dataset\\ncalled financial tweets, okay?\\nSo let's just search for that.\\n\\nAnd the one that we're looking for\\nis this one by David Wallach.\\nAnd I'm just going to click on that,\\nand we're going to download it.\\nAnd while that's downloading, let's actually take a look\\nat what this dataset includes.\\nSo this datasets pretty straightforward, as you'll see.\\nBasically it just includes a number\\nof different tweets, right?\\nThe text of those tweets, that is.\\nIt includes when those tweets occurred,\\nit includes the source that those tweets were received from.\\n\\nAnd then we have the stock symbols\\nthat are mentioned in the tweets,\\nas well as the company's name.\\nAnd then you have the URL of the actual tweet,\\nand whether or not that tweet is verified.\\nSo there's lots of information\\nthat you can use there, of course, right?\\nOne, the most obvious thing that comes to mind\\nwith sentiment analysis\\nis to maybe sort all of these tweets\\ninto the stocks that they mention, right?\\nSo you get, let's say, all of the tweets\\nthat mention Netflix,\\nyou might want to then sort it by the timestamp.\\n\\nAnd then using the sentiment analysis,\\nyou could actually take a look\\nat how the basic sentiments change over time\\nfor a given company.\\nThat's the first idea that comes to mind,\\nbut that's what a lot of people are trying to do\\nwith this kind of sentiment analysis,\\nthese kinds of sentiment analysis programs.\\nSo let's take that downloaded data,\\nand I'm going to load that into our JupyterLab here.\\nSo let me just find that real quick and drag it over.\\n\\nI'm in my other monitor right now.\\nAlright, so let's go to downloads.\\nWe're going to have to unzip the archive that we downloaded,\\nand we're going to want to take\\nthe stockerbot-export.csv file\\nand just drag that into your file tree here,\\nand that should upload it.\\nSo the next thing that we're going to do,\\nlet's actually just rename this file here\\nto something like sentiment-analysis,\\nand then we'll create a new notebook\\nby just clicking on Create new notebook.\\n\\nAnd we'll call this thing something like,\\nor rename it to something like stocks.ipynb.\\nSo we'll start off by just saying import pandas as pd.\\nWe'll use that to load our CSV file.\\nSo we'll say something like,\\nwe'll call this something like financial tweets.\\nWe'll just call ft to make it easier to type later.\\nAnd then we'll say, =pd.read_csv,\\nand we'll try and read that stockerbot-export.csv file.\\n\\nAlright, so let's just hit Enter here\\nand see how that goes.\\nAnd what we're going to see here is, oops,\\nit looks like there's an error,\\nand that is because there's some bad data in there.\\nSo there's a number of ways to deal with this.\\nProbably the easiest way for our purposes\\nis just to say on_bad _lines, as a keyword arg, equals,\\nand then the value for that would be skip, right?\\nSo basically what that does\\nis when our pandas.read_csv function runs into a bad line,\\nright, a line that it can't really figure out how to parse,\\nit'll just skip over that line\\nand that record won't appear in our data frame.\\n\\nSo let's run this again,\\nand we should see that that runs without incident.\\nSo let's just take a look at our data now by saying ft.head,\\nand sure enough, we see that it has the idea of that tweet,\\nthe text, the timestamp, the source, the symbols,\\nthe company names, the URL, et cetera.\\nSo the next thing that we might want to do here\\nis take a look at, well,\\nhere, let's just describe some of this data here.\\nJust it's usually a good idea\\nto get a good sense of what the data is like\\nwhen you're working with it.\\n\\nAnd if we hit Shift + Enter,\\nwell, really all that we can see here is the ID, right?\\nThat's because most of these are text-based values.\\nSo, unfortunately, in this case,\\nft.describe isn't going to do us a whole lot of good.\\nAnd in fact, it doesn't do us any good\\nbecause IDs are really a pretty useless thing\\nto see a mean or a standard deviation for.\\nSo let's just delete that cell.\\nAnd what we're going to try and do instead\\nis lets find out all of the unique stock symbols\\nthat this dataset contains, right?\\nSo for example, we see that there's a few of them here,\\nlike GS, M, AIG, et cetera.\\n\\nBut what we can actually do is using the Pandas DataFrames\\nbuilt-in methods, we can actually say ft, alright?\\nAnd then we're going to get the symbols column.\\nAnd then in order to find out all of the unique values\\nthat this column contains, we can just say .unique.\\nAlright, so dataframes really just contain\\na lot of great methods\\nthat really cover pretty much every use-case,\\nor all of the most common use-cases you'll encounter.\\n\\nSo let's just run this.\\nAnd what we'll see is that, sure enough,\\nthis prints out an array\\ncontaining all of those unique values basically, alright.\\nSo let's take a look at another one here.\\nWe might want to see what the basic,\\nor we might want to see how many tweets there are\\nor how many rows there are, for each of these values.\\nAnd you can do that by, instead of saying unique,\\nyou can say ft['symbols'].value counts.\\nAnd if we hit Shift + Enter there,\\nwe see that Netflix, for example, NFLX has 101,\\nAMAT, I'm not exactly sure off the top of my head\\nwhat that one is, that's 100, GPS, 100.\\n\\nMOMO is 100, (laughs) and HON is 100 as well.\\nAnd then we see that we have a few down here\\nthat really don't have that many tweets about it.\\nAnd actually you can see that these are, in fact,\\nmultiple values where someone mentioned Facebook\\nGoogle and Goog.\\nAnd same thing here with Amazon and GPS.\\nAlright, so, you know, we might want to do\\na little bit more data manipulation,\\nor figure out a way to separate those\\nso that we can still get those tweets.\\n\\nBut let's just start off by analyzing\\nall of the tweets about Netflix, for example.\\nNow, the first thing that you'll want to do here\\nis you'll want to be able to get only the rows\\nwhere the symbol is equal to NFLX.\\nAnd the good news here is that Python DataFrames,\\nor Pandas DataFrames, that is,\\nprovide us with a very nice syntax\\nfor filtering our dataframes.\\nAnd here's what that's going to look like.\\nWe're going to say ft, and then in these square brackets,\\nwhat we're actually going to do is put a Boolean expression\\nthat incorporates something about the data here.\\n\\nSo what this is going to look like\\nis we're going to say ft['symbols'], there we go.\\nAnd then we're going to say, oops, there we go,\\nis equal to 'NFLX'.\\nNow, this might look a little bit strange to you at first\\nif you're used to just using Python's basic filter\\nor list comprehension,\\nbut what this is actually going to do\\nis it's going to give us all of the pieces of data\\nfrom our financial tweets\\nwhere the symbols column,\\nor the value in the symbols column, that is,\\nis equal to NFLX.\\n\\nSo let's run this,\\nand what we'll see is that, sure enough,\\nthis gives us all of the Netflix related tweets, alright?\\nSo here's what this is going to look like\\nif we want to run some sort of analysis on this\\nnow that we have all of the Netflix tweets.\\nSo we'll just say something like Netflix tweets,\\nnt, (laughs) we'll say,\\nand we'll just assign that to a variable.\\nWhat we can actually do from there\\nis we can do something like\\nwe can sort it by the timestamp, let's say.\\n\\nAlright, so let's do nt.\\nYep, sure enough, it's nt.sort_values.\\nAnd then we're going to say by=,\\nand we basically get to determine the column\\nthat we're going to be sorting by.\\nIn our case here, that's going to be timestamp.\\nAnd let's just hit Enter and see what that gives us.\\nAnd yep, it looks like those are now in ascending order.\\nSo let's go back here now,\\nand we're going to need to assign this sorted values thing\\nto a new variable.\\n\\nIf we want to keep that around,\\nwe'll say something like nt_sorted = nt.sort_values\\n(by='timestamp'),\\nand we can now use this thing\\nin order to perform some sort of sentiment analysis\\nand see how that sentiment score changes over time.\\nSo here's what this is going to look like.\\nWhat we're going to do is we're actually going to loop through\\nall of these things and create a new list.\\nWe're just going to create a basic Python list.\\nYou could use something else like a dataframe\\nif you really wanted to,\\nbut we're going to create a new list\\ncontaining all of those compound scores\\nso that we can actually see what's going on.\\n\\nAlright, so here's what this is going to look like.\\nWe're going to say, we'll start off by saying\\nfor index and row in,\\nand then what we'll do is we'll loop through\\nall of the text, or all of the rows in this nt_sorted thing.\\nSo we're going to say index, row in nt_sorted.iterrows.\\nAlright, so basically in order to be able to loop\\nthrough the rows as if this were a list,\\nyou just need to say nt_sorted. Iterrows,\\nand that allows you, as you can see here,\\nto use that in a for loop.\\n\\nSo now that we have that,\\nthe next thing that we're going to do\\nis we're going to get the actual text from that row,\\nwhich we can do by saying tweet = row.\\nAnd then we'll take a look at the text column value\\nfor that row.\\nAnd last but not least,\\nwe're just going to need to perform\\na little bit of sentiment analysis on this.\\nSo let's actually go up and create a new cell up here.\\nWe're just going to create,\\nwe'll do it underneath this one, I suppose,\\nand this is where we'll actually set up\\nour sentiment intensity analyzer.\\n\\nSo let's just open up this here and copy what we had before.\\nWe'll copy that, paste that in here, and run it.\\nAlright, so now we should have\\nthis sentiment intensity analyzer.\\nYou should see all of this stuff pop up under here.\\nAnd now we just need to say, let's see,\\nsomething like analysis = sia.polarity_scores.\\nAnd we're going to pass the tweet to it like so.\\nAnd then we basically just take the compound score\\nby saying something like...\\n\\nWell, here, let's actually add that to a new list here.\\nWe'll just say something like scores =,\\nand here we'll call this compound_scores =,\\nwe'll start that off as an empty list.\\nThen we'll just say, compound_scores.append.\\nAnd we'll add analysis.,\\nor not ., sorry, the compound property of analysis,\\nthe compound entry, that is, to that compound scores.\\nAnd what we should end up with,\\nif we print out our compound scores here,\\nlet's just check to make sure that this works by running it,\\nis, sure enough, we see that we have a bunch of scores\\nin-between negative one and one\\nabout our company here, Netflix.\\n\\nSo the next thing that we can do here\\nis now that we have all of these compound scores,\\nif you wanted to plot those,\\nlet's say against the date on a line chart, right,\\nlet's just take a look at that.\\nLet's go back to our notebook demo,\\njust so we can copy this code here.\\nAll of that matplotlibs stuff like so.\\nWe're going to paste that into stocks.ipynb.\\nAnd what we're going to do here\\nis instead of just having fake numbers for the X-axis,\\nwe're going to have the dates, alright?\\nSo what that's going to look like\\nis we're actually going to use the dates\\nfor our Netflix tweets as the X-axis.\\n\\nAnd then these scores that we just generated here\\nare going to be the Y value.\\nSo let's just say nt_sorted,\\nand then we're going to get the date.\\nAnd let me just make sure I know the name\\nof the column there.\\nThat is going to be timestamp, right?\\nSo we'll say timestamp.\\nAnd then for the Y values here,\\nwe're just going to pass those scores.\\nSo let's just, there we go.\\nWe're going to want compound_scores.\\n\\nJust wanted to make sure I had the name correct there.\\nSo compound_scores, and let's see what this looks like.\\nInstead of profit, we're going to say something like score,\\nor Compound Score.\\nAnd if we hit Enter, we should see\\nhopefully something printed out to the console.\\nAnd sure enough, we see that that sentiment\\nis kind of bouncing all over the place.\\nAnd that may or may not be super accurate.\\nI certainly wouldn't want to buy\\nand sell Netflix stocks (laughs)\\njust by looking at this data here.\\n\\nBut nevertheless, this has given us a useful,\\na potentially useful visual that we could use\\nto make decisions after, of course, verifying\\nthat the sentiment intensity analyzer\\nhas actually done its job.\\nAlright, now if you wanted to see\\nwhat each of these scores was actually assigned to\\nso that you could kind of spot check it,\\nhere's how you might do that.\\nYou might want to say something like nt_sorted,\\nand then you could actually create a new column,\\nwhich is kind of interesting.\\n\\nWe'll say something like sentiment_score, alright.\\nAnd then you could say = compound_scores,\\nand hopefully that works here.\\nAnd let's see,\\nwhat we could now do is we could say nt_sorted,\\nand basically just print that out.\\nAnd then what you should see is that there's a score now.\\nThere's this extra column that you just added\\nthat contains these sentiment analysis scores\\nfor each of those tweets.\\nSo feel free to spot-check that\\nand see if you think that it matches up\\nwith the text values for each of those.\\n\\nBut anyway, that is how you might apply\\nthis sentiment intensity analyzer thing,\\nand really just sentiment analysis in general\\nto a real-world data.\\nSo obviously there's lots of possibilities here.\\nI really hope that you take what you've learned here\\nand start to apply it to some of your own personal projects.\\n\"}],\"name\":\"2. Perform Sentiment Analysis\",\"size\":100212489,\"urn\":\"urn:li:learningContentChapter:5984260\"},{\"duration\":1155,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5992055\",\"duration\":56,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Learning objectives\",\"fileName\":\"5979027_en_US_03_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":56,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3059605,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Welcome to Lesson 3, Work with Image Recognition.\\nIn this lesson, we're going to be diving into\\nthe fascinating world of image recognition in Python,\\nand in particular, we're going to be seeing how to use\\na library called OpenCV\\nto identify human faces in photographs.\\nSo, the basic structure of this lesson\\nis going to be like this.\\nWe're going to start off by discussing some basic tools\\nthat are used for this purpose,\\nand then what we're going to do is,\\nwe're going to focus on OpenCV, as I said,\\nwhich is a top-tier library\\nfor both image and video analysis,\\nand once we've gone through some of the basics,\\nwe're going to see how we can use\\nthat OpenCV library to identify,\\nas well as draw boxes around human faces\\nin real photographs.\\n\\nSo, that's our basic plan of attack, let's get started.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5989158\",\"duration\":120,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Learn about image recognition tools in Python\",\"fileName\":\"5979027_en_US_03_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":120,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7309925,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- All right, so, to get started here,\\nlet's just talk a little bit about image recognition tools\\nand what some of the major goals\\nof these tools are in Python.\\nSo, what we're going to be using here\\nis a library called OpenCV,\\nand OpenCV is, obviously, it's an open-source\\npiece of software that just contains\\na wide variety of low-level tools\\nfor doing basic image processing,\\nAI, that kind of thing on image data.\\n\\nSo, what we're going to be doing specifically here\\nis we're going to see how to use facial detection\\nas part of the OpenCV library\\nin order to highlight faces in a photo,\\nand you can do this\\nwith lots of different photos, of course.\\nNow, what this software is going to be doing specifically\\nis it's going to actually be detecting faces in real photos,\\nand it's going to be drawing boxes around them.\\nNow, this is really as far as open CV goes\\nmost of the time, because as I said,\\nit contains mostly low-level utilities.\\n\\nSo, while for for certain AI applications,\\nyou might want to be able to, let's say,\\ndetermine the expression on a person's face,\\nor maybe you want to be able to detect\\nwhat exactly is in a photo, right?\\nIs there a car in the photo, is there a plant in the photo,\\nis there, whatever, right?\\nBut OpenCV generally just contains low-level utilities\\nthat help you actually create those things by combining them\\nwith other tools.\\n\\nNow, that being said, what we are going to be doing here is\\nwell within the abilities of the OpenCV library,\\nso that's basically what we're going to be doing here.\\nSo, the next thing that we're going to be doing here\\nis taking a look at how to add OpenCV to our project,\\nas well as some of the basic operations\\nthat it will allow us to do.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5987195\",\"duration\":514,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Learn the basics of OpenCV\",\"fileName\":\"5979027_en_US_03_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":514,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":19074893,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- All right, so now that we're a little bit more familiar\\nwith the basic idea of the tool we're going to be using,\\nlet's see how we can add OpenCV to our project\\nand some of the basic things that it allows us to do.\\nSo first of all, let's add this\\nto our existing project, right?\\nThis is the folder that I created for when we were doing\\nthat customer-service.py demonstration.\\nSo what we're going to do is\\nif you still have a program running,\\njust stop that with Ctrl C,\\nand we're going to install a package here\\ncalled opencv-python, okay?\\nThat's the name of the Python version\\nof this OpenCV package.\\n\\nSo now that we've done that, right,\\nor let's just wait for that to finish there.\\nThere we go.\\nNow that we've done that, what we're going to do next\\nis create a basic file,\\nor just a new empty file that is,\\nand we'll call this something like face-detection.py.\\nNow we're not going to be doing face detection quite yet,\\nbut what we'll do in this file first is, as I said,\\njust take a look at the basics of the OpenCV library\\nand what kinds of things it allows us to do.\\n\\nBefore we do that though, let's actually add a photo\\nto our folder here.\\nAnd I already have just a basic stock photo.\\nIt's a group of people.\\nLet me just add that to my project here.\\nIt's just a, it's just a big group of people,\\njust your basic stock photo.\\nYou should be able to find something like this.\\nBut I would highly recommend that you find a photo\\nwhere most people are facing forward\\nand where their faces are well lit, et cetera.\\nAs you'll see, the face detection algorithm\\nthat we're going to be using isn't super robust\\nto things like that.\\n\\nSo anyway, once you've got that photo, the next thing\\nthat we're going to do is\\nwe're going to import the OpenCV library.\\nSo we'll say cv2,\\nand then basically what the cv2 library is going to do.\\nThat's just how you refer to OpenCV inside a Python program.\\nWhat it's going to do is it's basically going to allow us\\nto read an image, do something to that image,\\nand then either display the result\\nor write the result to a new file.\\n\\nSo what this is going to look like here is\\nwe're going to start off by reading in\\nthis group photo image.\\nNow, the way that we do this, it's fairly straightforward.\\nWe just need to say, we'll say group photo equals,\\nand then we'll say cv2.imread.\\nThis is the main function\\nthat's used on the cv2 module in order to read in images.\\nAnd then we'll say, group-photo.webp.\\n\\nAnd you might have a JPEG or a PNG or something like that.\\nThose will all work as well.\\nCool.\\nthere's a lot of different things\\nthat we might want to do to it, right?\\nSo you might want to transform the colors a little bit.\\nYou might want to change some other attributes\\nabout the image, right?\\nCertainly for a lot of face detection algorithms,\\nit requires that the image is actually in grayscale\\nversus in color.\\nAnd we might also want to add stuff to the image.\\n\\nWhat we're going to be doing once we actually add\\nthe face detection logic is we're actually going\\nto be adding some squares, right, boxes\\naround all of the faces that we've detected.\\nWe'll get to that shortly.\\nBut first, the easiest thing that we can do\\nis we can just display the image\\nto ourselves in a little window\\nby saying cv2.imshow,\\nand then the first argument that we pass to this\\nis going to be the title of the window.\\nSo we'll say something like group photo,\\nand then for the second argument here,\\nwhat this is going to be is the group photo image\\nthat we want to display, all right?\\nSo what this imshow function does is it just takes\\nwhatever photo that is,\\nand it displays it in a separate window.\\n\\nSo, what this is going to look like,\\nand actually before we do this, what we're going to want to do\\nis we're going to say cv2.waitKey.\\nWe're going to pass zero as an argument to that.\\nAnd I'll describe what that does in just a minute,\\nbecause after that we're going to say cv2.destroyAllWindows.\\nOkay?\\nSo basically what this is doing here is\\nit's displaying the photo, and then whenever we press a key,\\nit's basically waiting until we press some sort of key,\\nand then it allows the program to continue,\\nand it will actually destroy all of the windows\\nand the program will end.\\n\\nSo let's just run this thing just to see where we're at.\\nWe're going to say python3 face-detection.py.\\nAnd what we should see is that this will run\\nand in a window here, it will display our image,\\nand you can resize that if you want to.\\nYou can really do whatever you want with it.\\nSo really when we're developing something,\\nwhen we're trying to achieve some sort of result\\nwith this OpenCV library,\\nthat's how we're ultimately going to tell\\nwhether we were successful or not, right?\\nWhile it would definitely be possible for some people,\\nI'm sure, to look at just a list of numbers in Python,\\na list of coordinates, and tell\\nwhether those actually match up with the faces in a photo,\\nlet's say, for most people it's just going to be easier\\nto actually see those squares drawn onto the image,\\nwhich is what we're going to see how to do next.\\n\\nBut anyway, the last thing I wanted to show you is\\nif this window is currently active,\\nand you press some key like the enter key,\\nthat will make the program end, and you can come back here\\nand sure enough, the program has ended\\nin the terminal as well.\\nSo one last thing here that you'll want to know how to do\\nwith the CV library is write the results\\nof transforming an image in some way to another file.\\nAnd the way that you do that, it's pretty straightforward.\\nYou can just say cv2.imwrite, all right?\\nSo that's sort of the opposite of this imread thing.\\n\\nThis allows you to write an image to a file,\\nand we can just, we'll call this something like group photo.\\nAnd then since we're not doing anything to it yet,\\nwe'll call it copy.webp.\\nAnd then we need to pass in the second argument here,\\nthe actual photo variable.\\nSo we're going to say group photo,\\nwhich is the original photo that we loaded here.\\nAnd in between these two things here, imread and imwrite,\\nis where we can actually make changes\\nto this group photo thing.\\nSo for example, we might want to change the color, right?\\nAnd the way that we would do that,\\nI'll just show you that right here,\\nwould be, and oops, let's actually change that here,\\nwould be by saying something like,\\nwell, we'll change it to grayscale, right?\\nSo we'll say gray photo equals,\\nand then we can say cv2 dot convert color,\\nwhich is abbreviated as cvtColor.\\n\\nAnd then we basically specify the image\\nwe want to convert the color of, and oops,\\nthat should be in lowercase, group_photo.\\nAnd the next thing here is going to be a constant\\nthat indicates what transformation we want to make, right?\\nWhat color transformation we want to make.\\nIn our case, we're going to say cv2.color\\nand then underscore BGR2gray, all right?\\nSo basically that's just going to convert\\nregular color photo into grayscale.\\nAnd what you'll see now is if we show the gray photo,\\nwe'll say gray_photo, like so.\\n\\nAnd actually let's just remove that.\\nWe'll just write that photo\\nand we'll remove these last two as well.\\nActually, we'll just comment those out.\\nYou know what, let's comment this one out as well\\nbecause we might need that a little bit later on.\\nSo let's just write the gray version of the photo\\nto a new file.\\nAnd if we run our code now,\\nwhat we should see if all goes well is that our photo will,\\nor our program rather, will successfully run,\\nand we should have this group-photo-copy.webp in there,\\nwhich contains a grayscale version of the original photo,\\nwhich is pretty cool, right?\\nAll right, so the next thing that we're going to see how\\nto do is use another piece of the OpenCV library\\nto detect faces in that photo.\\n\\nAnd then we're going to use that data\\nto actually draw squares on the photo around the faces\\nthat we've detected to see how we did.\\nSo that's what we have coming up next.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5986168\",\"duration\":465,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Incorporate image recognition into an application\",\"fileName\":\"5979027_en_US_03_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":465,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":19978437,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Alright, so now that we've seen the basics\\nof working with OpenCV in Python,\\nlet's take a look\\nat how we can do some basic face detection,\\nas well as draw those results to an image\\nthat we can then validate visually.\\nSo here's what this is going to look like.\\nThe first thing that we're going\\nto do is create something called a classifier.\\nAnd there are different kinds of classifiers\\nthat we can use for face detection,\\nbut the one that we're going to use\\nhere is called a cascade classifier.\\n\\nAnd here's how this is going to work.\\nUnderneath the import statement,\\nwhat we're going to do is we're going to create a new variable\\nhere, which we'll call something like face_classifier.\\nAnd then we're going to say = cv2.CascadeClassifier()\\nall right? This is just a specific kind of object\\nthat we can basically use to run our images through\\nand it will give us back the coordinates of,\\nyou know, the boxes that should be drawn around the faces.\\n\\nSo you'll see what that looks like shortly.\\nSo anyway, what this is going to want us\\nto do now is pass a string\\nas an argument indicating the exact classifier we want.\\nSo what this is going to look like,\\nwe're going to say cv2.data\\nand then this specific cascade classifier\\nthat we're going to be using is called a Haar cascade\\nclassifier, hard thing to say there.\\nAnd that's spelled H-A-A-R and then cascades.\\n\\nAnd then we're going to say + 'haarcascade'.\\nAnd then we're going to use the frontalface_default\\nclassifier, right?\\nBasically this one is pretty good\\nat recognizing faces when it's like dead on.\\nIt's not so good when the face,\\nwhen the person's like looking\\nto the side or something like that.\\nBut for the most part, you can find,\\nit's pretty good at detecting faces\\nthat are looking at the camera.\\nSo anyway, this is going to create our classifier for us.\\n\\nAnd what we can now do is when we've loaded an image such\\nas our group_photo here, we can actually pass\\nthat to our face_classifier.\\nAnd as I said, the face_classifier is going\\nto give us back the coordinates of the,\\nbasically the corners of the faces in the image.\\nSo here's what this is going to look like.\\nWe're going to say, and actually one thing I should point\\nout is that this specific classifier does in fact require\\nthat the photo that it's trying\\nto detect faces in is in black and white.\\n\\nThat just makes it easier for the classifier\\nto find those faces.\\nSo we're actually going to use our gray_photo here.\\nAnd then what we'll do is we'll say faces =\\nand then we'll use our face_classifier\\nto say face_classifier.detectMultiScale()\\nis the name of the function here.\\nThat is in CamelCase, unlike some\\nof the other variable names which are in snake_case.\\nJust make sure you don't make that mistake.\\nAnd then we're going to pass our gray_photo there\\nas the first argument.\\n\\nThat's the one that we're expecting this classifier\\nto detect the faces in.\\nAnd then we basically just have to detect a few other,\\nor specify rather a few other things.\\nThe first thing, something called a scaleFactor\\nfor reasons that don't really matter right now,\\nwe're going to set that to 1.1.\\nWe're going to say minNeighbors, these are all just sort\\nof parameters for the classifier itself.\\nWe'll set that to 5,\\nand you can always play around with these\\nto see if they have an effect.\\nAnd then we're going to say minSize=\\nand we'll set that to something like (30, 30).\\n\\nAlright, so now that we've detected our faces,\\nwhat this faces thing is going\\nto be is basically a list containing all\\nof the coordinates of those faces.\\nSo what we're going to do next is in order\\nto be able to visually verify\\nthat this classifier has worked pretty well,\\nwe're going to loop through those faces\\nand actually draw them onto our photo.\\nAnd actually we're going to draw them onto our original photo\\nhere so that we can actually see the boxes\\non the color version of the photo.\\n\\nSo in other words, while the classifier had to use the black\\nand white photo, we can still basically just map those\\ncoordinates over to the color photo\\nand that won't cause any problems.\\nSo anyway, here's what this is going to look like.\\nWe're going to say for (x, y)\\nand then we have the width and height, right?\\nSo these are the X and Y coordinates of the box.\\nSo the, one of the corners of the box that is as well\\nas the width and height of the box\\nthat should be around the person's face, as I've said.\\nAnd then we're going to say in faces:\\nright? So essentially this faces thing is a list\\nor an iterable containing several tuples\\nwith all of those coordinates in them.\\n\\nAnd now we're just going to say cv2.rectangle\\nwhich will in fact draw a rectangle\\nonto our group_photo.\\nAnd then we need to tell it the coordinates\\nfor that rectangle. The first argument here is going\\nto be (x, y)\\nright? A tuple containing those.\\nThe second argument is going to be the coordinates\\nof the lower corner.\\nSo we'll say (x + w, y + h)\\nand then we need to specify the color that we want.\\n\\nAnd since these are in BGR, not RGB,\\nlet's just draw, let's draw a blue box\\nby saying (255, 0, 0)\\nand then we'll say 2 at the end of that.\\nSo anyway, that should draw the rectangles onto the faces.\\nLet's now display this group_photo\\nwith the rectangles drawn onto it\\nin a window so that we can see it.\\nSo what we're going to do, I'll just remove this,\\nimwrite thing, will comment\\nor will uncomment imshow\\nas the waitKey and destroyAllWindows.\\n\\nAnd let's make sure that this is showing the group_photo.\\nCool. So let's give this thing a try.\\nNow we're going to run our code\\npython3 face detection.py\\nand sure enough, we see that the window pops up\\nand for the most part, it got the faces right.\\nNow there are certain people\\nthat clearly the classifier didn't\\nquite get their faces right, right?\\nWe see that this guy with the sunglasses on here,\\nthere's no blue box around his face.\\nPresumably these sunglasses made that hard\\nto detect as well as the person with the hat here,\\nthat was probably a little bit difficult to detect.\\n\\nAnd you can also see that there's some boxes\\nthat are a little bit out of place.\\nLike this person here has two boxes.\\nThere's this big box in the middle here\\nthat's clearly not a face,\\nbut for the most part it did pretty well at detecting faces\\nand putting a box pretty much directly around them.\\nOh, and this person in the back here\\nthat didn't get detected either\\nbecause, well, presumably the hat, the sunglasses, etcetera.\\nSo anyway, that is the basics of doing face detection\\nas well as using that data to modify the photo.\\n\\nIf you wanted to write out that photo\\nwith the rectangles on it to a new file,\\nyou could always just use this, imwrite function as well.\\nSo, all right, so that is the basics of using OpenCV\\nfor basic face detection.\\nNow, as I said, this is just one of many things\\nthat we can do with the OpenCV library\\nand in general, as I've already said,\\nI believe the OpenCV library is going to be something\\nthat you'll want to combine with other libraries, right?\\nWith other things like machine learning,\\ndata science related,\\nand that's really where the real power comes in.\\n\\nBut anyway, the next thing that we'll be taking a look\\nat is how to use basic web scraping in order\\nto collect data from the internet.\\n\"}],\"name\":\"3. Work with Image Recognition\",\"size\":49422860,\"urn\":\"urn:li:learningContentChapter:5989160\"},{\"duration\":1749,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5984257\",\"duration\":56,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Learning objectives\",\"fileName\":\"5979027_en_US_04_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":56,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3043732,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Welcome to lesson four, Scrape Data from the Internet.\\nIn this lesson, we dive into the useful realm\\nof web scraping, which is a very powerful possibility\\nfor gathering data\\nfor your data science projects from the worldwide web.\\nSo in this lesson, we kick things off by getting acquainted\\nwith just the basic idea of web scraping in general,\\nas well as a tool called Beautiful Soup\\nthat will make data extraction a breeze.\\nOnce we have some of the HTML from a website,\\nand once we grasp the essentials of web scraping\\nand beautiful soup, we're going to really dive into how\\nto take HTML data\\nand convert it into data that's nicely formatted so\\nthat we can actually use it\\nas a data set in our data science projects,\\nand well, that's our plan of attack for this lesson.\\n\\nSo let's get started.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5986169\",\"duration\":296,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Learn about web-scraping tools in Python\",\"fileName\":\"5979027_en_US_04_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":296,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11884923,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Alright, so let's get started here\\nby talking about just the basic idea\\nof web scraping tools in Python in general.\\nAnd really the first thing to know about web scraping\\nis that at the end,\\nmost of the time it's just making basic requests\\nto different websites and taking the HTML\\nor other data that we get back from those websites\\nand parsing it in order to get some kind of data from it.\\nSo, to give you an example of this,\\njust to do a very brief demonstration,\\nthere are more specific tools that we'll be using\\nfor the actual parsing.\\n\\nWell, let's say that we wanted to collect data\\nabout different articles from Hacker News, right?\\nWhich is a, you know, a very popular site.\\nJust there's lots of interesting news article links on here.\\nSo let's say that you wanted to be able to pull the data\\nfor these articles\\nand sort of create your own data set from that.\\nWell, the first thing that you would have to do,\\nand this is going to be true, as I've said,\\nof most of the web scraping you're going to do,\\nthe first thing you're going to want to do is actually make\\na request to that site\\nand get the HTML that that site is made up of, right?\\nSo, in other words, when we put in news.ycombinator.com,\\nwhich is the URL for Hacker News, into our browser,\\nthat's exactly what our browser's doing, right?\\nIt's sending a request to that URL,\\nthe URL's sending us the HTML back,\\nand obviously from what we can see here,\\nall of the data we want is in that HTML, right?\\nSo really the main focus of a lot of these\\nweb scraping tools, including Beautiful Soup,\\nwhich is the one that we're going to be using here,\\nthe main idea is that they make the process\\nof actually parsing that HTML\\nthat we get back much easier than if we were to try\\nand do that through just basic text manipulation in Python.\\n\\nSo anyway, to get started here\\nand just to really see how easy\\nand straightforward web scraping can be,\\nobviously it can get much more complicated than this,\\nbut in general, the basic concept is pretty simple.\\nLet's just create a new file\\nand we'll call this something like scraping,\\noops, let's try that again, scraping_basics.py.\\nAnd inside of here what we're going to do is\\nwe're going to start off by importing\\na library called requests.\\nNow, this is used\\nto make requests across some sort of network, right?\\nSuch as the internet.\\n\\nAnd in order for this to work,\\nsince requests isn't included in Python by default,\\nwe're going to have to say pip install requests and hit Enter.\\nAnd that should install that library for you.\\nAlright, so the next thing that we're going to need to do,\\nand oops, I didn't mean to do that, let's try that again.\\nThe next thing that we're going to do is\\nwe're going to use this requests library\\nin order to send a request to the URL, right?\\nThe same URL that we have in our browser here.\\nSo what we're going to do is we're going to say\\nurl = https://\\nand then the URL is news.ycombinator.com, okay?\\nSo now that we have the URL,\\nwe can send a request to it\\nby saying requests.get, right?\\nIn many cases, we'll want to actually send, well,\\nin most cases I would say we'll want to send a get request,\\nsince that's generally the type of request that's used\\nto load data such as HTML.\\n\\nAnd then we're going to say url,\\nand what that's going to do is it's going to send a get request\\nto that URL.\\nAnd in order to get the response\\nso that we can use it later on,\\nwe're going to say response = requests.get(url).\\nSo at this point, the website has presumably responded\\nto our Python script's request with some sort of HTML.\\nSo in order to see that,\\nlet's just say print and then we'll say response.text.\\nAnd if we run our code now, what we'll see,\\nand we can run this with python3 scraping_basics.py,\\nis that sure enough, we have all of that HTML\\nthat is getting displayed in our browser right here\\nin our Python program, right?\\nSo you can see things like the weight of New York City.\\n\\nThat's what I'll see.\\nYou'll probably see something different\\nand you can see that that article is right here.\\nSo, in other words,\\nwe're taking a look at this HTML from two different sides.\\nOne we're taking a look at in the browser,\\none we're taking a look at in the Python program.\\nNow, the beauty about having this HTML data\\nin our Python program now is that we can actually parse\\nthis thing programmatically in order to create\\nour own data set from it.\\nAnd that's where things start to get exciting.\\nSo the next thing that we're going to see how to do\\nis use a library called Beautiful Soup,\\nkind of strange name,\\nin order to actually turn this jumble\\nof HTML into some usable data.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5990180\",\"duration\":397,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Learn the basics of the Beautiful Soup library\",\"fileName\":\"5979027_en_US_04_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":397,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17284439,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- All right, so now that we're familiar with the problem\\nthat we're going to have to address when web scraping, right,\\nwe've already got the HTML,\\nbut we need to actually have a way that we can parse this\\nand make some sort of sense out of it.\\nAnd in Python, the most commonly used library\\nfor this is called Beautiful Soup.\\nAll right, now, yes, that is a strange name.\\nWhere did this library get that name?\\nWell, it depends on where you read.\\nSome sources say that it got\\nthat name from the phrase \\\"Tag Soup\\\",\\nwhich refers to poorly formatted HTML.\\n\\nAnd some say that it's a reference to Alice in Wonderland.\\nWhatever the case, that's the name of the library\\nand it actually is a very useful library\\nfor taking just a jumble of HTML, like what we see here,\\nand turning it into real, usable Python data.\\nSo here's what this is going to look like.\\nWell, we'll start off by just doing things,\\nvery simple things,\\nlike getting all of the titles on our webpage here, right?\\nSo, let's say that we want to just print out all of the titles\\nof the articles that are currently on Hacker News.\\n\\nWell, in order to do that, as I said,\\nwe're going to use the Beautiful Soup Library\\nand we first need to install it.\\nSo, what we're going to need to do is say pip install,\\nand then it's going to be beautifulsoup4,\\nall one word, all lowercase.\\nAnd, because of the way that this library works,\\nwe actually need to install a parser along with it.\\nSo, what we're going to do is we're going to install,\\nhtml5lib, alongside it.\\nAnd, that should install both those things into our project.\\n\\nSo we can now add that to our scraping-basics.py file.\\nSo let's start off by importing this thing.\\nHere's what importing Beautiful Soup is going to look like.\\nWe're going to say, from bs4, which is the module name for\\nthe Beautiful Soup Library we just installed,\\nwe're going to say import capital B, beautiful,\\nand then capital S, soup.\\nAlright, so now that we have this thing installed,\\nthe next thing that we're going to do is actually use it\\nto parse the HTML that we got back as our response.\\n\\nSo, here's what this is going to look like.\\nWe're going to say, soup=BeautifulSoup,\\nand then we're going to pass response.text to it,\\nas well as, the parser that we want to use.\\nIn this case,\\nthat's going to be the one we installed alongside it, html5lib.\\nAnd, this- naming this soup is just a convention here.\\nBut basically what this soup thing is,\\na very useful Python object\\nthat we can actually use to navigate our HTML page.\\nSo here's what this is going to look like.\\n\\nLet's say that we wanted to, again,\\nfind out what all the titles on the page are.\\nWell, first let's just go back to here\\nand take a look at the basic HTML of the page so\\nthat we can find out what we're actually looking for here.\\nSo let's just take a look at one of these titles here.\\nAnd we can see that this title is\\ninside an anchor tag, right? An A tag.\\nThat, in itself, is inside a table row called athing.\\nNow, don't worry too much about Hacker News's\\ndifferent class names that they chose.\\n\\nThis is actually going to be very useful for us\\nto find the title\\nand the corresponding things like a link,\\nthe number of comments, the number of up votes, et cetera.\\nSo, here's what this is going to look like.\\nWe're going to go back to our code,\\nand what we'll do from here is we're going to say,\\nfor item in soup.find all, oops,\\nfind_all, that is.\\nAnd we're going to want to find all of the TRs.\\nAnd then what we'll do is we're going to say,\\nclass_=athing, right?\\nSo this is going to find us all of the table row elements\\nwith the class,\\nand it has this underscore here so that it doesn't interfere\\nwith Python's built-in class keyword, equal to athing.\\n\\nSo, let's just add a colon after this.\\nAnd this will allow us to loop through all\\nof the items, right?\\nAll of the HTML elements that match that description there.\\nAnd then what we're going to do is inside that item, right,\\ninside that athing, table row,\\nwe're actually going to have to do a slightly\\nmore complicated query here.\\nSo let's just go back to one of these other ones here.\\nWe'll take a look at that.\\nSo you can see that this is an anchor tag,\\nbut it's also inside a span called a Title Line.\\n\\nAnd basically, that is how we're going to have to find this.\\nSince, unfortunately, this anchor tag doesn't\\nhave a specific class that we can use.\\nSo let's go back here now,\\nwe're going to say, item.find, all right?\\nWe're just going to find one.\\nAnd we're interested in finding the span\\nwith the class equal to,\\nand let's just go back here\\nto make sure we have the exact string, title line.\\nSo let's just paste that in here now.\\nSo we'll say, class=titleline,\\nand then we're going to say dot find on that, right?\\nBecause, just like how we were able to\\nfind all of the athing table rows\\nand then find inside those, we can basically chain this find\\nto find one element inside another.\\n\\nSo now, we're just going to find an anchor tag inside of there,\\nand that should give us the title tag.\\nSo let's try this out.\\nWe're going to say, title tag=item.find.\\nAnd for now,\\nlet's just try printing out the text of that title tag.\\nAnd the way that we do that is we can say,\\nprint title tag.text, all right?\\nSo let's see where we're at so far\\nby running our code and oops uh-\\nlet's try that with python3 scraping-basics.py.\\nAnd sure enough, it looks like we hit the nail on the head.\\n\\nWe got all of the titles from Hacker News.\\nNow you're going to see different titles than what I have here,\\nbut you should see something that's pretty similar to this.\\nSo anyway, that's really the basics\\nof using the Beautiful Soup Library.\\nIt's, you know, there's obviously lots\\nand lots of different methods you can use,\\nlots of different, you know,\\nlots of different strategies\\nfor finding one thing inside another.\\nBut really all that it comes down to is\\nfinding something on the page, getting the contents\\nor getting some attribute of that tag,\\nand then using that as data.\\n\\nSo the next thing that we're going to see is\\nhow we can use a similar strategy to what we did here\\nto actually build ourselves a simple data set from the\\nHacker News site.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5991097\",\"duration\":467,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Format and use scraped data\",\"fileName\":\"5979027_en_US_04_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":467,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":21598634,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- All right, so now that we've seen how\\nto do something fairly straightforward, like print out all\\nof the titles from a website that we're scraping, let's see\\nhow we can do the same kind of thing,\\nbut in much more depth, right?\\nSo that we can actually create a simple data set\\nfrom the data on a website.\\nSo what we're going to do here is we're going to, in addition\\nto finding all of the titles of the articles on this site,\\nwe're also going to want\\nto get things like the number of up votes, right?\\nThat's the points right here, as well\\nas things like the actual link to that site.\\n\\nHow many comments, perhaps, there's lots of things\\nthat you can do with this.\\nBut anyway, let's just start off with the title, the link,\\nand the up votes.\\nAll right, so here's what this is going to look like.\\nWe've already found all of the table rows here\\nwith the class of a thing,\\nand that's where most of our content is going to be.\\nLet's just double check that by going back to our site\\nand just taking a look at these things here.\\nSo a thing, let's just take a look at this so\\nthat we can actually see what that covers on the page.\\n\\nAnd actually we're going to need to,\\nin addition to getting that, get the table row here,\\nit looks like they made the structure\\na little bit complicated.\\nSo for subline, inside that span with a class of subline,\\nthere is a span with a class called score.\\nSo hopefully, we'll be able to just get the scores for all\\nof these, right?\\nJust by looking for a span with a class of score.\\nWe'll see if that works or not,\\nand we'll just hopefully we'll be able\\nto get the string from inside of there as well as the link.\\n\\nSo let's start off though, by getting the title\\nand the link.\\nAnd the good news here, I believe, is\\nthat the link should be right on that anchor tag as well.\\nSo in other words, once we've found the anchor tag here,\\nwhich is the title tag here, we should be able\\nto get both the text, which is the actual title.\\nSo we'll say title equals title tag.text,\\nand we can get the, the actual link that that will take us\\nto by saying link equals title tag.\\nAnd then in order to get an attribute from that tag,\\nwe just say Get, and we're going to look for href,\\nAnd as a backup here, we'll try\\nand say something like an A, all right, just so\\nthat if there is no link for some reason,\\nwe can have a backup value.\\n\\nAll right, so that should give us the titles and links.\\nAnd really in this case, what we would want to do is just have\\na simple list.\\nSo we'll say something like articles equals empty list\\nthat we then add a dictionary to\\nwith these two pieces of data.\\nSo let's say something like New article equals,\\nand then we'll say Title, title,\\nand then we'll say link.\\nThat's the key there.\\nAnd then we'll have the link like so.\\nAnd then, of course, we'll want\\nto say articles.append new article.\\n\\nAnd then we will basically just print those out\\nto see where we're at.\\nNow we're going to run our code again,\\nand sure enough, it looks like let's just go down\\nto where we last ran our code.\\nIt looks like, sure enough, we end up with title\\nand link for all of the articles on that page.\\nSo cool, let's keep moving\\nand actually get the number of up votes\\nfor each of these things.\\nThe number of up votes, as I said, is going\\nto be a little bit trickier since in this for loop,\\nwhat we're doing is we're finding all of the table rows\\nwith the Class A thing,\\nbut the up votes is actually a separate table row\\nunderneath that, right?\\nAnd the way that they have this organized,\\nthis whole thing is just a series of table rows\\nthat we're going to need to find.\\n\\nBut what we should be able to do,\\nhopefully this works the way I think it will,\\nwhat we should be able to do is just loop back through all\\nof the article datas,\\nor all of the article dictionaries rather,\\nthat we've constructed so far,\\nand add those up votes on after the fact.\\nSo let's try this here.\\nWhat we're going to do is we're going to say four item in,\\nand then we're going to say soup.find_all.\\nAnd we want to find this time all spans\\nwith the class name of,\\nlet's just go back to make sure I have the right one.\\n\\nShould be class of score.\\nSo let's add that here.\\nI'm going to say class equals score.\\nAnd well, that should be pretty much all we need.\\nLet's loop through those\\nand basically just add those on to our new articles here,\\nor actually our articles list.\\nSo we're going to want the index as well.\\nAnd we should be able to say Enumerate soup, find all, so\\nthat we can actually access that index.\\nSo let's just say for index and item in enumerate.\\n\\nCool.\\nSo now all we have to do,\\nand again, hopefully, this works is say\\narticles index equals, and actually articles index.\\nAnd then we can set the property on there like up votes\\nto whatever text is inside that item.\\nSo we'll, say equals item.text.\\nAnd hopefully, this works.\\nLet's just give that a try.\\nIf we run our code again, we should see,\\nthere we go, that sure enough, each\\nof these has an up votes property.\\n\\nAnd let's check to see if that matches 331 points\\nfor the top one.\\nAnd that looks a little bit different,\\nbut I have a feeling that's because it changed since then.\\nSo sure enough, if we refresh, it says 331 points.\\nSo apparently, there's a lot of activity on this thing.\\nLots of people reading this article and upvoting it.\\nBut anyway, this here shows the basics of\\nhow you can use very simple web scraping.\\nI mean, this thing was less than 24 lines\\nof code altogether in order\\nto construct your own data sets from internet data.\\n\\nNow, a few caveats I should mention here\\nbefore we go on the first is\\nthat generally sites aren't going to be big fans\\nof you scraping them if you don't do it\\nin a respectful way, right?\\nSo what we've done here in our program is we've only loaded\\na single page once, right?\\nAnd we've then just taken that HTML\\nand pretty much just done some simple stuff with that data\\nthat's already in our program.\\nHowever, web scraping can obviously get much\\nmore invasive, right?\\nThere are certainly web scrapers out there that will try\\nand scrape every page in a website,\\nand that can actually put a lot of strain on those servers.\\n\\nSo what a lot of sites will do is they'll just, you know,\\nif they detect that you're loading pages too quickly\\nand being an annoyance to them,\\nthey'll block your IP address.\\nSo you may end up, you know, if you were to, let's say,\\ndo something like this, a requests dot get thing in a\\nfor loop and just make a bunch of requests over\\nand over again, that you might end up getting blocked\\nby their server for a little while.\\nSo that's just something to keep in mind\\nand really, you know, you just want to be respectful,\\nespecially while you're learning,\\nwhich generally will just mean being very careful not to\\nsend lots and lots of requests, one after the other, right?\\nYou'll want to at least, if you're going to do something\\nwhere you're looping through data\\nand sending requests to some, to a few different URLs,\\nyou're going to want to at least put a delay in there\\nto make sure that you're being respectful of that site.\\n\\nSo anyway, with that said, let's move on to seeing how\\nto actually modify the logic that we've written here\\nfor different sites.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5989156\",\"duration\":533,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Modify web-scraping logic for other websites\",\"fileName\":\"5979027_en_US_04_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":533,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":24839201,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- All right, so now that we've seen how\\nto scrape a basic website like, \\\"Hacker News,\\\"\\nand construct a dataset from it,\\nthe last thing that I want to do is just give you\\na basic idea of how to modify that scraping logic\\nfor other sites you might want to scrape.\\nSo what you see here is a pretty safe site\\nto practice your web-scraping on.\\nIt's called books.toscrape.com.\\nAnd as a matter of fact, toscrape.com in general,\\nis really just a website that's set up\\nto help people practice web-scraping.\\n\\nSo this isn't really, you know,\\nthis isn't a real website so to speak.\\nThis is just something that is meant to help you practice.\\nSo in our case here,\\nlet's say that we want to take this page here,\\nand I just went into the Travel section here,\\nyou can obviously pick whatever section you want,\\nand let's say that we want to create a basic data set\\nthat contains all of the titles and perhaps ratings\\nand prices for all of the books.\\nWell, what we would want to do in that case\\nis we'd want to start off more often than not,\\nby trying to find some identifier in the HTML structure\\nthat we could use to sort of hook into from our program.\\n\\nSo let's just take a look at these here.\\nWe'll take a look at the basic structure of the title here,\\nand we see that the title here is an anchor tag\\nthat's inside an H3 tag.\\nAnd that in turn is inside, it looks like an article tag\\nwith a class of product_pod.\\nAll right, so just from those few things,\\nyou should be able to find that,\\njust by looking for the article tag\\nwith the class of product_pod.\\nAnd as a matter of fact, I believe that will give us all\\nof these books here.\\n\\nAnd then inside that you can search for an H3 tag\\nwith an anchor tag inside of it.\\nAnd that's where you'll find the title.\\nFor the price, you can look apparently\\nfor a div called product_price.\\nAnd then that's in a paragraph tag.\\nFor the ratings, let's see how that works here.\\nLet's just select that like so.\\nFor the ratings, you might have to get a little bit more\\nintricate with this,\\nbut what you should be able to see\\nis that it says a paragraph,\\nand this has a class of star-rating Two.\\n\\nSo hopefully the rest of these will have either\\nThree or Four or Five next to it as well.\\nSo let's just check those,\\nand yep, sure enough, star-rating Four, star-rating Three.\\nSo what we're going to have to do is actually\\nlook at the class name here in order\\nto find out how many stars there are.\\nThat's one way to do it.\\nObviously there are other ways as well.\\nBut well, let's just give this thing a try.\\nSo here's what this is going to look like.\\nWe're going to head back to our IDE.\\nI'm going to start off by copying this\\nand just creating a new file.\\n\\nWe'll call this something like,\\nbook-site.py, (keyboard clicks)\\nand then we'll paste our code inside of there like so.\\nAnd we're just going to change this URL to the URL here.\\nI'm just going to copy and paste this whole URL like so.\\nThere we go.\\nAnd now we should be able to get that HTML\\nand do something with it.\\nSo let's give this a try now.\\nThe first thing we're going to want to do, as I said,\\nis find the, let's just scroll back up,\\nor you know what, let's just try and select this again.\\n\\nWe'll try and find the,\\nI believe it was called, product_pod, all right?\\nSo article class = product_pod.\\nThe way that that's going to work in our soup.find_all\\nis we're going to look for article tags\\nwith the class of product_pod.\\nAnd then for the title of the book,\\nwhat we're going to want to do is we're going to say\\nsomething like title = and then as I said,\\nwe're going to have to look\\nthrough this thing for an H3 tag with an anchor tag,\\nand that's going to have the text inside of it.\\n\\nSo here's what that's going to look like.\\nWe're going to say,\\nlet's see here, item.find, (keyboard clicks)\\nwe're going to look for an H3 tag,\\nand then we're going to say .find\\nand we're going to look for an anchor tag,\\nand that should have the text we're looking for.\\nSo let's just give that thing a little test run\\nto make sure that's working.\\nWe'll say something like, books = for the list here.\\nAnd then of course we'll want to append.\\nOops, I will say,\\nbooks.append, (keyboard clicks)\\nand we'll create that new book in there by saying, title.\\n\\nAnd we'll set that to title, like so.\\nCool, and let's just print out the books like so.\\nAnd let's give this a try now to see if this works.\\nWe're going to say, python3 book-site.py,\\nand sure enough, it looks like we're doing pretty well.\\nAll right, so let's go back now and we'll try\\nand get the rating and the price.\\nWe'll start off with the price, and yes, it is in pounds,\\nbut that shouldn't be a big difficulty.\\nSo let's first of all, just find the price here\\nto take a look at what we're going to need\\nto find in order to get it.\\n\\nSo sure enough, it looks like we're looking\\nfor something called price_color, right?\\nA paragraph tag with a class of price_color.\\nAnd we should be able to just get the text from there\\nin order to find the actual price.\\nSo let's give this a a try as well.\\nWe're going to say price = item. (keyboard clicks)\\nActually, I don't believe this is inside,\\nor actually, yes it is, it is inside product_pod.\\nSo let's find that paragraph tag with price_color\\ninside the item.\\nWe'll say, oops, there we go.\\nWe'll say, item.find. (keyboard clicks)\\nWe're going to find the paragraph tag\\nwith the class of product or price_color,\\nit was called, I believe.\\n\\nThere we go.\\nLet me just double check that.\\nYep, price_color.\\nAnd we should be able to get the text out\\nof there in order to get the price.\\nSo let's give that a try.\\nWe're going to say, price, price,\\nand let's give this a try.\\nSure enough, it's added the price.\\nNow, it didn't really like the pound sign apparently,\\nbut anyway, it works as well\\nas we need it to for now, so cool.\\nThe last thing we're going to do is we're going\\nto get the star-rating.\\nAnd again, in order to do that, we're going to have\\nto actually look at the class value there.\\n\\nSo here's what this is going to look like.\\nWe'll start off by looking for the class of,\\nor the paragraph tag with the class of star-rating.\\nAnd then what we'll do is we'll access\\nthe class names on there.\\nSo here's what this is going to look like.\\nWe'll get the star_rating_tag = item.find.\\nWe're going to find, as I said,\\nthe paragraph with the class of star-rating.\\nSo here's what that's going to look like.\\nWe'll say, oops, not group_photo.webp,\\nthat I'm not sure why that was in there.\\n\\nWe'll say paragraph with the class of star-rating.\\nAnd now that we found that, basically what we have to do\\nis we have to get the class names from that.\\nLet's just take a look at how that works.\\nWe're going to say something like,\\nwell, let's just print star_rating_tag.get,\\nand we're going to get the class attribute from that.\\nAnd let's just take a look at what that is.\\nSo I'll comment out print books for now,\\njust to see if we're in, we're going in the right direction.\\n\\nAnd actually look at what this is.\\nEach one of these class attributes is actually a list\\ncontaining the different class names.\\nSo we have star-rating and Two,\\nand that's really nice,\\nbecause we just have to take a look at index one\\nin order to get that.\\nAll right, so there we go,\\nTwo, Four, Three, Two.\\nSo we can now just use that as the rating.\\nAll right, so we'll say rating.\\nAll right, and we could obviously map that\\nto an actual number, but that's pretty straightforward.\\nSo I'll leave that up to you.\\nI'm just going to take this rating now and I'm going to,\\nand oops, we need the index at the end there.\\n\\nSo we'll say index one.\\nAnd let's add this now to rating.\\nWe're going to say, a rating, rating.\\n(keyboard clicks)\\nAnd that's all we need to do.\\nSo let's print out our books again\\nand run this one more time.\\nAnd sure enough, we see that that rating has been added\\nto all of these books.\\nSo anyway, this is just another example\\nof how we can use Beautiful Soup to take the pretty,\\nyou know, pretty jumbled data\\nthat's generally contained inside HTML\\nand actually convert it into something much more usable\\nin our program.\\n\\nSo I'm sure that you can think of plenty\\nof other applications for using Beautiful Soup to get data\\nand create data sites from the Worldwide Web,\\nand I really do encourage you to pursue those.\\nSo anyway, that is how to use Beautiful Soup\\nto do basic web-scraping in Python.\\n\"}],\"name\":\"4. Scrape Data from the Internet\",\"size\":78650929,\"urn\":\"urn:li:learningContentChapter:5986171\"},{\"duration\":72,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5986167\",\"duration\":72,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Skill up with Python: Summary\",\"fileName\":\"5979027_en_US_05_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":72,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4076284,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Well, we've covered a lot of ground in this course.\\nHopefully you enjoyed all the projects\\nand found them to be a fun\\nand effective way to get comfortable with the basics\\nof data science and machine learning in Python.\\nSo just to briefly recap all of the topics we covered,\\nthe first thing we did in this course was we took a look at\\nthe basics of data manipulation\\nand visualization in Jupiter,\\nand we learned about how to operate basic Jupiter notebooks.\\nAfter that, we delved into the essentials\\nof sentiment analysis in Python,\\nand we saw how that could potentially be applied\\nto some interesting data science projects.\\n\\nOur third endeavor was exploring basic image\\nand facial recognition in Python,\\nand we saw how to identify human faces in real photographs\\nas well as draw boxes around them for later use.\\nAnd lastly, to complete our exploration,\\nwe shifted our focus over to uncover the basics\\nof web scraping in Python, right?\\nWe saw how the internet can be a treasure trove of data\\nfor our data science projects.\\nSo that was the course.\\n\\nThanks again for joining me,\\nand I hope you enjoyed learning data science\\nand machine learning in Python.\\n\"}],\"name\":\"Conclusion\",\"size\":4076284,\"urn\":\"urn:li:learningContentChapter:5991100\"}],\"size\":361075324,\"duration\":8481,\"zeroBased\":false},{\"course_title\":\"Complete Guide to Python for Data Engineering: From Beginner to Advanced\",\"course_admin_id\":3896057,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":3896057,\"Project ID\":null,\"Course Name\":\"Complete Guide to Python for Data Engineering: From Beginner to Advanced\",\"Course Name EN\":\"Complete Guide to Python for Data Engineering: From Beginner to Advanced\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"Get up and running with the basics of Python before progressing to more advanced topics specific to data engineering. In this hands-on, interactive course, join instructor Deepak Goyal to practice performing a wide range of data engineering tasks in Python to boost your technical know-how, prepare for an interview, or land a new role. This course includes Code Challenges powered by CoderPad. Code Challenges are interactive coding exercises with real-time feedback, so you can get hands-on coding practice to advance your coding skills. Deepak helps you boost your skills as a Python programmer with six specific coding challenges. Explore language basics, Python collections, file handling, Pandas, NumPy, OOP, and advanced data engineering tools that use Python. The course ends with a capstone project focused on retail sales analysis.\",\"Course Short Description\":\"Practice fundamental skills using Python for data engineering in this hands-on, interactive course with coding challenges in CoderPad.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":20819005,\"Instructor Name\":\"Deepak Goyal\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Azure Cloud Solution Architect\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2024-08-01T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"Yes\",\"LIL URL\":\"https://www.linkedin.com/learning/complete-guide-to-python-for-data-engineering-from-beginner-to-advanced,https://www.linkedin.com/learning/complete-guide-to-python-for-data-engineering-from-beginner-to-advanced-coderpad-revision-fy25q1\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Advanced\",\"LI Level EN\":\"Advanced\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Data Science\",\"Primary Software\":\"Python\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":19581.0,\"Visible Video Count\":67.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":238,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4253283\",\"duration\":34,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Welcome to the course\",\"fileName\":\"3896057_en_US_00_01_4482577_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Welcome Movie Storyboard\\nhttps://docs.google.com/spreadsheets/d/1TUU9C1W7g_lj6kbRJA7Agk5RaeAkPlzjYhW-m__SG_k/edit#gid=2099676191\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":48,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the course and the overall learning goals.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1141125,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] ] Do you want to become a data engineer?\\nThen Python is the most important skill to have.\\nThis course is designed to prepare you\\nto develop the Python skills\\nneeded to become a successful data engineer.\\nYou will learn Python basics,\\nfile handling, Panda, NumPy, visualizations,\\nalong with practical examples.\\nHi, I'm Deepk Goyal,\\na senior certified Microsoft solution architect and trainer.\\nJoin me to accelerate your career in the data world.\\n\\nLet's get started.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4253282\",\"duration\":82,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"3896057_en_US_00_02_4482577_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":127,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"You do not need to have prior Python knowledge, but knowledge of coding in another programming language would be helpful. The first chapter reviews the basics but does not go into great depth.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1800715,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] It is important to understand\\nthat what is the prerequisite knowledge needed\\nto learn this Python for Data Engineering course?\\nThe good news\\nis that we don't need any prerequisite knowledge as such,\\neither about the Python or about the data engineering.\\nIn this course, we're going to start from the Python basics,\\nand we try to build all that knowledge needed\\nto learn Python from the data engineering perspective.\\n\\nAlthough, if you have any programming knowledge,\\nthen it will be a plus for you,\\nbut again, it is not mandatory.\\nFrom the data engineering side,\\nalso, we try to cover the basics\\nof the data engineering,\\nwhenever and wherever needed throughout the course.\\nAs this course is not just a theory,\\nbut a practical-driven course,\\nso for this, we're going to need the Google Colab.\\nIf you have any idea about the Google Colab, it is fine,\\nand if you are hearing this term in itself\\nfor the first time, then rest assured,\\nwe have covered how you can make your Google Colab account\\nand we have a quick tour of explaining\\nhow you can use that tool for practicing your Python.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4258006\",\"duration\":122,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"CoderPad tour\",\"fileName\":\"3896057_en_US_00_03_4482577_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":209,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about how to navigate the CoderPad environment.\",\"captionsStatus\":\"NOT_AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3528885,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":null}],\"name\":\"Introduction\",\"size\":6470725,\"urn\":\"urn:li:learningContentChapter:4258007\"},{\"duration\":3414,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4250311\",\"duration\":186,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to Python and data engineering\",\"fileName\":\"3896057_en_US_01_01_4482577_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":271,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Explore the basics of using Python for data engineering.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5379653,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Have you ever wondered\\nhow Netflix knows exactly\\nwhat movie you want to watch next,\\nor how Amazon seems to read your mind\\nand recommend the preferred product?\\nThe secret lies in the data engineering\\nand the magic wand that makes it possible is Python.\\nWelcome to the Python for Data Engineering.\\nLet's start with the basic.\\nWhat is data engineering?\\nIn simple terms, it's a process of collecting,\\nvalidating, storing, and processing data\\nto turn it into information\\nthat can be used to make decisions.\\n\\nThink of it like a factory.\\nOur raw materials,\\nor in our case, our raw data, comes in,\\nthey are processed and transformed,\\nand out come a finished product\\nor a useful information.\\nNow, question comes,\\nwhere does Pythons comes into the play?\\nPython is a programming language known\\nfor its simplicity and versatility.\\n\\nIt's like a Swiss Army knife of programming language.\\nIt's used in web development, game development,\\nand yes, in data engineering too.\\nPython is particularly popular in data engineering\\nbecause of its powerful libraries,\\nlike pandas for data manipulations,\\nNumPy for numerical computation,\\nand Matplotlib for data visualization.\\nLet's take a real-world example.\\n\\nImagine you are a farmer\\nand you want to predict the best time to plant your crops.\\nYou could collect data on temperature, rainfall,\\nand sunlight, and you can use Python to analyze this data\\nand predict the optimal planting time.\\nThat's data engineering in action.\\nOver the course of this series,\\nwe will dive deeper into the Python and data engineering.\\n\\nWe will explore Python's powerful libraries,\\nlearn how to clean and manipulate data,\\nand we'll also cover a capstone project.\\nBy the end, you will have the skills you need\\nto harness the power of data\\nand make informed decisions.\\nBut before we dive in too deep,\\nwe need to get comfortable in the shallow waters.\\nIn our next video,\\nwe will take our first step into the world of Python.\\n\\nWe will set up our coding environment\\nand write our very first lines of Python code.\\nSo if you have ever wanted to learn Python\\nbut didn't know where to start,\\nmake sure you don't miss our next video.\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4257013\",\"duration\":189,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Setting up your Python environment\",\"fileName\":\"3896057_en_US_01_02_4482577_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":270,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to set up your Python environment.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5187321,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Imagine having a powerful computer\\nthat you can access from anywhere, anytime,\\nwithout worrying about installation, configuration,\\nor even the cost.\\nSounds like a dream, right?\\nWell, with Google Colab, this dream becomes a reality.\\nFirst things first, let's understand what is Google Colab?\\nThink of it like a virtual notebook that lives in the cloud.\\n\\nIt allows you to write\\nand execute Python right in your browser\\nwith zero configuration required.\\nIt's like having a powerful Python machine\\nthat you can access from your phone, your tablet,\\nor your 10 years old laptop.\\nNow, why use Google Colab? There are certain reasons.\\nFor example, it is free,\\nit is easy to use,\\nand it comes filled with many of the libraries\\nwe will need for data engineering.\\n\\nAnd on top of it,\\nit allows us to share our work easily,\\njust like Google Docs.\\nNow let's go to the screen\\nand see how we can set up our first Google Colab account.\\nFor making Google Colab account,\\nyou need to have a Google account.\\nIf by any chance, if you are using Gmail or YouTube,\\nan account might got created.\\n\\nNow go to the URL colab.research.google.com\\nand you will find that Google Colab is loaded for you.\\nHere, you are seeing some of the notebooks.\\nThis is because I already have some notebook\\navailable for myself.\\nIt might be possible that this is completely empty for you.\\nSo on the left bottom corner,\\nyou will find here a button called + New Notebook.\\n\\nClick on that notebook\\nand this will open up the first notebook for you.\\nNow here, this portion is called cell.\\nNow in the cell, let's write our first line of code\\nthat is print Hello data engineering.\\nIf everything goes fine,\\nwhen you execute this by pressing Control + Enter,\\nthis is going to print Hello data engineering for you.\\n\\nCongratulations. It is printing Hello data engineering.\\nSo you have just run your first Python code\\nin Google Colab.\\nAnd in the next video,\\nwe will explore Python's variable and data types.\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4253281\",\"duration\":266,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Explore the Google Colab worksheet\",\"fileName\":\"3896057_en_US_01_03_4482577_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":347,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"This video explores the Google Colab worksheet.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7535335,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Imagine you are a painter\\nand you have just walked in into a new studio.\\nYou have all the tools and materials you need,\\nbut you need to know where everything is\\nand how to use it.\\nGoogle Colab is just like that studio.\\nAnd in this video,\\nlet's explore more about this Google Colab studio.\\nGoogle Colab, short for Colaboratory,\\nis a free Jupyter Notebook environment\\nthat requires no setup and runs entirely in the cloud.\\n\\nIt's like a virtual notebook that you can use\\nto write, run, and share Python code.\\nLet's start by creating our new notebook.\\nFor creating a new notebook, click on the File,\\nand here you will see an option, New notebook.\\nOnce you open a new notebook,\\nit has on our predefined name, example, Untitled23.\\n\\nIt may be Untitled01 for you.\\nNow let's just change the name of this notebook.\\nI call it like Python1.ipynb.\\nEach notebook is made up of cells.\\nSo here that rectangular gray box you are seeing\\nis called its cell.\\nNow here, when you hover around this, you will find\\nthat there are two options, Code plus Text.\\nSo this cell in itself can be of two type,\\nand based on your need, you can add any one of them.\\n\\nCode cell type is the one where you can write the code.\\nText type of cells are where you can write\\nsome information about your code.\\nFor example, you'd want to write\\ndescription about your code.\\nHere on the right-hand side corner around\\nyou will see a button called Connect.\\nWhen you click on this Connect it is connecting you\\nto a new runtime environment.\\n\\nYou can think like that\\nit is a kind of a server where your code will get execute,\\nso it is providing you the computation power.\\nAnother important feature that Google Colab gives you\\nis the auto suggestion.\\nWhen you type in something, you will find\\nthat Google Colab gives you the auto suggestion\\nto auto complete what you're trying to write.\\nSo even if you forget the entire syntax,\\nthis auto suggestion feature will help you\\nto guide in writing the right syntax.\\n\\nAnother powerful feature of the Google Colab\\nis its integration with the Google Drive\\nand other Google service.\\nFor example, if you want to save this file\\ninto a Google Drive, you can click on the File icon,\\nand here you will see an option, Save a copy in Drive.\\nUsing that, you can save the copy in the Google Drive.\\nEven at any point of a time\\nif you want to share this notebook with someone else,\\nso click on on the right-hand side top corner,\\nthe Share icon.\\n\\nYou click on the Share notebook,\\nand you can share this notebook\\nto your friends and colleagues.\\nEven at some point of a time,\\nyou want to download this specific piece of a code,\\nso again, you can click on File,\\nand here under the bottom you'll see the Download option.\\nYou also have this Revision history option there\\nif you have made some changes in your notebook\\nand you want to go back to see all the previous changes,\\nyou can see the entire revision history.\\n\\nOverall, Google Colab is a powerful tool for writing\\nand executing your Python code.\\nIn the next video,\\nwe will go a little deeper into the Python,\\nand we'll start from understanding\\nthe variables and data types.\\nSo see you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2126315\",\"duration\":319,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Variables and data types\",\"fileName\":\"3896057_en_US_01_04_4482577_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":498,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about Python data types and variables.\",\"captionsStatus\":\"NOT_AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8430830,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":null},{\"urn\":\"urn:li:learningContentVideo:4250310\",\"duration\":427,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Operators and expressions\",\"fileName\":\"3896057_en_US_01_05_4482577_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":529,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about operator expressions in Python.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11675460,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Assume you are a conductor\\nleading an orchestra.\\nEach magician plays their part,\\nbut it's their job to control how they play together.\\nIn Python, operators are like the conductor's baton,\\ncontrolling how different values interact with each other.\\nOperators in Python are special symbol\\nthat performs operation on one or more values.\\n\\nThere are several types of operators in Python,\\nbut for now we will focus on most common ones,\\nfor example, arithmetic, comparison, and logical operators.\\nArithmetic operators are used\\nto perform mathematical operations,\\nfor example, plus,\\nwhich is adding two numbers\\nor two floating numbers,\\nsubtraction or minus, which used to subtract the two values,\\nand we have another one, asterisk or division,\\nwhich is used for multiplication and division.\\n\\nLet's go to our Google Colab\\nand try these arithmetic operators.\\nLet me add one more cell.\\nNow imagine that I have a variable called num1 = 7.\\nAnother variable, num2 = 10.\\nNow if I want to do sum\\nof these two variable using arithmetic operator plus,\\nI can say that num1 + num2.\\nAnd if I use print(sum),\\nI should be able to get the sum as 17.\\n\\nIn fact, if I use another operator that is subtraction,\\nso I can say sub = num1 - num2.\\nSimilarly, let's try out mul = num1*num2,\\nand division = num2/num1.\\nAnd we can just print all that\\nto see how all these are performing, subtraction,\\nmultiplication, and this division.\\n\\nWhen I execute this and try to print it,\\nyou will see that it is printing -3, 70, and 1.4.\\nSo it is trying to do the subtraction,\\nmultiplication, and division.\\nNow this is the arithmetic operators for you.\\nNow let's go back\\nand see the another type of operators\\nthat is comparison operators.\\nComparison operators are used to compare two values.\\n\\nSome of the example of this comparator operators are equal,\\nnot equal, greater than, or less than.\\nFor example, let's just check our two variables\\nare equal or not.\\nSo I will say that var_equal = num1==num2.\\nIf num1 and num2 are equal,\\nthis variable will become true.\\n\\nOtherwise, this will become false.\\nSo let me execute and try to print.\\nYou will see that it is printing false. Why?\\nBecause my num1 and num2 is not equal.\\nIf I just wanted to check not equal,\\nI can use the not equal operator\\nlike num1 != num2.\\nNow if you see that, this will going to print me a True.\\n\\nWhy? Because num1 is not equal to num2.\\nSimilarly, if you have the integers or floating\\nor decimal type of values.\\nAnd if you want to compare based\\non some numerical value greater than or less than,\\nyou can also use that comparison operator.\\nFor example, num1 is greater than num2 like here,\\nnum1 > num2,\\nand then let me print(gret).\\n\\nThis will comes out to be very handy when we are going\\nto write these kind of conditions in our if statement\\nor for loop or those kind of a control statement.\\nDon't worry, all those control statement are coming along\\nin this course in next couple of videos.\\nNow beside this,\\nthere is another third important category of operators\\nthat is the logical operators.\\nLogical operators also used\\nto combine the conditional statement.\\n\\nFor example, you have a one Boolean condition,\\nand you want to add this Boolean condition with another one,\\nthen logical operators could be very useful.\\nIt could be AND, OR, or NOT.\\nSo these three are pretty common logical operators.\\nLet me just go back to the Google Colab and show you.\\nFor example, if we want to see\\nhow this logical operator works,\\nso if I just try to print(True and False).\\n\\nSo if you try to use the AND operator\\nbetween True and False,\\nit will definitely come out to be false\\nbecause for AND operator,\\nboth the left-hand side and right-hand side need\\nto be of true.\\nLet's say if you want to use an OR operator,\\nthen you can use a variable or a Boolean type,\\nand you will use the word called OR,\\nand then you will use another Boolean variable or a value,\\nsomething like this.\\n\\nAnd if we try to run this,\\nthis time, you will see the result as True\\nbecause for OR condition, any one of them could be a true.\\nSo that's how you can use the logical operators\\nin your Python.\\nIn our next video,\\nwe will explore Python's control structure.\\nFor example, it could be if statement,\\nfor loops, or a while loop.\\nThis structure will allow us to control the flow\\nof our code and make it more dynamic.\\n\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4250309\",\"duration\":418,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Control structures\",\"fileName\":\"3896057_en_US_01_06_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":577,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about control structures in Python.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9955214,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Imagine you are directing a movie,\\nyou have actors, you have scripts,\\nbut how do you control what happens, and when?\\nIn Python, control structures are like the director's chair,\\nguiding the flow of your code.\\nLet's learn control structures in Python.\\nControl structures in Python\\nare used to control the flow of execution in a program.\\n\\nThere are three popular types\\nof control structures in Python:\\nif statements, for loops, and while loops.\\nIf statement is used to test a condition\\nand perform an action\\nbased on the result of the test.\\nIt's like a fork in the road,\\nif the condition is true, we go one way,\\nand if it's not, we go another way.\\nLet's try it out in our Google Colab Notebook.\\n\\nLet me add another cell and declare a variable call,\\nnum = 7.\\nI am checking if this number is greater than 10 or not.\\nIf this number is greater than 10,\\nI have to print greater,\\notherwise I have to print smaller.\\nSo, after this num > 10:\\nI'm using this semicolon.\\nSemicolon is the syntax that your condition is finished.\\n\\nNow we are going into the true block.\\nAnother thing here you need to understand,\\nthat Python is having a specific indentation.\\nSo, you can see that,\\nwhen I press Enter after this colon,\\nI am having a one space forward in the next line.\\nThis is showing that, now, whatever I'm going to write\\nbelongs to that if-block.\\nIf you are coming from a Java, or a different background,\\nthere you have curly brackets.\\n\\nAnd here in Python, it's very shorthand,\\nso you don't need to add those curly brackets,\\nit automatically understand by your spaces.\\nSo, here, if I write print(\\\"Greater\\\").\\nNow, if I want to ensure that if this condition goes false,\\nI have to choose in another path.\\nSo, here I am writing else:,\\nand now if my if condition goes wrong,\\nthen whatever is in my else-block,\\nthat will go to get executed.\\n\\nSo now, you can see that this piece of a code,\\nit will try to check if number is greater than 10 or not.\\nIf it is greater, it will print Greater.\\nIf it is not, it will print Smaller.\\nNow, let me just execute this piece of a code,\\nand you can see that it is printing \\\"Smaller.\\\"\\nReason is, number seven is smaller than number 10.\\nGreat, we have learned our if statement.\\n\\nNow let's understand another control statement, that is for.\\nFor is like a repeat block.\\nWhatever you write in the for loop,\\nit is trying to repeatedly do that work for you.\\nFor example, let me type something like this.\\nfor i in range(5):\\nNow, in this case, what it's going to do is,\\nit will keep on printing the value of this i,\\nuntil unless we reach to a range of five.\\n\\nSo, for i in the range, it will start the value of i,\\nand every time, it will increment that value by one,\\nand it will go until unless it reaches to five.\\nSo here the idea is,\\nthis for statement will run five times,\\nand whatever you have written under that block\\nis going to get execute that many number of times.\\nGreat. That's how you can use the for loop.\\n\\nNow let's talk about the third type of control statement,\\nthat is while loop.\\nWhile loop is used to repeat a block of code\\nas long as the condition is true.\\nIt is pretty similar to the for loop,\\nonly the change is the way we write this loop.\\nFor example, let's say if I rewrite that example,\\nit could be number =5.\\nAnd instead of using either a for, I'm using the while.\\n\\nAnd in the while, you can put the condition,\\nfor example, number >0.\\nNow, this will go on repetitively\\nuntil unless this number is greater than zero.\\nAnd here, I'm trying to print the value of this number.\\nAnd after that, I'm doing one small thing,\\nthat is number = number-1.\\nNow, what will happen when this loop start running?\\nThe first time, the value of number is five,\\nwhich is greater than zero,\\nit will print the value.\\n\\nAnd once it print the value,\\nit will subtract the value of number by one,\\nso number will become four.\\nNext time, it will become 3, 2, 1.\\nAnd the moment this number will become zero,\\nthen this while condition will become false,\\nand this loop will terminate.\\nSo, let's just execute this.\\nAnd you can see that it starts from number 5, 4, 3, 2, 1,\\nand it keep on going till one.\\n\\nIf I do one small change here\\nand make it like while number >=0:\\nthen it will go up to zero.\\nSo you will see one extra number that is the zero this time.\\nGreat.\\nWe have learned about the control statement in Python\\nwhich allows you to control the flow of your code.\\nRemember, understanding these concepts\\nis key to write dynamic and interactive Python code.\\n\\nIn the next video,\\nwe will explore the functions of the Python.\\nFunctions are important to modularize your code.\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4256210\",\"duration\":405,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Functions\",\"fileName\":\"3896057_en_US_01_07_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":547,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about functions in Python.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10657971,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Presenter] Imagine you are building a house.\\nYou could build it brick by brick\\nor you could use pre-made sections to speed up the process.\\nIn Python, functions are like these pre-made sections,\\nreusable piece of code that make your program more efficient\\nand easier to manage.\\nLet's learn more about the functions in Python.\\nA functions in Python is a block of reusable code\\nthat performs a specific task.\\n\\nFunctions help break our program into smaller\\nand modular chunks.\\nAs our program grows larger and larger,\\nfunctions make it more organized and manageable.\\nCreating a function in Python is simple.\\nWe use the def keyword followed by function name\\nand parenthesis.\\nLet's just move to our workbook\\nand then see how we can create those functions.\\n\\nNow this time, let me create a one new notebook.\\nFor that, click on file, click new notebook.\\nHere is your notebook.\\nWe can just simply come here\\nand change the name, say functions in Python.\\nNow to create your first function,\\nlet's use the def keyword, D-E-F.\\nThat is for function definition,\\nand then you give the name of your function.\\n\\nFor example, let's have function called, Greet.\\nAnd once you define the name of the function,\\nyou have to put this column and presenter.\\nNow, whatever you write, this will become the body\\nof this function.\\nFor example, as a part of this function,\\nI just want to print, Hello world.\\nNow if somebody use this function,\\nthis function will always go and print Hello world.\\n\\nSo congratulation, we have created our first function,\\nwhich is doing nothing big but print, Hello word.\\nNow imagine that somebody want to run this function,\\nhow we can call that function?\\nSo for that click on code here\\nand calling that function needs, you have to give the name\\nof the function followed by parenthesis,\\nand just hit the run button.\\nFirst, let's click on correct to get the new runtime.\\n\\nIt'll take a while.\\nNow runtime got connected and now if I execute this cell,\\nyou will see that this is giving an another.\\nThe reason being is let's first execute\\nthis function cell as well.\\nNow our function got created\\nand if I run this cell again now it'll print Hello word.\\nNow let's try to modify this function a little more\\nand make it more effective.\\n\\nSometimes you don't just need a function\\nwhich is just not taking any parameters.\\nSometimes you want to pass few parameters as well.\\nHow you can do that? Let's see here.\\nNow this time I'm again creating this grid function,\\nbut this time I'm passing a parameter called name\\nso that I can reuse this parameter within the function.\\nMaybe for example, I want to say something like this,\\nprint hello,\\nand then whatever be the name comes in,\\nI want to give that name and that's it.\\n\\nNow let me just execute the cell\\nso that my function can create.\\nAnd here now when I call this function, I will pass a name,\\nlet's say Deepak, and when I enter run this,\\nyou will see that it is printing, Hello Deepak.\\nWhy it is happening?\\nBecause when you calling this function greed,\\nyou are passing the parameter name equal to Deepak\\nand wherever you have used this variable called name\\nwithin your function there, this name will be replaced\\nby the past value that is Deepak\\nand that's why it is printing Hello Deepak.\\n\\nNow function, not only just print the value\\nor takes the arguments,\\nthey also can return some value as well.\\nFor example, let's create another meaningful function.\\nThis time call def add numbers.\\nNow this time I'm creating a function which will take\\ntwo numbers and will return me these\\nsum of those numbers.\\n\\nNow how this can be done? This could be pretty easy.\\nWherever you want to return some value\\nfrom the function,\\nyou have to use the keyword called return.\\nAnd in that return I am passing num1+num2.\\nNow let me execute this so my function get create.\\nNow wherever I'm using this function,\\nlet's say for example I say sum is equal\\nto add numbers\\nand I pass 10,20,\\nand if I try to print the sum,\\nI should see the result as 30.\\n\\nNow see the magic.\\nHow it is happening?\\nSum is equal to add numbers.\\nThis will call a function with two values, 10 and 20.\\nThis will go to the function here\\nand this function is actually adding these two numbers\\nand returning the value.\\nReturn value will come here.\\nSo now my sum will become 30\\nand we are printing that sub\\nand that's how this is printing.\\n\\nThe answer is 30.\\nThat's it.\\nYou have learned about functions in Python,\\nwhich can take input parameters,\\nwhich can give you the output parameters\\nand you also understand how to clearly define them.\\nIn the next video,\\nwe will explore Python's module and packages.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4257012\",\"duration\":309,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Modules and packages\",\"fileName\":\"3896057_en_US_01_08_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":468,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about Python modules and packages.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9092849,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Imagine you are assembling a car.\\nYou could create every part from a scratch,\\nor you could use pre-made parts to speed up the process.\\nIn Python, modules\\nand packages are like these pre-made parts, reusable pieces\\nof code that make your program more efficient\\nand manageable.\\nLet's understand modules and packages in Python.\\nStarting with the modules,\\na module in a Python is a file containing Python code.\\n\\nThis code can define functions, classes, and variables.\\nIt's like a toolbox that we can dip into\\nwhenever we need a specific tools.\\nTo use a module, we need to import it in our program.\\nIt's like opening the toolbox\\nso we can use the tools inside.\\nLet's import our first module in Python,\\nand see how it works.\\n\\nFor that, let's go to our Google workbook.\\nClick on this file, and let's have another file.\\nI give it a name, \\\"ModulesAndPackage\\\".\\nTo import a module, type a keyword, \\\"import\\\",\\nand then your module name.\\nFor example, let's use the math module to import.\\nOnce you execute this line,\\nthis will add the math module in your code,\\nwhich will be used for doing all the mathematical operation.\\n\\nFor example, we want to find out the square root\\nof a number.\\nFor that, I can use this square root function\\nfrom this math module.\\nThat is math SQRT, and I can type a number,\\nand you will see that it will give me an output as 4.0.\\nThis is just the tip of a iceberg.\\n\\nThe math module contains many more functions\\nthat you can use in your program.\\nFor example, floor, seal, factorial,\\nand many more.\\nNow let's move back to the packages and understand.\\nA packages in Python is a way of organizing related modules\\ninto a directory hierarchy.\\nIt's like a folder that contains multiple files.\\n\\nTo use a package, we need to install it first.\\nThis is because packages are not built into Python.\\nThey are created by other developers,\\nand shared with the community.\\nNow let's again go back to our Google collab,\\nand see how can we install our first package.\\nI'm going to use a example of a Pandas package\\nto import in Workbook.\\n\\nTo import in Workbook, you have to use exclamation mark,\\nand the command \\\"pip install\\\", and package name.\\nThis code will install the pandas package\\nin our environment.\\nCongrats, you have installed your first package\\nin the Python.\\nNow let's import this package to use some\\nof its functions.\\nI call it \\\"import pandas as pd\\\".\\n\\nAnd now I'm using this panda to create a data frame,\\nwhich is more like a table in database, consists\\nof rows and column.\\nAlthough we are going to learn about this pandas\\nin quite little details in coming videos within this course.\\nNow let's create a data frame using this pandas,\\nsaying data frame is equal to pd dot data frame,\\nand I pass this data inside it.\\n\\nNow, if I try to print this data frame, this will print me\\na entire table consists of two columns and three rows.\\nThat's how we have installed a external package\\nthat is pandas, and we have used it by importing it\\nin our code.\\nIts strengths are one of the most frequently used pieces\\nin data engineering transformation.\\nIn our next video,\\nwe will explore string manipulations in Python.\\n\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4252317\",\"duration\":492,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"String manipulation\",\"fileName\":\"3896057_en_US_01_09_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":696,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about string manipulation in Python.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13917968,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Close your eyes\\nand picture yourself as a sculptor.\\nYou have a block of marbles.\\nYou can shape it, carve it\\nand polish it to create a beautiful statue.\\nIn Python, strings are like these block of marbles,\\nand with the right tools,\\nyou can shape them to fit your needs.\\nLet's explore string manipulations in the Python.\\nStrings in the Python are sequence of characters.\\n\\nThey include letters, numbers, symbols,\\nand even spaces.\\nYou can think of string as a sentence or a word.\\nIn Python, we define a string by enclosing characters\\nin single quotes or double quotes.\\nLet's jump to our Google Colab and see how we can do it.\\nLet's create our first string,\\ngiving a variable name greeting = \\\"Hello Data engineering!!\\\"\\nNow, if I want to use this variable to print it,\\nI just use the statement, print(greetings) and that's it.\\n\\nYou have created your first string\\nand you have used it to display its value.\\nNow, this is a pretty preliminary thing.\\nNow, let's do something more advanced.\\nAssume that we have a two string.\\nAnother one, let's say name,\\nand I want to combine these two strings\\nor concatenate these two strings.\\nCan I do it? Yes, you can do it.\\n\\nHow?\\nThis is the way, message =.\\nExpand a little more so that you can see it well,\\nmessage = greeting + another message that is name.\\nAnd now, if I try to check this message,\\nit will print me, \\\"Hello Data engineering!!John.\\\"\\nSo now you can see that that's how you can combine\\nor concatenate two strings together.\\n\\nSometimes, we need to replace a part of a string\\nwith another string.\\nThis can be achieved by using replace method.\\nIt's like replacing a word in sentence with another word.\\nFor example, let's say\\na new_string = greeting.replace.\\nNow here, I am replacing Hello with Hi.\\n\\nNow, if you look at the new strings which has been created,\\nthis will become Hi Data engineering!!\\nSo here, we have replaced Hello in that string with Hi.\\nEven another use case could be is\\nyou try to remove some leading and trailing spaces\\nfrom your string.\\nYou can do it using a method called strip.\\n\\nSo, strip function will go and remove those extra spaces.\\nFor example, I call sentence =.\\nI give spaces, Hello World, and then I give multiple spaces.\\nNow, if I want to trim all of those spaces around it,\\nI could have done something like this and let me enter.\\n\\nNow, if I printed this sentence directly,\\nyou can see that it has some spaces, right?\\nBecause this has not been trimmed yet.\\nIf I use here sentence.strip,\\nand if I rerun this, you will find that now,\\nthere is no leading and trailing spaces.\\nThat's how strip can be used\\nto trim those extra white spaces.\\n\\nAnother useful method is find\\nwhich returns the index of a sub string in a string.\\nIt's like finding a position of a string\\nwithin another string.\\nFor example, if I use index = greeting.find('Data').\\nNow, you remember greeting contains\\nHello Data engineering!!!\\nNow, we are trying to find out the index position\\nof this data in this greeting.\\n\\nSo, if I just go and print this index, you will find that\\nit is showing me that this data word\\ncomes at the sixth position in my entire string.\\nAnother one of the most useful method in the string\\nis format method.\\nIt's like placing the value,\\nwriting the string at the runtime.\\nFor example, I have a value something like this,\\ntemplate, hello and curly brackets.\\n\\nNow, when I'm trying to use this,\\nprobably, I'm replacing this curly brackets\\nwith a dynamic value.\\nHow?\\nI just say template.format,\\nand in that format, I will pass\\nwhatever be the value of my choice.\\nLet's say I say Deepak.\\nNow if you run it, this will going to print hello Deepak.\\nYou can see that from this template, the hello word comes\\nand this curly brackets has been replaced\\nby whatever the value you have provided.\\n\\nSo here, we provided Deepak, so it will become hello Deepak.\\nAnother few common string methods are upper and lower.\\nFor example, you want to convert your entire string\\ninto an upper letters or the capital letters,\\nso you can use greeting dot function called upper.\\nThis will going to print me all the letters\\nwithin that string into an capital letters.\\n\\nSimilarly, we have another method, lower,\\nwhich will convert each one of them into a lower case.\\nSometimes, you want to split the words in a sentence,\\nso that, you can do it through a split function.\\nFor example, I can say words = greeting.split.\\nNow automatically, this will convert my words\\ninto a list of words, and you see.\\n\\nSo here, I got three words.\\nHello, data and engineering. Why?\\nBecause in my actual string, it was separated by spaces.\\nSo, you have learned a lot of manipulations methods\\nin the string world in Python.\\nThis will be a fundamental skill in programming\\nand data engineering.\\nIn our next video,\\nwe will explore Python's error handling mechanism,\\nwhich could play an important role\\nin writing the high quality code.\\n\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4252316\",\"duration\":325,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Error handling\",\"fileName\":\"3896057_en_US_01_10_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":549,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about error handling in pynb.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9365086,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Imagine you are driving a car.\\nSuddenly a warning light appears on the dashboard.\\nIt's telling you there's a problem,\\nbut it's not stopping you from driving.\\nIn Python, errors are like these warning lights\\nand with the right tools you can handle them gracefully.\\nLet's understand it in more detail.\\nErrors in Python are inevitable.\\nNo matter how carefully you are,\\nyou will eventually write code\\nthat doesn't work as expected, but that's okay.\\n\\nPython provides tools to handle error gracefully,\\nallowing your program to recover from the errors\\nor even prevent them from happening in the first place.\\nLet's start with an example in our Google Colab.\\nLet's try to print(10/0).\\nNow you will see a zero division error,\\nwhich is Python's way of telling you\\nthat you cannot divide by zero.\\n\\nBut what if you don't want your program\\nto crash when it encounters an error,\\nthat's where the error handling comes in.\\nIn Python, we can do error handling\\nusing the try accept block.\\nThe try block contains the code that might\\ncause an error, and except block contains the code\\nthat will be executed if error occurs.\\nLet's modify our code\\nto handle zero division error.\\n\\nType try, and now inside that I'm trying to print,\\nand if something goes bad, I am saying\\nexcept ZeroDivisionError.\\nAnd if this error occurs,\\nwhat I want to do\\nis print(\\\"Sorry you are dividing by Zero\\\").\\nNow let me try to run this now this time\\nit runs very gracefully.\\n\\nWe try to divide by zero,\\nbut instead of giving an exception,\\nit is giving me, sorry you are dividing by zero.\\nThis happened because we have handled\\nthe expected error in a very graceful manner.\\nIn some cases, you might not very sure\\nwhat specific error can occur.\\nIn those case, you can,\\ninstead of typing an error type,\\nyou just type except and let's run the code.\\n\\nThis time it tried to handle any kind\\nof error which occurs in this piece of a code.\\nThat's how you can cut a general error in Python.\\nFinally, we can use the else\\nand finally blocks to add more control\\nto our error handling.\\nThe else block contains the code\\nthat will be executed if no errors occurs.\\nFor example, let me retype this code\\nand here after this except I put a code,\\nsomething like else block\\nand there I'm saying,\\nprint(\\\"Nothing wrong happened\\\").\\n\\nWe can also have an another block that is finally\\nthat will execute no matter what happened.\\nWhether you got an error,\\nor whether you don't got an error,\\nthis will execute always.\\nNow, if you run this code,\\nlet's say instead of an error,\\nI try to run this with a right number and execute.\\nThis time you'll see that this got executed.\\n\\nThis doesn't got execute because there is no error.\\nElse block get execute because we say that\\nwhenever no error occurs in our try block,\\nthis will get execute,\\nand that's why it prints, nothing wrong happened.\\nAnd finally block is always execute\\nwhether you got an error, or whether you don't got an error.\\nSo that's how this output get come.\\nThis could be, now you must be thinking\\nhow this could be useful.\\n\\nThis could be pretty useful\\nwhen you do the file read write,\\nlot of time error can come in file read write\\nthat time you can keep your code\\nwithin that try except block.\\nAnd if all goes fine,\\nyou probably wanted to do something,\\nor maybe whether the error comes\\nor the error not comes end of that block\\nyou want to close all the connections\\nso you can put that connection closing thing\\nunder that finally block.\\nThat's how you can handle the errors in the Python.\\n\\nIn the next section, we will talk about the collection.\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:66997fe3498eb00df4b54c4c\",\"duration\":300,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Code challenge: String manipulation\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:930080\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:4256209\",\"duration\":78,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Conditions\",\"fileName\":\"3896057_en_US_01_12_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":244,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"This video presents the instructor's solution to the CoderPad challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2306451,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] The question is how to reverse the string.\\nFor reversing a string,\\nremember that a string is a sequence of characters.\\nSo what we are going to do, we are going\\nto use the slicing operator on the string\\nbecause string is the list of character also.\\nSo what I'm going to do is I will say that return text::-1.\\n\\nNow understand this.\\nWhen I'm putting the first colon, I'm saying that,\\nfor the slicing, my start is from the beginning,\\nanother colon is representing\\nthat I am slicing 'til the end,\\nand minus one is saying that slice the whole string,\\nbut in the reverse order.\\nSo if everything goes fine, this will going\\nto reverse the string.\\nNow let me click on the test my code\\nto check if it is working.\\n\\nYou can see that it is working perfectly fine.\\nSo do remember that whenever somebody asks you\\nto reverse the string, you have the option of slicing\\nin the reversing order.\\nSee you into the next video.\\n\"}],\"name\":\"1. Python Basics\",\"size\":93504138,\"urn\":\"urn:li:learningContentChapter:4253284\"},{\"duration\":1640,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4252315\",\"duration\":141,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Collection overview\",\"fileName\":\"3896057_en_US_02_01_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":216,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the different collection types in Python. Collections are crucial for storing and manipulating data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3805672,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Assume that you are a librarian\\nand you have a vast collection of books.\\nYou need to organize them by genre, author,\\nand title so that readers can find\\nwhat they are looking for.\\nIn Python, collections are like this library,\\nand with the right tools, you can organize\\nand manipulate your data efficiently.\\nIn Python, a collection is a container\\nthat holds multiple items.\\n\\nThese items can be of any type,\\nand a single collection can hold items of different types.\\nPython provides several types of collections.\\nList, a dynamic arrays that can grow or shrink in size.\\nImagine them as flexible containers\\nwhere you can keep adding or removing items.\\nTuples, immutable sequences,\\noften used to represent records or structures.\\n\\nThink of them as a fixed set of data that don't change,\\nlike the specification of a machine.\\nSets, an order collection of unique items.\\nThey are like a bag where you throw items.\\nAnd even if you throw in duplicates,\\nthe bag only keeps one of each.\\nDictionaries, key value pairs perfect for structured data.\\nImagine a massive library where each book\\nor a value can be quickly found\\nby its unique title that is key.\\n\\nIn data engineering, choosing the right collection\\ncan make your operation faster, more efficient,\\nand more intuitive.\\nRemember, while each collection has its strengths,\\nits combination of them that gives Python its power\\nin data manipulation and engineering.\\nThat's wrap up of our broad overview of Python collection.\\nWe will dive deep into each\\nof these collection in coming sessions.\\n\\nI'll see you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2126314\",\"duration\":365,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Python collections: Tuples\",\"fileName\":\"3896057_en_US_02_02_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":568,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the Python collections known as tuples. The tuple collection type is important from a data engineering perspective.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10066893,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] For a moment, think you are a time traveler,\\nand you have a device that can take you\\nto any point in time.\\nBut once you have set the coordinates,\\nthey can't be changed.\\nThis is pretty similar to tuples in the Python.\\nLet's understand more.\\nIn Python tuples, you can have items\\nwhich are ordered and unchangeable.\\nIt means that they are immutable.\\n\\nOnce you have created a tuple, you cannot change it.\\nYou can access them via index as well.\\nLet's understand that by an example.\\nCreate an employee tuple which represent a row\\nin employee table.\\nFor example, employee = (),\\nand there you can type employee ID, name,\\ncity, and maybe phone number.\\n\\nThat's it, this is your tuple.\\nIf you want to print that tuple,\\nyou just simply say print and that tuple name.\\nThis will go and to print the tuple for me.\\nRemember, tuple creation can be done\\nwhile defining under the brackets.\\nAlso, one thing you notice that this tuple,\\nelements are heterogeneous,\\nlike first element is my integer, second a string,\\nthen another string, and then integer.\\n\\nSo it is not mandatory that all\\nof the tuple elements are of the same type.\\nThey could be of heterogeneous type.\\nIn fact, even if you want to access any\\nof the specific element, you can access them via index.\\nSo you can give the index something like one.\\nSo this will going to print me the name that is Deepak.\\nRemember that the index are zero indexed.\\n\\nIt means that the first position would be zero,\\nsecond position would be one.\\nSo if you want to get this second element from the tuple,\\nyou will use the index one.\\nThere is another trick.\\nImagine you have to print the last element of this tuple\\nand you don't know the size of this tuple.\\nIn that case, either you can use this function that is len,\\nwhich gives you the length of this tuple\\nto get the size of the tuple,\\nand then you will print the last element based on that.\\n\\nOr else you can use the indexes -1.\\nSo the -1 index represent the last element.\\nAnother most important point\\nin the tuple is tuples are immutable.\\nWhen we say immutable,\\nit means that once we have created this tuple,\\nI cannot change it.\\nEven if I try to change, it will not going to work.\\n\\nSo the workaround could be is if you want to change,\\nyou can convert this tuple into the list.\\nSo you can call it like new_emp = list(employee)\\nbecause list are mutable in nature.\\nSo we will come here\\nand we will just simply change the element\\nin the list by saying new_emp[1] = 'Deepak Goyal'.\\n\\nAnd then I can reconvert word this list into the tuple.\\nSo I again say that employee = tuple new_emp.\\nNow if I go down and try to print the employee,\\nit'll going to print the new name as Deepak Goyal.\\nSo remember, tuples are not mutable.\\nYou cannot change it directly,\\nbut you convert the tuple into the list, change the element,\\nand then again reconvert from list to the tuple\\nto make that change.\\n\\nOne more thing.\\nSometimes, you want to iterate upon this tuple.\\nYou can do that. It would be very easy.\\nYou can say like for x in employee.\\nSo this will going to give me all the elements available\\nin this employee tuple.\\nAnd if I go print this,\\nthis will go line by line and printing one element each.\\nIn some cases, you might want it\\nto find the position of any specific item.\\n\\nThat also you can do by using the function called index.\\nSo you can say print(employee.index),\\nand then you give your item.\\nFor example, I want to find the index\\nof my city in the employee tuple.\\nThis will give me the indexes two.\\nSo this element Indore is available\\nat third position in my tuple,\\nand that's it.\\n\\nYou have learned more about the tuples\\nin the Python including how to access,\\nhow to update, how to iterate,\\nand how to use variety of couples method.\\nNow it's time to understand another interesting collection\\nin the Python that is list.\\nList could be very useful in data manipulation.\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4256208\",\"duration\":328,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Python collections: Lists\",\"fileName\":\"3896057_en_US_02_03_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":459,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the specific Python collection type known as lists.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9993523,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] For a while assume\\nthat you are a project manager\\nand you have a dynamic to-do list.\\nAs tasks get complete, new ones emerge.\\nPriorities shifts and some tasks get dropped\\nand Python lists are like this ever evolving to-do list.\\nLet's understand more.\\nLists are used to store multiple items in a single variable.\\nLists are ordered,\\nmutable, means you can change it,\\nand they allow duplicate values.\\n\\nWhen we say that it is ordered,\\nit means that the view you have inserted,\\nthat order gets remembered.\\nItem number one, item number two,\\nitem number three, like that.\\nLet's understand lists more with an examples.\\nLet's start by creating our first list\\ncalled tasks = ['development', 'testing', 'monitoring']\\nNow if you want to print this specific list,\\njust use print function and type tasks.\\n\\nThis will print your first list in Python.\\nNow, in case you don't want\\nto use the entire list at a time,\\nyou want to access a specific element,\\nyou can access it via index.\\nFor example, if in the curly brackets,\\nif I say tasks[1],\\nthis will going to print me the second task\\nthat is the testing.\\nRemember, the indexes in the list are zero indexed,\\nso zero position would be the first one.\\n\\nAs I said at the very start, that list are mutable.\\nIt means that if I want to change the element\\nwithin the list, it is possible.\\nHow?\\nFor example, I want to change the element\\nat the second position.\\nI will say tasks[1] = 'New_testing'.\\nSo now if I go\\nand try to print tasks list entirely,\\nyou will find that this will going\\nto print me a 'New_testing' as a second element.\\n\\nThat shows that our list are mutable in nature.\\nSometimes you want to append more number\\nof elements within the list.\\nYou can do that by using an append function.\\nYou'll say tasks.append,\\nand then you can provide another element,\\nfor example, 'Production_deployment'.\\n\\nNow, if we try to check our tasks list,\\nyou will find that it has now become four element.\\nIn fact, you can also insert the element\\nat a specific index also.\\nFor example, I can say that tasks.insert,\\nand then I can say that\\ndon't change the first position element,\\ninstead just add in another one at this position,\\nfor example, 'Unit testing'.\\n\\nNow, if I try to check my list again,\\nyou will find that it has now become five elements\\nand this new element got added up at this index.\\nYou can also remove the element from the list\\nthat you can do using a remove function.\\nEither you can give the name of it\\nor you can try to remove based on the index.\\nFor example, if I say 'Unit testing',\\nthis will going to remove the unit testing element\\nfrom this list.\\n\\nYou can also remove it by the index\\nby using a function called pop,\\nand inside that you can provide the index number.\\nNow this pop function will remove the second element\\nfrom this list.\\nLastly, in case if you want to simply iterate on the list,\\nthat is also possible.\\nYou can just say for x in the list\\nand then you just simply try to iterate on that list.\\n\\nPython list comes with variety of methods.\\nWe've already seen append, remove, pop,\\nindex, and two others.\\nIt serves a unique purpose in manipulating the list.\\nRemember that the difference between the tuple\\nand the list is tuples are immutable\\nwhile lists are mutable.\\nIn the next video, we'll talk about the sets\\nin the Python collections.\\n\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4256207\",\"duration\":280,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Python collections: Sets\",\"fileName\":\"3896057_en_US_02_04_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":438,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the specific Python collection type known as sets. The set collection type is important from a data engineering perspective.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8439706,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] To understand sets with real world example,\\nassume that you are a detective.\\nShifting through clues, you have a pile of evidence,\\nbut you only want unique pieces.\\nNo duplicates.\\nIn Python, sets are like this collection of unique clues.\\nLet's understand it more.\\nLet's start by creating a simple set\\nof unique tasks in our Google Colab Notebook.\\n\\nAgain, here, let task equal to curly brackets.\\nRemember, in the list we have given the square brackets.\\nHere, in the task, we are giving the curly brackets.\\nThere we can add the task, for example, development,\\ntesting\\nand\\nmonitoring.\\nNow, if I go and print this specific sets,\\nit will print me three items.\\n\\nNotice something?\\nSee, sets does not have an order.\\nSo here at the time of creation you have given something,\\nbut at the time of printing, it is printing in random order\\nbecause sets are not ordered.\\nWhile our list and couples were ordered collections.\\nIn fact, sets cannot have two items with the same value.\\nFor example, if I recreate this task set\\nwith a duplicate item like testing,\\nnow here you will find that\\ntask will just bring me only the three values\\nand it will remove that duplicate testing.\\n\\nSo, sets are meant to be holding the unique values.\\nLike the list in the sets also,\\nyou can add more items using the function called add,\\nand then you can provide a value, broad deployment.\\nAnd then let's print that.\\nEven you can make two sets,\\nand you can add one set to another set.\\nHow? Let's see here.\\n\\nLet's say new task\\nis equal to\\nunit testing\\nand SIT testing.\\nNow, if I want to add this specific set on an existing set,\\nI can use an update method\\nand there I can pass this a new set.\\nThis will going to update the task set for me.\\n\\nIf now I try to print the name is new task, yeah.\\nNow if I go and try to print it, you can see that new task\\nhas been added up into an existing set,\\nusing the update method.\\nLike an addition,\\nyou can also similarly remove the elements.\\nHow?\\nYou just have to use the method called remove.\\nAnd there you can give the item which you want to remove.\\n\\nFor example, I want to remove testing,\\nand if I just try to check by printing it,\\nyou will find that this will get removed.\\nIn fact, if you completely want to empty your set,\\nyou can use the clear method.\\nFor example, new underscore tasks dot clear.\\nAnd if you go and\\ntry to print this,\\nnew underscore task, it will comes as empty.\\n\\nBesides this, there are several other\\nmore methods in the sets.\\nFor example, you can do union, you can do intersection.\\nAll that is possible.\\nSets a powerful tool for ensuring uniqueness in your data.\\nIn the realm of data engineering where data activity\\nand quality are paramount, sets can be invaluable.\\nIn our next video, we'll continue our journey\\nthrough Python collection, focusing on dictionaries.\\n\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4250308\",\"duration\":462,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Python collections: Dictionaries\",\"fileName\":\"3896057_en_US_02_05_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":787,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the specific Python collection type known as dictionaries. The dictionary collection type is important from a data engineering perspective.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14080811,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Imagine you are in a vast library\\nwhere each book has a unique title that helps you find it\\namong thousands of others.\\nIn Python, dictionaries work similarly\\nwhere each piece of data has a unique key.\\nLet's understand it in more detail.\\nIn Python, dictionaries are like indexes.\\nThey help us quickly locate and manipulate\\na specific data point in large data sets.\\n\\nA dictionary is a collection which is ordered,\\nmutable and it does not allow duplicates.\\nAs of Python version 3.7, dictionaries are ordered.\\nIn Python 3.6 and earlier, dictionaries was unordered.\\nNow, let's go back to our Google Colab\\nand see how we can use dictionaries.\\nDictionaries are written with curly bracket\\nand have keys and values.\\n\\nLet's start by creating a simple dictionary,\\nwhich is of a user type,\\nand let's call a name, which has a element called name,\\na city, and I'm going to add the age, as well.\\nNow, if you want to print this dictionary,\\nwe can just say print and give the dictionary name.\\nThis will print me an entire dictionary.\\n\\nIn case I just want to print a specific value,\\nso I will give the index as a key,\\nsaying print(user), and in the brackets,\\nI will type that key name.\\nFor example, city.\\nThis will try to print me the city\\nof that specific dictionary.\\nThere is also a method called get.\\nThat will also give you the same reserve.\\n\\nFor example, user.get, and there, I can pass the key.\\nThe keys method returns you a list\\nof all the keys in a dictionary.\\nFor example, I would say keys = user.keys.\\nAnd if I just print these keys,\\nyou will see the entire list.\\nSimilarly, we can also get the values from us dictionary\\nusing function called values,\\nand then we can print that.\\n\\nAnother interesting method is to data mine\\nif a specific key exists in a dictionary or not.\\nThe way of doing this would be using the if function.\\nWe'll say if 'name' in user.\\nSo, I'm trying to check whether that specific key exists\\nin this or not.\\nSo, if this is available, when I try to run it,\\nit will say yes.\\n\\nThe key with name as name is available in the user.\\nThat's why it is showing yes.\\nEven if you want to change some of the values\\nof a specific key, you can change it like user\\nand you give the city something like this\\nand then you pass a new value.\\nLet's say Mumbai.\\nAnd if you try to print the dictionary,\\nnow you will see a different values.\\n\\nAt some point of a time, you might want to add\\na one more entire key value into the dictionary.\\nYou can do that using an update function.\\nYou will say that user.update,\\nand in the brackets you will pass another element.\\nLet's say I will say mobile\\nand then I can pass some number.\\nNow, if I try to print my user dictionary,\\nI will get a new key value got added up.\\n\\nAt some point of a time,\\nyou might also want to remove the specific key value.\\nHow you can do that? You can do it using a pop method.\\nYou will say user.pop,\\nand in that, you will pass that specific key.\\nFor example, I pass mobile.\\nNow, if I try to print this user dictionary,\\nyou will find that the mobile key value is remote.\\n\\nFor example, if you want to iterate this entire dictionary\\nbased on the keys, you can say something\\nfor x in user.keys,\\nand then, you'll probably use like print(user) keys.\\nSo this will going to print the values for all the keys.\\nNow, there is a one important thing you need to notice\\nis that you cannot copy one dictionary from another one.\\n\\nFor example, if I say new_user = user, in this case,\\nboth the dictionary new_user and user\\nwill point to the same dictionary.\\nIf by any chance I say,\\nlet's say I add a few more element\\ninto this new dictionary,\\nthis will get added up\\ninto another user dictionary, as well.\\n\\nFor example, I will say mobile is equal to this.\\nNow, if I go and check my user dictionary,\\nthat will also got change.\\nFor example, if I add a new key value\\nin this new_user dictionary by saying user.update,\\nand there, I define something\\nmobile equal to a random number.\\n\\nNow, this will probably get added up\\ninto my old user dictionary, as well.\\nSo, remember that the dictionaries,\\nyou cannot copy like this.\\nIf you want to just create a copy of it,\\nin that case, you have to use something like this\\nusing a copy function.\\nNow in that case, only the values get copied.\\n\\nNow, if you make any change into the new_user,\\nthis will not get changed into a actual user dictionary.\\nThat's wrap up our exploration of dictionaries in Python.\\nThey are the condo store in data engineering,\\naligned for efficient data retrieval and manipulation.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:669980533450c1ef4bffe550\",\"duration\":300,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Code challenge: Analyze list\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:930082\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:4256205\",\"duration\":64,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Collections\",\"fileName\":\"3896057_en_US_02_07_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":74,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Compare your challenge solution to that of the instructor.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1771824,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] Problem statement says\\nwe have to find out the smallest number from the list.\\nThis would be pretty straightforward.\\nReason being is there is a built-in method available\\nin which if you pass the list,\\nthis will give you the smallest number out of it.\\nThe method name is min.\\nSo the min is a built-in function.\\nAnd in that, if you pass your list,\\nthat is the numbers,\\nthis will automatically give you the smallest number.\\n\\nLet me just click on test my code,\\nand you can see that we are getting the smallest number\\nfrom the list.\\nIn the bottom, you can see that the number list is this,\\nand out of that, the smallest number is five.\\nSo the way to find out the smallest number from the list\\nis simplest way using the min function.\\nSee you into the next video.\\n\"}],\"name\":\"2. Python Collections\",\"size\":48158429,\"urn\":\"urn:li:learningContentChapter:4250312\"},{\"duration\":971,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4253280\",\"duration\":141,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"File I/O overview\",\"fileName\":\"3896057_en_US_03_01_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":211,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the basics of file input and output in Python. File I/O is crucial for reading and writing data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4416480,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's assume yourself as a librarian\\nwho is organizing a library of information.\\nYou have books, each containing valuable knowledge.\\nIn the world of programming, files are like these books,\\nand we need tools to read and write their content.\\nLet's understand it in more detail.\\nIn data engineering,\\nworking with files is like handling data in the rawest form.\\n\\nFrom logs and configuration to structured data and reports,\\nfiles are at the core of data manipulation.\\nPython provides various ways to read and write files.\\nAs per the project need,\\nmany times, you might need to read the files\\nof a specific format, for example, CSV, JSON,\\nXML, Excel, and few others.\\n\\nHowever, from the data engineering perspective,\\nCSV, JSON, XML, and Excel are most commonly used.\\nIn the real-world scenario,\\nfile reading/writing comes very handy.\\nFor example, you need it when you want to analyze\\nthe server logs,\\nor maybe you want to analyze the data available in CSV,\\nor you may have IoT device telemetry data available\\nin JSON format that needs to be processed.\\n\\nOne of the biggest challenges\\nwhich generally comes while reading\\nand writing the files in programmatic manner in Python\\nis handling the bad data within the files,\\nwhich might result in lot of errors.\\nHence, handling errors in effective manner\\ncould be one of the important skill.\\nIn coming videos in this section,\\nwe will dive deeper into the specific file formats\\nlike CSV, JSON.\\n\\nWe will explore their structures, method,\\nand how they play a crucial role in data engineering.\\nStay tuned, and see you in the next video.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2128324\",\"duration\":360,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Working with CSV files\",\"fileName\":\"3896057_en_US_03_02_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":518,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to read from and write to CSV file formats. CSV data sources are one of the common file formats in the data engineering world.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14315732,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Think from a data detective perspective.\\nWho is looking to get treasure of hidden information,\\nyou have got data neatly organized in rows\\nand columns like a spreadsheet.\\nIn the world of data engineering,\\nthis kind of a data can make your treasure dream come true.\\nA CSV file is like a table with row and columns.\\nEach row represent a record\\nwhile the columns represent attribute or fields.\\n\\nValues are separated by commas.\\nHence that's why it is called comma separated.\\nPython makes it easy to read CSV files.\\nYou can use the CSV module to read the CSV file.\\nLet's go to our Google Colab workbook\\nand see how we can read the CSV file.\\nThe first thing probably you need to read a CSV file\\nthat you need to have a CSV to read.\\n\\nEither you can click on this files tab on the left\\nand upload here your own CSV file\\nor else for learning purpose,\\nyou can use the sample CSV given.\\nFor now we are using the sample CSV here.\\nNow, the first thing that we should do is import CSV module.\\nThis will help us to get all the CSV functions.\\n\\nNow we try to use the function open\\nto read the CSV file.\\nNow for that we need a path of a CSV file.\\nSo just right click on any one of the CSV here\\nand copy the path.\\nLet's paste that path here\\nand then you have to give the mode.\\nFor now, we just want to read the file.\\nSo I keep the mode as R\\nand then as file.\\n\\nNow let's read the CSV file\\nusing the function called csv.reader.\\nand in that we will pass our file object.\\nNow using our iterator,\\nwe can iterate upon each row\\nas we move and let's say we want to print the each row,\\nI just use the function print\\nand I will print this row object and that's it.\\n\\nLet me execute this.\\nIf everything goes fine, you can see that we are able\\nto read this CSV file.\\nSo congratulations, you have read your first CSV file.\\nHowever, at some point of a time,\\nyou just don't only want to read the file.\\nMaybe you are interested in writing the file as well.\\nFor that also, things almost remain the same.\\nYou have to use your CSV import,\\nthen you have to have some data available.\\n\\nSo let's say for example,\\nI have a data =,\\nlet's assume we have a data something like this,\\nwhich has a one row as in header\\nand two row as in like a normal data row.\\nNow, if I want to save this information in terms of a CSV,\\nI will again use the same thing with open.\\nNow I give whatever be the name of CSV file\\nthat I want to write.\\n\\nI call it like output.csv.\\nAnd then this time, at the time of giving mode,\\nwe choose the mode = 'w'\\nbecause this time we want to write the file\\nand say as file.\\nNow this time we choose instead of the CSV reader,\\nwe choose the csv.writer.\\nSo I call it that function csv.writer.\\nAnd there I pass this file object.\\n\\nNow to write I have to just say\\ncsv_writer.writerows\\nand in that rows I will pass this data,\\nwhich I have created and that's it.\\nSo if I just execute this\\nand everything goes fine,\\nour data output file will got create.\\nLet me just come here and say refresh.\\nHere you can see that our output.csv got generated.\\n\\nLet me click on that output.csv\\nand here you can see that that data is coming\\nwith name, age, and then your third parameter,\\nthat is country.\\nSo that's how easy it is to read and write the data\\nusing CSV in Python.\\nBeside this, there is another way of reading\\nand writing the CSV file\\nand that is using the Pandas library.\\n\\nWhat we have seen for now is a classical way\\nof dealing with CSV.\\nIn the later part of this course,\\nwe will deep dive into the Pandas library\\nand we'll analyze CSV file with it.\\nThat's conclude our exploration\\nof the working with CSV file.\\nIn the next video,\\nwe will see how to manage JSON files with the Python.\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2126313\",\"duration\":306,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Working with JSON files\",\"fileName\":\"3896057_en_US_03_03_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":494,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to read from and write to JSON file formats. JSON data sources are one of the common file formats in the data engineering world.\",\"captionsStatus\":\"NOT_AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9752484,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":null},{\"urn\":\"urn:li:learningContentAssessment:669980853450195708ad604a\",\"duration\":300,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Code challenge: Read and write text to file\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:930085\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:4258005\",\"duration\":164,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: File handling\",\"fileName\":\"3896057_en_US_03_05_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":201,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Compare your challenge solution to that of the instructor.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4520673,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"`(upbeat music)\\n- [Instructor] Let's understand the problem statement.\\nThe problem statement says, first, we have a string\\nwhich we want to write in a file.\\nThen we have to read the same file,\\nand then we need to ensure that\\nwhatever we have written in the file,\\nare we getting the exact same string or not?\\nNow, for this purpose, we are going to use our knowledge\\nof file input output that we have learned\\nduring this module.\\n\\nSo how you can write the file, the function is with open,\\nand let's have our file name as example.TX3.\\nNow I want to write the file.\\nSo I will open it in the mode that is W.\\nW and I will say S file.\\nNow what I can do is I have to say file.write so\\nthat I can write the text inside it\\nand the string which I want to write inside it,\\nis available to me as an input parameter here\\nthat is text.\\n\\nSo I will just pass the text here.\\nThis will going to write the input text\\ninto a file called example.txt.\\nNow the other piece of the problem statement says we have\\nto read the same file.\\nSo I will say with open, again, same example.TX3,\\nand this time I want\\nto open this file into the read mode.\\nSo I'll say read S file.\\n\\nNow to read the text out of this file,\\nwhat I can do is I will say that result equal to file.read.\\nThis will read the text of the file\\nand return it into the result variable.\\nAnd finally, I can pass on this result variable.\\nSo overall, you can see\\nthat in this function I have a text as an input.\\nI use the open function to write a file\\nwith the same text.\\n\\nThen I read the same file again,\\nand I use the read method to get\\nthat data back from the file.\\nAnd I return this from this function.\\nLet's click on test my code and see. Perfect.\\nThis is absolutely working fine.\\nSo that's how we have solved this problem.\\nSee you in the next video.\\n\"}],\"name\":\"3. Python File Handling\",\"size\":33005369,\"urn\":\"urn:li:learningContentChapter:4253285\"},{\"duration\":2123,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4258004\",\"duration\":202,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to pandas\",\"fileName\":\"3896057_en_US_04_01_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":295,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the pandas library.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5945400,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Imagine you have a magical van\\nthat can effortlessly transform heap\\nof data into valuable insights.\\nWell, in the world of data engineering, that van is pandas.\\nWelcome to the world of data analytics using\\nthe most popular library, that is pandas Library in Python.\\npandas is a Python library used for working with data sets.\\nPrior to the pandas, Python was majorly used\\nfor data munging and preparation.\\n\\nIt had very little contribution towards data analysis;\\npandas solved this problem.\\nUsing panda, we can accomplish five typical steps\\nin processing and analysis of data, that is load, prepare,\\nmanipulate, and analyze data.\\npandas can open the data door for you.\\npandas can open and read different types of data files\\nlike spreadsheets, CSV, JSON, XML, and many more.\\n\\nYou can sort and filter.\\nThink of it as a way of quickly putting the things in order.\\nYou can tell partners\\nto show you the data in a specific way,\\nlike sorting ages from youngest to oldest\\nor showing only the people who loves pizza.\\nYou can ask pandas to do math with your data.\\nIt's like having a calculator\\nthat can add up ages, find averages,\\nor do other calculations based on the information.\\n\\nSometimes there are missing\\nor wrong piece of information in your data.\\npandas can help you spot those mistakes,\\nand either filling the missing parts or fix the errors.\\nJust like mixing the ingredients to make a delicious dish,\\npandas lets you combine different piece of data together\\nand create new way of looking at the things.\\n\\nThink of pandas as a bridge between raw data\\nand insightful analysis.\\nIt helps you to scrub away inconsistencies,\\nfill in missing values,\\nand organize data in a way\\nthat even the most complex analytics can be done smoothly.\\nIn other words, it's like a magical toolbox\\nthat helps you organize, clean,\\nand work with data effectively.\\n\\nWhether you are a data engineer, data scientist, analyst,\\nor just someone who wants to play with numbers,\\npandas make working with data as easy as pie.\\nIn our next video, we'll be delving deeper into pandas,\\nexploring its powerful data manipulation techniques\\nand handling real-world scenarios.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4252313\",\"duration\":345,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Read files as DataFrames\",\"fileName\":\"3896057_en_US_04_02_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":432,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to read a file in as a DataFrame.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13266406,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Picture Pandas as your data detective,\\na Schlock Holmes for spreadsheets.\\nIt can scan those files, gather the pieces,\\nand assemble them into something called a DataFrame.\\nIt's like turning scattered nodes\\ninto a well organized story.\\nA Pandas DataFrame is a two dimensional data structure,\\nlike a two dimensional array\\nor just like a table with rules and column.\\n\\nNow, without wasting any time,\\nlet's get down to our Google collab notebook\\nand see how Pandas work in real time.\\nAssume that we have a CSV file\\nand we want to read it via Panda's library.\\nFor this use case, the first step would be\\nis to have that CSV loaded.\\nEither you can click on this folder icon\\nand use existing sample CSV\\nor click on this upload to load your own file.\\n\\nThis time let me load our own CSV file.\\nNow, once this file gets loaded up here,\\nright click on it and copy the file path.\\nThe first statement, which is needed to use the Pandas,\\nis to import it, that is import pandas as pd.\\nNow to read the CSV file,\\nwe can use df = pd.read_csv function,\\nand we will pass our path.\\n\\nThis will convert our file into a DataFrame and that's it.\\nYou can use just a print function\\nto print your DataFrame.\\nRemember that by default,\\nthis print function print first five rows only.\\nIn case if you want to see the entire DataFrame,\\nyou can use the two string function\\nthat is df.to_string.\\n\\nNow this function will help you\\nto print the entire DataFrame.\\nYou can check it here.\\nThe beauty of this Pandas DataFrame code is simplicity.\\nYou can see that we have read the CSV file\\nwith just one line\\nand that too in terms of rows and column.\\nIf you remember few sessions back,\\nwe read the CSV file using Python file input output method.\\n\\nThat was a little cumbersome process.\\nNow, there might be possibility\\nthat instead of having the CSV,\\nyou might be trying to analyze the IoT telemetry data\\nand that might be in the form of JSON,\\nso you can again use Pandas\\nto read the JSON file as DataFrames\\nand you can continue doing your operations.\\n\\nFor that, again, the same thing is import pandas as pd.\\nAlthough in this file,\\nwe have already imported this Pandas library,\\nso that is not needed to do it again,\\nbut I'm still repeating it in case if you are taking\\nthe code snippet directly and using it in a new notebook,\\nthen this import pandas as pd is mandatory step.\\n\\nNow, once you added this,\\nyou can read the JSON using\\ndf = pd.read_json\\nand inside that, you can pass your input file.\\nHere, you can use the existing JSON file available,\\ncopy the path and just paste it here and done.\\nYou can now just use print(df).\\nThis will print the DataFrame for you,\\nwhich is of the JSON format.\\n\\nThat's how easy to read the JSON using the Pandas.\\nSimilarly, you can also read the Excel file.\\nFor that, you have to just use df = pd.read_excel\\nand then you can give Excel path.\\nLet me upload a one Excel file for you.\\nClick on this upload icon\\nand let me select this Customer_excel.\\n\\nOnce this get uploaded, I just right click on it,\\ncopy the path and paste it here.\\nNow I can simply say print(df.to_string())\\nto print this entire Excel.\\nYou can see that this has imported this entire Excel data.\\nNow, so far we have learned the easiest way\\nof reading CSV, JSON, or Excel file using the Pandas.\\n\\nIn the next video,\\nwe will learn how we can do data cleaning\\nand its pre-processing for data analysis\\nusing the power of Pandas.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4256204\",\"duration\":435,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data cleaning and preprocessing\",\"fileName\":\"3896057_en_US_04_03_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":584,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about data cleaning and preprocessing.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17513093,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's get real.\\nMany times, you might have encountered with data\\nthat's all over the place, missing values,\\nincorrect entries and duplicate values,\\nand you have been asked to do the data analysis\\nand find business insights from such type of data.\\nFirst thing probably you would need\\nis to clean and pre-process this data.\\n\\nAnd the good news is with the help of pandas,\\nall this tasks can be done in the most easiest manner.\\nLet's go back to our Google Colab\\nand take an example of our Order.csv.\\nThis file contains the data with lot of problems\\nlike null, wrong format, duplicate and few others.\\nLet's go step by step and clean it up,\\nstarting with removing all the rows\\nwhich contains an empty cell.\\n\\nThere exists a function called dropna.\\nYou can say like new data frame = data frame .dropna.\\nThis will drop all the rules\\nwhere any column is having the null value.\\nNow, if you go ahead and use new data frame .to_string,\\nyou will get a data frame\\nwhich does not have any unique value.\\n\\nIt's happened because the first cell is not executed.\\nNow, it will work.\\nNow, you can find that all the rows here\\nwill have non null value.\\nAnd this happened because\\nwe have used a function called dropna.\\nNow, in some situation, you don't want to delete\\nall those null columns, rows.\\n\\nIn fact, if you find that there are some null values,\\nyou might want to replace it with some fillers.\\nFor example, you want to say that data frame .fillna.\\nThat means fill all the null columns\\nwith a default value 100.\\nAnd I say inplace=true.\\nThis inplace=true say that don't give me the new data frame.\\n\\nJust make the change in this data frame itself.\\nNow, what this function tries to do,\\nthis function will try to replace all the null columns\\nwith a default value 100.\\nThe true probably should be like this.\\nSo now, the new data frame, you might find that\\ncouple of columns which having a null value\\nreplaced by 100.\\n\\nNow, here is one catch.\\nThe catch is, let's say I have multiple columns.\\nFor example, I have a column called country and region.\\nNow, wherever the value is null,\\neverywhere this 100 will be replaced\\nif I choose like this, fillna(100).\\nNow, if I want to categorically say that\\nonly fill the null with some value for a specific column,\\nyou can use the same function in this manner.\\n\\nFor example, I want to say\\nfill all the values for units sold.\\nSo I can say like df[],\\nand in the code I can say that this is my column name,\\nUnitsSold, and then I can choose fillna function.\\nNow what it will do is\\nfor all the null values in this specific column\\nwill be replaced by 100\\nand it will not try to fill any other null values\\nin other columns.\\n\\nSo this way also, you can go and utilize this fillna.\\nAnother challenge could be in handling the duplicate rows.\\nTo remove the duplicate rows,\\nthere exists in another function\\nthat is use drop_duplicates.\\nThis method will drop all the rows\\nwhich is having a duplicate rows.\\nYou can say df_new = df.drop_duplicates.\\n\\nThis will going to remove all the duplicate rows\\nfrom your data frame.\\nThere might be some column where your date format\\nor some other format might not be correct.\\nIn that case, you can also sort that out\\nusing some functions.\\nFor example, I would say df[\\\"Order_date\\\"] column.\\nNow, should be equal to pd.to_datetime,\\nand here, I can define data frame,\\n[\\\"order_date\\\"] column.\\n\\nAnd I will say that here, use the format = %d%m%Y.\\nAnd if none of them matches in this format,\\nwe will error it out using errors = 'ignore'.\\nNow this will try to format the date\\ninto this specific format.\\n\\nIf the existing value cannot be formatted,\\nit won't cause any error\\nbecause I have chosen errors = 'ignore'.\\nThis will replace those error with the null\\nor the none value.\\nThat's how you can clean and format your data\\njust before answering the business question by analyzing it.\\nNow, our stage is set for data analysis.\\n\\nIn the next video, we will use this clean data\\nand learn how to use pandas to analyze it.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2126312\",\"duration\":406,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data manipulation and aggregation\",\"fileName\":\"3896057_en_US_04_04_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":625,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about data manipulation and aggregation.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12687743,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] As we already cleaned our data\\nusing data manipulation techniques,\\nnow it's time for analyzing it\\nto get the hidden information from data.\\nFor this, we again going to use the Pandas library.\\nLet's go to our notebook and see how we can do it.\\nAssume that in our clean data frame\\nfor our audit dot CSV,\\nwe want to find out the average unit solds\\nfor every category.\\n\\nThen we can find it very easily in the Pandas\\nusing the group by and average function.\\nSo my code would be something like this,\\nmy average sale equal to new underscore df dot group by\\nitem type, and here I will print out that unit sold.\\n\\nI just put it under the single codes\\nand then I just find out the mean for it.\\nNow if you look at closely, again,\\nwe doing the group buy on this data frame\\nbased on our item type that is category,\\nand we are finding out the mean of those units sold\\nfor each category.\\nAnd now we can just use print average sale.\\n\\nHere you can see that for every different category\\nof item you are seeing the unit sold,\\naverage unit sold for them.\\nIn just a few lines of code,\\nwe have calculated the average sales\\nfor each product category.\\nSometimes you would need\\nto apply the aggregation on your dataset.\\nIn that case, you can think\\nof using the data frame aggregation functions.\\n\\nIt is used to apply some aggregation across one\\nor more columns.\\nSome of those common aggregation functions are sum,\\nmax, and mean.\\nLet's take an example to see that as well.\\nFor example, I want to find out the sum\\nof all units sold.\\nSo I can say new underscore data frame dot aggregate,\\nand under the bracket,\\nI can pass those functions, sum comma mean.\\n\\nThis will help me to find out the sum\\nand mean of all the columns.\\nSo you will find that this is a sum\\nof region column, sum of mean column.\\nIdeally this makes sense only for your numerical data types.\\nSo you can see that the sum of my units sold is this much\\nand minimum of my units sold is this much.\\nAs we have not given any column name here,\\nthat's why it is doing this sum\\nand mean aggregation for all the columns.\\n\\nSimilarly, you can also use the other functions\\nlike max, average, and other functions on this data frame\\nto get those statistical values.\\nIn fact, sometimes you might not just do the data analysis\\non one file,\\nbut instead you might want\\nto combine or join two dataset based on some common value.\\n\\nFor example, I have a one more order file,\\nwhich is coming in for new month.\\nSo I pick this file\\nand I want to merge this new file\\nwith the existing one.\\nThat also can be done using the merge function\\nof our pandas.\\nHow? I can reload this new file,\\nnew order file equal to pd dot read underscore CSV\\nand let me copy the path and paste it here.\\n\\nNow I can use here\\nmerged data equal to pd dot merge,\\nour new data frame, and this new order file.\\nSo I am merging these two data frames together\\nand I am doing based on one specific column\\nthat is on order ID.\\n\\nI can just now do print\\nand see how this merged data looks like.\\nWe got the data.\\nSo the question come is why is data manipulation\\nso important or exciting?\\nImagine you are a store owner\\nand you have got a sales data and product info.\\n\\nBy merging and manipulating the data,\\nyou can pinpoint\\nwhich product category brings the most revenue.\\nThat's powerful business knowledge right there available\\nin your files.\\nData manipulation and aggregation\\ndon't just give you the answers,\\nthey give you insights on which you can act upon.\\nNow, brace yourself for the next step\\nof our journey.\\n\\nIn our upcoming video,\\nwe are diving into the data visualization,\\nthe art of turning those insights into captivating visuals\\nthat tells you a story.\\nTrust me, it's a chapter you won't want to miss,\\nsee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4257010\",\"duration\":373,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data visualization\",\"fileName\":\"3896057_en_US_04_05_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":705,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about data visualization.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11657240,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Assume you are handed a spreadsheet\\nfull of numbers, columns, and rows,\\nbut how do you quickly make sense of it all?\\nThat's where data visualization comes like a superhero,\\nturning the sea of data into clear actionable insights.\\nAnd guess what?\\nOur trustee Python library is not just for data wrangling,\\nit's got some impressive visualization tricks too.\\n\\nLet's go back to our workbook\\nand make our first basic line plot.\\nLet's import our pandas\\nand then we have to import one more extra library\\nfor our matplot.\\nNow I'm taking a sample data like this,\\nwhich is a date and have a price.\\nAnd let's first make a small data frame\\nfor this data using df is equal to pd.dataFrame.\\n\\nAnd now let's convert this date column into a date type\\nusing df single quotes date\\nequals to pd.to_datetime\\nand then let's pass data frame column date.\\nNow let's plot our first chart using df.plot.\\n\\nHere, I would say my x-axis would be date column.\\nMy y-axis would be my price column\\nand I want it to be of a line type.\\nNow let's say plt.xlabel\\nthat is Date\\nand plt.ylabel\\nequal to Price.\\n\\nAnd let's say plt.show.\\nWe have to use the data frame as D capital and let's run it\\nand we can see that our graph is ready.\\nSo our line chart is ready for the stock prices.\\nOn the bottom x-axis, you can see it is Date\\nand y-axis is Price.\\nSo that's how you can make your first basic chart,\\nthat is the line chart.\\n\\nNow, imagine that I have another dataset,\\nwhich is something like this,\\nwhich is having three items in the product\\nand sales for those three products.\\nAnd now let's make a bar chart for this.\\nHow you can do that?\\nYou can say like df.plot x equal to your Product\\ny-axis equal to your sales\\nand your kind or the type of the chart is equal to bar.\\n\\nAnd then finally, you can say plt.show.\\nHere, you can see that your plot is ready.\\nIn the x-axis, you have a Product,\\nand in the y-axis, you have the Sales.\\nNow imagine you have a data frame\\nwith two numerical columns.\\nFor example, our study and exam score.\\n\\nAnd you want to visualize the relationship\\nbetween these two variables using a scatter plot.\\nHow you can do that? Let's see this.\\nOn this data frame,\\nagain, we're going to use to plot a chart.\\nI will say df.plot then my x is equal to our study\\nand my y-axis is is equal to exam scores\\nand I will say kind equal to scatter.\\n\\nAnd then I will just say plt.show.\\nNow, this will going to display the scatter plot\\nfor this set of data.\\nSimilar to this,\\nit is easy to draw the plot for a pie chart as well.\\nLet's assume I have a data for pizza toppings.\\nNow, for this piece toppings,\\nagain, let me create the plot for this,\\ndf.plot my x-axis is equal to toppings\\nand my y-axis is equal to count\\nand kind is equal to the pie chart.\\n\\nNow, if I just go and print this plot,\\nthis will look like this.\\nRemember, these visualizations are your windows\\ninto data insights\\nthat can drive decisions and actions.\\nIn the next video,\\nwe will dive into how to write data frame as files\\nand save our analysis in a persistent manner.\\n\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4256203\",\"duration\":266,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Write a DataFrame to a file\",\"fileName\":\"3896057_en_US_04_06_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":326,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to write a DataFrame to a file.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9089918,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":null},{\"urn\":\"urn:li:learningContentAssessment:66998392498eb00df4b54c4e\",\"duration\":300,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Code challenge: Play with pandas\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:930086\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:4257009\",\"duration\":96,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: pandas\",\"fileName\":\"3896057_en_US_04_08_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":117,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Compare your challenge solution to that of the instructor.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2786499,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- The problem statement says that we have a dataframe,\\nwhich you can see in the bottom,\\ndata, which has the two columns, order_id and the price,\\nand a Pandas dataframe has been created out of it.\\nNow our goal is to find out the sum of all these prices.\\nHow I can do that?\\nThis will be, again, a pretty straightforward\\nif we use the Pandas dataframe functions.\\n\\nSo in the example, here, you can see that the function,\\nfind_sum has this dataframe as an input.\\nSo this df will give me the dataframe.\\nAnd on this dataframe I can use the column\\nthat is the price column.\\nAnd if I use the sum function of this column,\\nthis will going to give me the sum of all the prices.\\nI just click on test my code,\\nand then you can see that it works perfectly fine.\\n\\nAnd I can see that the sum is coming as 15,000.\\nSo if you look at our example window in the bottom,\\nhere in the test code, the price are 1,000,\\n2,000, 3,000, 4,000, 5,000.\\nIf you add all of them, the answer will become 15,000.\\nAnd that's what it is showing.\\nSo that's how we can solve this problem\\nusing Pandas inbuilt function that is sum function.\\nSee you into the next video.\\n\"}],\"name\":\"4. pandas DataFrame API\",\"size\":72946299,\"urn\":\"urn:li:learningContentChapter:4257014\"},{\"duration\":2239,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4257008\",\"duration\":191,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to NumPy\",\"fileName\":\"3896057_en_US_05_01_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":350,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the basics of the NumPy library. NumPy is a fundamental library for numerical computation in Python.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6095262,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Imagine this.\\nYou are standing at the edge of a vast numerical landscape,\\nand you need a powerful compass to navigate it.\\nWell, look no further.\\nNumPy is your compass,\\nyour guiding light through the world of data.\\nIn this section of the course,\\nwe are going to uncover the superpowers of the NumPy\\nand learn how it seamlessly integrates with pandas\\nto make your data analysis much faster and efficient.\\n\\nNumPy, short for Numerical Python,\\nis the backbone of numerical computing in Python.\\nIt's like having a Swiss army knife tailored for numbers.\\nYou can perform lightning fast mathematical operations\\nusing it.\\nIn fact, if you need to manipulate arrays\\nof data effortlessly,\\nNumPy is the library which you have to look for.\\nLet's just dive in\\nand see what are the different offerings\\nby this NumPy.\\n\\nManaging an array.\\nNumPy's heart and soul are its arrays.\\nWhether you are working with one dimensional data\\nlike stock market prices\\nor multi-dimensional data like images,\\nNumPy arrays are your go-to containers.\\nThese arrays play beautifully with panda's data frame,\\nallowing you to seamlessly switch between both worlds.\\nNumPy's ability to perform element-wise operations\\non arrays is where the real magic happens.\\n\\nIn case if you have to multiply each element of array\\nby 10 or X, Y, Z, with NumPy, it's a breeze,\\njust like spreading butter on toast.\\nOr imagine you want to add some specific number\\nto every element of an array\\nor perform operations on array for different shapes,\\nNumPy broadcasting makes it feel like\\nyou're performing feats of arithmetic acrobatics.\\n\\nStatistics and aggregations.\\nNumPy makes statistical analysis a walk in the park,\\ncalculation of a mean, median, the standard deviation,\\nand many more other values all in a matter of seconds.\\nAnd that is your sneak peek into the world of NumPy.\\nWith NumPy, you are not just analyzing data,\\nyou are orchestrating symphonies of insights.\\n\\nIn our next video,\\nwe will dive deeper into the world of advanced NumPy,\\nfeatures like creating of an array,\\nslicing and indexing arrays,\\nand using it to manage the missing data\\nand many more advanced operations.\\nSo see you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4253279\",\"duration\":398,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Array creation and attributes\",\"fileName\":\"3896057_en_US_05_02_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":609,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to create and inspect NumPy arrays. Arrays are a fundamental data structure in NumPy.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11207585,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's kick things off by understanding\\nhow to create arrays in NumPy.\\nIt's like having a blank canvas\\nthat you can fill with your data.\\nLet's go back to our Google Colab\\nand see variety of methods to create arrays.\\nThe first thing probably which needed to use the NumPy\\nis to import the library using import import numpy as no.\\n\\nNow, let's see our first method of creating an array\\nfrom a list.\\nFor example, my numbers list is something like this,\\n1, 2, 3, 4, 5.\\nNow, if I want to create an array out of it,\\nit could be done something like this,\\nnp.array,\\nand in that, you can pass your list.\\nNow, this will become your array.\\n\\nJust use this print\\nand you can see that you got an array.\\nNow, this is not only one method.\\nThere are other multiple functions also available\\nthrough which you can generate an array\\nwith some specific values,\\nmaybe like arrays of zero ones\\nor even some range of numbers.\\nFor that, let's say we want to create an array of zero,\\nso I would say zero_array is equal to np.zeros,\\nand then you can define the size.\\n\\nThis will going to create an array of zeros.\\nLet's just go and see how this looks like.\\nSo we got an array of five elements.\\nIn case if we want to have an array of ones,\\nwe can simply use a function called ones, np.ones.\\nAnd then again, we can define the size and that's it.\\n\\nThis will going to create an array of size three\\nwith all values one.\\nIn fact, as I said,\\nyou can also create an array of the range.\\nThat can be done through like this.\\nWe create a variable range_array\\nequal to np.arange.\\nAnd in that, you can define something like this.\\n\\nAnd if you print it,\\nyou will get to know that it has created an array\\nsomething like this.\\nSo here, you have the start one and ending is 10,\\nand it's step by two,\\nso we got 1, 3, 5, 7, 9.\\nIn fact, you can also create an array\\nfor random numbers, belongs to random integers.\\nFor example, it could be random_array\\nequal to np.random.randint.\\n\\nAnd here, you can define\\nthe random number between one to 100.\\nIf you just try to print it multiple times,\\nyou will see different values.\\nHere, we have created an array of five elements\\nthat belongs to a random number between one to 100.\\nIf you want to create totally random matrix,\\nyou can also create that\\nusing random_matrix equal to np.random.rand.\\n\\nAnd there, you can define the matrix size.\\nNow, if we try to print this random matrix\\nusing print random_matrix,\\nyou will find that there is a matrix got printed\\nof size three cross three and with some random values.\\nSo far, we have learned a variety of methods\\nto create an array.\\nNow let's just see some of the methods which can be used\\nto get attributes and understanding about this matrix.\\n\\nFor example, if you want to identify\\nthe shape of your matrix,\\nyou can choose shape equal to range_array.shape.\\nAnd then we can print the shape to check\\nwhat is the shape of this matrix.\\nOr else, if you want to identify the size of an array,\\nin that case, you can use a method called size on it.\\n\\nWe can copy this,\\nand instead of shape, I want to identify the size.\\nLet's print the size of this array.\\nYou'll get to know that the size of this array is five.\\nIn fact, you might want to know\\nthe data type of this array as well.\\nThat can also be identified\\nby just using a function called dtype.\\nSo I could say that range_array.dtype.\\n\\nThis will allow me to get the datatype for my array,\\nthat is integer in this case.\\nEven if you want to know the dimension about your array,\\nyou can use the ndim function.\\nThat gives the dimension of an array.\\nFor example, let's print the dimension of this range_array\\ndirectly using the function, using the ndim attribute.\\n\\nSo this is a one-dimensional array.\\nWe learned the variety of methods\\nand the attributes available with our array.\\nIn the next video, we're going to learn about the sizing\\nand indexing of NumPy arrays, which could be useful\\nfor doing a lot of mathematical operations\\nand exploring the advanced attributes.\\nSee you in the next video.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4252312\",\"duration\":426,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Array operations\",\"fileName\":\"3896057_en_US_05_03_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":830,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the different operations that can be performed on NumPy arrays. Array operations are key for numerical computation.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11876913,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] You are not just looking at the numbers,\\nyou are conducting a symphony of data.\\nIn this video, we are handing you the conductor's baton,\\nand showing you how to perform incredible data operations\\nusing NumPy.\\nWhether you're a beginner,\\nor experienced data wizard,\\nthe skills you acquire here will elevate your data analysis\\nto new heights.\\nLet's go back to our Google workbook and see all of them.\\n\\nNumPy arrays can perform arithmetic operations\\nelements wise.\\nIt's like a magic with the numbers.\\nLet's see that.\\nFirst, let's import our NumPy.\\nAnd let's create two arrays.\\nI call it like, array1 is equal to np.array.\\nAnd there, we're going to pass one list of element\\ncalled ([1,2,3,4,5]).\\nNow let's say we have an another array2\\nof a similar size,\\nand let's create that array2 using np.array([6,7,8,9,10]).\\n\\nNow if you want to add these two arrays,\\nit would be pretty easy to add it.\\nYou have to just say like,\\nsum = array1+array2.\\nAnd that's it.\\nIf you just go and try to print the sum,\\nyou will get an array,\\nI'm sorry, here is one extra comma.\\nAnd let's just see the print again.\\nAnd there you get it.\\n\\nSo, you can see that the sum happens element by element,\\nlike six plus one, seven,\\nseven plus two, nine,\\n8 plus 3, 11,\\nlike that.\\nIn fact, the same thing applies for subtraction as well.\\nIf you just want to subtract two arrays, element by element,\\nyou would just say like array1 - array2.\\nAnd let's print that subtraction.\\nSimilarly, the multiplication can also be done.\\n\\nWe can do that like multiplication,\\nmul = array1*array2.\\nAnd you can just check the output for the same.\\nLastly, we can also apply another arithmetic operation,\\nthat is the division.\\nThat also follows the same suit,\\ndiv = array1/array2.\\nAnd then you can check it out by printing the same.\\nAfter these arithmetic functions,\\nyou can also do couple of aggregation functions as well\\non these arrays.\\n\\nNumPy offers a range of aggregation function\\nwhich helps you to summarize your data.\\nFor example, I may want to find out the mean of my array.\\nSo I could say that mean is equal to np.mean,\\nand in that, I can pass my array.\\nThis will help me to get the mean of my array.\\nThat is 3.0.\\nSimilarly, I may want to find out the sum of this array,\\nso, I could say like, np.sum(array1),\\nand then again I can pass the array inside it.\\n\\nAnd we can check out the sum of all those elements.\\nSorry, there is a typo in the array spelling,\\nand let's again rerun it.\\nSo, we got an addition of all those elements\\nwithin an array.\\nIt also allows you to have the functions like max and min.\\nFor example, if you want to find out\\nthe maximum from your array,\\nyou can use the max function, like np.max,\\nand then again you can pass your array inside it.\\n\\nSimilarly, you also have a min function.\\nThat is, min = np.min,\\nand then, again, you can pass your array inside it.\\nIf you want to check out,\\njust print the value which you got,\\nprint(min) and print(max).\\nSo, you'll get the maximum and minimum element\\nfrom our array.\\nBesides this aggregation function,\\nyou can also do couple of the boolean operations as well.\\n\\nMaybe you want to perform element-wise comparison\\nand logical operation,\\nthat can also be done using something like this.\\nFor example, let's say I want to call it like,\\ngreater_than array.\\nAnd inside it, if I say something like this,\\narray1 >3,\\nnow if you try to print it, you will get an array,\\nwhich is a kind of a boolean array,\\nand that represent the values based on this comparison.\\n\\nFor example, the first element was not greater than three,\\nso the first element here is false.\\nThe second element, also false.\\nThe third element is false.\\nBut the fourth element is true,\\nand fifth element is also true.\\nSo, this kind of a boolean operation,\\nor the logical operations,\\nyou can do on your array, element-by-element.\\nBesides this, NumPy also allows you\\nanother interesting feature, that is broadcasting.\\n\\nFor example,\\nif you want to apply some operation on your array,\\nthen you can do that in this manner.\\nFor example, I would say that array1 +5.\\nNow what I'm trying to do here is,\\nI am adding this scalar value of 5\\nto all the elements of an array.\\nSo I would say that I am broadcasting this element, 5,\\nto all the elements of an array.\\nAnd if I just go and try to see the result,\\nyou will find that this is scalar value to all the elements.\\n\\nThis is a little different from addition of two arrays.\\nEarlier, we have seen addition of two arrays,\\nwhere we are adding, element by element.\\nBut here, we are broadcasting the single value\\nto all the element,\\nand that is called as the broadcasting in NumPy.\\nSo, we covered the couple of methods,\\nlike arithmetic, logical operations,\\nand this kind of a broadcasting in this video.\\n\\nWe have a few more operations to cover\\nthat we'll do in the next video.\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4250306\",\"duration\":501,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Indexing and slicing\",\"fileName\":\"3896057_en_US_05_04_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":795,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to index and slice NumPy arrays. Indexing and slicing are important for accessing array elements.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13966465,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's dive right in\\nand explore the magnificent world\\nof indexing and slicing using NumPy.\\nLet's go to our Google Colab worksheets.\\nNumPy arrays can be indexed just like Python lists,\\nbut with some added powers.\\nYou can also access individual element\\nby their positions in the array.\\nFor that, let's just start with importing our NumPy,\\nand create a small array for our exercise,\\nsay, array1 = np.array,\\nand let's have us some few elements inside it.\\n\\nNow, if you want to get an element,\\na specific at the position number four,\\nsecond, third, fourth, you can get it.\\nBut do remember that\\nthe index position starts from zero, okay?\\nFor example, I need to get the second element.\\nNow, my second element would be something like array1,\\nand in the bracket, instead of the 2, I have to give 1,\\nbecause it's zero-indexed.\\n\\nAnd now if I try to print the second element,\\nyou will get the second element that is 20.\\nHere, you got the output.\\nMaybe if you don't know the count,\\nand you want to have the second last element of your array,\\nyou can also get that.\\nYou have to just say like,\\nprint(array1[-2]).\\n\\nThis -2 two will try to index from the end.\\nSo you can see that, from the end,\\nthe first element would be 50,\\nand the second element would be 40.\\nSo, when I say my index is -2,\\nthis will allow me to get the second last element.\\nWe can also do the slicing.\\nSlicing means you take a piece out of the whole chunk.\\nFor example, I want to have my sliced_array,\\nwhich is something of a little lesser size.\\n\\nFor example, I would say array1,\\nand that, I would say like, 1:4.\\nNow, I am saying that I have to get the element\\nfrom the first position up to three.\\nSo, again, this is the index position I'm giving.\\nOkay? So one is representing the second element.\\nAnd up to four,\\nthat means like you get the element up to three.\\nSo, let's say if you print this,\\nyou will see something like this,\\n[20 30 40].\\n\\nSo, 20 is at the position number second,\\nthat is the index, 1.\\nAnd last element you got is 40,\\nthat is at the position number four,\\nbut index is 3.\\nSo, I'm saying get the index number one,\\nand up to four, that means four minus one.\\nSo it will go up to less one, okay?\\nSometimes you don't want to get all those element\\none by one, you get it in some step manner.\\n\\nSo that, also you can do.\\nLike step_array.\\nIf I do something like this, array1,\\nand then I say step like two colons,\\nand say 2.\\nSo what I'm saying is I need an entire array,\\nbut every second element, I want it.\\nSo if I just try to see this,\\nyou will see that it got the first element,\\nthen the third one, and then the fifth one.\\nSo we are getting this step_array\\nthat is stepped by two,\\nso every second element, I need from this array.\\n\\nIn fact, this indexing and slicing\\ncan be done on the multi-dimensional array,\\nthat is on the matrix as well.\\nFor example, let me make a matrix.\\nSo, this is my 2D array, with three rows and three column.\\nIf, in this array,\\nI want to get the second row, second element,\\nI can get that using element = matrix,\\nand in there, I pass one [1,1].\\n\\nThis will give me the second column of the second row.\\nNow just print this element,\\nand you will see that it's going to print five,\\nbecause five is our second row and second column.\\nWe can also slice this matrix into an array as well.\\nFor example, I just want to get this second row,\\nso I can just say like second_row = matrix,\\nand this time I give [1,:].\\n\\nNow, this will tell Python that I need an entire row,\\nso if I just use this print(second_row),\\nyou will see that this will print me an entire array.\\nThat is, the second row fully.\\nYou can similarly,\\nalso you can get the second column as well.\\nFor that, you have to just change 1,:\\nto the :,1.\\nSo, this will give you the second column.\\n\\nSo, second column is [2 5 8],\\n2, 5, 8.\\nBeside this slicing and getting some index-based data,\\nyou can also do some of the missing data management\\nin the NumPy.\\nHere, for example, let's say I have some data\\nwhich is containing some null values.\\nLet me create a one data, something like that, np.array.\\nAnd let's have an array like,\\n[1,2,3, np.nan, ]\\nthat is not a number,\\nand let's say 4,5.\\n\\nNow, from this data,\\nif I want to find out the elements which are missing,\\nI can use missing = np.isnan,\\nand then I can pass an entire array inside it.\\nIf you just go and print this result,\\nyou will find that we got a one boolean array,\\nwhere it will say that the first element is a valid number,\\nthat's why it is coming as False.\\n\\nOnly the fourth element is actually not a number,\\nand that's why that is coming True.\\nSo, using this isnan function,\\nyou can find out all the missing, or the null,\\nrecords available in your array.\\nIn fact, if you want to change all those elements\\nwhich is null to some specific number,\\nyou can fill that missing data also.\\nYou can say data_fill = np.nan_to_num,\\nand then you can pass your array,\\nand the number which you want to set.\\n\\nLet's say all the null number\\nI want to replace with 100,\\nso I can do that now.\\nAnd if you just check the new result,\\nyou will find that the null value, or the nan value,\\nhas been replaced with 100.\\nBy this, we have covered the indexing, slicing,\\nand we have learned how to find out\\nthe missing values within your array\\nand how to replace them.\\n\\nIn the next video,\\nwe are going to learn about the algebraic\\nand the statistical functions available in the NumPy.\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2126311\",\"duration\":348,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Linear algebra and statistics\",\"fileName\":\"3896057_en_US_05_05_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":558,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to perform linear algebra and statistics operations in NumPy. These are key mathematical operations in data analysis.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9605576,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructors] NumPy provides an important methods\\naround the linear algebra and statistics.\\nThis could be very helpful when we do\\na lot of calculations needed\\nfor advanced mathematical purpose.\\nNumPy provides methods for matrix multiplication\\nand inversion, and that is for the variety\\nof these sizes of the metrics.\\nIt also provides the ways to calculate\\nall those eigenvalues and eigenvectors.\\n\\nRemember that this could be very helpful\\nin the advanced mathematical operations.\\nIt also provide the statisticals methods\\nto calculate the means, standard deviations,\\ncorrelation coefficient,\\nand all those statistics parameters.\\nLet's just go back to our Google Colab\\nand see all of them in a practical manner.\\nFor the metrics multiplication,\\nlet's just first create our two arrays.\\n\\nLet's call it like array1.\\nThat is a two dimensional array,\\nso I created using np.array,\\nand inside, that let's have\\na multi-dimensional array pipe this way, 1,2,\\nand another one is 3,4.\\nLet's create another array that is array2 = np.array,\\nand this will also be a two dimensional array.\\nThat is the metrics with 5,6 and 7,8.\\n\\nNow, if I want to do the metrics multiplication\\nfor these two, then this can be done\\nthrough a function called .function.\\nSo I will say np.dot, and inside it, you can pass\\nyour metrics 1, comma your metrics 2.\\nAnd if you want to see the result, just print(mul)\\nand then you will see an output of this multiplication,\\nand you got 19 comma 22 and 43 comma 50.\\n\\nSometimes, you might want to do the inversion of it.\\nThat inversion can be done using another function called\\nnp.linear algebra.\\nThat is linalg.inversion,\\nand in that, you can pass in metrics\\nwhich you want to invert.\\nSo let's say I say array1,\\nand then I can just print my inversion.\\n\\nSo, you can see that this is an inversion\\nof your metrics array1.\\nIf you want to find out the eigenvalues\\nand eigenvectors, then again,\\nyou can use the method from the linear algebra.\\nLet's say like evaluate comma eigenvector = np.linalg.eigen,\\nand inside that, I can again pass my metrics\\nto get those values.\\n\\nIf you want to do some kind of statistical analysis,\\nmaybe you want to find out standard deviation,\\ncorrelation coefficient, or some hypothesis testing,\\nyou can do that, as well.\\nLet's create a one simple 1D array.\\nI call it like np.array.\\nIn the bracket, I pass 1,2,3,4,5,\\nand let's find out the mean of it.\\nSo, mean will be = np.mean,\\nand in that you can pass your entire array.\\n\\nIf you want to find out standard deviation,\\nyou can just use a function\\ncalled a standard deviation function.\\nThat is std, and then you pass data.\\nYou can print all those value using this print function,\\nand then you can check the result.\\nHere, you have to just press a column.\\nThere is one more shot typo here is in the print.\\nAnd then, you can see that we got the mean\\nand we got the standard deviation.\\n\\nEven if we want to have a correlation coefficient,\\nso let's create a one more array.\\nLike, let's call it like data2\\nand let's say is 11,12,13,14,15,\\nand let's find the correlation between this data\\nand data2 array.\\nThat, you can do it using a correlation function\\ncor = np. correlation coefficient.\\n\\nAnd inside that, you can pass your both arrays\\nand you can just print your correlation coefficient.\\nYou can also do the performance hypothesis testing.\\nThat can also be done using the statistical functions.\\nNumPy provides you the variety of this linear algebra\\nand statistical functions.\\nAll this could play a major role in the data engineering\\nand in the data science world.\\n\\nWe have done all these correlations,\\nanalysis and everything.\\nNow, time has come how to save all this information\\nin a persistent manner.\\nIn the next video, we will learn\\nhow you can persistently write all these information\\nin terms of files.\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4256202\",\"duration\":277,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Write a NumPy array to a file\",\"fileName\":\"3896057_en_US_05_06_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":385,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to write NumPy arrays to files. This allows for data to be saved and shared.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8386448,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] You have worked tirelessly\\nto uncover hidden patterns in your data\\nand now it's time to share your discoveries.\\nIn this video, we're going to learn\\nhow you can save your data in various file formats.\\nWhether you are a data analyst, scientist,\\nor just someone who loves numbers,\\nthese skills are your passport\\nto sharing insights with the world.\\nLet's just go back to a Google Colab and see it.\\n\\nNow, we have all our data\\nthat we have manipulated or calculated.\\nNow, you can save all these information\\nin terms of a CSV file or comma-separated files.\\nAlthough we have learned to save the file\\nusing file input/output or through the pandas,\\nsimilarly, you can save the CSV file\\nusing the NumPy as well.\\nLet's see how you can do that.\\n\\nFor that, let's say I have a one matrix,\\nwhich is this multiplication matrix.\\nNow, let's save this multiplication matrix here in a file.\\nUse np.savetxt.\\nNow, this savetxt function will save your data\\nin terms of a text file\\nand here, I am passing my file output.csv\\nand I am passing my matrix,\\nthat is, my matrix name is mul,\\nthat is multiplication.\\n\\nAnd I can use the delimiter\\nthat is nothing but a comma separator.\\nIf everything goes fine, we got a one file.\\nSo just see this option,\\nhere you got that output.csv.\\nDouble click it\\nand you can see that our two by two matrix got saved,\\nand this matrix came from this multiplication matrix.\\nSo that's how you can save your matrix calculation\\nin the CSV file.\\n\\nIn fact, you can also save this\\nin terms of a binary files,\\nthat is the NPY files.\\nThese file would be used to faster loading\\nand saving of NumPy arrays.\\nLet's save in that format as well.\\nLet's use np.save.\\nNow, when you do np.save function,\\nthere you need to provide a file with output.npy,\\nthat is a binary one,\\nand then you can pass on what you want to save.\\n\\nFor example, I want to save this multiplication matrix\\nand this will save multiplication matrix\\ninto the NumPy binary format.\\nIf later on somebody want to use it,\\nhe can just simply say np.load\\nand you can pass on this same file\\nto load this matrix in the fastest manner.\\nAnd you can hold this new matrix\\nmaybe in some new_matrix variable.\\n\\nSo that's how using this NumPy binary format\\nalso you can save and load your data.\\nYou can also save your information in terms of an Excel.\\nBut for that, we going to use the pandas along with.\\nSo we'll say that import pandas as pd\\nand then we can create a data frame\\nsaying, df = pd.DataFrame and inside that,\\nwe can pass our matrix,\\nand then we can again use the DataFrame function\\nto save in terms of an Excel.\\n\\nSo I can say df.to_excel\\nand then I can pass on the path inside it.\\nThis way also, you can go and save your information\\nin terms of Excel.\\nThese are the different ways\\nthrough which you can persistently save\\nall your calculations which you have done using the NumPy.\\nRemember, with NumPy, you are not just analyzing the data,\\nyou are making it accessible and actionable.\\n\\n\"},{\"urn\":\"urn:li:learningContentAssessment:6699847f498e1f4bf83c51ed\",\"duration\":300,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Code challenge: NumPy array operation\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:930088\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:4258003\",\"duration\":98,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: NumPy\",\"fileName\":\"3896057_en_US_05_08_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":385,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Compare your challenge solution to that of the instructor.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2721241,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(bright music)\\n- [Instructor] The problem statement says\\nwe have to identify the sum of all the numbers\\nfrom the list,\\nbut we have to use the NumPy arrays.\\nSo we are going to utilize our knowledge of creation\\nof the array using the NumPy functions,\\nwhich we have learned during this module.\\nThe function is array one.\\n\\nI'll just give a variable name, array one,\\nequal to np.array, and under that,\\nI can pass the list of the numbers,\\nwhich is available to me as an input argument\\nof this method, numbers.\\nSo this numbers in the bottom, you can see that.\\nIt is a two dimensional list, which I'm getting in.\\nI can pass this two dimensional list in this np.array,\\nwhich will create an array for me.\\nAnd I can just say array one dot sum.\\n\\nHere is a typo.\\nI'll just shade this.\\nYeah, so now you can see that I'm saying this array dot sum.\\nAnd if I click on this test my code,\\nyou can see that the addition of all of these number\\nis coming out to be 61.\\nSo using this sum function on top of the array,\\nit'll add all the elements of this metrics\\nand will give you the sum.\\nSo that's how you can calculate this sum.\\n\\nSee you in the next video.\\n\"}],\"name\":\"5. NumPy\",\"size\":63859490,\"urn\":\"urn:li:learningContentChapter:4250313\"},{\"duration\":666,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4253278\",\"duration\":108,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understanding classes and objects\",\"fileName\":\"3896057_en_US_06_01_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":162,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the concept of classes and objects. Many real-world problems can be break down into the classes and objects.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3232477,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] Ever wondered how applications\\nlike banking, online warehouse management,\\nsocial media platforms are built?\\nIt all starts with classes and object.\\nThink of classes as a blueprint for creating objects\\nwith certain attributes and behaviors.\\nFor example, if we consider car as a class,\\nthen its state or attribute would be color, weight,\\nmodel name, and its behavior would be run,\\nreverse, apply brakes.\\n\\nObjects are considered to be an instance of the class.\\nFor example, car represents the class,\\nwhile a specific car, like BMW, Mercedes,\\nwith specific name, color, and model number\\nwould be an object.\\nIn real world, most of the complex software system\\nbuild using the object-oriented programming\\ndesign principles.\\nThat is, the breaking of things in terms of classes\\nand object.\\n\\nAnother good example would be banking application,\\nwhere account could be the class\\nand an individual account for John\\ncould be an object.\\nClasses are reusable in nature.\\nYou define it once and can be used many time.\\nNow think about the objects\\nthat are currently surrounding you as you watch the video.\\nMake a mental list of their attributes and their behaviors.\\n\\nThink about this for a moment,\\nand then go to the next video\\nwhere you learn how you can implement\\nall of these in practical manner.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4258002\",\"duration\":225,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Implementation: Classes and objects in Python\",\"fileName\":\"3896057_en_US_06_02_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":337,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to implement classes and objects in Python. This allows for the implementation of the solution to many real-world problems in the programming world.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7229902,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's create a class.\\nWe are defining a class named Employee.\\nIt has attributes like name, position, and salary.\\nWe're also creating a method to display info\\nto show the employee details.\\nThink of this like creating a template for employee records.\\nLet's go back to our Google Colab sheet\\nand see how we can create Employee class.\\nHere, we have our class Employee.\\n\\nFor creating an Employee class,\\nyou have to give the class as a keyword\\nand then you define the name of the class.\\nClass has a innate function, which is a special function\\nthat gets invoked automatically\\nwhenever you create an object of Employee class.\\nHere it contains the self,\\nwhich is a reference to the object.\\nIt could be used to access the attributes\\nand the behaviors of the objects within the class.\\n\\nAnd this Employee class has three attributes:\\nname, position, and salary.\\nThen we have defined the operation\\nfor this specific class, that is display_info,\\nand that again, we are passing the self reference.\\nThis display_info is returning us the name, position,\\nand salary for that specific object.\\n\\nThat's how we can create the Employee class.\\nNow let's create the objects for this Employee class.\\nHere, we have created two objects for the Employee class:\\nemployee1 by passing the values like John Doe, manager,\\nand 75000 as a salary.\\nSimilarly, we have created another object employee2\\nwith some specific values.\\nYou can access the attributes,\\nall the methods of the class using the dot operator.\\n\\nFor example, if I want to access the operations\\nor the method of this Employee class,\\nI can say employee1.display_info.\\nThis will display the employee1 object detail\\nbecause I am calling it in reference to employee1.\\nIn case I want to check it for employee2,\\nI could have called it something like this,\\nemployee2.display_info.\\n\\nI can also change the specific attributes of an object\\nor I can access them as well.\\nFor example, if I want to change the salary of employee1,\\nI could say something like this, that is 100K.\\nNow, the salary for employee1 has got changed.\\nIf I want to see that, I can just again use display_info\\nand this will show me\\nthat salary has been changed successfully.\\n\\nThat's how we have learned how to create the class,\\nhow to create the object,\\nand how to access all these attributes and methods.\\nIn our next video,\\nwe will cover basic ops concept,\\nwhich help us organize our code,\\nmake it reusable and easy to understand.\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2128322\",\"duration\":182,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand OOP features: Abstraction, inheritance, and more\",\"fileName\":\"3896057_en_US_06_03_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":314,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about OOP principles. OOP features are frequently used while writing the code.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4557798,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] Ever wondered how big software applications\\nlike online banking system, online restaurant system,\\nall these are built up?\\nIt's all thanks to the incredible feature\\nthat OOPs offers.\\nObject-oriented programming makes it possible to write code\\nthat is reusable, scalable,\\nand reducing the overall development time,\\nalong with increasing the code efficiency.\\nThere are certain powerful features of the OOPs.\\n\\nLet's understand them.\\nAbstraction.\\nThink about your television.\\nWhen you press the remote, you expect the channel to change\\nor volume to adjust, right?\\nBut do you really need to understand\\nhow every tiny circuit works inside the TV?\\nNot really.\\nHere is where the abstraction steps in.\\nSimilarly, in OOPS, we have a concept of abstraction.\\n\\nWe create a class that have methods\\nto perform a specific task.\\nUsers of this class does not have to know\\nhow those tasks are carried out.\\nThey only need to know what to do.\\nIn nutshell, abstractions hides unnecessary code details\\nfrom the user.\\nLet's see another powerful concept.\\nThat is the inheritance.\\nInheritance is the capability of one class to derive\\nor inherit the property from another class.\\n\\nThe class that derives the property\\nis called the derived class or the child class,\\nand the class from which the properties are being derived\\nis called the base class or the parent class.\\nSo just like traits passed down in families,\\ninheritance lets classes share their traits and behavior,\\nmaking our code more powerful and efficient.\\n\\nThere is a one more concept in the OOPs\\nthat is called polymorphism.\\nPolymorphism, the word itself is made up of two words,\\npoly and the morph.\\nPoly means many, and morph means shape and form.\\nIn this concept, we can perform one task in different ways.\\nFor example, we could have multiple functions\\nwithin the class with the same name.\\n\\nIn nutshell, polymorphism makes our code super flexible.\\nWe can use the same method with different types of objects,\\nand it just works.\\nIt's like having a tool that can transform to fit any job.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:669984d034507dd25ce0eb73\",\"duration\":300,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Code challenge: Accessing object attributes\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:930089\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:2126310\",\"duration\":151,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: OOP\",\"fileName\":\"3896057_en_US_06_05_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":190,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Compare your challenge solution to that of the instructor.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4669145,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] The problem statement says,\\nwe have a class that is a BankAccount class.\\nIn the BankAccount class, here,\\nyou can see that we have an _init_ method,\\nand it has a two parameters or the attributes,\\naccount_id and balance.\\nBut the balance is having a default value that is 1,000.\\nIt also has a deposit method,\\nwhich will deposit the given input amount into the balance.\\n\\nAnd we have another method that is a get_balance method,\\nwhich is returning you the balance.\\nNow, the question says that, first,\\nwe need to create an object of this BankAccount class.\\nThen we have to deposit the amount,\\nand then we need to get the balance.\\nAgain, we're going to use the concept\\nthat we have learned in this module.\\nI'll give a name called as account1, that is my object name,\\nand I will use the class BankAccount.\\n\\nAnd in that, I will pass a random BankAccount number,\\nlet's say 12345 and balance will be the default\\nthat is 1,000.\\nThen let me call the deposit method on this account1 object\\nto deposit some 500 rupees.\\nOkay, we have given this amount here,\\nso let me just pull up this amount directly here,\\nwhich is available in our function.\\n\\nAnd finally, for returning it,\\nI'm going to use account1.get_balance method\\nbecause I know that this get_balance method\\nreturns me the balance.\\nThis deposit method will automatically\\nwhatever be the amount I pass, will add to the balance.\\nAnd get_balance method will return me the balance amount.\\nSo that's it.\\nJust click on Test my code,\\nand you will see that it working perfectly fine.\\n\\nThe default balance was 1,000.\\nYou have deposited 500,\\nand then you can see that the final balance is coming 1,500.\\nSo that's it.\\nSee you into the next video.\\n\"}],\"name\":\"6. OOP with Python\",\"size\":19689322,\"urn\":\"urn:li:learningContentChapter:4258008\"},{\"duration\":1254,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4250304\",\"duration\":205,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Tips to write efficient Python code\",\"fileName\":\"3896057_en_US_07_01_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":287,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to write Python code efficiently. Efficient code can run the code faster and save execution time.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4956182,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Writing efficient code\\nis like finding the quickest route to your destination.\\nIt saves time and resources.\\nSo we will explore different tips to write Python code\\nthat not only works, but works efficiently.\\nTip number one.\\nChoose the right data structure.\\nWhen dealing with large datasets,\\na set or a dictionary might outperform a list.\\nFor example, you might be using an entire array.\\n\\nIn that case, if you put that\\ninto the set or the dictionary,\\nand when you try to access it, a specific element,\\nprobably this can improve the performance.\\nTip number two.\\nMinimize function calls in the loops.\\nRemember, function is used to do the reusable piece\\nof a code, but if you call this functions\\nwithin the loops, then every time, this will get called,\\nand it might be doing a very small piece of work internally.\\n\\nSometimes over-calling the functions\\nmany times can slow down the process.\\nIt's a very similar thing that,\\nif you have a friend and if you call that friend\\nagain and again and again for a very small work,\\nit might not work in that efficient manner.\\nSame thing happens into the Python world as well.\\nIf you use the functions too much\\nfor a very, very small piece,\\nit probably slowed down the process\\nbecause there is a time it takes to start a function,\\nto assign the memory internally, and do your work,\\nand then do the cleanup internally\\nso we can get rid of all that\\nif we do not overuse it within the loop.\\n\\nTip number three.\\nAvoid global variables.\\nGlobal variables can lead to unexpected behavior\\nand slower code.\\nIt's better to encapsulate your code within functions.\\nTip number four.\\nUse built-in functions and library.\\nPython offers a bunch of built-in functions and libraries.\\nUtilizing them can save a lot of computation time\\nand boost the performance.\\n\\nFor example, think, you might be creating a function\\nto find out the maximum out of two numbers,\\nor a five numbers,\\nbut let's say we have a one max function\\navailable within the Python.\\nNow, by default, if you use that max function,\\nyou will find that that probably works much better.\\nReason being is that\\nthat function has been specifically designed\\nby using all the best practices\\nand has been tested over a large dataset.\\n\\nSo when we use the built-in function,\\nthe probability is very high that they are more optimized\\nand a better solution rather than writing the things\\nright from the scratch.\\nSo wherever possible, use the built-in functions\\nand libraries to improve the performance of your code.\\nSo these are some of the most common quick hacks\\nwhich can improve the performance of your code in Python.\\nStay tuned for the next video.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2126309\",\"duration\":207,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"What is ETL in the data engineering world?\",\"fileName\":\"3896057_en_US_07_02_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":263,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the ETL process. ETL is one of the most popular processes in the data engineering world.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5076521,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Imagine, you are a chef preparing a feast.\\nYou start by gathering the fresh ingredients,\\nthen chop, dice and season to create a masterpiece.\\nIn the data world, ETLs follows a similar process,\\nturning raw data into meaningful insights.\\nLet's break it down for you.\\nExtraction.\\nExtraction is like going shopping for your ingredients.\\n\\nYou get the data from various sources.\\nIt could be database spreadsheets, CSV, JSOD\\nor any other type of files, or even a REST API.\\nIt's like collecting all the elements you need\\nfor your recipe.\\nFor example, consider a retail chain expansion project\\nwhere the retail chain is trying to gather\\nall the data from their variety of sources.\\n\\nIt could be their databases,\\nit could be their competitor's data.\\nIt could be some data available freely\\nover the internet to analyze.\\nSo, probably the first step would be\\nis to pull in all that data, that is extract the data.\\nThe step number two comes is transformation.\\nTransformation is the place where the actual magic happens.\\nIt's like you have all the ingredients, right?\\nYou get into the kitchen, you put on all those spices\\nand season with all that seasoning and all.\\n\\nSame thing happens in the transformation.\\nIn the transformation, you have the data.\\nAfter the process of extract, you clean it,\\nyou remove all the nulls, all the outliers,\\ntry to remove the duplicates, filter it,\\nand then you finally make a data,\\nwhich is more useful, more meaningful\\nand more insightful for the business.\\nThen comes the step three.\\nThat is the loading part of the ETL.\\n\\nThe loading is something like you prepare the food\\nand now you are trying to serve it, okay?\\nAnd you are keeping it into the containers.\\nThe same thing as the loading here.\\nYou extract data, you transform it\\nby removing all those nulls and outliers or duplicate.\\nIn loading part, you go and push it,\\nor save it or store it into a final data source.\\n\\nIt could be, again, a database, a data warehouse\\nor any other meaningful data source.\\nIdea is the different users within the company,\\nrather than using the raw data,\\nnow they will point to this final destination data-source\\nto get this meaningful data for their data analysis.\\nSo, basically, in the real world,\\nyou have OLTP databases,\\ndata move from OLTP database into the data warehouse.\\n\\nThat is OLAP via ETL process,\\nand that's how your entire ETL process looks like.\\nIn the next video, we'll talk about\\nanother exciting framework that is Hadoop.\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4252311\",\"duration\":241,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand PySpark for data engineering\",\"fileName\":\"3896057_en_US_07_03_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":321,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about PySpark. PySpark is one of the most popular data analytics frameworks.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5313713,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Picture yourself as a traffic controller\\nmanaging planes in a busy airport.\\nYou need speed, precision and scalability.\\nIn the data world,\\nPySpark provides just that,\\na high speed engine for processing vast amount of data.\\nIn other words, or in a plain layman terms,\\nPySpark is a distributed computing engine\\nthat is used for data processing.\\n\\nIt's a Python library that works with powerful framework\\nlike Apache Spark.\\nThe combination allows us to process huge amount of data\\nwith incredible speed.\\nLet's understand why this PySpark\\nis so important and so popular.\\nThe first reason is scalability and the speed.\\nApache Spark, the engine which is behind the PySpark,\\nis designed to handle big data.\\n\\nIt can distribute the task across multiple machines,\\nwhich makes it incredibly fast,\\nand it also allows you to scale the solution.\\nIf today you have a five node cluster,\\nyou can increase it to any number.\\nReason being is every time words get divided\\ninto number of tasks,\\nand all those tasks can run in parallel on these nodes.\\n\\nThis speed is even faster than the speed\\nwhich you get into the Hadoop.\\nAnother benefit is distributed computing.\\nSo as I was talking,\\nthat Spark divides the work into number of tasks,\\nit's doing this using the distributed computing principle.\\nWhenever a work or your job get comes in,\\nit'll divide the work into task,\\nall those task runs on the worker nodes\\nwithin the PySpark cluster,\\nand all these runs in parallel to give you the speed,\\nand gives you the distributed computing environment.\\n\\nEventually, this helps to manage and process\\na huge amount of data,\\neven with a very small processing time.\\nAnother benefit is real time processing.\\nSome applications like fraud detection in online transaction\\nmight require an immediate response.\\nThe moment somebody is trying to make a transaction\\nand you want to catch him, that moment itself.\\n\\nSo probably that kind of real time\\nstreaming solution is needed,\\nand PySpark fortunately can provide you\\nthat functionality as well.\\nIt also benefits in streamlining the ETL processes.\\nETL process are fundamental in data engineering.\\nPySpark streamlined this process\\nby handling the extraction, transformation,\\nand loading of a data at scale,\\nreducing the time and efforts required.\\n\\nIn natural, PySpark empowers data engineer\\nto tackle the challenges posed by big data,\\nreal time processing, and complex computations.\\nIt's like having an high performance engine under the hood,\\nwhich enable the data-driven decision making\\non a massive scale.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4253277\",\"duration\":170,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"What is Hadoop\",\"fileName\":\"3896057_en_US_07_04_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":205,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about Hadoop in the big data world. Hadoop is the base for all big data solutions.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3797362,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Think about social media platforms\\nlike Facebook, Twitter, or Instagram.\\nThey have billions of users,\\ngenerating an enormous amount of data every second.\\nAnd let's say, if I ask you to analyze that trend\\nby reading all that with, probably you don't have anything\\nwhich can do that.\\nThe answer for such kind of an analysis is the Hadoop.\\nHadoop is the framework which can give you the superpower\\nto analyze even and unstructured data,\\nand that too in the form of a very huge data,\\nwhich we call big data.\\n\\nHadoop is an engine for processing\\nand analyzing the massive amount of a data.\\nYou might be wondering\\nhow they get this power in the Hadoop.\\nThe reason is that it follows the distributed computing\\nor the parallel processing.\\nSo it has a cluster.\\nSo when you work in the Hadoop, you have to have a cluster\\nof a multiple nodes,\\nand your work get divided in these number of nodes,\\nwhich works in parallel for analyzing this data.\\n\\nIt also implement default tolerance\\nbecause the moment you go to the distributed computing,\\nthere is chances that a few of the machine might go down\\nand then your entire work can get disrupted.\\nBut Hadoop handles that situation\\nby using the fault tolerance mechanism,\\nwhere it'll handle any abrupt failure of any of the node.\\nIt also have the scalability feature on.\\n\\nThat means that, today, if you have a 10-node cluster\\nand tomorrow your data size grows like anything,\\nyou can easily add 10 machines more\\nand you can double the computation power.\\nSo it has a very horizontal scalability option.\\nAnd lastly, when you have\\nthe horizontal scalability option there,\\neventually it allows him to manage a huge amount\\nof a data storage in an efficient manner as well.\\n\\nSo overall, Hadoop is the framework which helps you\\nto store a huge amount of data\\nand that can help you to analyze such a massive data\\nwith very good performance.\\nIn the next video, we'll see an another technology\\nthat is the Spark, which is even far better than the Hadoop.\\nSee you into the next video.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4253276\",\"duration\":219,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Importance of visualization tools in data engineering\",\"fileName\":\"3896057_en_US_07_05_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":306,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the importance of data visualization in data engineering. This will be a high-level theoretical overview featuring Power BI.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6561773,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] You might have heard a statement,\\n\\\"Picture can be worth a thousand words.\\\"\\nThat statement still true hold\\nin the data engineering world as well.\\nVisualization tools are like the magic ones\\nof the data engineer.\\nThey take complex data and transform it into a visual\\nthat tells a clear, compelling story.\\nFor example, assume that if I give you a GPS app\\nand instead of list of directions,\\nwhen you see a map with a clear path,\\nhow much it helps you?\\nIt guides you effortlessly,\\nmaking the journey more enjoying.\\n\\nSame thing happens with the data as well.\\nVisualization tools turns complex data\\ninto a easy-to-understand visuals,\\nproviding you the instant insights without the need\\nfor extensive data mining.\\nFor example, look at just a sales dashboard,\\na chart displaying of monthly revenue,\\nprobably can give you the complete idea\\nin fraction of a second.\\n\\nAnother important point is spotting the trend.\\nTrends might be hidden in the rows and columns,\\nbut that can be very visually apparent\\nonce we draw them in form of a chart.\\nThis allows for a quicker identification\\nof pattern entrance for a business,\\nmaybe just looking at the temperature chart\\nor a price for any stock.\\nJust by looking at that chart, you can get to know\\nthat whether it is an uptrend or it is a downtrend.\\n\\nIt also improves the data quality assessments\\nbecause when you see the visualization of a data,\\nyou can identify something is missing,\\nsomething might get duplicated,\\nor something has an outlier values.\\nSo even by looking at the visual diagram of your data,\\nyou can identify the quality of your data.\\nAnother benefit is the storytelling with the data.\\n\\nVisualization allows data engineers\\nto create narratives around the data,\\nproviding the context, and making it easier for the others\\nto understand the significance.\\nFor example, a chart showing customer demographic\\ncan help in tailoring marketing strategies\\nto a specific target audience.\\nLastly, it also helps to improve\\nthe engagements and understanding.\\n\\nPeople generally move away from the text,\\nbut they are attracted towards the pictures\\nand that strategy apply here.\\nVisual representation of a data capture more attention\\nand increases the comprehension.\\nSo probably, if you are showing some kind of a report\\nto a top management, they would be more interested\\ntowards a visual part of it, and that can put a more impact.\\n\\nIn nutshell, visualization tools are the bridge\\nbetween the raw data and the actionable insights.\\nThat's it, see you into the next video.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4253275\",\"duration\":212,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"On-premises vs. cloud data engineering\",\"fileName\":\"3896057_en_US_07_06_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":329,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Explore the comparison between on-premises and cloud data engineering. Many organizations are moving to the cloud from on-premises environments.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4572132,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] On-premises data engineering involves\\nmanaging your data infrastructure\\nwithin your own physical location.\\nThis means owning and maintaining servers, storage,\\nand networking equipments on your own.\\nOn-premises solutions gives you more control\\nand customization because, as you have a full control,\\nyou can customize your data environment\\nas per your own need.\\n\\nFor example, it's might like you're building your own house,\\nso you can decide what are all equipments you want.\\nAnother advantage of on-premises is security\\nand the compliances.\\nAs you are the full owner,\\nyou can decide what are the different accesses you want\\nto give to anyone, and you can decide\\nwho are allowed to let in and not.\\n\\nIn general, if you are looking\\nfor the highest security-based solution,\\nprobably on premises could be one of them.\\nOn the other hand, if you talk about\\nthe cloud data engineering,\\nit actually leverages the third party services\\nto store, manage, and process data.\\nThis turned the ownership to a limited piece only.\\nHere in the cloud data engineering,\\nyou are using the services offered by some third party.\\n\\nYou can correlate this with an example\\nof renting an apartment.\\nYou have access to the well-maintained space\\nwithout the responsibility and ownership,\\nand even you don't have to buy everything together\\nat the first spot.\\nYou just pay the rent, use it,\\nand as soon as you're done with your work,\\nyou move out of it.\\nSo that is the advantage of the cloud data engineering,\\nthat it has lower upfront cost\\nand you use it on a paper-use model.\\n\\nThe strength of cloud data engineering lies\\nin the scalability and the flexibility.\\nAs you are taking the things on the rent in the cloud,\\nso you have the opportunity or the facility,\\nif at any point of a time, if you want\\nto increase the number of services, you can increase it.\\nSo you have the scalability option.\\nFor example, today, you are using the two virtual machine.\\nAnd if the traffic get increased on your website\\nand you want to increase the server, it would be very fast\\nbecause cloud allows you that scalability option.\\n\\nAnother strength of the cloud data engineering\\nis the cost efficiency\\nbecause here you are just paying for the usage.\\nThere is no capex involved.\\nIt is just an opex or the operational expenditure model.\\nHence the choice between on-premises and the cloud\\ndepends on the factors like organizational preferences,\\nbudget, security, and scalability requirements.\\nIt's about finding the right fit\\nfor your unique data landscape.\\n\\n\"}],\"name\":\"7. Advanced Data Engineering\",\"size\":30277683,\"urn\":\"urn:li:learningContentChapter:4252318\"},{\"duration\":3012,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4250303\",\"duration\":420,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"HTML basics\",\"fileName\":\"3896057_en_US_08_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":592,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn the fundamental concepts of HTML for web scraping. This is essential for understanding page structure.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10015545,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Understanding HTML is crucial\\nbecause it forms the backbone of web pages.\\nAnd knowing how to navigate and extract information from it\\nis a powerful skill in your data engineering toolkit.\\nSo, let's dive in and start decoding\\nthe structure of web pages.\\nWhat is HTML?\\nHTML stands for HyperText Markup Language.\\nIt is a standard language,\\nwhich is used to create webpages.\\n\\nIt structures the content on the web\\nand it allows browsers to display your text,\\nimages, and other elements correctly.\\nHere it's a simple example of HTML.\\nYou can look at it.\\nIt has the basic structure,\\nwhich includes the document declaration,\\nangle brackets not equal to document type\\nthat contains the root element brackets and HTML.\\n\\nYour HTML tag is a root element\\nand it has multiple nested elements\\nlike head, title, body, h1, and others.\\nHTML is made up of tags and elements.\\nTags are the building blocks that define elements.\\nMost tags comes in pairs and opening tag,\\nand there is corresponding closing tag for them.\\n\\nFor example, you can see that h1 and correspondingly /h1.\\nSo, first one is an opening tag.\\nThe second one is the closing tag.\\nAnd whatever you put in between\\nthe opening tag and the closing tag\\nwill be a content between these two tag.\\nSimilarly, we can have a paragraph p tag and /p.\\nThat is the closing tag for this paragraph tag.\\n\\nSome tags are self-closing, such as the tag for images.\\nIn cases when you are defining\\nsome kind of an image onto your webpages,\\nin that case, you can just define the tag img,\\nwhich has not any closing tag.\\nSimilarly, we have another tag for breaking the line.\\nThat is br tag.\\nThat also does not have any closing tag.\\nTags can also have attributes\\nthat provide additional information about the elements.\\n\\nAttributes are always included in the opening tag\\nand comes in name/value pairs.\\nFor example, if I show you here,\\nyou can see that in this example\\nwe have an href is an attribute for the anchor tag.\\nAnchor tag starts with a,\\nand it has a attribute href,\\nwhich is representing a specific URL,\\nand then Visit Example.com.\\n\\nThe text will become a hyperlink.\\nIt happens because we have added in anchor tag a.\\nAnd when somebody clicks on that Visit Example.com,\\nit will go to the URL,\\nwhich we have kept under the attribute href.\\nAn HTML document typically includes several key sections.\\nHTML, the root element.\\nHead contains the meta information,\\nwhich is an information about the document\\nsuch as the titles, lengths to style sheet, and few more.\\n\\nWe also have a body tag\\nwhich contains the content of the document\\nsuch as text, images, and the links.\\nUnderstanding these structures help you navigate\\nand extract data when you do web scraping.\\nImagine you are working on a project\\nwhere you need to gather product information\\nfrom an e-commerce website.\\nThe website's HTML structure will guide you\\non where to look for data like product name,\\nprices, description, and few other details.\\n\\nBy inspecting the HTML, you can identify the tags\\nand attributes that contains the information.\\nModern browsers comes with developer tools\\nthat allow you to inspect the HTML of webpages.\\nYou can simply right click on any element of the webpage\\nand select inspect, or you can press F12.\\nThat will open the Developer tool for you.\\nThis will show you the HTL structure\\nfor the corresponding webpage\\nwhich you are seeing in the browser.\\n\\nThis will make it easier to identify\\nthe elements you need to scrap.\\nWhen you do scraping of the webpages,\\nthere are few of the commonly used HTML elements\\nyou will find.\\nFor example, div.\\nDiv stands for a container element.\\nIt's basic purpose is used\\nto combine the content in a group.\\nA span, it is an inline container\\nused for styling a part of the text.\\n\\nSo, there are some of the piece of the content\\nfor which you want to put a specific type of styling.\\nSo you can add this entire content within the span tag.\\nAnchor tag, it is used to link one webpage to another.\\nSo for example, from your webpage,\\nif you want to navigate to any other page,\\nyou can put an hyperlink kind of a stuff.\\nThat hyperlink can be done through this anchor tag.\\n\\nImg tag is used to display the images.\\nAnd table tag is used to organize data\\ninto the rows and column.\\nKnowing these elements will help you target the right parts\\nof a webpage during the scraping.\\nGreat job.\\nWe let the foundation by understanding the basics\\nof HTML in the context of web scraping.\\nThis knowledge is crucial as it allows you\\nto navigate and extract meaningful data from the webpages.\\n\\nIn the next video,\\nwe will dive into the HTML parents, child and descendants\\nto further enhance your web scraping skills.\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2126308\",\"duration\":376,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"HTML parents, children, and descendants\",\"fileName\":\"3896057_en_US_08_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":650,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about HTML element relationships for targeted web scraping. This facilitates accurate data extraction.\",\"captionsStatus\":\"NOT_AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9528057,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":null},{\"urn\":\"urn:li:learningContentVideo:2128321\",\"duration\":492,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand web scraping\",\"fileName\":\"3896057_en_US_08_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":715,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Grasp the importance and applications of web scraping for data collection and analysis.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11844413,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Web scraping is a powerful technique\\nfor extracting data from websites, which is crucial\\nfor many data engineering tasks.\\nLet's dive in, and understand\\nthe fundamentals of web scraping.\\nThe first question come in the mind is:\\nWhat is web scraping?\\nThe web scraping is the process\\nof automatically extracting the information\\nfrom the webpages.\\nThis techniques allows you\\nto gather the data from websites\\nand transform it into a structured format.\\n\\nFor example, it could be a CSV file, a Excel file,\\nor a database,\\nand you can use this information for the further analysis.\\nFor example, imagine you want to analyze the prices\\nof products on amazon.com.\\nWhat you can do is, instead of manually copying each prices,\\nyou can use the web scraping,\\nand that will automatically pull out the prices\\nof all the products from the webpage,\\nwhich you are seeing there.\\n\\nThat could help us to do lot of data analytics,\\nso web scraping work is usually\\nto extract the information from the pages\\nand make it available for the analytics.\\nBefore diving into the web scraping, it's essential\\nto understand the legal and the ethical consideration.\\nIt's always, respect the websites robots.txt file.\\n\\nThis file generally indicates the parts of the sites\\nthat can or cannot be scraped.\\nAdditionally, we need to be little mindful\\nof the website's terms of service\\nand avoid overloading the servers with too many requests.\\nI'll give an example.\\nSometimes we have a popular website,\\nlet's say an amazon.com,\\nand if all the people across the world try\\nto do the web scraping of this amazon.com too much,\\nthen eventually their server might go down,\\nbecause all these people try to request a lot of pages from\\nthat and that to automatic manner.\\n\\nSo we should be, have a little bit\\nof ethical consideration also into our mind\\nwhile we do the web scraping.\\nLet's understand how web scraping works.\\nWeb scraping involves multiple steps.\\nThe first step would be is to sending a request.\\nYou send an HTTP request to the website server\\nto retrieve the HTML content of the page.\\nThe second step, which you do once you get the request,\\nis parsing the HTML page.\\n\\nNow, as you have received the HTML content for the request,\\nwhich you have sent, now your work is\\nto parse this HTML content to identify\\nand locate the desired information.\\nFinally, the third step would be the data storage.\\nYou try to store the extracted information\\ninto a sum storage\\nand in a specific structured format,\\nthat could be CSV, JSON, or a database.\\n\\nEach of these steps can be implemented\\nusing various Python libraries,\\nwhich we'll explore in further sessions.\\nNow, as we know\\nthat we can do this web scraping using the Python,\\nthe next question come in the mind is how we can do that.\\nSo for this, there are multiple common web scraping\\nlibraries are available.\\nSome of them are Requests,\\nwhich is used for sending the HTTP requests to the website.\\n\\nWe have a library that is named Beautiful Soup,\\nit is a library for parsing the HTML and XML documents.\\nWe have Scrapy, a powerful web scraping framework\\nfor a large scale scraping.\\nLet's talk about a real-world use case.\\nSuppose you want to compare a price\\nof smartphone across a different e-commerce websites.\\nNow, web scraping allows you\\nto collect this data automatically\\nand help you to build a comprehensive data set for analysis.\\n\\nMaybe what you need to do for this is,\\nidentify the URLs of the product pages.\\nMaybe, for example, you have looking\\nfor some specific mobile brand.\\nYou will try to pick out all the related website,\\nwhich is selling this product.\\nYou can collect the URL of these websites,\\nthen, you can inspect this URL to look at the price.\\nYou can write the script to scrape these prices\\nfrom all these websites automatically,\\nand then you can store the data\\nin a CSV file for comparison.\\n\\nThis approach saves time\\nand ensure you have up-to-date information\\nfor your analysis.\\nMaybe, let's say, you want to check it today,\\nand maybe after a month,\\nso when you come next time, you don't have\\nto go to each and every pages,\\nyou just run your web scraping code,\\nand that will pull out all the latest comparative analysis.\\nSome websites load the content dynamically.\\n\\nFor loading the dynamic content,\\nthey are using some kind of a scripting,\\nlike Java scripting,\\nthat can make your scraping little bit complicated.\\nBut the good news is, there are tools like Selenium,\\ncan automate this interaction with such a websites.\\nBy controlling a web browser, Selenium allows you\\nto interact the web pages\\nas if you are browsing them manually.\\n\\nThat helps you to scrape the dynamic content also.\\nWeb scraping can present some challenges as well.\\nSometimes your website structure is keep on changing.\\nNow, if the layout of the website is changing,\\nprobably the way you have written a code to extract,\\nyou also need to change that.\\nSometimes some websites do the IP blocking.\\nIn that case, you cannot go and access them.\\nLastly, ensuring the accuracy\\nand the completeness\\nof the scrape data can also be little challenging.\\n\\nHowever, there are some option to overcome these challenges.\\nFor example, you can use proxies to avoid the blocking.\\nYou can set appropriate request headers.\\nAlso, you can simply go\\nand regularly update your scripts\\nto accommodate all the changes which the website is doing.\\nGreat job.\\nWe explored the fundamentals of web scraping\\nand understand what it is, how it works,\\nand the tools available in the Python.\\n\\nWe also discussed the real world application challenges\\nand ethical considerations.\\nNow, in the next video, we are going to talk about\\nthe BeautifulSoup, a library which is used\\nto parse the HTML content.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4256201\",\"duration\":417,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"BeautifulSoup basics\",\"fileName\":\"3896057_en_US_08_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":654,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn the basics of the BeautifulSoup library for HTML parsing. This is crucial for web scraping tasks.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10066174,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":null},{\"urn\":\"urn:li:learningContentVideo:4256200\",\"duration\":251,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Installing BeautifulSoup\",\"fileName\":\"3896057_en_US_08_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":520,\"exerciseFileDisplayText\":\"08_05.ipynb\",\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Install the BeautifulSoup library for Python. This enables the utilization of its powerful features.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7452581,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Now it's time to install the beautiful soup,\\nand start doing our practice.\\nNow before just jump in and install the beautiful soup,\\nunderstand why we need to do this.\\nImagine if you are using a normal Windows,\\nnot using the Google collab,\\nthen by default, in your normal Windows\\non a Mac machine where you are having your (indistinct),\\nif you are trying to do the web scraping\\nusing the beautiful soup there,\\nthen you must need to install the beautiful soup.\\n\\nIn Google collab, it is there, but we can go\\ninto the Google collab and run it,\\nthe installation there also,\\nand check whether it is there or not.\\nLet us go to the Google collab.\\nHere let's click on this code tab.\\nLet's have our own cell.\\nTo install the beautiful soup,\\nyou need to say pip install\\nand then you will you say beautiful soup4.\\n\\nSo this will install the beautiful soup.\\nI'll go as far as I'm aware of it.\\nGoogle collab give you this by default,\\nso you don't need to install it manually.\\nBut imagine if you are running this\\nexamples on your local machines,\\nthen you probably will not have\\nthe beautiful soup installed.\\nSo for that you need to do this activity.\\nLet me execute.\\nSo this is how you can install\\nthe beautiful soup4 using pip install.\\n\\nNow you can see that\\nit is saying the requirements already satisfied,\\nso it means that it is already there\\nin your Google collab sheet,\\nbut as I said, may be possible it is not there\\non your local machine,\\nso when you do this practice\\non your Windows, Mac, or any local machine,\\nthen in that button program,\\nyou go there and you install this.\\nYou also need to have a one parcel along with it.\\n\\nSo you need the parcel.\\nSo let's install the parcel as well.\\nThe parcel is a common parcel is lxml.\\nYou can see that that is also already installed.\\nNow, if you want to check at some point in time\\nthat whether it is already installed\\nor it is not installed there.\\nSo what you can do is,\\nyou can just simply type in\\na specific amount to check the versions of the same.\\nHow you can do that?\\nSay print, under the print I would say the bs4,\\nbeautiful soup, dot, underscore, underscore,\\nversion and underscore, underscore.\\n\\nIf I execute this, I can see that\\nthis is installed with the version 4.12.3.\\nSo great, so we have installed our\\nbeautiful soup to practice our web scraping.\\nSometimes you might encounter issues\\nduring the installation.\\nNow the common problem includes\\nyou're missing the parentheses,\\nor compatibility issues.\\nIn those cases, you just check whether your\\npip is updated or not.\\n\\nSo for example, you can run the command like,\\nso use pip install--upgrade\\nto just update your pip so that\\nif sometimes dependency issues are there,\\nand not allowing you properly\\nto install the beautiful soup,\\nyou can use this command,\\npip install --upgrade your pip,\\nand then you can try again to install.\\nGreat, we explored the process\\nof installing the beautiful soup,\\nwhich is an essential tool for our web scraping.\\n\\nIn the next video, we will dive into the practical\\nimplementation and start using it\\nto pass our HTML and extract the data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4257006\",\"duration\":329,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Get HTML from a web page\",\"fileName\":\"3896057_en_US_08_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":510,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Retrieve HTML content from web pages for scraping. This forms the basis of the web scraping process.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8066568,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] To scrap a webpage,\\nfirst we need to learn\\nhow to get the HTML page from the website.\\nThis is a crucial part of a web scrapping\\nas it allows us to retrieve the raw HTML content\\nand we will later parse and analyze this.\\nSo let's dive in and explore\\nhow to fetch HTML using Python,\\nalong with some real-world application.\\nWe can obtain the HTML content from the webpage.\\n\\nThis raw HTML contains all the information displayed\\non the webpage and it has structure\\nusing the tags and attributes.\\nBy fetching these HTML, we can gain access\\nto the underlying data, process, and analyze.\\nOne of the most popular library for fetching\\nan HTML content in the Python is Requests.\\nIt's simple to use\\nand provides a straightforward way to send HTTP request\\nand receive responses.\\n\\nSo to have this, first you need to import\\nthe Requests library.\\nThen you can use requests.get function\\nto sign an HTTP GET request.\\nWhen you send this request\\nor call this method,\\nthe HTTP request returns you an HTML content\\nthrough the response object.\\nWe are not diving into the code right now\\nand let's understand the process little more.\\n\\nHandling HTTP responses\\ncan be sometimes a little cumbersome.\\nWhen you send an HTTP request\\nusing the request library,\\nyou receive an HTTP response.\\nThis response contains various pieces of information\\nsuch as status code, headers,\\nand the HTML content of the page.\\nThe status code indicate\\nwhether the request is successful or not.\\n\\nFor example, if you get the status code as 200 OK,\\nit means the request was successful\\nand you are getting the page.\\nBut sometimes when you hit the request,\\nyou get the status code as 404.\\nThat is representing that either the webpage\\nwhich you're trying\\nis not available, or maybe you are giving the wrong URL.\\nAlong with this status code and the headers,\\nyou also going to get the HTML content,\\nwhich you are most interested in.\\n\\nWebpages can serve the content in various formats.\\nIt could be HTML.\\nIt could be JSON, and it could be XML.\\nWhen scrapping, it's crucial to ensure\\nyou are fetching and parsing the correct type\\nof the content.\\nFor example, when scrapping an API\\nthat returns the JSON data,\\nyou need to handle the response differently\\nthan an HTML page.\\nFor an instance, fetching the stock market data\\nfrom the financial API\\ntypically involves working with the JSON responses,\\nwhich you can process and analyze accordingly.\\n\\nFetching HTML content is not always very straightforward.\\nMany time, when you hit a URL,\\nyou get the few errors.\\nFor example, it may be website is down.\\nIt may be permission not granted.\\nIt may be it is overloaded.\\nHandling these errors gracefully\\nis essential for your robust web scrapping system.\\nCommon issues could be, as I said,\\n404 that showing the URL is wrong, not exist.\\n\\nAnother type of error\\nthat could come is internal server error,\\nthat representing something is wrong at the server side.\\nIf you get the status code as 403,\\nthat is showing that you have the access denied.\\nLet's consider a real-world use case\\nof monitoring the news websites.\\nAssume that you have a scenario\\nwhere you are building a tool\\nto monitor news websites for breaking stories.\\nFetching HTML content from various news sites\\nallows you to extract headlines,\\narticles, summaries, and publication dates.\\n\\nThis automated process helps you to stay updated\\nwith the latest news without manually visiting each site.\\nGreat job.\\nWe explored the process\\nof fetching the HTML from a webpage,\\nwhich is an essential step in web scrapping.\\nWe also discussed the importance of fetching HTML,\\nhandling the HTTP responses,\\nand dealing with different content types.\\nIn the next video, we will dive into the implementation\\nwhere you will get hands-on experience\\nwith Requests library and Beautiful Soup.\\n\\nI'm sure you don't want to miss, so let's see you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4250301\",\"duration\":450,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Scrape the web page\",\"fileName\":\"3896057_en_US_08_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":569,\"exerciseFileDisplayText\":\"08_07.ipynb\",\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Implement web scraping using BeautifulSoup. Extract valuable data from web pages.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16508811,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We understood the web scrapping,\\nwe understood the request.\\nNow it's time to write the code and do it practically.\\nSo first thing, we need to have the import request\\nbecause you remember, this request library is going to use\\nto get the HTTP get request.\\nSo we'll hit our one URL and get the web content from it.\\nSo that's why we need this request library.\\n\\nSo I'm trying to import this request.\\nSecond thing is once my page get returned, I wanted to,\\nI trade upon it and analyze it.\\nSo for that we need the BeautifulSoap.\\nSo I would import that as well, saying from bs4\\nimport BeautifulSoap.\\nNow we need a one URL\\nto practice our web scrapping.\\n\\nSo right now I have a one URL,\\nwhich I am using to fetch some data.\\nYou can use anything of your own choice.\\nNow on this URL, what I'm going to say,\\nI'll say response equal to request dot get URL.\\nNow when I execute this,\\nthis request dot get will send me the entire HTML content\\nalong with some other information.\\n\\nTo get the HTML content from the response,\\nI would say response dot content and that's it.\\nWe can just simply say print this HTML content.\\nIt'll try to print the entire HTML of that webpage.\\nYou can see that it is showing some big lot\\nof content, right?\\nNow, time to let's further deep down inside it,\\nhow we do that?\\nSo we going to parse this specific HTML content\\nusing our BeautifulSoap.\\n\\nSo I would say soap equals to BeautifulSoap\\nand in the bracket I will pass my HTML content,\\nwhich I get it from our request\\ncomma and we need to pass the parser.\\nRight now my content is HTML.\\nSo I will pass an HTML parser.\\n\\nNow this will passe this HTML content.\\nLet's say I want to find out\\nall the text which is put under the H1 or headline tag\\nso I can traverse it using this soap object.\\nHow I can do that?\\nI will say for example,\\nH1 underscore text equal to soap dot find.\\nSo you remember what was our function?\\nFind will find the first occurrence of the tag,\\nwhatever you put in.\\n\\nSo I'm looking for H1 tag.\\nSo I can say this print H1 underscore tags dot text\\nand you can see that.\\nSo in that webpage I have the header\\nas Azure interview question and answer.\\nSo that H1 tag is printed here.\\nIf I go down to this website\\nand if I try to show you this website,\\nthis website looks something like this.\\n\\nSo here you can see that we have this\\nas a H1 tag and that is what we are able\\nto get it from our Python code itself.\\nGreat, let's do a little more.\\nLet's try to find out\\nhow many images are there on this page, how I can do it?\\nI will say images equal to,\\nthis is just a variable name I'm giving images,\\nit can be anything.\\n\\nAnd then I will say soap dot find underscore all.\\nSo find underscore all will find all the occurrences of it\\nand I'm using the tag as image tag.\\nThen I will say, this time,\\nremember, you have used find underscore all.\\nSo it'll return all the image tags.\\nSo it'll be an array or a list.\\n\\nSo how I can do it? I will use for image in images.\\nSo I'm now iterating upon this entire list.\\nAnd I will say that let me print\\nall these images source,\\nI can use something like this.\\nNow if I try to print,\\nyou will find that it is printing all the image sources.\\n\\nSo in this webpage, all these are the image tag there.\\nAnd all these image source are these specific URLs, correct?\\nLet's do a little more.\\nYou remember we have a UL tag for having our list.\\nSo let's find out that specific tag.\\nI will say that my list is equal to soap dot find,\\ninverted comma UL.\\n\\nThat will give me the UL tag.\\nAnd under that UL, you have the LI content.\\nSo I'll just show you, here,\\nif I say print my list dot text,\\nyou will find that it'll print all the list of items.\\nSo this UL has these many items, got it?\\nSo that's how you can choose all the text\\nof your own choice and you can iterate upon them\\nor get the text as per your need.\\n\\nIn a real world environment, as I said for example,\\nyou are doing web scrapping for your rival company.\\nSo what you can do, you can send a request to their website\\nand you can scrap all the data.\\nNow this data is on our command prompt,\\nisn't it good if we can save this information in some kind\\nof a CSV\\nand then we can send it to our data analytics team\\nto do further analysis.\\n\\nSo this is what we are going to see in the next video.\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4250302\",\"duration\":277,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Export data as a TXT file\",\"fileName\":\"3896057_en_US_08_08_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":382,\"exerciseFileDisplayText\":\"08_07.ipynb\",\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to export scraped data to a TXT file. This enables further analysis and manipulation of data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7765742,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now it's time to store all our analysis\\nfor extraction which we have done from the webpages\\ninto a text file.\\nThis is crucial for data engineering.\\nThe reason is you generate this data\\nexport it into the text files\\nand you share it with the data analytics team\\nso that they can do the further analysis on it, right?\\nSo let's dive in\\nand see how we can export the data in text format.\\n\\nNow, the first question come into mind is,\\nwhy we are exporting the data to text file?\\nThe reason is the text file are simple, it's human-readable,\\nand it widely supported across various platforms\\nand the programming languages.\\nThey're also easy to parse and manipulate.\\nThis makes them one of the best choice for the data storage.\\nNow let's go back to our Google Colab\\nand see how we can extract the data into the text file.\\n\\nSo in the previous file,\\nwe have got the list of all the items.\\nNow, let me put that into a variable.\\nSo I will say my text which I want to write\\nis equal to mylist.text.\\nThis is in entire this data.\\nNow, if I wanted to write it further down, what I will do is\\nI will use the file read write thing.\\n\\nI would say with open\\nand I give the name of my file,\\nI would say web-scrap.txt,\\nand I want to open it into the right mode,\\nso I would say w as file.\\nAnd I can just simply say here, file.write.\\nAnd under that write I can pass this variable.\\n\\nDone.\\nIf this code get executed successfully,\\nthis will create a file called web-scrap.txt,\\nand that is storing all the text\\nwhich we got it from ul tag.\\nLet me execute this.\\nDone. Looks successful.\\nJust let me open up here the folder on the left-hand side.\\nHere I can see that.\\nLet me double click and try to open.\\n\\nYou can see that the data has been captured\\ninto the text file.\\nGot it?\\nThis is how you can do the web scraping.\\nWhatever the data which you get from this web scraping,\\nyou can automatically put that data into a text file,\\nand then this file can be shared with anyone\\nwho is interested to do the further analysis on it.\\n\\nSometimes you need to be a little more careful\\nwhen we are doing the parsing for a little bigger pages.\\nIf you have a large chunk of data\\nwhich you wanted to store as a text file,\\nthen rather than storing completely in a one shot,\\nyou want to store that data in some chunk.\\nMaybe you can add on some automation\\nof a scheduling as well,\\nthat a file can automatically execute\\nand store the data in our respective files.\\n\\nGreat job.\\nWe have explored the process\\nof fetching the HTML from the webpages,\\nan essential step in web scraping.\\nWe have also understood how you can pass this data\\nand push it into the text file.\\nSo overall, this is how you can do the web scraping\\nusing the Python.\\nRemember, this is an important skill to have\\nas a data engineer.\\n\\nIn the next chapter, we're going to talk about\\nanother interesting thing in the Python world\\nthat is generators.\\nSee you there.\\n\"}],\"name\":\"8. Web Scraping with Python\",\"size\":81247891,\"urn\":\"urn:li:learningContentChapter:4252319\"},{\"duration\":1880,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2128319\",\"duration\":240,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Generators in Python\",\"fileName\":\"3896057_en_US_09_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":428,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Understand generators and their benefits for memory-efficient iteration. This improves code performance.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5836800,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Generators are the powerful tools\\nthat can enable the efficient\\nand memory-friendly data processing,\\nwhich is crucial for handling the large datasets.\\nLet's explore how generators works,\\ntheir benefits and their real-world applications.\\nWhat are generators?\\nGenerators are a special type of iterator\\nor iterator, depends upon you how you pronounce.\\n\\nThese iterators allows you\\nto iterate over a sequence\\nof values without storing the entire sequence\\nin the memory.\\nThis makes them incredibly efficient\\nfor handling the large datasets.\\nFor example, imagine you are processing\\na massively large log file.\\nNow, loading the entire file into the memory\\nmight not be feasible.\\n\\nIt may crash your server.\\nGenerators allows you to read and process the file\\nline by line and using the minimal memory.\\nAnd remember, whenever we talk about the memory here,\\nwe are talking about the main memory or the RAM.\\nThere are several benefits of using the generators.\\nThe first one is the memory efficiency.\\nThe generators generate the item on the fly,\\nwhich is ideal for the large datasets.\\n\\nSecond one is lazy evaluation.\\nGenerators produce values only when needed.\\nThat reduces computation overhead significantly.\\nAnother benefit is readable code.\\nGenerators can simplify the code\\nby replacing complex loops\\nand conditionals with clean and readable logics.\\nOne of our real-world example\\ncould be is processing this streaming data\\nfrom the sensor network.\\n\\nGenerators can handle continuous data streams\\nwithout consuming large amounts of memory.\\nHow generator works.\\nGenerators are defined using the functions\\nand the yield statement.\\nUnlike the regular functions that return a single value\\nand terminate, generator yield multiple values,\\npausing and resuming their state between each yield.\\n\\nFor example, look at the code snippet in front of you.\\nYou need to generate a large sequence\\nof number for a simulation.\\nUsing a generator,\\nyou can yield each number one at a time.\\nWhenever you use generators,\\nyou can follow some certain best practices.\\nSome of the best practice are, keep generators simple.\\nEach generator should have a clear and a specific purpose.\\n\\nYou can combine the generators.\\nWherever you feel that it is possible\\nto break down the complex data processing task,\\nwe can do it using the generator.\\nImplementing an error handling to manage the exception\\nis also one of the best practice while using the generators.\\nGreat.\\nWe have understood the generator,\\nwhich is a powerful tool that can help you write\\nefficient and readable and memory-friendly code.\\n\\nIn the next video,\\nwe will dive into the Python generator classes\\nand iterators where you will learn more.\\nLet's jump in there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4250299\",\"duration\":683,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Python generator classes and iterators\",\"fileName\":\"3896057_en_US_09_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":1211,\"exerciseFileDisplayText\":\"09_02.ipynb\",\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to create custom generator classes and iterators in Python. This enables efficient data processing.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":19194357,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Python generator classes\\nand integrators are powerful ways\\nto create custom iterators\\nand manage complex data processing workflows.\\nLet's understand how you can leverage\\nthese features in your projects.\\nGenerator classes and iterators are\\nadvanced tool in Python\\nthat allows for more customized\\nand efficient data iteration\\nwhile generators provide a simple way to itrate\\nover sequences\\ncustom iterator classes gives you\\ngreater control over the iteration process.\\n\\nImagine you need to process the large dataset in chunks.\\nUsing custom iterators, you can design a class\\nthat reads data in manageable pieces,\\nmaking your processing pipeline more efficient.\\nLet's just go to our Google Collab\\nand create our own custom iterator.\\nTo create a custom iterator in Python,\\nyou need to define a class that implements two methods,\\nunderscore, underscore iterator method ETR,\\nand another method is the next method.\\n\\nLet's us see how we can do that.\\nI give a class,\\ncounter.\\nNow in this class\\nI will define the two method for\\ncustom iterator.\\nThe first method is\\ndefinition underscore underscore\\ninint.\\nSo this is the first one,\\njust for initializing our class\\nand I would say self comma,\\nstart comma and\\nand I will say\\nself dot\\ncurrent is equal to start\\nand self dot\\nend is equal to end.\\n\\nNow let's define the iterator method.\\nThat is def underscore underscore iter.\\nAnd there I will pass the self.\\nThis iterator method is used\\nto return the iterator object of itself.\\nSo how we can do that, we can just say return\\nself.\\nThen we can think of adding another method.\\n\\nThat is the custom iterator method next method.\\nThis next method is used\\nto return the next value in the sequence.\\nSo how I can do that,\\nI can just say if,\\nmy current value is greater than our end\\nthen probably I need to stop\\nor maybe I need to raise\\nsome stop\\niteration\\nas I would say that\\nmy self dot current\\nis equal to\\nor I can use shorten operator say\\nplus equal to one.\\n\\nThat means I am trying to increase it by one\\nand I will return\\nthis self dot\\ncurrent minus one.\\nSo I increase it\\nand then I am returning one minus\\nso that I can return the current value.\\nGot it?\\nNow this will be my custom counter class,\\nwhich can be used to iterate from a start value\\nto an end value.\\n\\nI can do one thing.\\nI can say counter\\nequal to this counter class object,\\nwhich is having an iterators between one to five.\\nAnd I will say\\nfor numbers\\nin my counter,\\njust print\\nthe number.\\nSo automatically this iterator\\nwill keep on iterating using the iter object\\nand the next two method.\\n\\nSo let me run this.\\nIt's saying counter does not take any input.\\nArgument reason is on the top\\nI have a spelling error.\\nI just remove it.\\nSo now if I try, it works\\nand you can see that it is printing\\none, two, three, four, five.\\nSo that's how we can make our own custom iterator.\\n\\nWe can also create the generator classes with the if.\\nGenerator classes combine the simplicity of generators\\nwith the flexibility of the classes.\\nInstead of using the next method\\nyou can use in within a method\\nto produce the sequence of value.\\nLet's just see how we can do that.\\nLet's have a class called it Fabonacci.\\n\\nNow in this class I define the init method,\\nwhich is taking just a one argument of max\\nand I will say self dot max\\nis equal to max.\\nNow let's have our iterator method saying\\niter and under that\\nI will pass the self and I would say self\\ndot A comma self\\ndot B equal to\\nzero comma one.\\n\\nSo I will reset the values A to zero\\nand B to one\\nbecause we are trying to find out the Fabonacci values.\\nAnd I will just say return self.\\nNow in the next method,\\nthis time I will use\\nthe yield.\\nNow I create the next method\\nand let's generate the next value from here.\\n\\nHow?\\nSay self dot A is greater than\\nself dot max.\\nIf that happens up, we have to stop, right?\\nSo how I would do that, I will say here,\\nraise\\nand stop iteration error\\nelse.\\nNow I reduce self dot A comma\\nself dot B equals to\\nself dot B comma\\nself dot\\nA plus self dot self B.\\n\\nAnd then we simply say return.\\nSelf dot A minus\\nself dot B.\\nThis will generate a new values from me.\\nNow we have a generator method.\\nI will say def\\ngenerator\\ngenerate\\nself\\ncolon\\nwhile\\nself dot A\\nis less than equal to\\nself dot max.\\n\\nWe keep on hydrating\\nand there we use yield self dot A.\\nAnd then we'll say self dot A comma\\nself dot B is equal to\\nself dot B comma,\\nself dot\\nA plus self dot B.\\nNow this Fabonocci class generate the Fabonocci numbers\\nup to the specified maximum value.\\n\\nAnd how you can use it?\\nThe using would be pretty simple.\\nYou will say fib equal to Fabonacci.\\nI have used the wrong spelling to be honest,\\nbut that's fine.\\nI will say I need 10 values,\\nso Fabonacci 10,\\nand then,\\nI will use\\nfor\\nnum in\\nfib dot\\ngenerate.\\n\\nAnd then I can simply use print\\nmy number\\nand if everything goes fine, we get something.\\nBut here is a problem.\\nIt's saying that Fabonacci is not defined.\\nLet's copy this and let's paste it\\nand let's have a properly indented.\\n\\nDone.\\nNow let's try.\\nIt's saying that the self does not have an A and B.\\nSo to solve this error, let me just\\nintroduce the A here equal to zero\\nand self dot B is equal to one.\\nAnd let me execute.\\nNow you will find that our Fabonacci series is working\\nperfectly fine.\\n\\nThe important thing here is\\nwe have using the generator class\\nand with having the yield.\\nGreat job.\\nSo we have seen how we can create the custom iterator\\nand how we can use the generator class.\\nIn the next video, we are going to talk about the iterables.\\nSee you there.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4257007\",\"duration\":375,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Iterables in Python\",\"fileName\":\"3896057_en_US_09_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":508,\"exerciseFileDisplayText\":\"09_03.ipynb\",\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Understand iterables and their role in Python programming. This facilitates looping over data structures.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9934039,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Iterables are fundamental part\\nof Python programming that enables you to loop\\nthrough the data structures efficiently.\\nLet's explore what iterables are, how they work,\\nand their real-world applications.\\nWhat are iterables?\\nIn Python, an Iterable is an object\\nwhich is capable of returning its members one at a time.\\nThat's allowing it to be a iterator over in a loop.\\n\\nThis includes data structures\\nlike lists, tuples, dictionaries, and sets.\\nLet's take a real-world example.\\nImagine you have a list of student names\\nand you want to print each name.\\nThe list is an iterable that you can loop through\\nto access each name sequentially.\\nTypes of iterables.\\nPython provides several built-in types that are iterables.\\n\\nFor example, lists that is ordered collection of items;\\ntuples, which is immutable ordered collection;\\ndictionaries, which is collection of key-value pairs;\\nsets, which is unordered collection of unique items;\\nstrings, which is collection of characters.\\nLet's just go to our Google Colab\\nand see how we can do it practically.\\nLet's create a quick list.\\n\\nI would say fruits\\nand I give three names inside it:\\napple,\\noranges,\\nand grapes.\\nNow, I can iterate upon this list very easily\\nbecause this is an iterable.\\nHow?\\nI would use for f in fruits, right?\\nAnd then I can just say inside that loop, print f.\\n\\nThis will go and print each fruit one by one.\\nSimilarly, we can do iteration on the tuples\\nand other stuffs as well.\\nNow let's take a different approach.\\nNow, imagine that you have the list like above,\\nyou can use an iterable function\\nor a method to iterate from an iterable.\\n\\nHow? Let's just see.\\nFrom above, let's get the iterator.\\nHow?\\nI would say\\nit is equals to\\niterable from my fruits.\\nRight.\\nNow, this iter function give me an iterable object\\nand I can do something like this.\\nI can say print.\\n\\nTo get the first item, I have to just say next\\nunderscore my iterator object.\\nFor getting the second item,\\nI should say print\\nnext, the second item.\\nThen for the third item, again,\\nI would say print next third item.\\nAnd if I execute this,\\nI can get apple, oranges, grapes, everything.\\n\\nCorrect.\\nGreat.\\nYou can also write a code\\nto check whether a specific object is iterable or not.\\nFor example, we can write a function,\\nsay def is eatable\\nand pass an object inside it.\\nI can use try iterator\\nand I would say object,\\nso it will check whether it is an iterable object.\\n\\nIf it is there, I will say return true.\\nIf it was not an iterator object,\\nwe would have gotten exception,\\nso I will say except TypeError\\nand I will catch that error here\\nto say return false.\\nAll right?\\nT of True should be capital\\nand F of false should be capital.\\n\\nCorrect.\\nNow what I can do is I can come here\\nand say print check is_iterable\\nmy fruits, which I give, is iterable or not.\\nI can also check if I just pass\\na simple iterable like this.\\nLet's say I just pass 10 here\\nto check whether is iterable or not.\\n\\nLet me execute this.\\nNow you can see that\\nthe first item it's saying is true.\\nThat means the fruits is iterable, which is right,\\nbecause that is the list which has to be iterable.\\nThe second item, which I pass is just a number 10,\\nand 10 is not an iterable, that's why I got false.\\nGreat.\\nThat's how we've explored the iterables in the Python.\\n\\nNow in the next session, we will dive into the filter\\nand map functions where you will learn how to transform\\nand filter the data effectively.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4256198\",\"duration\":348,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"filter() and map() functions\",\"fileName\":\"3896057_en_US_09_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":558,\"exerciseFileDisplayText\":\"09_04.ipynb\",\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Explore the filter() and map() functions for data manipulation. This enhances efficiency in processing data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9012572,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] There are two more powerful functions\\nin the Python for data engineers.\\nThe filter and the map.\\nThese functions are essential for transforming\\nand filtering the data,\\nmaking your data processing workflows more efficient\\nand concise.\\nLet's explore how they work and where you can use them.\\nThe filter and the map functions\\nare built-in Python functions that allow you to transform\\nand filter the data in a functional programming style.\\n\\nThe filter function takes a function\\nand an iterable as an argument.\\nIt returns an iterator\\nand then those items of the iterable.\\nIn short filter will help you\\nto filter out the items from your iterables.\\nThe map function applies a given function\\nto all the items in the iterable.\\nFor example, imagine you have a list of numbers\\nand if you want\\nto double all this number values individually,\\nyou can think of using the map function\\nand that map function, you can pass this iterable list\\nand pass your function,\\nwhich is like doubling the values, correct?\\nLet's just go to our Google Colab\\nand see how we can do that.\\n\\nLet's assume that we have a list of numbers.\\nIt'll be two like this,\\n3, 4, 5, 6, 7, 8, 9, 10.\\nNow if I wanted to filter out all the items\\nwhere these items are only the even numbers,\\nhow I can do it using the filter function?\\nLet's just see here.\\n\\nI can say something like\\neven_numbers = filter function,\\nand now in this filter function, I will pass in\\nmy list of numbers and a function.\\nAnd what could be my function?\\nMy function would be something like this,\\nthe Lambda function, lambda x: x&2 == 0.\\n\\nSo I pass this one liner function\\nthat will take the number, take the reminder of it by two,\\nand if that reminder comes out to be equal to zero,\\nthen that will be filtered out.\\nI just removed this extra equal.\\nAnd the second argument which I need\\nto pass in this filter function is my list,\\nthat is, numbers.\\nNow if I just go here\\nand print all this list of even numbers,\\nyou can see that we got all the even numbers\\nout of the list,\\nand here we have used our filter function.\\n\\nNow, let's take in another use case.\\nImagine that on the above numbers list,\\nI want to find this querying of all the numbers.\\nCan I do that?\\nThe answer would be yes, possible through the map function.\\nWhat I will do is I will say\\nmy squared_list = the map function,\\nand within the map function,\\nI will say lambda x: x**,\\nthat is, whatever be the number,\\ntake the square of it,\\nand here I will pass my entire list.\\n\\nGot it?\\nNow, if I just do print the list of my squared numbers,\\nI would see something like this.\\nGot it?\\nSo my entire list got squared,\\nand what I did is I go one element at a time\\nusing this map function,\\nand this map function applies on every number\\nto make it become square of it.\\n\\nYou can also write some functions or a piece of a code\\nwhere you are combining the filter and map logic\\nwithin the same code snippet.\\nThe real world use case\\nof this could be like you have the sales data\\nand you want to process it.\\nSuppose you need to filter out the sales\\nbelow a certain threshold and then apply a discount to it.\\nThen you can use this combination of filter and map.\\n\\nThere could be multiple different examples\\nor scenarios of using this map and filter function.\\nSee you into the next video.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4253273\",\"duration\":234,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"any() and all() functions in Python\",\"fileName\":\"3896057_en_US_09_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":329,\"exerciseFileDisplayText\":\"09_05.ipynb\",\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to use the any() and all() functions for boolean evaluation. This is useful for logical operations in code.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5932343,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] After exploring the filter and map function,\\nwe have two more functions, any() and all().\\nThese functions could play a significant role\\nin performing logical operations on iterables\\nand enabling you to make decisions\\nbased on the content of your data structure.\\nLet's explore how they work\\nand their real world applications.\\nThe any() and all() functions are builtin python functions\\nthat evaluate the truthfulness of element in the iterable.\\n\\nAny() function returns true if at least one element\\nin the iterable is true.\\nAll() returns true\\nif all the elements in an iterable are true.\\nFor example, imagine you have list of test results,\\nand you want to check if at least one student passed.\\nYou can use the function any() or if you want to know\\nif all the student get passed,\\nthen you can use the function called, all().\\n\\nLet's just go to our Google Colab\\nand see how we can use these functions.\\nImagine that we have a list of numbers,\\nsomething like this, -1, 0, 1,\\n- 2, 2, 0, something like this.\\nNow, we are interested to find out all the positive numbers.\\nHow you can find?\\nWe will use the all() function to find out\\nif all the numbers in this list are positive or not.\\n\\nSo we can write something like this, print(all),\\nand in this all() function,\\nI will pass a condition something like this,\\nif my numbers are greater than zero.\\nAnd I will pass for num in numbers.\\nNow, imagine what this function will do.\\n\\nThis function will iterate upon every item in the list,\\nand for every item, it will check\\nwhether it is greater than zero or not.\\nIf it is greater than zero, it will find for each number,\\nand once all the numbers are greater than zero,\\nthen only this will return true.\\nOtherwise, this will return false.\\nYou can see that we got a result as false.\\n\\nThis is saying that in this list,\\nnot all the numbers are positive.\\nNow, instead of this, let's say I have a situation\\nwhere I wanted to check\\nif any one of the number is positive.\\nSo I can use, print(any), and under that any() function,\\nI can just past the condition like number>0\\nfor num in numbers.\\n\\nNow, in this case, again, it will check for each\\nand every number, whether it is greater than zero or not.\\nEven if it finds any one of the number which is positive,\\nit will return as true.\\nGreat, so we got this true from this any() function.\\nThat's how your any() and all() works.\\n\"}],\"name\":\"9. Advanced Built-in Functions\",\"size\":49910111,\"urn\":\"urn:li:learningContentChapter:4250314\"},{\"duration\":843,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4253274\",\"duration\":279,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"What is logging?\",\"fileName\":\"3896057_en_US_10_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":395,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Grasp the concept of logging for capturing runtime information. This is important for debugging and monitoring.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7169802,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Logging is an important aspect\\nof software development\\nthat allows you to track events, debug issues,\\nand understand the behavior of your applications.\\nLet's explore what logging is, why it is important,\\nand how you can effectively implement it\\nin your Python projects.\\nWhat is logging?\\nLogging is the process of recording events that happen\\nduring the execution of a program.\\n\\nThese events can include errors,\\ninformational messages, and debugging information.\\nLogs provide a way\\nto understand what is happening within your application\\nand it then gives you an invaluable information\\nto diagnose the problems.\\nImagine you are running a web application.\\n\\nLogging can help you track the user activities,\\nserver errors, and maybe it can help you\\nto perform the performance metric analysis,\\ngiving your insights\\ninto how your application is being used,\\nand it can help you to understand\\nwhere the improvements are needed.\\nLogging, its essential,\\nit has multiple advantages.\\n\\nLogging is important for debugging.\\nIt helps to identify\\nand diagnose the issues within the code.\\nLogging is also helping in monitoring\\nand tracking the application behavior and performance.\\nLogging is also crucial for auditing stuff.\\nMany times you need the auditing\\nfor some compliance perspective or maybe\\nbecause of some security reasons.\\nAuditing can be done through the logging.\\n\\nInformational, logging also provides the insights\\ninto the application's operations.\\nImagine, in a financial application,\\nlogging can track transactions,\\nensuring that any anomalies or errors are quickly identified\\nand resolved.\\nPython has a built-in logging module,\\nwhich provides a flexible framework\\nfor emitting your log messages from your Python program.\\n\\nThis built-in Python logging has multiple logging levels.\\nYou can define the several levels of severity\\nfor log messages.\\nFor example, DEBUG,\\nthat can give the detailed information typically\\nof interest only when you want\\nto diagnose some problem.\\nINFO, this kind of level,\\nyou need it for having the confirmation\\nthat things are working as expected.\\n\\nWARNING level, it is an indication\\nthat something unexpected happened\\nor indicative of some problem in the near future.\\nIt is kind of putting your attention to these problems.\\nERROR, a more serious problem the software\\nhas not been able to perform some function.\\nCRITICAL level, a very serious error indicating\\nthat program itself may be unable\\nto continue running.\\n\\nIn a real world, think of an e-commerce application.\\nYou might log an info message that a user logs in.\\nYou do a warning log\\nwhen a user enters incorrect credentials multiple time\\nand an error log if there is a database connection failure.\\nGreat job, we explored the concept of logging in the Python,\\nunderstanding its importance,\\nand how to implement it effectively.\\n\\nWe discussed some of the real world applications as well.\\nLogging is a vital tool for debugging, monitoring,\\nand auditing your application.\\nLet's get down a little more deeper in the next video.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4252309\",\"duration\":289,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Custom logging\",\"fileName\":\"3896057_en_US_10_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":382,\"exerciseFileDisplayText\":\"10_03.ipynb\",\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to customize logging behavior for specific requirements. This enhances flexibility in logging setup.\",\"captionsStatus\":\"NOT_AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8484060,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":null},{\"urn\":\"urn:li:learningContentVideo:2128320\",\"duration\":275,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Logging best practices\",\"fileName\":\"3896057_en_US_10_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":413,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Explore best practices for effective logging implementation. This improves code readability and maintainability.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6849298,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] Proper logging is important for debugging,\\nmonitoring, and maintaining your applications.\\nBy following the best practices,\\nyou can ensure your logging strategy is efficient,\\neffective, and scalable.\\nLet's dive in and explore\\nhow to optimize your logging practice.\\nUse appropriate logging levels.\\nUsing the correct logging levels,\\nit helps to categorize and prioritize the log messages.\\n\\nHere's a quick overview of the levels and when to use them.\\nFor example, debug.\\nIt should be used for detailed information,\\nwhich is needed to diagnose the problems.\\nThis level is important for development\\nand the debugging purpose.\\nInfo, this is general information\\nabout the application's operation.\\nUse this level for events\\nthat are part of the normal workflow.\\n\\nWarning, an indication of a potential problem\\nor an unexpected event,\\nuse this level for recovery issues.\\nError, a serious issue that has been occurred.\\nUse this level for the errors that need immediate attention.\\nCritical, a very serious error indicating a major failure.\\nUse this level for severe problems\\nthat may cause the application to terminate.\\n\\nFor example, in a payment processing system,\\nuse info to log successful transaction,\\nwarning for retriable errors,\\nand error for failed transaction\\nthat need to be investigate.\\nAnother important best practice you need to keep in mind\\nis avoid logging sensitive information.\\nNever log sensitive information like your passwords,\\ncredit card numbers, personal data.\\nBecause logging sensitive data\\ncan lead to the security concerns and compliance issues.\\n\\nUse consistent formatting.\\nConsistent log formatting make it easier\\nto read and analyze log.\\nUse for matters to include important information\\nlike timestamp, log levels, and message content.\\nLog rotations help manage log file size\\nand prevent disc space issues.\\nSo you should keep on rotating your logs.\\nLeveraging the logging configuration file.\\n\\nIt totally makes sense to have the configuration file\\nseparate for setting the logging properties.\\nThat keeps your code clean,\\nand it is easier to manage complex logging setups.\\nCentralized logging.\\nCentralized logs from different parts of your application\\nor from multiple application\\ncan help in monitoring and debugging.\\nUse centralized log solutions like ELK,\\nthat stands for Elastic Search Log Stash Kibana, or Splunk.\\n\\nYou can also think of setting up the monitoring\\nand alerting on specific log events or patents.\\nThis ensures you are notified\\nof critical issues in real time.\\nIn a production environment, setting up alerts for error\\nand critical log levels can ensure prompt response\\nto the severe issues.\\nAnother best practice you should follow is\\ntest your login configurations.\\n\\nYou should regularly test to ensure it works as expected.\\nThis includes verifying log rotation, format,\\nand integrity of log data.\\nYou can also document your logging strategy\\nand configuration to ensure consistency\\nand ease of maintenance.\\nGreat, we have explored the logging\\nbest practices in the detail.\\nIf you follow these practices,\\nthis will make your life as a developer very easy,\\nand this will also help your code to have the clean,\\nscalable, and better solution.\\n\\n\"}],\"name\":\"10. Logging in Python\",\"size\":22503160,\"urn\":\"urn:li:learningContentChapter:2128325\"},{\"duration\":1250,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4256197\",\"duration\":203,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Capstone Project: Retail sales analysis\",\"fileName\":\"3896057_en_US_11_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":268,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the course capstone project challenge.\",\"captionsStatus\":\"NOT_AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5149610,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":null},{\"urn\":\"urn:li:learningContentVideo:4253272\",\"duration\":1047,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Capstone project\",\"fileName\":\"3896057_en_US_11_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":1396,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Compare your challenge solution to that of the instructor.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":38258192,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] Let's just see how we can build\\nthis Capstone project\\nto analyze our retail sales data.\\nThe step number one would be,\\nis first to import our data.\\nThat is CSV file.\\nI click on the files, I select here\\nto import my CSV file that is retail sales,\\nand let me import.\\nNow, my retail_sales.csv is available here.\\n\\nLet me minimize this left window.\\nNow our CSV file is here.\\nThe first step probably we want is to load the CSV,\\nbut before loading that, understand the point.\\nWe have to do everything in terms of a classes and object.\\nSo what I'm going to do is first I'm going\\nto create a class.\\nSo let me add a class.\\nI'll call it class.\\nAnd as per the project we need,\\nRetail Sales Analyzer is the name of the class.\\n\\nSo this is my class I give, colon.\\nAnd then I have to define the function inside it.\\nFirst function, which we have is our init function.\\nSo that is init.\\nAnd then under that I just pass the self.\\nSo this is creating our initialization function.\\nAnd there, let me create our attribute, that is data.\\nSelf.data, equal to.\\nNow here I'm going to import our CSV file\\nand read it as a data frame.\\n\\nSo how I can do it?\\nI can say that pd.read_csv,\\nand then in the bracket, I will pass the name\\nas retail_sales.csv.\\nSo this is going to read my file.\\nBut here you see under that PD that there is some error.\\nReason is we have not imported our Pandas dataframe library.\\nSo how I can do it, I just say import pandas as pd.\\n\\nSo this error will be gone by now\\nand this function just simply goes and reads our file.\\nWe also need to understand that our CSV file,\\nif I just come down here, if I just show you this file,\\nyou'll see that this is a date column.\\nNow once you do a normal reading,\\nthis date column probably is read as a string.\\nSo let's convert it into the date format itself.\\n\\nHow I can do it, I just come down here\\nand I will say that self.data,\\nin the bracket, I'll say, date column is equal\\nto pd.to_datetime, and then I will pass self.data,\\nbracket, again, date.\\nSo this will convert date column from an string\\ninto the To date time and that's it.\\n\\nSo our first function has been created now.\\nThe first step which we have done,\\nthat is we have read the data from the CSV file\\nusing the pandas.\\nNow what is the next thing we want to do?\\nWe wanted to do the data cleaning.\\nHow we can do that.\\nSo let's have another function, I call it like def,\\ndata_clean, and again, inside, also I will pass the self.\\nAnd how we can clean the data.\\n\\nThe cleaning of data would be pretty easy.\\nI just say that self.data.dropna.\\nAnd inside that I will just say that,\\ninplace equal to true.\\nSo what this is going to do is this will going\\nto clean the data for me\\nby removing all the rules where any\\nof the column having the value as null.\\n\\nSo our step number two, that is data cleaning,\\ncheck for and remove rules with missing values\\nin any column, is done.\\nNow the third step would be is to do the data manipulation.\\nNow under the data manipulation we have\\nto find out multiple things.\\nFor example, the first thing which is needed\\nis to calculate total sales per product.\\nHow you can find out, let's just do that.\\nLet's add in one more function, we'll call it def,\\ntotal_sales_per_product.\\n\\nFor this function, also let me pass self.\\nAnd now you tell me how I can find out the total sales\\nbased on the product.\\nThe answer is pretty simple.\\nI need to do the group by because I'm grouping\\nbased on the product.\\nSo I just use, let's just say,\\nreturn self.data.groupby,\\nand inside that groupby, I have to give the column name\\nand my column name is what?\\nMy column name is Product\\nbecause I want to do groupby based on this product column.\\n\\nAnd what I'll do is, for this column, Sales,\\nI want to have the sum of it.\\nSo I'm just saying that,\\ngroup the data based on this product column\\nand for every product, do the sum of all the sales.\\nThat's how I can get the total sales per product.\\nNow the next thing that I want,\\nto identify the best-selling product.\\n\\nNow how I can identify the best selling product?\\nThe best selling product, I can find out in this manner.\\nSo let's say I have one more function,\\nwhich I'll call def best_selling_product.\\nAnd if you want to find out the best selling product,\\neventually you need how much sale is there for each product.\\nSo eventually I need total sales per product,\\nand out of that I want to find out which is the maximum.\\nSo what I'll do is I will again just say like,\\nreturn self.total_sales_per_product.\\n\\nThat's how I will get it.\\nAnd whatever I'm getting, this total sales per product,\\nI will do this sort.\\nAnd how I will do, I will say that sort values\\nand I will sort it based on descending order.\\nSo what I'll say is ascending equal to false.\\nSo then it'll give me the, in the higher to lower order,\\nthat is the descending order.\\nAnd out of that what I will do is I will just say like,\\ndot-index, zero element.\\n\\nThat means that whatever be the dataframe I get,\\nit is a sorted data frame and the first element\\nin the data frame eventually will become\\nthe highest sold product.\\nSo that's how I can get the best selling product\\nfrom total sales per product.\\nNow this is also done.\\nNow we're left with the third option here,\\ncompute average daily sales.\\nSo again, average daily sales means is you are trying\\nto find out the mean.\\n\\nSo how I can do that,\\nI just created one more function I call,\\naverage_daily_sales.\\nAnd again inside that I will pass that self\\nand then I will just say return, self.data.\\nAnd I have to find out the average of the sales.\\nSo inside that I will pass the column name as Sales,\\nand for that I will say mean.\\nSo this is going to give me the mean of all the sales.\\n\\nSo by this we have finished up this data manipulation part.\\nNow the full thing comes as visualization.\\nPlot sales trend over time.\\nHow I can do that?\\nLet's just, for that, also, let's have one more function.\\nWe call it like def, plot_sales_trend.\\nAnd in that also, let me pass the self here.\\nNow if you remember correctly, if you want to show the plot,\\nthen you have to import a couple of the libraries.\\n\\nRemember what are those libraries?\\nThe library which I want to import is the matplot one.\\nI will say import matplotlib.pyplot,\\nand then I call it like as PLT.\\nNow in this plot sales trend function, how I will do that?\\nAgain, I need the same thing again,\\nI have to use self.data,\\nand this time I'm plotting the sales trend.\\n\\nSo basically what I'll do is I will plot it\\nbased on the days, right?\\nSo I will do the groupby,\\nand this time I will do this groupby based on the column\\nthat is date column.\\nAnd here, on a date basis, I want what?\\nI want sales per day, right?\\nSo I will say groupby date and you get the sales sum,\\nand that's how you will get a dataframe\\nwhere your one column is date\\nand another column is the total sales\\nfor that specific date.\\n\\nAnd if I want to plot, I will just say dot-plot,\\nand inside that, I will say that kind equal to line.\\nSo this will be a line plot.\\nAnd just to set a few more things, like plot or title\\nand give the title, like give sales trend over time.\\nYou can also set the x axis and y axis.\\n\\nSo I will say that plot X level is date\\nand Y label is our total sales.\\nSo I will say that plot Y label,\\nis total sales.\\nAnd finally I will say plt.show.\\nSo this is going to display our plot which is showing\\nthe trend of the sales over time.\\n\\nNow let's just see the next item.\\nThe next item is, display sales per product\\nin the bar chart format.\\nSo let's have one more function.\\nWe call it def, plot_sales_per_product.\\nAnd then again I will pass self.\\nThen I will say like self.total_sales_per_product,\\nbecause if you remember on the top we already\\nhave this function, total sales per product,\\nwhere we are getting the sales count\\nbased on the product.\\n\\nSo rather than finding it, again,\\nI'm just reusing this function here so\\nthat I will get the dataframe which has the product\\nand the sales for that product.\\nSo I will use this simple function,\\nand on top of it I just say dot-plot.\\nAnd under that I will say that it's a kind of what?\\nIt's a kind of bar.\\nSo this will become a bar chart,\\nand then, rest remains the same.\\n\\nI will just say like plt.title\\nand I give the title name as Sales Per Product,\\nand then we'll say plt.xlabel,\\nand our X label would be Product,\\nand plt.ylabel,\\nand that is Total Sales.\\nAnd finally we'll say that plt.show.\\n\\nSo we have created this visualization part as well\\nthrough the function manner.\\nSo up to this we are covered.\\nNow we have already created the classes and functions.\\nSo finally we're left with a script\\nwhich creates an instance of this class\\nand calls all these methods in a one by one manner.\\nSo let's just quickly do that.\\nSo let me create one more cell here,\\nand in the cell what I'll do is I'll create an object\\nof this class that is Retail Sales Analyzer,\\nand in that I'll just pass an empty.\\n\\nAnd now just start printing the things.\\nHow?\\nWhat is the first thing which we wanted to print?\\nTotal sales per product.\\nSo I'll say total sales per product.\\nAnd here I will say analyzer-dot, calling that function,\\nthat is total sales per product.\\nSo this function will get called\\nand print me the total sales per product.\\n\\nNow the next thing pro I wanted to do\\nis wanted to identify best selling product.\\nSo I will just say Best Selling Product.\\nThis will become again using the same object analyzer\\nand analyzer.best_selling_product.\\nSo this function will get called, which will go\\nand print me the bestselling product.\\nFinally, I wanted to have our average daily sales.\\n\\nSo I will say Average Daily Sales,\\nand then inside that average daily sales, I will say,\\nanalyzer.average_daily_sales function.\\nAnd lastly, we want to show the plots.\\nSo how I can do that?\\nI just use the function analyzer.plot_total_sales_product\\nand analyzer.plot_sales_trend, and done.\\n\\nWe are almost done.\\nIf you see the sixth point, it's saying that,\\nwriting a script that creates the instance\\nand calls methods.\\nSo this is what we have done.\\nWe have created the instance\\nof the retail sales analyzer class,\\nand I'm using that object to call all those methods.\\nSo let's quickly run it.\\nFirst, run this.\\nSo we have executed this cell, so our class got created.\\n\\nAnd then let's run this.\\nWe got one small error.\\nWhat is that?\\nI think I have done a little bit mistake\\non this label spelling.\\nThe spelling has to be L-A-B-E-L.\\nSo that is a typo.\\nLet me just change that quickly,\\nand let's rerun this cell and rerun this again.\\n\\nNow you can see that you got everything\\nwhat you are looking for.\\nSo let's start with this.\\nWe got total sales per product.\\nSo product E got 130, B got 140, C got 150.\\nThen we got best selling product is product C.\\nThat is true because this product C has a maximum sale\\nand our average daily sale is 52.5.\\nAnd you can see the sales per product\\nin the graphical manner.\\n\\nThat is the bar chart.\\nAnd you can see the sales trend over time.\\nSo on January 1, we have this,\\nJanuary 2, we have the highest,\\nJanuary 3, it's decreasing,\\nand 4th, we have very low sales.\\nSo that's how we have finished our capstone project.\\nI hope you have enjoyed this entire path,\\nyou learned a lot and you have,'\\nable to practically implement all the knowledge\\nwhich you have gathered throughout this course.\\n\\n\"}],\"name\":\"11. Capstone Project\",\"size\":43427141,\"urn\":\"urn:li:learningContentChapter:2126316\"},{\"duration\":51,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4250298\",\"duration\":51,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"3896057_en_US_12_01_4482577_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":86,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1268330,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Deepak] That brings us to the end of the course.\\nYou did great in understanding\\nand learning Python for data engineering,\\nbut we have only scratched the surface of the data world,\\nso by no means is this the end.\\nI encourage you\\nto keep moving forward in your learning journey.\\nYou can start by taking my live data engineering course,\\nMission 100 Azure Data Engineer.\\nYou can find the program on my LinkedIn profile page itself.\\n\\nAgain, my name is Deepak Goyal,\\nand it has been both my pleasure and honor\\nto teach you the skill of Python for data engineering.\\nThank you for watching, investing in yourself,\\nand for allowing me\\nto be a small part of your learning journey.\\n\"}],\"name\":\"Conclusion\",\"size\":1268330,\"urn\":\"urn:li:learningContentChapter:4258009\"}],\"size\":566268088,\"duration\":19581,\"zeroBased\":false},{\"course_title\":\"Data Science Foundations: Python Scientific Stack\",\"course_admin_id\":3084641,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":3084641,\"Project ID\":null,\"Course Name\":\"Data Science Foundations: Python Scientific Stack\",\"Course Name EN\":\"Data Science Foundations: Python Scientific Stack\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"Join instructor Miki Tebeka as he dives into the Python scientific stack and shows you how to use it to solve problems. Miki covers the major packages used throughout the data science process: numpy, pandas, matplotlib, scikit-learn, and others. He also guides you through how to load data, analyze data, run models, and display results.&lt;br&gt;&lt;br&gt;This course is integrated with GitHub Codespaces, an instant cloud developer environment that offers all the functionality of your favorite IDE without the need for any local machine setup. With GitHub Codespaces, you can get hands-on practice from any machine, at any time\u00e2\u20ac\u201dall while using a tool that you\u00e2\u20ac\u2122ll likely encounter in the workplace. Check out the \u00e2\u20ac\u0153Using GitHub Codespaces with this course\u00e2\u20ac\u009d video to learn how to get started.\",\"Course Short Description\":\"Learn about the Python scientific stack, with an emphasis on how to use it to solve problems.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":7418954,\"Instructor Name\":\"Miki Tebeka\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"CEO at 353Solutions\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2022-11-09T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/data-science-foundations-python-scientific-stack-17064277,https://www.linkedin.com/learning/data-science-foundations-python-scientific-stack-project-mojo\",\"Series\":\"Deep Dive (X:Y)\",\"Limited Series\":null,\"Manager Level\":\"General\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Data Science\",\"Primary Software\":\"Python\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":8738.0,\"Visible Video Count\":66.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":506,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4360136\",\"duration\":50,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"The Python scientific stack\",\"fileName\":\"3084641_en_US_00_01_3006561_WX30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"The Python scientific stack is widely used in data science. In this video, learn why it's very popular and what its main components are.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3327678,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Miki] Data science is booming,  \\n and Python is a major player there.  \\n Hi, I'm Miki Tebeka, and together we'll crunch some data,  \\n train some models, create visualizations,  \\n and generally have fun with Python and data.  \\n We'll solve many exercises while learning.  \\n I highly recommend that you follow along and write code.  \\n From my experience, there's some magic happening  \\n between the fingers and the brain  \\n that greatly improves the learning process.  \\n Don't be afraid to make mistakes,  \\n Python is very forgiving language  \\n and recovering from errors is easy.  \\n During this course we'll cover tools and techniques  \\n that I find useful in my daily work processing data  \\n and gaining insights from it.  \\n I hope you'll find these tools  \\n and techniques beneficial as well.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4365164\",\"duration\":20,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"3084641_en_US_00_02_3006561_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"This course assumes some prior knowledge. This video explains the skills and knowledge you need in order to succeed in this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1045352,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] This course is about Python and data science.  \\n It'll help if you know a bit of both.  \\n I'm not going to teach you Python.  \\n If you don't know Python, check out  \\n our offerings and then come back.  \\n I'm not going to teach you the basics  \\n of data science either.  \\n You can get by with a little math.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361151\",\"duration\":361,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Using GitHub Codespaces with this course\",\"fileName\":\"3084641_en_US_00_03_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14203918,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Ray] This is Ray Villalobos,  \\n senior staff instructor at LinkedIn Learning,  \\n and I'm going to show you how to work  \\n with LinkedIn Learning courses using GitHub Codespaces.  \\n Codespaces is a code editor in the cloud  \\n with the full power of Visual Studio Code.  \\n It allows for real-world hands-on practice  \\n that mirrors software development in the workplace.  \\n This course was created using GitHub Codespaces.  \\n Using Codespaces, you have everything you need to get going  \\n without needing complex installations or build tools.  \\n One click and you're ready to go.  \\n First, make sure you've signed up for a free GitHub account  \\n and that you've logged in.  \\n You just see your profile icon right here.  \\n When you go to the repository for one of our courses,  \\n look for the Code button.  \\n Click on Create codespace.  \\n The first time you open up a codespace,  \\n it might take a few minutes to create the virtual machine.  \\n Once it's done, you'll enter the code editor environment.  \\n If you're familiar with Visual Studio Code,  \\n this is a special version of that editor  \\n running on GitHub's servers.  \\n The cloud editor's menu lives in a hamburger icon  \\n inside the activity bar.  \\n The menu will let you see the keyboard shortcuts  \\n for your operating system.  \\n Let's find the shortcut for the Command Palette.  \\n Look in the hamburger menu, under View,  \\n and then look for the keyboard shortcut  \\n for your operating system.  \\n The activity bar has the most common tools you'll need  \\n to work with projects.  \\n You can, for example, show or hide the explorer,  \\n do a search and replace for content,  \\n manage features of the repository, and much more.  \\n By the way, if for any reason you lose the activity bar,  \\n you can get it back using the Command Palette.  \\n Your course can have one or more extensions pre-installed  \\n based on the course you're taking.  \\n Those can be found in the Extensions panel.  \\n Because this is a browser,  \\n it's easy to inadvertently close the browser window  \\n and lose the editor.  \\n If you do that, don't panic.  \\n Codespaces saves everything you're doing  \\n on a virtual machine.  \\n You can always get the codespace back for this repo  \\n by going to the Code menu  \\n and finding your codespace right here.  \\n Notice that it even remembered  \\n that I had opened the Extensions panel.  \\n The sample course I'm looking at  \\n is what is known as a flat branch repository.  \\n You can tell because there's a single branch  \\n when you click on the Branch icon on the status bar.  \\n You can also tell because there are different folders  \\n for each of the videos in the course.  \\n Your instructor will let you know  \\n what the folder structure is for your course.  \\n Forking lets you create your own copy  \\n of the repository on your account  \\n so that you can keep any changes that you've made,  \\n even if you've deleted the codespace.  \\n To create your own fork,  \\n you can click on the Fork button on the repository.  \\n I'm going to hit Create fork.  \\n I have my own copy of this repository,  \\n and if I want to,  \\n This fork version  \\n is almost exactly like the original repository,  \\n but it will let you push your own changes.  \\n Notice that the URL of the repository is slightly different.  \\n Let's go ahead  \\n and make a simple change to this file right here.  \\n I'm going to save it.  \\n You'll note that there is an M right here,  \\n as well as an asterisk on this branch.  \\n In the Source Control panel,  \\n you can also see a 1 for the change that we just made.  \\n Let's go ahead and try to commit this change.  \\n I'm going to hit the Commit button,  \\n and I'll ask it to go ahead and stage the changes,  \\n and then I'm going to hit this button here to commit this  \\n onto my own version of the repo.  \\n I'll hit OK.  \\n You can also let it go ahead and run git fetch.  \\n That way, it'll automatically sync with your forked repo.  \\n Now, that change will be stored  \\n in your own version of this repository.  \\n Don't worry if you forget to fork a repo  \\n and then try to push changes.  \\n Codespaces will also ask you  \\n if you want to create a fork automatically.  \\n Look for additional course-specific tips  \\n from the instructor.  \\n Now, let's get back to the course.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4359127\",\"duration\":75,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Setup\",\"fileName\":\"3084641_en_US_00_04_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"There are several tools and packages used throughout the course. In this video, learn about the tools used and how to install them.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2592266,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] This course is using GitHub's Codespaces,  \\n which mean everything is already preconfigured for you.  \\n To get started, head over to the repository on GitHub.  \\n Click on Code, check the Codespaces tab,  \\n and click on Create codespace on main.  \\n And now you are going to wait a bit.  \\n Once you see Visual Studio starting up,  \\n wait a bit before you jump into the code.  \\n The sign that Visual Studio Code is ready  \\n is when you see the \\\"Get started with Jupyter Notebooks\\\"  \\n on the right-hand side.  \\n And you see that the terminal now is free of output.  \\n Now you can go on, close the terminal and these two windows,  \\n and you're ready to go.  \\n You have a machine with Python, all dependencies installed,  \\n and Visual Studio Code ready to run everything.  \\n This is going to save you from all the trouble  \\n of installing Python and all the scientific applications.  \\n But sooner than later, you will need to learn  \\n about how to install Python packages with tools such as pip,  \\n and also about managing virtual environments.  \\n Go and check our offerings on the subject.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":21169214,\"urn\":\"urn:li:learningContentChapter:4361167\"},{\"duration\":310,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4360137\",\"duration\":143,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Use code cells\",\"fileName\":\"3084641_en_US_01_01_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Code cells allow you to write code and run it in parts. In this video, learn how to write code cells and how to use them for efficient development.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4429323,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Writing code when exploring data  \\n is a bit different than writing code for applications.  \\n You write small pieces of code  \\n and you want to see them in action.  \\n For that end Visual Studio Code supports code cells.  \\n They start with a special comment,  \\n so we have a comment and then the two percent signs.  \\n And you see now that we have some actions  \\n that we can do on the top.  \\n You can run the cell either by clicking on one cell  \\n or hitting shift and enter.  \\n This is going to open an interactive prompt on the right.  \\n The first time you run a cell,  \\n this might take a bit of time.  \\n And here we see the result.  \\n Shift and enter will also  \\n automatically advance to the next cell,  \\n so you can run the next cell again with shift and enter.  \\n The interactive prompt can display more than just text.  \\n Here we have a piece of code that is going  \\n to display the sin of x between minus six and six.  \\n If you don't understand the code, don't worry about it.  \\n We'll get to it.  \\n So shift and enter again.  \\n And now we can see also the chart.  \\n Let's try and run the code from the command.  \\n So I'm going to open a terminal with new terminal,  \\n and then python Ch01/01_01/code_cells.py,  \\n and nothing is being printed out.  \\n The reason is that the interactive prompt  \\n is what is known as a REPL: read, eval, print and loop.  \\n When you run code from the command line,  \\n it only does read and eval.  \\n It's not going to print unless you explicitly print.  \\n And it's not going to loop unless you do a loop.  \\n Let's close the terminal.  \\n You can also write code in the interactive terminal  \\n without adding it to your code.  \\n So let's do 355 divided by 113,  \\n and I can hit play to run the code.  \\n And this is an approximation of pi.  \\n Sometimes you run Command and it takes a lot of time,  \\n and you forget to add the result to a variable.  \\n If you look at Visual Studio Code,  \\n you will see small numbers out here.  \\n These are the input and output numbers.  \\n So I can do, for example,  \\n output number 4 times 10 and run it.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4359128\",\"duration\":96,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Extensions to the Python language\",\"fileName\":\"3084641_en_US_01_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"The Python scientific stack adds some extensions to the Python language. In this video, learn about these extensions and how to use them.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3547488,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - IPython  \\n which the Visual Studio Code Python extension is built on  \\n offers extensions to the Python language to  \\n make interactive development easier.  \\n The first extension is called magics.  \\n They start with a percent sign  \\n and when I'm going to run the cell, the PWD Magic is  \\n going to tell me the present working directory.  \\n These extensions are extensions to Python.  \\n If you're going to run Python on this file,  \\n it is going to complain and the execution will fail.  \\n Some cells work on a single line.  \\n This is the time magic, and it works  \\n on a single line to check how much a sleep  \\n of 0.1 seconds  \\n takes.  \\n But this magic sometimes can work  \\n on more than a single line.  \\n Starting with 2% signs.  \\n These are known as cell magics  \\n and I'm going to run this one.  \\n It is going to time the whole execution of the cell.  \\n Another extension is running shell commands  \\n with the bank sign.  \\n If I'm going to run this,  \\n IPython or the interactive prompt are going to  \\n execute the LS command on the Varlog directory  \\n and I'm going to see the output.  \\n You can do fancier stuff, for example  \\n assign dirname to a viable, get the output of  \\n ls  \\n of this dirname when I'm passing it with a dollar sign  \\n as a parameter and now I have files which is like a list  \\n in Python.  \\n So I can print out how many files I have.  \\n Visuals to the code offers more features  \\n that will help you become more productive.  \\n Do your homework and learn what it has to offer.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4360138\",\"duration\":71,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand markdown cells\",\"fileName\":\"3084641_en_US_01_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Markdown cells allow you to write markdown, as opposed to code. Learn how to use markdown cells to write effective documentation.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3821578,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Speaker] Markdown is a common format  \\n to add accomodation with rich text.  \\n People even published full books using Markdown  \\n in a scientific notation.  \\n In Visual Studio Code, you can write Markdown cells  \\n by adding the Markdown inside square brackets.  \\n When you run the cell,  \\n Visual Studio Code is going to render the Markdown as html.  \\n So, we have a header and then a list.  \\n Markdown can do more.  \\n Here's an example of a link  \\n and adding an image.  \\n Right, so you have Python.  \\n If you're going to click that,  \\n this is going to take us to Python  \\n and the Python image.  \\n In scientific documentation,  \\n you write a lot of math.  \\n There is special support for LaTex  \\n inside Visual Studio Code  \\n markdown cells.  \\n You write laTex inside the dollar sign  \\n and this is the laTex language  \\n if I'm going to run this cell now,  \\n you are going to see  \\n the equation  \\n If you're not familiar with laTex  \\n it's a language for writing scientific paper  \\n and it's a bit out of scope of this video.  \\n There's a lot of good resource for writing Latex out there.  \\n \\n\\n\"}],\"name\":\"1. Visual Studio Code\",\"size\":11798389,\"urn\":\"urn:li:learningContentChapter:4362143\"},{\"duration\":1115,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4365165\",\"duration\":116,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"NumPy overview\",\"fileName\":\"3084641_en_US_02_01_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"NumPy is a fundamental package. In this video, learn the main features it offers.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4393313,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Speaker] Let's look at one  \\n of the building blocks of the scientific Python stack  \\n which is NumPy sometimes pronounce NumPee.  \\n NumPy gives us fast homogenous arrays or matrices  \\n math functions, linear algebra, randomization, and more.  \\n You can ask,  \\n why do we need matrices when we have python lists?  \\n Python lists are powerful and very fast.  \\n However, since they need to accommodate many use cases  \\n there's a performance and memory penalty you pay.  \\n Let's have a look.  \\n So here's the Python integer,  \\n and if I'm going to run this one,  \\n I'm going to get two to the power of 1000,  \\n which is great.  \\n but it's going to take a lot of memory.  \\n in NumPy, If I'm going to take the N64 and run this cell  \\n I'm going to get zero.  \\n Because what is known as an overflow,  \\n the number cannot fit in a 64 bit integer.  \\n In scientific computation,  \\n Usually a 64 or 32 bit integer or float  \\n can hold all the values you need.  \\n under the hood.  \\n NumPy uses highly optimized C and Fortran code.  \\n Don't look down at Fortran,  \\n the Fortran Compiler can sometimes do optimizations  \\n that the C compiler can't.  \\n especially when dealing with arrays.  \\n NumPy offer more than speed, selecting, reshaping  \\n multiplication and more.  \\n It also provides a lot of scientific functions.  \\n All of these functions work both on scalers  \\n integers or floats and on arrays.  \\n So here we have NumPy sine and you compute the sine of 27.  \\n or we can run sine on an array of three elements  \\n and get the sine for every element in the array.  \\n We import NumPy as np  \\n since NumPy has a lot of functions inside.  \\n So instead of doing from NumPy import and then a long list  \\n we import NumPy as np, and then np dot np dot.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361152\",\"duration\":202,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"NumPy arrays\",\"fileName\":\"3084641_en_US_02_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"NumPy arrays are super fast, multidimensional arrays. In this video, learn about its capabilities.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7043253,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's start looking into NumPy arrays.  \\n So first we input numpy, as np.  \\n And now, I'm creating an array  \\n of three elements with one, two, and three.  \\n Run the cell.  \\n And we have an array one, two, and three.  \\n For starts, arrays look very much like lists.  \\n We can ask for the len, we can get the second element.  \\n And arrays are zero-based indexing like the rest of Python.  \\n But if you look closer,  \\n for example, type of the first element,  \\n we see that it's not a Python integer but a numpy.int64.  \\n You can ask array what is the data type  \\n of the array using the dtype attribute.  \\n When you create an array,  \\n NumPy will determine the default type  \\n for the array for the input.  \\n However, you can specify explicitly the data type  \\n of the array to be created.  \\n This here is saying, dtype=np.int32.  \\n And we are going to get an array of int.32.  \\n Comparing to int.64, where this takes half the memory,  \\n but you need to make sure  \\n that all of your values fit inside a 32-bit integer.  \\n You can multiply an array by itself.  \\n And what you're going to get  \\n is an element-wise multiplication.  \\n So one times one, two times two, and three times three.  \\n I talked about NumPy being fast.  \\n Let's have a look.  \\n So I'm creating two arrays,  \\n each one with 1 million random elements, which are floats.  \\n And then, I'm going to use the time matrix  \\n to see how much time it takes to multiply them.  \\n And that took 6.27 milliseconds.  \\n If you'd like to get the dot product  \\n of a matrix, use the @ sign,  \\n which is the Python Matrix Multiplication Operator.  \\n If you run the cell now, we are going to see 14,  \\n which is a dot product of these two arrays.  \\n We can move to more dimensions.  \\n So here's an array with three rows and three columns.  \\n And we can run this one, and now we can see the array.  \\n That's a lot of code to write for a small sample matrix.  \\n Let's use one of the many utilities we have in NumPy  \\n to create arrays.  \\n np.arange,  \\n it's very much like Python Range, but returns an array.  \\n So if you do V equal arange of 12,  \\n you are going to get an array from zero to 12.  \\n And now, we can use the reshape to reshape the array.  \\n So if you're going to run this one,  \\n now we're going to get an array  \\n with for rows and three columns.  \\n So in a single line,  \\n we can run mat = np.arange(12).reshape((4, 3)),  \\n and then, get our matrix.  \\n Can call the dot shape attribute  \\n to know what is the current shape  \\n or how many rows, columns,  \\n and maybe, other dimensions the matrix has.  \\n Let's create another matrix  \\n by reshaping the original one.  \\n Instead of four rows and three columns,  \\n three rows and four columns.  \\n And now, we're going to change a value  \\n in this reshaped matrix.  \\n And then, take a look at the original one.  \\n And we see that the original matrix is affected as well.  \\n When you do a reshape,  \\n you get a view of the underlying data.  \\n It's not a copy.  \\n There are many other methods  \\n and function that work with NumPy arrays.  \\n For example, you can transport the matrix  \\n by calling the dot T attributes.  \\n You'll pick more as you go, but I think this is enough  \\n for understanding the basics of NumPy arrays.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361153\",\"duration\":109,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Slicing\",\"fileName\":\"3084641_en_US_02_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Slicing is a way to get parts of an array. In this video, learn how to slice an array and use the parts of the data you need.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3590054,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Python lists support slicing.  \\n Here's an example.  \\n nums is a list of 1, 2, 3, 4, 5,  \\n and we do a slice from 2 to 4.  \\n If you're going to run it,  \\n you're going to get 3 and 4.  \\n We start with the index number 2, index number 3,  \\n and up to but not including index number 4.  \\n These are half-open ranges.  \\n You can do the same for NumPy arrays,  \\n so we create a vector with a range  \\n and then get a slice of this vector or matrix.  \\n Slicing is useful  \\n since many times you'd like to take a subset of your data  \\n for training, testing, and other tasks.  \\n NumPy takes slicing to a whole new level.  \\n Let's have a look.  \\n Let's create an array with three rows and four columns.  \\n When we take the array at location 0,  \\n we are going to get the first row.  \\n We can also say the second element in the second row,  \\n and this is 5, so this one here.  \\n We can also do it in another way by saying I want 1, 1  \\n and this will give me the same element.  \\n What happens if you want a column?  \\n For a column, we're going to use slicing.  \\n Say we want all the rows  \\n but only the second element for every row,  \\n and now we're going to get 1, 5, and 9,  \\n which is the first column.  \\n Note that we'll get a flat array and not an array of ones.  \\n If you want to get the full array of ones,  \\n you will need to reshape,  \\n so now we get it in a vertical format.  \\n You can do slicing on both axes,  \\n so all the rows from the second row  \\n and the columns from the third column.  \\n You can also use slicing to set values,  \\n so I'm doing the slicing as before, but now equals 7,  \\n and now when I'm looking at the array,  \\n I'm seeing that the bottom right is all sevens.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4362135\",\"duration\":168,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Learn boolean indexing\",\"fileName\":\"3084641_en_US_02_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Boolean indexing lets you select parts of an array based on a condition. In this video, learn to effectively use boolean indexing.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5606070,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Sometimes you'd like to select parts  \\n of an array, not by indices but by some logic,  \\n say all the values that are bigger than some threshold.  \\n For this we're going to use Boolean indexing,  \\n let's see how it works.  \\n So we input NumPy and create an array with three elements,  \\n let's run it.  \\n You can index an array with another array in the same shape  \\n containing Boolean values, true or false?  \\n So here's the array  \\n and I'm adding it an index with true or false, true.  \\n And I'm going to run it,  \\n I'm going to get only the elements where the index was true.  \\n For now this does not seem that helpful,  \\n but let's see one more thing  \\n and this will make it super useful.  \\n If I'm doing array bigger or equal to one,  \\n I'm going to get a Boolean array  \\n with false, true and true,  \\n and now I can combine these two and say array  \\n at the location where the array is bigger or equal to one,  \\n and I'm getting one and two but not the zero.  \\n You can combine these conditions  \\n or mask using Boolean operators.  \\n However, these are not the normal Boolean operators  \\n that we have in Python.  \\n You're going to use 1% for and,  \\n the vertical bar or and, the Tilda for no.  \\n So I have a bigger array now with 10  \\n and I'm saying all the elements that are bigger than two  \\n and smaller than seven,  \\n note that the parenthesis here is mandatory.  \\n And here's an example of negation,  \\n so everything that is not bigger than seven.  \\n Usually you're going to write it as everything  \\n that is smaller or equal to eight.  \\n Let's do a more realistic example,  \\n we're going to find outliers using the standard score.  \\n First we're going to create our data.  \\n So I'm going to create an array of thousand elements  \\n with values between zero and 10  \\n with the normal distribution,  \\n and then I'm going to add two out outliers  \\n at location 33, at location 832.  \\n Now I'm going to calculate my mask.  \\n So the mask is where the absolute value of the value itself  \\n minus the mean, meaning the distance from the mean,  \\n is bigger than two times the standard deviation  \\n inside the array,  \\n and then I'm going to calculate values in the mask,  \\n I'm going to see exactly my outliers over there.  \\n I can even use this mask to change the values,  \\n let's say to the mean of the current array.  \\n This is an example for the power of Boolean indexing.  \\n You can do a lot of things in very few line.  \\n Apart from the cool effect, this is also very fast.  \\n In numerical Python, you try hard not to do any follow ups.  \\n This method of computation is called vectorization  \\n and once you use it, everything runs  \\n at the sea or photon level of NumPy.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4363229\",\"duration\":79,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand broadcasting\",\"fileName\":\"3084641_en_US_02_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Broadcasting allows operations between unequal shaped arrays. In this video, learn how to use broadcasting for effective computation.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2602677,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's have a look at the piece of code.  \\n We input NumPY as p.  \\n We create a range and then add four to it.  \\n Let's run this one.  \\n We added the scalar to a vector.  \\n NumPY does what makes sense,  \\n it adds the scaler to every element of the array.  \\n This is known as broadcasting.  \\n Broadcasting work for other dimensions as well.  \\n So we're going to create three to three matrix  \\n and then add a vector of three  \\n and then matrix plus the vector.  \\n And this time we add the vector to every row.  \\n We can even do a horizontal vector  \\n and a vertical vector and add them together.  \\n NumPY will extend these vectors  \\n and when we add them,  \\n we'll get a matrix of three by three.  \\n It'll expand V1 vertically  \\n and V2 horizontally to match each other.  \\n Here's an image from the astroML site  \\n that will help you understand what broadcasting is doing.  \\n This is roughly the operations we just did.  \\n NumPY tries hard to make broadcasting possible  \\n but sometimes it is going to fail.  \\n I highly recommend you watch  \\n \\\"Losing your Loops\\\" by Jake VanderPlas  \\n to understand fully what broadcasting can do for you.  \\n Try to follow what Jake is doing in his kNN example.  \\n Run the code, tinker with the data, and generally, have fun.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4364132\",\"duration\":142,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand array operations\",\"fileName\":\"3084641_en_US_02_06_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"NumPy arrays offer a wide variety of operations. Learn about these operations and how to use them.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5170870,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's see what an array can offer us.  \\n So, \\\"import NumPy as np\\\",  \\n and then create an array of three rows and three columns.  \\n Let's run it.  \\n I'm going to do \\\"v.\\\".  \\n You can see a lot of things  \\n that can be accessed from the array.  \\n Let's take a look at some of these attributes.  \\n So, \\\"v.T\\\" returns the transpose of the matrix,  \\n \\\"v.any\\\" will return true  \\n since there is at least one element in the array  \\n which is true.  \\n As a reminder, everything in Python is true  \\n except the zero value numbers, empty collections, none,  \\n and of course, false itself.  \\n We also have \\\"v.all\\\", and this one is going to return false.  \\n This is because we have one false element at the beginning,  \\n which is zero.  \\n Any and all are useful functions,  \\n and you'll find out you're going to use them  \\n in various situations.  \\n Know that the truth value of an array is undefined,  \\n so if I'm going to \\\"if v:print (ok)\\\" and run the cell,  \\n I am going to get an exception.  \\n The Zen of Python states that,  \\n \\\"In the face of ambiguity, refuse the temptation to guess.\\\"  \\n Experience shows that it's better to be safe  \\n than guess something that would lead to a wrong computation.  \\n This is why NumPy refuses to guess the truth value  \\n of an array, and you will need to be explicit  \\n and state either any or all.  \\n NumPy also provides some mathematicals like min, max,  \\n mean, SD, sum, prod, and others.  \\n Let's take the product of the array,  \\n which is zero because we have zero at the beginning.  \\n This method work by default on the whole array,  \\n but you can specify which axis  \\n you'd like the operation to happen.  \\n For example, if you do axis equal one,  \\n you'll get the sum of the rows,  \\n and if you'll do axis equal zero,  \\n you'll get the sum of the columns.  \\n This will work with array  \\n with more than two dimensions as well.  \\n Just provide the right axis.  \\n Remember we said that NumPy works hard not to copy anything,  \\n but sometimes you do want to copy and mutate an array  \\n without affecting the original one.  \\n For this, you can use the copy method, so V1 is a copy.  \\n We change v1, the first element, to be minus one  \\n and then look at the original one  \\n and see that nothing has changed.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4362136\",\"duration\":159,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand ufuncs\",\"fileName\":\"3084641_en_US_02_07_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"ufuncs are functions that work both on arrays and scalars. In this video, learn about the advantages of ufuncs and how you can write your own.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5664778,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - There are many functions defined in numpy,  \\n how many, let's see,  \\n will input numpy,  \\n and then look at how many elements the dir command returned.  \\n And you're going to see 603,  \\n this might change depending on the numpy version you have.  \\n We are not going to cover all of the 600 plus attributes,  \\n but as you start working with numpy  \\n you'll find the ones useful for you  \\n and commit them to memory.  \\n However, there's one aspect of numpy functions  \\n I like to discuss.  \\n So if I'm doing the sine of 90  \\n I'm going to return a number,  \\n remember, sine gets its values as radians and not degrees.  \\n You might wonder when numpy is defining function  \\n that's already defined in the built-in math model,  \\n the reason is, that we can take creative vector  \\n which is np.arange from minus three to three  \\n and one numpy sine on that,  \\n and it's going to work, going to get the sine  \\n of every element in dir,  \\n the built-in math sine will not work.  \\n A lot of numpy functions are ufuncs or universal function,  \\n it means they work both on scales,  \\n regular numbers and arrays.  \\n This is very handy and let's us work with the same function  \\n regardless of the input type.  \\n What happens if you like to write your own function?  \\n Do you need to check the type of the input every time?  \\n Well, you can, but numpy provides an easier way.  \\n Let's have a look.  \\n So let's define ReLu,  \\n which if N is less than zero  \\n return zero, otherwise return N.  \\n And if you run it on a scaler it is going to work.  \\n However, if you try to run it on our vector,  \\n this is going to raise an exception,  \\n it'll try to do if N smaller than zero  \\n and the true value of a numpy array is not defined.  \\n What you can do is use the np.vectorize,  \\n the N at the beginning  \\n meaning we're using the np.vectorize as a decorator.  \\n If you're not familiar with decorators,  \\n we have several courses on the subject,  \\n so the same function, but now vectorized.  \\n And now when we run it, we can run it on the vector  \\n but we can also run it on the number,  \\n which is something that is not familiar.  \\n This is an array with one value,  \\n however, this array behaved just like a number,  \\n so if I'm doing our function on minus two  \\n and removing seven from it,  \\n I'm going to get minus seven.  \\n Ufuncs are also NaN aware,  \\n which means that if they see NaN,  \\n the special floating viral,  \\n that means not a number,  \\n they will return NaN as well.  \\n So we create an element with NaNs  \\n and we get the result with the NaN at that location.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4362137\",\"duration\":58,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Working with an image\",\"fileName\":\"3084641_en_US_02_08_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2225504,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Say we have this image of a flower.  \\n We can use Matplotlib to load this image into memory.  \\n So, we use the imread method, we create a copy  \\n to make it readable,  \\n and then I'm going to use plt to show it.  \\n If you're going to run the cell,  \\n you're going to see the flower image.  \\n If you look at the type of the image,  \\n you're going to see that it's a numpy array.  \\n And when we look at the shape of the array,  \\n we see it's 1200x900 on three.  \\n These are the number of pixels, so 1200 on 900,  \\n and three values for the color, red, green and blue.  \\n What I want you to do is draw a blue square  \\n around the flower.  \\n The top left should be at 190 on 350.  \\n The bottom right should be on 680 and 850,  \\n and the line width should be five.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4358207\",\"duration\":82,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Working with an image\",\"fileName\":\"3084641_en_US_02_09_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3481315,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Computer User] Let's have a look at my solution.  \\n First, we're going to run the code.  \\n So we're going to load the image.  \\n And we're going to print the type and the shape.  \\n Now we're going to look at our code.  \\n Let me hide the left side.  \\n So we define the top left X and the top left Y,  \\n and the bottom right X and the bottom right Y and the width.  \\n Also the color, which is blue, right?  \\n We have RGB, red, green, blue.  \\n Red and green are zero and blue is full.  \\n And now we draw the lines using slicing.  \\n So from the top left to the top left plus width  \\n up to the top left Y to the bottom right Y equal color.  \\n This is the top line.  \\n Then the bottom line, which is bottom right X.  \\n The bottom right X plus width, top left Y, bottom right Y.  \\n And then we're doing the vertical lines.  \\n So top left X to bottom right X,  \\n top left Y to top left Y plus the width.  \\n And then finally the right line.  \\n Top left X to bottom right X,  \\n bottom right Y minus width to bottom right Y.  \\n And then we are going to show the image.  \\n So let's scroll a bit up and run this cell.  \\n And now we see the picture  \\n and we have a square around the flower.  \\n \\n\\n\"}],\"name\":\"2. NumPy Basics\",\"size\":39777834,\"urn\":\"urn:li:learningContentChapter:4361168\"},{\"duration\":1040,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4361154\",\"duration\":79,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"pandas overview\",\"fileName\":\"3084641_en_US_03_01_3006561_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"pandas is the most used library in data science. In this video, discover why it is popular and some of its capabilities.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3182610,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Data in the real world is often composed  \\n from values that have mixed types.  \\n Some can be numeric: age, for example.  \\n Some can be textual: names, for example.  \\n Some can be time, like the year of birth,  \\n etcetera, etcetera.  \\n Pandas is a library built for real-world data,  \\n data that is messy, incomplete, and from various types.  \\n Once pandas was released,  \\n it was adopted by the scientific community  \\n as the tool for dealing with data.  \\n Pandas also excels with working with time-series data.  \\n You can easily select subsets of data by time  \\n and also change the frequency of the events  \\n to suit your needs.  \\n You can think of pandas  \\n as providing in-memory Excel data sheet,  \\n which we call DataFrame,  \\n with many, many extra features.  \\n Panda's DataFrame can process large amount of data.  \\n I've personally worked with millions of rows,  \\n and it can draw charts, join other DataFrames,  \\n pivot tables, and more.  \\n Apart from being very useful,  \\n pandas is one of the best-documented projects out there.  \\n Documentation contains many, many code examples.  \\n It is also worth reading the book,  \\n \\\"Data Analysis with Python,\\\"  \\n which was written by panda's creator, Wes.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4360139\",\"duration\":192,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Loading CSV files\",\"fileName\":\"3084641_en_US_03_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"CSV is a very common data format. In this video, learn how to load data from CSV files.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7768758,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Going to have a look at the log  \\n of one of my runs a few years back.  \\n We'll start by looking at the data, which is in CSV format.  \\n Pandas is going to load the whole CSV into memory,  \\n and it's a good idea to have a quick look  \\n at the data before you load some corrupted data  \\n or a file that is too big to fit in memory.  \\n If you don't have enough memory on your computer,  \\n you can spin a machine in the cloud  \\n with a lot of memory, do your work, and then tear it down.  \\n This approach is usually very cost effective  \\n and much easier than using a big data platform.  \\n If you think your data is too big,  \\n remember that you can spin a machine  \\n with several terabytes of memory.  \\n So here is our data.  \\n We have four columns, time, latitude,  \\n longitude and height.  \\n So let's start.  \\n First, let's look at the size of the file on this.  \\n So I'm going to use pathlib  \\n and then define what is a kilobyte.  \\n Define the CSV file and use the start method to get the size  \\n and divide it by kilobyte.  \\n ST size is in bytes.  \\n So let's run this one and I'm going to hide the sidebar.  \\n So 42.6 kilobytes.  \\n You can do the same thing with the shell command.  \\n So we start with the bank sign, telling the IPython  \\n or the interactive prompt to run a shell command.  \\n And this is an extension to Python.  \\n And we pass the name of the CSV file  \\n prefixed by the dollar sign as a variable.  \\n Note that this command will work  \\n only on Linux or on Mac, not on Windows.  \\n Now, let's see how many lines we have.  \\n So we do with CSV file open  \\n and you should open always file with a with statement.  \\n This makes sure that they are closed  \\n when you are done with them.  \\n So we're going to run it,  \\n count the lines and also print the first five lines.  \\n And we can do the same with the shell command.  \\n First, the head utility to see the first five lines,  \\n and then the WC dash L to see  \\n how many lines we have in there.  \\n Now we can load the data frame into pandas.  \\n So we input pandas as PD, and DF is PD read CSV.  \\n The variable holding the data frame is traditionally  \\n called DF, and we are going to print  \\n how many rows are in the data frame.  \\n So len of DF.  \\n And now we are going to get 740,  \\n while WC showed us 741.  \\n Pandas is loading the first row as the column names  \\n and counting only the rest of the rows,  \\n which are the data rows.  \\n Let's have a look at the column.  \\n So df.columns, and we see we have time, latitude,  \\n longitude and height, which matches  \\n what we actually see in the CSV.  \\n We can also run the info method to get  \\n some information about every column.  \\n So the column, how many non values we have,  \\n and what is the data type of every column.  \\n We see that the first column is an object,  \\n which means a string.  \\n And then we have floats for latitude, longitude, and height.  \\n You can also run the describe method,  \\n which will give you information about numerical columns.  \\n So only the latitude, longitude, and height.  \\n You'll get the count, the mean, standard deviations  \\n and other interesting statistics.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4358208\",\"duration\":107,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Parse time\",\"fileName\":\"3084641_en_US_03_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Time needs special treatment in CSV files. In this video, learn how to parse time from a CSV file.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5014540,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's load our DataFrame.  \\n So we import pandas, and we read the CSV,  \\n and we display the data types for each column.  \\n You can see the latitude, longitude, and height are floats,  \\n and the time is an object,  \\n which usually means a string in Pandas.  \\n In some cases, it's okay for data to be a string,  \\n but in our case, you expect some kind of a timestamp.  \\n Unlike JSON and other formats,  \\n CSV don't have type information embedded with the data,  \\n which means that everything comes out as a string.  \\n Pandas does a good job at guessing types,  \\n but here, it needs our help in parsing the time.  \\n There are many ways to write time as a string.  \\n Pandas parser know most of the common formats.  \\n If you need to write time as a string,  \\n do yourself a favor and use a known format,  \\n such as RFC 3339.  \\n Also, pick a format without spaces in it.  \\n And have the year first,  \\n so sorting time as a string will work as intended.  \\n Let's take a look at the read_csv method.  \\n It has many, many arguments.  \\n One of these arguments  \\n is parse_dates, where you can pass a list of columns  \\n to be parsed as times.  \\n This is where our initial look at the data came handy.  \\n You know, before loading,  \\n which column you'd like to parse as time.  \\n Let's do this.  \\n Going to hide the sidebar.  \\n So now we doing pandas with CSV, with the csv_file,  \\n and tell it to parse the dates with only a single column,  \\n which is a time column.  \\n And run the cell.  \\n And now, we see that the time is datetime64[ns],  \\n which means a 64-bit timestamp in a nanosecond resolution.  \\n On your machine,  \\n this might be a little bit different.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4365166\",\"duration\":254,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Access rows and columns\",\"fileName\":\"3084641_en_US_03_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"The main data type in pandas is a two dimensional DataFrame. In this video, learn how to access rows and columns.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9467062,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Let's access some data.  \\n So first we load the data frame  \\n and I'm going to hide the side column.  \\n To get a column, you can use the square bracket  \\n and the name of the column, so DF latitude  \\n and I'm getting the whole latitude column.  \\n You can also use df.la like an attribute access  \\n but I don't recommend using the dot plantation.  \\n In some cases, column names have spaces in them  \\n and then you can't use the dot plantation anyway, so get  \\n in the habit of using square brackets, which always works.  \\n You can select more than a single column.  \\n So here I'm passing a list with two column names  \\n and I'm going to get back at data frame with two columns.  \\n If you want a specific value  \\n you can first select the column and then the row.  \\n So selecting the latitude column and then the first value.  \\n Remember that Python is zero based and not one based.  \\n The first item is at zero location.  \\n If you want an entire row  \\n you can use the dot lock accessor, so dot lock.  \\n And now we get the rope.  \\n So we see the time, latitude, longitude, and height.  \\n Lo can also work with slices as well, so from two to seven.  \\n Unlike slicing in Python, which are half open  \\n the dot lock in pandas slices  \\n from the start to the end, including the end.  \\n You can also combine slices and column selection  \\n so we're selecting only the latitude and longitude columns.  \\n And then getting from two to seven.  \\n This time plant is going to use the Python style slicing  \\n which is a half open range, meaning you get the first index  \\n up to, but not including the last one.  \\n And now it's a bit confusing.  \\n The best way to make these things sink in, is to practice  \\n take some data frames, slice  \\n and dice them until you get comfortable  \\n with the results and it'll be just fine.  \\n Every column in Panas is as serious.  \\n One of the differences between seriousness  \\n and regular Python lists is that they have a labeled access  \\n called an index.  \\n All the columns in the data frame share the same index  \\n so we can do df.dot index and then run the cell.  \\n And this is a range index starting from zero until 740  \\n with steps of one.  \\n However, it doesn't have to be this case.  \\n Let's create a small example.  \\n So we create a data frame.  \\n We have some values with number by a range, five rows  \\n and two columns.  \\n We specify the columns and then we say the index is A, B, C,  \\n D, and E.  \\n If you're going to run disco code, you see now  \\n that the index series A, B, C, D, and E.  \\n And now if you're going to use dot lock zero  \\n this is going to fail because there's no row labeled zero.  \\n But there is a row labeled A, so I can df dot lock A.  \\n I can also slice between A and D and Panas can handle it  \\n just as well.  \\n As a side note, labels don't have to be unique.  \\n If they're repeating labels, you  \\n will get all the rows with this label.  \\n Sometimes you'd like to access the first row regardless  \\n of the label.  \\n In this case,  \\n So df dot I lock at location zero  \\n and this is going to work and bring us the first row.  \\n Another kind of index you can have is time-based index.  \\n So let's change the data frame index, which currently  \\n if we look at it is a range index and we are going to set it  \\n to the time and have a look at it.  \\n And now it is a daytime index with all of these values.  \\n Now, if you're going to run the lock zero  \\n we are going to see that it's going to fail  \\n because we don't have any role that is labeled with zero.  \\n However, I can access a row  \\n by time and I can pass the time as a string.  \\n and Panas is fine with it.  \\n You can even access time index with the time unit.  \\n So here we are going to access everything that is  \\n at 3:48 and we we're going to get all the roles that fall  \\n in this minute.  \\n By the way, I'm not jogging at these crazy times.  \\n These times are in Newt Sea and I'm at Israel  \\n which means the time was actually 6:48 AM. Still early,  \\n but not that early.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4360140\",\"duration\":200,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Calculate distance\",\"fileName\":\"3084641_en_US_03_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"You calculate speed from latitude and longitude. Learn how to move data and write functions.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7859211,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's calculate how slow I jog.  \\n For speed, we need time and distance.  \\n We have latitude and longitude,  \\n and we're going to cheat and use Euclidian distance  \\n instead of distance on a sphere.  \\n So first, we're going to load the data with read_csv.  \\n I'm going to hide the side frame.  \\n So let's have a look.  \\n One latitude in kilometers is about 92 kilometers,  \\n and one longitude in kilometers is about 111.  \\n It varies depending on where you are on earth,  \\n but it's good enough for what we're going to do.  \\n So we are going to define a distance function  \\n which takes the delta latitude, the lateral longitude,  \\n multiply them by the constants,  \\n and then use NumPy to get the Euclidian distance.  \\n So, we're going to run the cell.  \\n There is no output, but now we have the function definition.  \\n Let's have a look.  \\n So, let's take first longitude, then latitude,  \\n and the second one.  \\n So we'll take 200 and 201,  \\n and we are going to calculate the distance between them,  \\n and this is going to work.  \\n Now we need to calculate the distance between every row.  \\n Remember, we want to avoid for loops as much as possible.  \\n Let's use the shift method to avoid looping.  \\n Here's an example:  \\n So, I'm creating a series of five elements, so zero to four.  \\n Now if I'm going to run the shift operator,  \\n I'm going to get none as the first one  \\n and every element is shifted downward.  \\n Shift can also work in  \\n the other direction if I give it minus one.  \\n So now I have none at the bottom and everyone is shifted up.  \\n Now we're going to use shift.  \\n So we get a distance, we take the latitude  \\n and longitude and the shifted latitude and longitude.  \\n If you're going to run this one,  \\n we're going to get none again for the first value  \\n and then distances which are in kilometers.  \\n Let's do a sanity check and sum all of the distances.  \\n 4.7 kilometers? That seems about right.  \\n Now we'd like to calculate the difference in times.  \\n This one, we're going to use the diff method.  \\n And if you're going to run it now, we're going to see  \\n again, not a time for the first one and then the difference  \\n in times: 17 seconds, zero seconds, one second, et cetera.  \\n If you look at a stick, we can specify the length  \\n of the stick in various ways, right?  \\n We can use centimeters, millimeters,  \\n nanometers, parsecs, and more.  \\n Same goes for time deltas.  \\n It's a span of time, and we need to convert it  \\n to a number by specifying the unit.  \\n So, what we are going to do is times_hour.  \\n We are going to take times and divide it  \\n by panda's time delta saying one hour.  \\n And now we see the numbers as fractions of an hour.  \\n Finally, we can calculate the speed.  \\n So, we do distance divided by times_hour,  \\n and we are going to see the speed. Right?  \\n And this speed is in kilometers per hour,  \\n and I'm not running 35 kilometers per hour.  \\n That's the speed of the fastest man alive, probably.  \\n This is an error in the measurement.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4363230\",\"duration\":128,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Display speed box plot\",\"fileName\":\"3084641_en_US_03_06_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"The pandas Timestamp class allows for math arithmetics. In this video, learn how to calculate speed and create a box plot to display it.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4511073,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's run our code,  \\n load the data and calculate the speed.  \\n Now we can use the describe method  \\n of the speed to get some statistics  \\n about what is the mean standard deviation and other values.  \\n I'd like to use box plot for first impression of the data.  \\n So let's do that speed.plot.box.  \\n If you're going to run this, you're going to see a box plot.  \\n A box plot shows the distribution of data.  \\n The line in the middle is the mean.  \\n Now we have the quintiles  \\n and the whiskers are the inter quantile ranges.  \\n And all of the dots outside  \\n of the whiskers are considered outliers.  \\n These quick plots are great when exploring data, but when  \\n you present the data, you better have some nicer charts.  \\n Don't underestimate nice, it sells.  \\n Most of the time when I explore the data  \\n I plot basic charts, but when it's time to present  \\n I invest some time in making them nicer.  \\n It helps the readability  \\n but also makes a better impression.  \\n There's no end to how much you can fiddle with your charts.  \\n Here are some basic things you can do to  \\n make your charts nicer.  \\n So I'm going to input matplotlib, and I'm going to say  \\n that the figure size is 10 over six.  \\n This is in inches, but will make the figure bigger.  \\n And then I'm using style.  \\n Matplotlib has several styles.  \\n I like the seaborn-whitegrid  \\n and now I'm going to plot again.  \\n And now we get slightly bigger chart.  \\n There is a difference at look  \\n and I have these lines telling me what are the values.  \\n It's easier to see with the grid.  \\n I can also set some other things.  \\n So I have a none here at the bottom.  \\n I'd like to remove it  \\n so I'm going to say that the name is nothing.  \\n And then I'm going to plot and give it a title.  \\n Miki's run.  \\n And then I'm going to set the Y label  \\n with the running speed off.  \\n And then here between these dollar sign  \\n I'm writing later for showing equations.  \\n When I run this one, I get a chart.  \\n It has the label, Mickey's run, the none is gone.  \\n And here you see running speed  \\n and then kilometer over hours.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361155\",\"duration\":22,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Taxi data mean speed\",\"fileName\":\"3084641_en_US_03_07_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":711264,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] I'm asking you to calculate the mean speed  \\n in miles per hour in taxi.parquet.  \\n To get the file taxi.parquet,  \\n you will need to run the download data Python script  \\n in order to get it,  \\n and you're going to use the Pandas read parquet  \\n instead of read CSV to load the data.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4364133\",\"duration\":58,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Taxi data mean speed\",\"fileName\":\"3084641_en_US_03_08_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2226527,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's take it, my solution.  \\n As a reminder, you will need to run the download data  \\n or copy taxi.parquet from the challenge  \\n in order to be able to work.  \\n So importing Pandas  \\n and using the read_parquet to read the file.  \\n We have some bad lines in the data.  \\n The good lines are the ones that the drop off time  \\n is bigger than the pickup time.  \\n So I'm going to create a mask, a boolean mask,  \\n and then get only the rows that match this mask  \\n from the data frame.  \\n Now I can do the calculation.  \\n So times is the drop of time minus the pickup time.  \\n Times in hours is the times divided by delta of one hour,  \\n and the speed is the distance, which is in miles,  \\n divided by the hour.  \\n And now, we can get the mean speed,  \\n which is 17 miles per hour in New York.  \\n \\n\\n\"}],\"name\":\"3. pandas\",\"size\":40741045,\"urn\":\"urn:li:learningContentChapter:4359134\"},{\"duration\":628,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4360141\",\"duration\":89,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Create an initial map\",\"fileName\":\"3084641_en_US_04_01_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Maps are great for showing geospatial data. In this video, learn how to create an initial map.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3493898,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor} Folium is a library  \\n for creating web-based interactive maps.  \\n Under the hood Folium uses leaflet.js  \\n JavaScript library and leaflet.js uses  \\n Openstreetmap to render maps.  \\n Let's load the log of one of my runs.  \\n So we're going to load track.csv we're going  \\n to tell pandas that the time column  \\n should be passed as a timestamp,  \\n and we also say that the index  \\n column is the time.  \\n So now if you're going to run this  \\n you're going to see that we have  \\n only latitude, longitude, and height.  \\n The time column is now in the index.  \\n So we look at the index the first five rows  \\n we're going to see that.  \\n Okay, so how are we going to work with volume?  \\n We're going to input volume and  \\n then get the center of the map  \\n which is the mean of the latitude and longitude.  \\n We're going to get the center of the map  \\n which is the mean of the latitude and longitude.  \\n and then we're going to create a map  \\n which is folium.map.  \\n We say the location is the center  \\n and we're going to say the zoom level  \\n which is 15.  \\n I don't have a good rule of thumb  \\n for the initial zoom level.  \\n Just play with it until you get it right.  \\n So let's run this one.  \\n And now we have an interactive map, right?  \\n We can move it around, zoom in, zoom out,  \\n and do everything we do with maps.  \\n You can even save the map to an HTML file  \\n and then open this file in the browser.  \\n If you go back to look at the files  \\n we see now that we have track.html,  \\n which is an HTML file you can open it in browser.  \\n You'll get an interactive map.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4359129\",\"duration\":176,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Draw a track on map\",\"fileName\":\"3084641_en_US_04_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Adding your own data to maps is great story telling. In this video, learn how to draw a jogging track on a map.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7650657,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now let's draw a track on the map.  \\n So first we're going to load the data frame,  \\n and I'm going to hide the sidebar.  \\n And now I'm going to use volume, create a map,  \\n and then get a location, create a marker  \\n on this location, and add this marker to the map,  \\n and finally, show the map.  \\n Right, when we run this cell now,  \\n we see a marker we can tweak around  \\n with the marker and style them.  \\n For example, we can use a red circle.  \\n So I'm going to use now a circle marker instead  \\n of a regular marker and say that the color is red.  \\n And now we get a circle marker.  \\n Another nice thing you can do is add a popup.  \\n So we have the circle marker in the location  \\n and color is red, and I'm adding a popup with, \\\"Hi there.\\\"  \\n So if I'm going to run the cell, now I have the marker.  \\n But now when I'm clicking on the marker,  \\n I'm going to see my popup.  \\n There are many styles of markers in volume,  \\n and the popup can be HTML.  \\n Check out the volume documentation for more details.  \\n Now that we know how to place one marker on the map,  \\n let's throw the entire track on the map.  \\n For this I'm going to create a function  \\n that will add a marker from a row in the data frame.  \\n So I'm going to get a tuple from  \\n the latitude and longitude, create a circle marker in  \\n the location, which has a radius of five, red color,  \\n and the pop-up is going to be the time.  \\n So I'm getting it from the row name,  \\n which is the index, and I'm going to format it  \\n as our column and edit the marker.  \\n Now check it out.  \\n Let's take one row first and see that.  \\n So I'm calling add marker on the 200th row,  \\n we're using the .iloc, and then showing the map.  \\n Let's run the cell.  \\n And now we see the marker.  \\n And if you're going to click on the marker,  \\n we're going to see the time.  \\n Once we see that the code is working for a single row,  \\n we can do it for the whole data frame.  \\n So I'm going to create another map  \\n and then call apply from the data frame on end marker  \\n with access equal one, meaning do it  \\n on every row and display the map.  \\n This took a while, and the markers overlap.  \\n You need less data points.  \\n And to do so, we are going to resample data.  \\n So I'm going to create a new map.  \\n And then I'm going to create a data frame by using resample.  \\n And I'm going to say minute frequency  \\n and take the mean of every longitude  \\n and latitude that falls inside that minute frequency.  \\n And now I'm going to apply this  \\n ad marker function, but on the minute df.  \\n Again, with access equal one.  \\n This looks much nicer.  \\n This workflow is very typical with visualization.  \\n You need to do some iteration until you get a good one.  \\n To get a great one, you need to  \\n invest more time tweaking it.  \\n But for now, I think we're good.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4358209\",\"duration\":284,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Using geospatial data with shapely\",\"fileName\":\"3084641_en_US_04_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Manipulating and selecting geospatial data is an important tool. In this video, learn how to use shapely to run calculations on geospatial data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11636237,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Sometimes we'd like to do more  \\n than just draw points on a map.  \\n For example, you might like to know if a location,  \\n a point, is inside the city limit.  \\n Cities are represented as polygon.  \\n And the question maps to, is this point inside that polygon?  \\n There are several libraries in Python to work with  \\n what is called GIS, Geographic Information System  \\n and geometry in general.  \\n We are going to have a look at one library called Shapely.  \\n The two main data structures that we're going to use  \\n are a point and a polygon.  \\n So from shapely geometry, import point  \\n and we are going to create a point and show it.  \\n And the interactive prompt is showing the point as,  \\n well, a point.  \\n But you can ask the point,  \\n what is the dot X and the dot Y of the point?  \\n Now let's take a look at the Polygon.  \\n So input the Polygon and create it.  \\n And when we run it, again, the interactive prompt  \\n is going to show us a square because this is the Polygon.  \\n You can ask Polygon several questions.  \\n For example, what is the area?  \\n And what is the center of the polygon?  \\n You can also ask whether this point is inside  \\n the polygon by using the intersects method.  \\n So for this point, this is true.  \\n Let's take a point, 10, 20,  \\n which is outside the polygon,  \\n and now it's going to be false.  \\n So let's load our tracking data.  \\n So we're going to use pandas read CSV from our track.  \\n We are going to say parse the time column  \\n and use it as the index column.  \\n Finally, we're going to resample the data  \\n in a minute frequency.  \\n And now we're going to add a new column to the data frame.  \\n This column is going to be a column of points.  \\n So we only take the latitude and the longitude columns,  \\n apply point to them,  \\n and we use the head command to see what we have.  \\n So now we can see we have a new column called point  \\n and every value inside of it is a shapely point.  \\n So let's define our polygon.  \\n We're going to take the middle latitude  \\n and the maxim latitude  \\n and the middle longitude and the maxim longitude.  \\n And we're going to create our polygon.  \\n Which again is going to look like a square.  \\n And now we can say which rows in the data frame  \\n are inside that polygon.  \\n So we can say data frame point  \\n and apply the polygon intersect,  \\n which is going to return us a series of Boolean values.  \\n So Boolean indexing to find the rows.  \\n And we're going to see several rows  \\n that are inside the polygon.  \\n Let's plot the polygon on the map.  \\n This can get a bit tricky since folium expects a series  \\n of latitude longitude.  \\n While if you ask the polygon for its exterior  \\n you're going to get two arrays, one of X's and one of Y's.  \\n We are going to do some data shifting here.  \\n This is quite common, but lucky for us,  \\n Numpy makes it super easy.  \\n We're going to use the stack function from Numpy.  \\n Let's run Numpy stack on the exterior.  \\n And this is almost what we want.  \\n We just need to rotate the array by 90 degrees  \\n to get the pair of coordinates per row.  \\n This is called a transpose  \\n and it's done with the dot T attribute.  \\n Now we have the data in the right shape.  \\n It's easy to add it to a map, right?  \\n So we're going to create a folium map  \\n with the location in the mean with the zoom start.  \\n And then do the stack to get the points.  \\n Add a child with the poly Line from volume and show the map.  \\n And now you see we have this polygon on the map.  \\n Now let's draw our track using different color  \\n for points that are in the polygon or outside.  \\n So we're going to define the center, create the map,  \\n and then we're going to add the poly Line.  \\n And now we have our add marker that's going to  \\n get the location from the raw latitude  \\n and check if we are inside the polygon  \\n using the polygon intersects with the point.  \\n And now we create a marker and location.  \\n The radius is five.  \\n And then the color is going to be yellow  \\n if you're in the polygon.  \\n Otherwise it's going to be green.  \\n And we also add the popup for the time.  \\n And add the marker to the map.  \\n We call the data frame apply with the add marker function  \\n and axis equal one, meaning running it on every row.  \\n And let's run the cell.  \\n And now you can see the track.  \\n All the points outside are green  \\n and the one inside are yellow.  \\n I think it's pretty cool what you can do  \\n with very few lines of code to show map interactive  \\n with GIS information inside of them.  \\n If you work a lot with GIS information,  \\n I suggest you look at the package called GeoPandas,  \\n which adds GIS data types to data frames.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4360142\",\"duration\":17,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Draw the running track\",\"fileName\":\"3084641_en_US_04_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":540511,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Draw the running track from track.csv.  \\n Sample the data to a minute interval,  \\n and then markers should be blue  \\n if the height is below 100 meters, otherwise red.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4363231\",\"duration\":62,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Draw the running track\",\"fileName\":\"3084641_en_US_04_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2420405,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] So here is my solution.  \\n First, we're going to load the dataframe,  \\n pass the dates, the index call,  \\n and then resample permanent frequency.  \\n Next, we're going to create the map, so importing folium,  \\n and then creating a map with the location at the mean,  \\n and the zoom start is 15.  \\n And now the add marker function,  \\n so getting the location from the latitude and longitude,  \\n creating a circle marker at that location with radius five.  \\n And now the color,  \\n it is blue if the height is less than a hundred,  \\n otherwise it's going to be red.  \\n And we are going to add a pop up for the time.  \\n Finally, we add the marker to the map  \\n and then we call the dataframe apply method on add marker  \\n with access equal one, meaning walk on every row,  \\n and display the map.  \\n And now, we can see the track.  \\n Most of the dots are below 100 meters.  \\n The beginning is a bit higher, so the points are red.  \\n \\n\\n\"}],\"name\":\"4. Folium and Geospatial Data\",\"size\":25741708,\"urn\":\"urn:li:learningContentChapter:4365169\"},{\"duration\":1198,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4359130\",\"duration\":77,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Examine data\",\"fileName\":\"3084641_en_US_05_01_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Looking at raw data can give you insights about the data. In this video, learn how to look at the raw data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2756277,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We're going to look at some data  \\n about taxi rides published by the New York City.  \\n Before you can start,  \\n you need to run the download data script  \\n in the video folder.  \\n This will download the taxi.csv file.  \\n It might take a little bit,  \\n so I suggest that you'll keep this file around  \\n and copy it over from chapter to chapter.  \\n Let's see if you can safely load this data into memory.  \\n So from pathlib we import path,  \\n we create the csv file,  \\n define what is a megabyte,  \\n and then use the stat method of the file to get the size,  \\n which is in bytes,  \\n and divide it by a megabyte.  \\n Let's run this.  \\n And it's about 163 megabytes.  \\n It is safe to load into memory.  \\n Next, we're going to have a look at few of the initial lines  \\n and see how many lines there are in total.  \\n Initialize the number of lines,  \\n open the file,  \\n and use enumerate.  \\n If the line is one of the first five lines,  \\n I'm going to print it out.  \\n And always increment the counter.  \\n Finally, print out the number of lines.  \\n And we're going to run this one,  \\n you're going to see we have the vendor id,  \\n the pickup and drop off time,  \\n how many passenger in the passenger account,  \\n the distance,  \\n and many more attributes.  \\n And we have about a million lines.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4363232\",\"duration\":91,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Load data from CSV files\",\"fileName\":\"3084641_en_US_05_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Loading data from a CSV file can be tricky. In this video, learn how to load complicated data from a CSV file.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3317429,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's load the data from the csv.  \\n You need to either run the download csv,  \\n or copy over taxi.csv from one of the other chapters.  \\n So let's start.  \\n Import pandas as pd,  \\n and we are going to use pd read csv to load the file.  \\n And finally, we're going to print out the length,  \\n adding a thousand comma separator  \\n for the number of rows.  \\n Let's run the cell.  \\n And you see we have a thousand rows.  \\n Let's have a look at the first one  \\n by using the iloc accessor for row number zero.  \\n So run the cell,  \\n and we're going to see we have a vendor ID,  \\n we have the pickup and drop off time,  \\n how many passenger,  \\n trip distance,  \\n and many other attributes.  \\n Let's look at the data types.  \\n So I'm calling df.dtypes,  \\n and this is going to show me the data type per column.  \\n We see that the vendor ID is an integer,  \\n but the pickup and drop off are objects,  \\n which means strings.  \\n Let's fix that.  \\n We know they are time,  \\n so we define the time columns  \\n as the pickup and the drop off.  \\n And now we use the read csv,  \\n and tell pandas to parse dates in these time columns.  \\n And now we can look at the data type again.  \\n Now we see that both the pickup time  \\n and the drop off time are date time as we want them.  \\n A lot of time you'll get data  \\n which is a bit messy and not formatted properly.  \\n This is why it's important to have a look at the data  \\n and see that what you load looks okay  \\n before doing anymore work.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4362138\",\"duration\":168,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Working with categorical data\",\"fileName\":\"3084641_en_US_05_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Categorical data is found in many data sets. In this video, learn how to work with categorical data types and save memory.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6531083,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Let's load our data.  \\n So we import Pandas as pd.  \\n We define what are the time columns  \\n and we use the read CSV method  \\n to load the data passing the time columns  \\n and looking at the data types.  \\n And you can see that the Vendor ID is an integer.  \\n Let's see how many unique values are there  \\n using the unique method of the column.  \\n And we see we have only two values.  \\n We'd like to refer them by name.  \\n You can access the metadata  \\n or the scheme of the data.  \\n And here we see that Vendor one is Creative  \\n and Vendor two is Verifone.  \\n So, let's create a map saying one is Creative,  \\n two is VeriFone.  \\n And now we're going to add a new column, called Vendor  \\n which is applying this map on the Vendor ID.  \\n And we're going to look at the head  \\n to see the first few values.  \\n And you see VeriFone and Creative.  \\n The Vendor column is a string column.  \\n Since there are only two Vendors,  \\n it seems wasteful to generate string per row.  \\n Let's see how much memory this column takes.  \\n So we define a megabyte  \\n and then we call the memory usage method on the column  \\n with deep=true to calculate the strings  \\n divided by megabyte.  \\n And you see it takes 67 megabyteS.  \\n This kind of data, when there are few possible values  \\n that repeat is called Categorical data.  \\n Pandas has a special type for Categorical data  \\n that is much more space efficient.  \\n So, we are going to do  \\n data frame Vendor=df Vendor.map Vendors  \\n just as before.  \\n But now we are going to add S type category  \\n and then look at what we have.  \\n And you see that it looks like before,  \\n VeriFone and Creative  \\n but now the data type is a category.  \\n Let's see how much space the Vendor column is consuming now.  \\n So right now it's a single megabyte versus the 67,  \\n as before.  \\n You can get the underlying data with the .codes attribute.  \\n So if you're going to run this one,  \\n we're going to see now,  \\n that we get the underlying ones and zeros.  \\n Not that these are different  \\n from the one and two we had in the data.  \\n These are the categories and numbers,  \\n not what was in the CSV file.  \\n Categories are easy to work.  \\n For example, you can compare a category to a string.  \\n So let's check how many rows are there,  \\n that the vendor is VeriFone.  \\n And you see about half of them.  \\n There are more things you can do with categories.  \\n For example, you can order them.  \\n This is useful when you have answers from a survey  \\n and you'd like to say that very much is bigger than somewhat  \\n read the excellent documentation  \\n for more details and examples.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361156\",\"duration\":231,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Work with data: Hourly trip rides\",\"fileName\":\"3084641_en_US_05_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"pandas offers many ways to reshape data. In this video, learn how to group data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9677835,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor} Let's plot how many rides  \\n we have on each day.  \\n To do this, we're going to use grouping.  \\n If you're familiar with SQL,  \\n you already know about grouping,  \\n it's span equivalent of SQL's group by.  \\n We probably want to do something like  \\n group by the date and count from the rides  \\n and group by the date again.  \\n If you don't understand SQL, don't worry about it  \\n but I highly recommend you learn some SQL.  \\n As a data scientist,  \\n this is practically a required language.  \\n So, first we start by uploading the data frame.  \\n Remember to run the download data or copy taxi dot csv.  \\n So import pandas as pd define the time column, read it  \\n and then set the vendor name from the vendor ID.  \\n And let's run this.  \\n And now we can look at the times.  \\n So going to look at the pickup time.  \\n So let's do the head.  \\n And we see several times  \\n these times have hour, minute, and second  \\n component and we are only interested in hours.  \\n Let's take a look at the timestamp.  \\n So we create a new timestamp from November 1st, 2021  \\n and we look at this timestamp  \\n and now we can use the floor method to round it up for days.  \\n capital D stands for days.  \\n Pandas has several offset aliases  \\n You should read the documentation  \\n and to know which one to use.  \\n We are going to use, as I said, the day  \\n which is calendar day frequency.  \\n Now let's try flooring up some rows.  \\n So pick up daytime, take the head and we do a floor.  \\n And this is going to fail.  \\n Panda series can hold a lot of types  \\n and the floor makes sense only for time data.  \\n So how can you access the timestamp object  \\n inside this specific series?  \\n There is a dot DT attribute that you can use.  \\n So we are going to do the same.  \\n Pick the column, then go ahead and then dot D  \\n and only then floor of D.  \\n Let's run this one.  \\n And this one is working for us.  \\n Now that you can generate the keys, let's group.  \\n When you group, you can either specify a column to group by  \\n or group by values, which would have the same length  \\n as the data frame.  \\n Since you're interested in the hour  \\n we can use the latter form.  \\n So we define the keys and then we group by the keys.  \\n You get a group by object.  \\n This object waits for you to tell it what to do  \\n with every group.  \\n Extract some columns, maybe calculate some statistics.  \\n In this case,  \\n we'd like to count how many rides are per hour.  \\n So we're going to use count.  \\n So data frame, group by keys, and then count.  \\n We are going to get the counts for every day.  \\n So we see first of the month,  \\n second of the month, third of the month,  \\n and then the same number for every column  \\n because we are just counting.  \\n So what we're going to do  \\n is we're going to extract just one column.  \\n We're going to use the vendor ID.  \\n So we go by the keys, do the count,  \\n take only the vendor ID column and do a bar plot.  \\n And now we have a bar plot for every day in the month,  \\n how many rides we have.  \\n Let's tidy it up.  \\n So we are going to run the same group by operation  \\n but we're going to change the index name for the day  \\n and we're going to call it, the date.  \\n And then we're going to create a bar plot with  \\n a title of daily access in January 2016.  \\n And we tell it we don't want to rotate the axis.  \\n We get this a x object, which is the monthly access  \\n and we do set y label for the number of rides.  \\n Once we run this cell, we have a much nicer task.  \\n We have a title,  \\n we have labels for the X axis and the Y axis.  \\n And we can see the day of the month just below.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4362139\",\"duration\":204,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Work with data: Rides per hour\",\"fileName\":\"3084641_en_US_05_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"You can group data by more than one column. In this video, learn how to group data by several columns and run calculations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8121355,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Let's calculate the median hourly rides over all days.  \\n First, we'll group the rides by day and hour,  \\n and then we'll calculate the median  \\n for each hour across all days.  \\n To make things more complicated,  \\n let's display the results by vendor.  \\n Let's take it one step at a time.  \\n First, you need to either download the data or copy it over.  \\n So you'll have taxi dot csv.  \\n And now we're going to load our data frame.  \\n I'm going to create two auxiliary columns.  \\n First, is the hour column, which is taken  \\n from the hour attributes of the datetime.  \\n And the second one is the day,  \\n again taken from the date attribute.  \\n So if you look at them right now  \\n we have the day, and then we have the hour in the day.  \\n And now we can do a groupby  \\n by the vendor, the day, and the hour.  \\n And I would like to do a count and I want to  \\n extract only a single column, which is the vendor.  \\n Oops. The vendor column is not there anymore.  \\n What happened is that pandas uses the groupby keys  \\n as indices, to the new generated data frame.  \\n Let's have a look.  \\n We see now that we have what is known a multi-index  \\n or hierarchical index.  \\n We won't go into these practically  \\n because we have a simpler method,  \\n and partially because they make my head explodes  \\n every time I work with them.  \\n What we're going to do is tell pandas not to  \\n place the columns from the keys  \\n as an index by using as index equal false.  \\n So if I'm going to do df groupby  \\n and we group by vendor, day, and hour,  \\n and as index equal false.  \\n And now we get the count and I'm going to have a look  \\n at the columns that were generated.  \\n I'm going to run this one  \\n we are going to see now that I have the vendor,  \\n the day, and the hour as columns in the new data frame.  \\n So now I'm going to create a daily data frame  \\n from the data frame grouping by  \\n with as index equal false and getting the count.  \\n And then do another groupby,  \\n this time by the hour, to get the median hourly.  \\n So we're doing the daily df, groupby,  \\n this time only with the vendor and an hour.  \\n Again, as index equal false.  \\n We are going to get the vendor ID, and get the median.  \\n And we can look at the hourly df.  \\n Right? So we get the data frame, where the columns are  \\n the vendor, the hour, and the vendor id,  \\n which is actually the amount of rights.  \\n And the index itself is the day.  \\n What you'd like to have is the hours in the index,  \\n the vendor as a column,  \\n and the median value from any column as the values.  \\n To reshape the data frame like this  \\n you're going to use pivot.  \\n If you work with Excel or sql  \\n you probably know or heard about pivot.  \\n Let's see, the fancier transpose,  \\n where role becomes columns, and the other way around.  \\n The documentation on pivoting and reshaping in  \\n pandas is excellent with a lot of visual examples.  \\n I highly recommend you read it.  \\n In our case, I'm going to pivot  \\n telling that the columns are the vendor  \\n the index in the hour  \\n and the values are the vendor id, which holds the count.  \\n Then I'm going to do a plot with the bar plot  \\n rotation equals zero.  \\n And once we have that, we have our chart.  \\n I hope by now we can appreciate the power of pandas.  \\n With the very few lines you can slice, dice,  \\n and reshape the data, making exploring data super easy.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361157\",\"duration\":240,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Work with data: Weather data\",\"fileName\":\"3084641_en_US_05_06_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"pandas lets you join several DataFrames together. In this video, discover how to join DataFrames.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9828825,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Sometimes your data will be  \\n in more than one place.  \\n Let's add some weather data to our taxi rides  \\n and see if temperatures have a correlation  \\n with the number of rides.  \\n First, we're going to load the weather data,  \\n reminding you either to run down data script  \\n or copy over the taxi dot CSV file.  \\n So input pandas, define the time columns with the data,  \\n and then add the vendor column.  \\n And let's run this out.  \\n And now for the weather.  \\n You will need to run the download weather script  \\n in order to get weather.csv.  \\n Let's have a look at the weather CSV file.  \\n Right, so we have the station,  \\n the date, latitude, longitude,  \\n elevation, and much more.  \\n We see the date starts around 1948  \\n and we would like the data column  \\n to be at timestamp.  \\n So, we are going to read the CSV of the weather.  \\n Parse dates is the date  \\n and we want to set the date as the index column.  \\n And we're going to call this WDF  \\n for weather data frame.  \\n So we see the date as the index,  \\n we see the station, the latitude,  \\n the longitude, and several other attributes.  \\n Let's run describe  \\n to have a quick look at the data.  \\n Remember, describe only shows numerical values, right?  \\n So we have latitude, longitude,  \\n snow, and then TMAX, and TMIN.  \\n The values of the temperature looks interesting.  \\n They don't match either cell use or fahrenheit .  \\n To understand what they are,  \\n you will need to go to the schema definition.  \\n If you read this PDF file,  \\n you're going to see  \\n that the temperature are 10% in celsius.  \\n Let's convert this to fahrenheit using  \\n the convert temperature function from SciPy.  \\n So from SciPy concepts,  \\n we import convert temperature,  \\n and we're going to create  \\n a column called temp F for fahrenheit.  \\n And we are going to take the maximal,  \\n divide it by 10,  \\n and then from celsius to fahrenheit,  \\n and we are going to look at 10 random roles  \\n from the generated column.  \\n And now it looks better.  \\n Now that you have the data,  \\n we can join the two data frames.  \\n Pandas has various ways of joining data.  \\n Like an SQL, there are many kinds of joints.  \\n Inner, outer, left.  \\n Let's read the commentation to learn more.  \\n By default,  \\n pandas is going to merge on the index.  \\n However, you can specify column  \\n or other values to join by.  \\n The weather data frame is already indexed by date.  \\n So we're going to group by the drives index,  \\n the taxi.csv.  \\n By the date as well.  \\n Ddf for the daily data frame,  \\n we're going to group by  \\n and use the pickup_daytime.dt.date  \\n and get the count.  \\n And if you're going to run ahead on this one,  \\n we are going to see that we have  \\n a date here on the index.  \\n And then every column has the same value  \\n which is the count.  \\n And now we are going to run our merge.  \\n So we are going to merge the daily data frame  \\n and the weather data frame.  \\n And we say on the left index  \\n and on the right index.  \\n And we are going to do ahead.  \\n So now we get the daily data frame with all the rows  \\n that has the numbers of the rides.  \\n And then on the days that we don't have anything matching  \\n we'll get NaN's.  \\n And we have the temperature which does match.  \\n All that is left is to plot.  \\n And we are going to use a scatter plot this time.  \\n So JDF plot scatter,  \\n the X is the temperature in fahrenheit,  \\n and Y is the vendor.  \\n And we see that there's not a lot of correlation  \\n between the weather and number of rides.  \\n No correlation is also a good finding.  \\n Note that the data we have here  \\n is only for one month.  \\n Maybe if you look on a yearly data,  \\n you will see more correlation.  \\n This might be an interesting exercise for you.  \\n Go to the taxi data website,  \\n download the files for a year,  \\n and then try to see if you find the correlation  \\n between the weather and the number of rides.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4365167\",\"duration\":30,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Graphing taxi data\",\"fileName\":\"3084641_en_US_05_07_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":992606,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Narrator] In this challenge, you should load  \\n the taxi data, then remove all the rows  \\n with either total amount that is less than zero  \\n or the passenger count is zero.  \\n And then I would like you to create two charts.  \\n One is a bar chart of the average tip per passenger count  \\n and the second one is a bar chart  \\n of the average tip per day.  \\n I've included two images which show roughly  \\n how the charts should look like.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4363233\",\"duration\":157,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Graphing taxi data\",\"fileName\":\"3084641_en_US_05_08_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5687649,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Teacher] Let's have a look at my solution.  \\n First, we're going to load the data frame.  \\n So defining time columns, reading the CSV,  \\n and telling pandas to pause the time columns.  \\n And finally creating a vendor name from the vendor ID.  \\n And I'm going to print out how many rows  \\n are in the data frame.  \\n So we have a million something rows.  \\n Now we're going to drop the bad rows.  \\n So we define the bad rows as the data frame  \\n where the total amount is less or equal to zero,  \\n or and this is the vertical sign for or,  \\n the passenger count is zero.  \\n And then we are going to use the data frame drop method  \\n with the index of the bad row and telling it  \\n in place equal true meaning change the current data frame.  \\n And finally,  \\n we're going to print out again the number of rows.  \\n So let's run this out.  \\n And we see that we have a bit less row,  \\n about 500 less of them.  \\n Next, for the charts.  \\n I'm going to create an axially column,  \\n which is the tip percent.  \\n And this is the tip amount  \\n divided by the total amount times a hundred.  \\n And now for the chart of the tip per passenger count.  \\n So we're going to go by the passenger count.  \\n Get the tip percent and get the mean.  \\n And then I'm going to do a bar plot  \\n saying the title is tip percent by the passenger count.  \\n And finally,  \\n I'm going to set the Y label to the tip percent  \\n So let's run this one.  \\n And for some reason, looks like groups with eight people  \\n are doing the biggest tips.  \\n Next is the weekday.  \\n So I'm going to get the day abbreviation function  \\n from the calendar model.  \\n And then the day of the week  \\n is coming from the pickup daytime column, .dt.weekday.  \\n And then by day is we are grouping by the day of the week  \\n and then taking the tip percent and mean,  \\n and now I can plot.  \\n So first I'm going to set the index of by day  \\n to day of the week,  \\n so it'll be displayed in the resulting chart.  \\n And now I'm going to plot,  \\n saying, the title is tip percent by day  \\n and I'd like a 45 degree rotation.  \\n Next I'm going to change generate chart  \\n to add the Y label as tip percent  \\n and then finally set the X ticks  \\n that will show under the X-axis  \\n as a day abbreviation instead of the day number.  \\n Let's run it.  \\n And we see that there's no much different  \\n in the percent of tipping throughout the week,  \\n maybe Thursdays.  \\n \\n\\n\"}],\"name\":\"5. NYC Taxi Data\",\"size\":46913059,\"urn\":\"urn:li:learningContentChapter:4359135\"},{\"duration\":1225,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4362140\",\"duration\":73,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"scikit-learn introduction\",\"fileName\":\"3084641_en_US_06_01_3006561_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"scikit-learn is a library that contains many machine learning algorithms. In this video, learn about scikit-learn and what it offers.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3209917,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Scikit-learn is a package for machine learning.  \\n It implements a lot of algorithms and tasks such  \\n as classification, regression, clustering, and more.  \\n Apart from providing algorithms  \\n Scikit-learn also provides a lot  \\n of utilities that help us with the day  \\n to day machine learning tasks,  \\n such as pre-processing parameter selection,  \\n model evaluation, and more.  \\n Scikit-learn documentation is one  \\n of the best there is in the open source world.  \\n There are tutorials, examples reference to articles,  \\n and much, much more over there.  \\n For example, for people who are new at machine learning  \\n there's a choosing an estimator section.  \\n You can follow the flow chart  \\n until you find the right algorithm for your problem.  \\n Scikit-learn is huge and has many parts.  \\n The developers of Scikit-learn made the API consistent  \\n so you can learn one algorithm  \\n and it's pretty easy to use the rest of them.  \\n We'll cover the interesting parts  \\n and learn some guidelines  \\n on how you can use and understand Scikit-learn better.  \\n As a bonus, it's very easy to get an algorithm created  \\n by the data science team  \\n and put it in production as is.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4363234\",\"duration\":412,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Linear regression\",\"fileName\":\"3084641_en_US_06_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Linear regression is a simple and effective way to predict the value of a variable. In this video, learn how to use linear regression and the basics of working with scikit-learn models.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17042957,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In regression, we train a model  \\n to predict a continuous target such as a price.  \\n Scikit-learn comes with several datasets.  \\n You can learn on these datasets.  \\n Makes it easy to start learning.  \\n But you need to remember that they are cleaned up for you.  \\n Real world data will be much, much messier.  \\n We are going to predict the prices of houses  \\n using the California housing dataset.  \\n So let's start.  \\n So from sklearn.datasets import fetch_California_housing,  \\n and run it.  \\n Cal_housing equal fetch_California_housing.  \\n And let's run the cell.  \\n Let's have a look at what we got back.  \\n So we'll use the Python-type function to see what it is.  \\n This is called a bunch.  \\n All through datasets, you're going to load from sklearn,  \\n comes as bunches, and have some common attributes.  \\n Bunches behave like Python dictionaries,  \\n and this is how we're going to treat them.  \\n So we're going to have a look at the keys.  \\n And we see we have the data, the target,  \\n the frame, target names, feature names, and the description.  \\n If you look at the type of the data, this is a NumPy array.  \\n And looking at the feature names,  \\n we see that every column in this NumPy array  \\n is corresponding to one of these names.  \\n So the first column is the median income.  \\n The second column is the house age.  \\n The third one is the average rooms, et cetera, et cetera.  \\n If you want to see the whole description,  \\n you can print the description.  \\n We're going to print it out,  \\n and then we see that we get some long description.  \\n It says the output exceeds the size limit.  \\n And you can click on here  \\n to see the full thing in a text editors.  \\n And now we see some description  \\n about California housing dataset,  \\n explanation about what is every attribute or feature,  \\n and some more references to have a look at.  \\n Let's close this one.  \\n Let's look at the target.  \\n These target are numbers.  \\n And these are the prices of the houses  \\n in hundreds of thousands of dollars.  \\n And this is what we'd like to predict.  \\n There are several regression algorithms in Scikit-learn.  \\n Let's go with a random forest regressor.  \\n So from scikit-learn.ensemble,  \\n we import the random forest regressor.  \\n We're going to extract the data and the target.  \\n Traditionally, these are called X, with a capital X, and Y.  \\n And then we are going to create a random forest regressor,  \\n and then train it.  \\n The training function is called fit.  \\n And we're going to get back a regressor.  \\n And this is the way that the notebook is rendering it,  \\n like some kind of an image.  \\n How well did our model do?  \\n You can use the score method.  \\n So we're going to call the CLF.  \\n CLF stands for classifier.  \\n Score with Xs and Ys.  \\n And we're going to get 0.97 and several digits others.  \\n What does this number mean?  \\n Is it good or bad?  \\n It depends on the algorithm.  \\n Each algorithm has a default scoring function  \\n that makes sense for it.  \\n In our case, we can have a look.  \\n So we're going to use the extension  \\n that the scientific computation does.  \\n This is not a valid Python, but it's going to run here.  \\n And the question mark at the end  \\n is going to show the documentation for the score function.  \\n And this says this is the coefficient of determination  \\n for the prediction, or sometime known as R-squared.  \\n You can go to the Scikit documentation,  \\n and see the documentation of R-squared.  \\n And in R-squared, one is a perfect score.  \\n We are very close to it.  \\n Scikit-learn has many other estimators,  \\n and you should know the problem and the domain  \\n to pick the right estimator for your problem.  \\n So what did our model learn?  \\n Let's run the built-in DIR command on the classifier.  \\n And again, this is a bit too long,  \\n so we're going to open it in a text editor.  \\n And we see a lot of attribute.  \\n The convention in Scikit-learn  \\n is that everything that ends with an underscore.  \\n A single underscore is something that the estimator learned.  \\n For example, if you're going to look at n_features_in,  \\n we're going to see that it says eight features.  \\n And if you're going to look at the shape of X,  \\n which is the data, we see that it is 20,000 rows,  \\n but we have eight columns, which are the features.  \\n The nice thing about decision trees,  \\n it's also easy to see the reasoning behind them.  \\n Scikit-learn has an export_graphviz,  \\n which is exporting the graph into a .file.  \\n This .file is the .language from the Graphviz toolkit,  \\n which is a generic toolkit for generating graphs.  \\n It is pretty awesome.  \\n I recommend you try it out.  \\n This means that you will need .tool or Graphviz  \\n installed on your machine.  \\n Of course, in code spaces, this is already done for you.  \\n So let's do it.  \\n First, we're going to call export_graphviz.  \\n We're going to take the first estimator output two,  \\n tree.dot.  \\n We're going to say where it can get the feature_names.  \\n Won't show us, just the values.  \\n And then I want the tree of up to five.  \\n And let's run this one.  \\n Going back to our explorer,  \\n now we see that we have a tree.dot here.  \\n This is the .file for Graphviz.  \\n And now we can use the Graphviz utility, dot.  \\n I'm saying I want the output to be SVG.  \\n The output file name is tree.svg.  \\n And the input is tree.dot.  \\n So you're going to run this one.  \\n Now you see, we have also tree.svg.  \\n What I'm going to use is IPython display,  \\n and basically display the SVG in the interactive prompt.  \\n So let's run the cell.  \\n And now we get the picture of how the algorithm  \\n is deciding where to go, right or left,  \\n and what are the thresholds.  \\n Once you train your model,  \\n you'd like to use it to make predictions.  \\n Let's pick a row, and see what our model predicts.  \\n So I'm taking row 17,  \\n and then I'm calling predict on this row.  \\n And this is going to fail,  \\n since the predict method worked on several rows at once.  \\n If you look at the shape of our rows,  \\n this is just eight or nothing.  \\n We would like one row of eight features.  \\n So we are going to use slicing to get a single row.  \\n But this time, the row is going to have a different shape,  \\n one over eight.  \\n And now we can call predict on this row.  \\n And you can see the predicted value is 1.59.  \\n Let's look at the target, which is the actual value,  \\n and see what it is.  \\n So 1.55.  \\n Not so bad.  \\n This covers most of the basic functionality of Scikit-learn.  \\n You call fit to train your model.  \\n You call score to validate the model.  \\n And you call predict to, well, predict values.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4359131\",\"duration\":131,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand train/test split\",\"fileName\":\"3084641_en_US_06_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In order to validate a model, you need to split your data into test and train sets. In this video, learn how to use facilities from scikit-learn to split data for testing.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5346486,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In real life, your model  \\n will need to predict values from data it hasn't seen yet.  \\n It is common practice to split the data  \\n to training and testing parts.  \\n This reduces the chances of overfitting  \\n which means your model is accurate for data itself  \\n but will behave poorly on new data.  \\n SecondLearn makes it easy. First, let's slow the data.  \\n So we're going to fetch the California housing,  \\n and then we're going to take the X  \\n and the Y from the California housing.  \\n And let's look at the shape of the X.  \\n And you see we have 20,000 rows and eight features.  \\n SecondLearn provides a trained test split function  \\n that splits the data into a training and testing.  \\n So from model selection, we import the train test split,  \\n and then we are going to call the  \\n train test split with X and Y.  \\n And say, the test size is 0.3, which means 30%  \\n and this is going to return four values:  \\n the X for the training, the X for the testing,  \\n Y for the training, and Y for the testing.  \\n And let's look at the X train shape again.  \\n And we see it's about 70% of the data.  \\n And now we can train our model.  \\n So, we take the random forest regressor, fit it only  \\n on the X train and the Y train, meaning train it on there.  \\n And then we are going to calculate the score  \\n on the test data that it hasn't seen yet.  \\n And we get the score of 0.8.  \\n There are several other ways to split data,  \\n and you can look at the site documentation for  \\n splitter classes and splitter functions,  \\n things like K folding with splitter data to K parts,  \\n then take one out, and others.  \\n Note that sometimes splitting data to test  \\n and train randomly might not be a good strategy.  \\n For example, in fraud, most of your data  \\n will contain valid transactions.  \\n There's a good chance that randomly  \\n you'll pick only valid samples,  \\n and then your model will learn only to say no fraud.  \\n Learn about your data, and figure out  \\n the best way to split it to a good model.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4363235\",\"duration\":195,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Preprocess data\",\"fileName\":\"3084641_en_US_06_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Most of the time, data needs to be preprocessed before you use it. In this video, learn how to use scikit-learn's preprocessing facilities.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8247137,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In most cases,  \\n you will need to pre-process the data  \\n before you feed it through your model.  \\n Some algorithms are sensitives to the range  \\n of feature values.  \\n Let's look at our data.  \\n So, from sklearn.datasets  \\n we import the fetch_california_housing,  \\n and we're going to load the data.  \\n And this time saying as_frame=True, meaning we're going  \\n to get the data as a data frame instead of numpy arrays.  \\n And then we're going to get the housing data,  \\n and call describe on the dataframe.  \\n You can see for example  \\n that the medium income maximal value is 15.  \\n While if you go to the population,  \\n the maximal value is more than 35,000.  \\n It is a huge difference, and will cause problems  \\n for some algorithms.  \\n Let's have a look.  \\n So I'm going to get a train test split,  \\n and split the data to training data and testing data,  \\n 30% saved for testing.  \\n Let's run this.  \\n And now I'm going to import the SVR support vector regressor  \\n from the SVM model.  \\n Create one, fit it on the training data,  \\n and score it on the testing data.  \\n Ouch. This is a really bad score.  \\n What we're going to use is the scaler  \\n from the pre-processing model.  \\n This is going to scale all the values.  \\n So from sklearn, we import pre-processing,  \\n and then X scaled is pre-processing scale of X.  \\n Let's see what the scaler did.  \\n So we're going to use pandas for that.  \\n So we import pandas as pd,  \\n and we create a dataframe from the scale data,  \\n telling it the column names are the feature names.  \\n And then call describe.  \\n And we see now, that all the values are  \\n between minus one and one.  \\n So now we're going to split the data again,  \\n calling train test split, but instead of X,  \\n we're going to do it on X scale.  \\n And Y is not changed with a test size,  \\n create a regressor, train it, and then score it.  \\n And this is a much, much better score.  \\n We have eight features in our original data.  \\n We can try and reduce them to four.  \\n This is known as dimensionally reduction.  \\n One of the most common algorithms is called PCA,  \\n which stands for Principle Component Analysis.  \\n So importing PCA from sklearn.decomposition,  \\n creating a PCA with four components,  \\n and then using the fit transform method to transform the X  \\n to a different shape.  \\n And when we look at the shape of the resulting data,  \\n it has the same number of rows,  \\n but right now only four columns.  \\n There are many other facilities  \\n for pre-processing data in SciKit-Learn.  \\n It all depends on the algorithm you use,  \\n and what's the best data format for it.  \\n Sometimes you will do the initial pre-processing in pandas.  \\n So you're going to load the data to a dataframe,  \\n maybe join it with data from other sources,  \\n filling missing values, et cetera, et cetera.  \\n And finally, before you move the data into SciKit-Learn,  \\n you are going to use the two numpy method  \\n to convert the data into a numpy array.  \\n And then pass this numpy array into SciKit-Learn,  \\n and continue as usual.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4358210\",\"duration\":143,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Compose pipelines\",\"fileName\":\"3084641_en_US_06_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Most machine learning flows are composed from more than a single step. In this video, learn how to use scikit-learn pipelines to compose complex flows.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5461344,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - There might be several steps you need to perform  \\n before you finally feed your data to your algorithms.  \\n (indistinct) provides us pipelines  \\n which are a way to group together several steps.  \\n Let's create one, we'll first scale data,  \\n then reduce the number of dimensions,  \\n and then use an SVR regressor.  \\n So first, let's load the data.  \\n So we import California housing  \\n and we import the train test split.  \\n We're loading the data,  \\n getting X and Y,  \\n and then splitting it to train and test.  \\n And now we're getting PCA  \\n from the composition, pipeline for the pipeline,  \\n standard scaler from the pre-processing,  \\n and SVR from the SVM model.  \\n And we can create a pipeline.  \\n So pipe is now a pipeline, which have three steps.  \\n First step is called scale and it uses a standard scaler.  \\n The second one is doing PCA,  \\n and it's using four components for the PCA.  \\n And the final step is the SVR, which uses an SVR.  \\n Once we have this pipe  \\n it behaves like every other classifier in cyclic learn.  \\n We can call fit to train the data  \\n and score to get the data.  \\n Pipes remove air prone manual steps from your process.  \\n You can combine as many steps as you want.  \\n Let's look at the steps we currently have.  \\n We can call pipe dot steps.  \\n And you see we have three steps.  \\n You can change the parameter of one of the steps.  \\n We can iterate over the list  \\n of steps until we find the object  \\n we want to change the parameter.  \\n However, there is an easy way.  \\n We can use the get params method.  \\n And if you see that we are going to get  \\n all the parameters for every step.  \\n Each parameter is prefaced by the step name.  \\n So PCA double underscore  \\n or scale double underscore or SVR double underscore.  \\n And we can set a parameter.  \\n So for example,  \\n if you want to set the SVRC to 0.9,  \\n we do set params  \\n and then we say SVR underscore underscore C equals 0.9.  \\n And we can do that.  \\n And set params returns the pipeline.  \\n And this is how a pipeline looks  \\n inside the interactive prompt.  \\n You can also write the pipeline parameters  \\n the configuration file and load them,  \\n and then run the pipeline.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4363236\",\"duration\":113,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Save and load models\",\"fileName\":\"3084641_en_US_06_06_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Once a mode is trained, you need to save it and then load it to use it for predictions. In this video, learn how to save and load scikit-learn models.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5122230,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Narrator] At the end, you'd like your model  \\n to run in production.  \\n How can you pass a model you've trained  \\n and experimented with to production?  \\n Scikit-learn makes it easy.  \\n Python has a built in model called Pickle  \\n which can store and retrieve almost any Python object.  \\n The Pickle Model is Python specific,  \\n so you can't save Scikit-learn Pickle Model  \\n and have Java loaded.  \\n But if your production is using Python,  \\n let's say in the microservice architecture,  \\n this becomes a non issue.  \\n Let's have a look.  \\n So we import the California housing, the PCA,  \\n the train test split,  \\n the pipeline, the standard scaler, and the SVR.  \\n We load the data, split it to test and train,  \\n create a pipeline, train it with the fit method,  \\n and then finally look at the score.  \\n And let's run this cell.  \\n Now we have our model, and we'd like to save it.  \\n So I'm going to import Pickle,  \\n and then name the output file model.pkl.  \\n You can pick any name you want.  \\n And then use with open of the output file in WB Mode,  \\n which means write and binary data.  \\n And this is important as output.  \\n We do pickle.dump the pipe to the output.  \\n And let's run this cell.  \\n If you look at the files, now you see  \\n that we have also model.pkl.  \\n You can give this file to operations,  \\n and now they can use it.  \\n Inside the code, they just need to open the file  \\n in read and binary mode, and do pickle.load to get it.  \\n And to make sure we got the same thing,  \\n I'm going to call the score function,  \\n and we see that we got exactly the same score.  \\n The same way you save the model to a file,  \\n you can send it over the network,  \\n store it in a database, or do whatever you like with it.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361158\",\"duration\":89,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Handwritten digits\",\"fileName\":\"3084641_en_US_06_07_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3135157,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Computer User] One of the data sets  \\n in scikit-learn is handwritten digits.  \\n Let's load it.  \\n So we import the load digits,  \\n and call the load digits extract X and Y.  \\n One of the things inside digits is called images.  \\n This images is the actual image.  \\n So let's take a random image.  \\n Let's say image 353.  \\n We're going to print it out,  \\n and we're going to also use matplotlib to show it.  \\n Let's take a digit.  \\n So we're going to pick 353.  \\n We're going to print Y, which is the label of this digit,  \\n and then take the images and print out the current image.  \\n Once we run it, we see that the digit is 4,  \\n and this is roughly how the image looks like.  \\n Kind of look like a 4.  \\n If you look at the image shape, it is eight by eight pixels.  \\n But if you look at the X shape,  \\n you see that the data is flattened.  \\n So we have 1700 images,  \\n and they are flat vectors of 64, 8 and 8, flatten out.  \\n What I'd like you to do, and I'm going to close the prompt.  \\n Write a pipeline that will learn to predict digits.  \\n Should reduce the number of features to 10  \\n and use K neighbors classifier.  \\n Split the data to train and test, and answer  \\n what is the score of the pipeline on the test data,  \\n and what is the size in kilobyte of the serialized pipeline?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4359132\",\"duration\":69,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Handwritten digits\",\"fileName\":\"3084641_en_US_06_08_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2700468,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's look at my solution.  \\n So load the data, take a look at an image,  \\n look at the shape, and look at the shape of the data.  \\n And now, I'm going to create the pipeline.  \\n So I'm going to use PCA to reduce the number of features.  \\n Train test split the data,  \\n the pipeline to create a pipeline,  \\n and KNeighborsClassifier.  \\n Now I'm going to create a pipeline  \\n with PCA and KNeighborsClassifier.  \\n And then, I'm going to split the data  \\n to testing and training,  \\n call fit to train the pipeline on the training data,  \\n and print out the score for the test data.  \\n The score is 096.  \\n To get the size of the data, I'm going to use pickle,  \\n define what is a kilobyte,  \\n and then I'm using the pickle dumps,  \\n which is going to return bytes instead of saving to a file.  \\n And I'm going to print out the length of the data,  \\n which is in bytes, divided by kilobytes,  \\n and we see it's about 154 kilobytes.  \\n \\n\\n\"}],\"name\":\"6. scikit-learn\",\"size\":50265696,\"urn\":\"urn:li:learningContentChapter:4365170\"},{\"duration\":1008,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4362141\",\"duration\":58,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview of matplotlib\",\"fileName\":\"3084641_en_US_07_01_3006561_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Plotting is a very important part of the data story telling process. In this video, discover the most common plotting library.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2048701,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Displaying charts is an important part  \\n of the data science process and storytelling  \\n both in the exploratory state and when presenting reports.  \\n So far, we used Pandas' plotting capabilities  \\n to display charts.  \\n Under the hood, Pandas uses a library called Matplotlib.  \\n There are several other plotting libraries in Python,  \\n but Matplotlib is by far the most used.  \\n Matplotlib is very powerful and has many features.  \\n However, it can be a bit low level at times.  \\n I prefer to use Pandas whenever I can  \\n and drop to Matplotlib only when I need to.  \\n Pandas abstracts a lot of the low level details,  \\n but at times, you will need to work directly  \\n with Matplotlib to get optimal results.  \\n The way I usually work is start at the gallery  \\n and then find a chart that looks like what I want to do.  \\n Click on it, and start with the code below  \\n tweaking it to my needs.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361159\",\"duration\":100,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Use styles\",\"fileName\":\"3084641_en_US_07_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Styles help make your plots look nicer. In this video, learn how to use styles to make your plots better looking.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3744608,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Beauty is in the eyes of the beholder.  \\n Let's start with some basic examples  \\n and see how you can customize Matplotlib  \\n and make it look better.  \\n I usually get someone with a good sense of aesthetics  \\n to give me an opinion before I publish a report.  \\n So, here's our code.  \\n We import Matplotlib as plt and numpy.  \\n We create linear space between -6 and 6  \\n with a hundred points.  \\n The Ys are the sync, which is sign X divided by X,  \\n and then we plot the Xs and the Ys.  \\n Let's run the cell  \\n and this is going to generate a line plot.  \\n It looks good,  \\n but sometimes we'd like to get a different look.  \\n There are several styles available  \\n for you to use in Matplotlib.  \\n We're going to run plt.style.available  \\n and we're going to get a list  \\n of several styles that are available.  \\n Personally, I like to use the seaborn whitegrid style.  \\n So, we're going to do plt.style.use seaborn-whitegrid  \\n and then do the same plot again.  \\n And now we see a slightly different plot,  \\n with grid lines a bit more spaced.  \\n Note that once you change the style,  \\n it's going to change the styles of all the charts  \\n from that point onward.  \\n Let's try another style.  \\n This is the fivethirtyeight style,  \\n and again, we use the style and then plot the same data.  \\n Now we see that the line is wider  \\n and we still see the grid lines.  \\n I encourage you to try out a few styles  \\n and see which one you like.  \\n You can head over to the gallery and check them out.  \\n If none of the styles fits your needs,  \\n you can create your own.  \\n This is a bit more involved, but is well documented.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361160\",\"duration\":196,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Customize pandas output\",\"fileName\":\"3084641_en_US_07_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"pandas DataFrames have support for styling. In this video, learn how to use styles for better visualization.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7823030,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Visual Studio Code  \\n can render Pandas DataFrame nicely.  \\n Let's have a look.  \\n So we're going to load the iris dataset  \\n from scikit-learn doing load_iris  \\n with as frame equal true to get a data frame.  \\n And we're going to get the first five rows  \\n from the data and look at this data frame.  \\n Let's run the cell and you see it rendered nicely.  \\n We have headers, we have different colors  \\n for even and odd rows, et cetera, et cetera.  \\n Let's highlight the maximum value in each column.  \\n This can be done with data frame style highlight max.  \\n And we run this one.  \\n We see that we get the value highlighted.  \\n On the last column of the petal width  \\n every value is the maximal value,  \\n so all of them are highlighted.  \\n There are several other built-in styles.  \\n For example, instead of highlighting the maximal value  \\n you can use a gradient.  \\n The higher the value  \\n the darker blue the cell will be painted.  \\n So let's run this one.  \\n And now we see that we get values.  \\n Another nice style is displaying a bar on every cell.  \\n The bar width is proportional to the value of the cell.  \\n So df.style.bar.  \\n And now we see the bars.  \\n There are several other nice default styles.  \\n It is also easy to create your own.  \\n Let's say you'd like to color all the cells  \\n with a round number in blue.  \\n So, I'm doing color blue of value.  \\n If rounding the number equal to the number  \\n we return color column blue.  \\n Otherwise we return none.  \\n And we do apply map on our function.  \\n When we run the code  \\n we're going to see only two values colored in blue.  \\n You can combine styles  \\n by chaining them with the dot operator.  \\n Here's another function.  \\n This time we're coloring odd numbers.  \\n So if integer of the value model two equal one  \\n we change the background color to orange  \\n and now we do apply map of color the round dot, apply map  \\n and then color the odds.  \\n And if you run this one, we are going to see some cells  \\n have orange background  \\n and some cells have both orange background and blue color.  \\n Styles can also be applied only to a subset  \\n of the columns.  \\n So we can do apply map with coloring the odd ones  \\n and say subset is the first two columns.  \\n If you're going to run this one, you see it affects  \\n only the first two columns  \\n and the third one is not affected.  \\n Apply map works on every element.  \\n You can also apply style to a whole column.  \\n Let's make the numbers in the top 50% bold.  \\n So top 50% of the column.  \\n We say is top 50 equal to column  \\n bigger or equal to the median of the columns.  \\n And then you use the list comprehension  \\n to create values per cell.  \\n We say the font weight is going to be bold if V,  \\n otherwise the empty string meaning no change in style  \\n for every value in the top 50.  \\n And then we apply this function.  \\n Now let's run it.  \\n And you see that the top 50 values are now bold.  \\n I think you get the general idea.  \\n When you generate report and would like to make it easier  \\n for people to find interesting point in your data frame,  \\n use styling.  \\n The documentation is as always very good.  \\n And there are many more examples there.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4363237\",\"duration\":138,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Plotting with pandas\",\"fileName\":\"3084641_en_US_07_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"pandas offers higher level plotting out of the box. In this video, learn how to use the built-in plotting facilities of pandas to generate charts.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4745227,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Pandas bases its plotting on matplotlib.  \\n It simplifies plotting, and also adds some advanced charts.  \\n In order to work,  \\n you'll need to run the download data script,  \\n which will generate stocks.csv.  \\n First, we're going to load this data.  \\n So, import pandas as pd,  \\n and then read csv of stocks.csv,  \\n parsing the date as timestamp,  \\n inserting it to the index of the data frame,  \\n and then we're going to show the first few lines.  \\n And we see that the index has times in it,  \\n and in the columns we have the symbol,  \\n opening, high, low, close, adjusted close, and volume.  \\n This is a very common trading data.  \\n Let's have a look at msft price over time.  \\n So first, we are going to get only the rows  \\n that shows Microsoft,  \\n and then we are going to plot the closing price  \\n using the .plot method,  \\n which is going to generate a line plot.  \\n You can notice that the X axis, which is time based,  \\n is rendered nicely by pandas.  \\n When you want to look at the distribution  \\n of a single feature,  \\n it is common to use a histogram.  \\n Pandas also offers a KDE, Kernel Density Plot.  \\n So you are going to look at the closing price plot,  \\n and now .kde.  \\n And this is going to generate a density plot.  \\n We see that most prices are around the 220 mark.  \\n Let's show the monthly volume by stock.  \\n So we get the data frame,  \\n we do a pivot with the columns of the symbol  \\n and the values of the volume.  \\n We resample to monthly frequency,  \\n and we sum up the volume for month.  \\n And finally we do a plot.  \\n This time a bar plot.  \\n And we say the xticks is a range of 12,  \\n and the rotation is zero.  \\n When you have very long lines  \\n you can break them by putting parenthesis,  \\n and then putting one operation per line with the dot.  \\n And now we can see the volume per stock.  \\n Pandas offers much more advanced charting capabilities.  \\n For example,  \\n scatter matrix that are great to see the initial correlation  \\n between values.  \\n The documentation is excellent.  \\n Do read it.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4358211\",\"duration\":135,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Use Matplotlib with pandas\",\"fileName\":\"3084641_en_US_07_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"pandas uses matplotlib under the hood. In this video, learn how to alter the charts generated by pandas.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4835511,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Pandas does a lot of the heavy lifting  \\n when it comes to plotting.  \\n But sometimes you'd like to have more control  \\n over the generated output.  \\n To this end pandas returns the generated matplotlib access.  \\n Once you have the access object  \\n you can use any method that matplotlib provides.  \\n Let's see an example.  \\n To run the example,  \\n you first need to run the download data.py script  \\n which is going to generate \\\"stocks.csv\\\".  \\n So we're going to use read CSV to read the CSV  \\n pass the dates on the date column  \\n and set it to the index of the data frame.  \\n And we're going to look at the first few lines.  \\n Let's take only the Microsoft part  \\n so query symbol equal msft.  \\n And now we want to plot a median of the daily volume.  \\n So daily equals msft,  \\n we take only the volume column.  \\n We do group by,  \\n by the index weekday  \\n and take the median and finally do a bar plot.  \\n For the data exploration stage  \\n this chart might be good enough.  \\n When you present,  \\n you'd like to make it better.  \\n So we are going to run the same chart,  \\n daily plot bar.  \\n This time you're going to add a title \\\"MSFT\\\"  \\n and say we don't want rotation on the x-axis.  \\n These are the numbers here that are rotated 90 degree.  \\n And now we are going to set the tick labels  \\n to the days of the week.  \\n So Monday, Tuesday through Friday.  \\n We're going to set the X label to the weekday  \\n We're going to set the X label to the weekday  \\n and the Y label to the volume,  \\n and now we can run it.  \\n Now we have the same chart but now we have a title  \\n we have labeling on the axis  \\n and we have the name of the weekday below.  \\n matplotlib API is huge it has a lot of power  \\n but you'll need to invest some time learning it.  \\n Lucky for you that the computation is great.  \\n I usually start from the gallery,  \\n find something that looks like what I want to do  \\n Another way of working with matplotlib and pandas  \\n Another way of working with matplotlib and Pandas  \\n is to pass the a-x the axis of matplotlib  \\n And then pandas is going to render the chart  \\n on this provided access.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361161\",\"duration\":110,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Tips and tricks\",\"fileName\":\"3084641_en_US_07_06_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Matplotlib is very powerful and packed with features. In this video, learn some tips and tricks for effective charts.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4270563,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Man] Let's write the function to plot sin  \\n between a limit.  \\n So, plot sin gets a limit.  \\n We create Xs which is a linear space  \\n between minus limit to limit with a hundred points.  \\n And then we use plt plot with the Xs  \\n and NP sin of Xs and we give it the label  \\n and we call this function plot sin with six  \\n and we get the generated function,  \\n but this is a static function.  \\n Even if you click on the controls and do zooming and panning  \\n it is still going to show you  \\n only parts of this chart,  \\n and you can change the limit and see the results right away.  \\n Jupyter Widgets, or ipy widgets,  \\n is the library that does exactly that.  \\n It gives you widgets that you can display either  \\n in Jupyter notebook or visual to the code, interactive.  \\n So we're going to import the interact,  \\n and we are going to use it as a decorator to our function.  \\n And we're going to set the limit to six.  \\n Let's run the cell,  \\n and now we see that we have a small widget here.  \\n Once we move the widget, the chart will adjust accordingly.  \\n However, you see that the limit is not good.  \\n We can go below zero, which is not something we want.  \\n So, what we're going to use is an integer slider  \\n from the widget list.  \\n Can set the value, the maximum number,  \\n and the minimal number.  \\n Right, so we do the IntSlider.  \\n We do interact, and now we set limit is an integer slider  \\n between zero and twenty.  \\n If you're going to run this one, now the limits  \\n are going to be between zero and twenty.  \\n Right, it start with zero,  \\n which is probably not what we want.  \\n So, let's do,  \\n value equal ten,  \\n and now when we are going to run the cell  \\n the initial value is going to be ten.  \\n Ipy widgets comes with a lot of widget you can play with.  \\n Go and experiment.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4364134\",\"duration\":163,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Other plotting packages\",\"fileName\":\"3084641_en_US_07_07_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"There are several other plotting libraries out there, each one with its own offering. In this video, learn about some other plotting packages and their capabilities.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6256139,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Matplotlib is the library for plotting  \\n in Python, but it's not the only player in town.  \\n Matplotlib is old, which means it's working,  \\n but it generates static images.  \\n Most of the newer plotting libraries,  \\n use the browser to get more interactivity.  \\n I'm going to share an example with Plotly,  \\n and mention some other libraries  \\n you might want to have a look at.  \\n Plotly is a graphing library  \\n that supports several programming languages.  \\n Plotly generates interactive plots, for example,  \\n you can see here, I can see the tool tips.  \\n I can do zooming and panning all within the browser.  \\n Let's see an example.  \\n I'm going to load the track data,  \\n so input pandas as pd, and read csv.  \\n Parse the time column and set it to the index.  \\n And now I'm going to re-sample the data frame,  \\n with a three minute interval with mean and reset the index.  \\n And I'm going to show you the first few rows.  \\n Right, so we have time as a column again,  \\n due to the reset index, the latitude, longitude,  \\n and the height.  \\n And Plotly works with data frames.  \\n So we import plotly.express as PX,  \\n and we say we want a bar plot from the data frame.  \\n And we say where to get the X and the Y values.  \\n What are the column names?  \\n And finally show the figure.  \\n And now I have an interactive plot,  \\n with automated tool tips and I could do zooming and panning,  \\n and all the other things you're used to do with charts.  \\n You can even write this chart into an HTML file,  \\n using fig.write_html.  \\n And if you're going to look back at our code,  \\n now we see we have track.html,  \\n which can be opened in the browser,  \\n and we show exactly this chart interactively.  \\n For most sophisticated reports, look at Plotly's Dash.  \\n Let's take a look at some other plotting libraries.  \\n The list is long, and you should evaluate,  \\n and pick the ones that fit your needs.  \\n Seaborn adds a lot of chart types on top of Matplotlib.  \\n Bokeh generates interactive plots in the browser.  \\n Also supports custom visualization.  \\n For example, this is an interactive plot by Bokeh.  \\n We have movies,  \\n each movie we have the title of the year and the revenue.  \\n And we can filter out movies by number of reviews,  \\n genre and other properties.  \\n Altair uses a concise data language,  \\n to generate interactive plots.  \\n As rule of thumb, if you're just starting start with Pandas,  \\n it'll probably be the easiest path.  \\n If you need some interactive plotting,  \\n probably go with Plotly.  \\n You can also consult the PyViz website,  \\n which lists a lot of plotting libraries,  \\n and their capabilities.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361162\",\"duration\":28,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Stock data bar charts\",\"fileName\":\"3084641_en_US_07_08_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":856927,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] For this challenge,  \\n you'll need to run the 'download_data' script  \\n in order to get stocks.csv.  \\n Once you have them,  \\n you should load the data  \\n and then create a bar chart  \\n that has the X as the month,  \\n the Y is the median closing price,  \\n and each stock should have its own chart.  \\n The output should look like this.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4360143\",\"duration\":80,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Stock data bar charts\",\"fileName\":\"3084641_en_US_07_09_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3168437,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Speaker] Here is my solution.  \\n We import pandas as pd and we read the CSV file  \\n passing the date column as dates  \\n and let's have a look at the first few rows.  \\n Now I'm going to create a month column by saying  \\n df month equals df date.dt.month  \\n and now I'm going to rotate the data with a pivot table.  \\n The columns are going to be the symbol.  \\n The index is going to be the month.  \\n The values are going to be the closing price  \\n and the aggregation function for all the values that follow  \\n within the same month is going to be the median.  \\n Let's have a look at the generated data frame.  \\n So we have the stocks, the month  \\n and then  \\n the values,  \\n which are the median  \\n and now we are going to generate the plot.  \\n So I'm going to take the month abbreviation from calendar  \\n and then I'm going to create a bar plot with a title  \\n which is median monthly close and no rotation on the X axis.  \\n Now I'm going to set the axset labels  \\n to the month abbreviation  \\n and set the Y label for the closing price.  \\n Once I run this,  \\n I have this chart with the title,  \\n the Y axis,  \\n the X axis,  \\n and the legend for every symbol.  \\n \\n\\n\"}],\"name\":\"7. Plotting\",\"size\":37749143,\"urn\":\"urn:li:learningContentChapter:4363239\"},{\"duration\":1163,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4358212\",\"duration\":67,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Other packages overview\",\"fileName\":\"3084641_en_US_08_01_3006561_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"The Python scientific stack offers many packages. In this video, learn about of some the interesting packages and what you can use them for.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3059901,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] People sometime  \\n find the world of open source intimidating.  \\n There are many packages to choose from,  \\n some of them with overlapping capabilities.  \\n Which one should you choose?  \\n Here are some guidelines I use when evaluating packages.  \\n Ask around.  \\n The community is helpful and most people are knowledgeable  \\n about which packages are good and which are not.  \\n Note that people tend to like the shiny new stuff,  \\n but if you ask enough people, you will get the right idea.  \\n Sites such as Stack Overflow are a great place  \\n for asking questions.  \\n Most packages are hosted on GitHub.  \\n Go over and check the activity, the number of committers,  \\n the number of stars, et cetera.  \\n Don't be intimidated by the number of issues.  \\n Some of them are feature requests  \\n and there are many duplicates.  \\n In general, packages without issues  \\n are ones that nobody uses.  \\n After you found the package you like, go and evaluate it.  \\n Write some code using real data and see how it behaves.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4359133\",\"duration\":161,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Go faster with Numba\",\"fileName\":\"3084641_en_US_08_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Numba is a JIT compiler for Python that can significantly speed up your code. In this video, learn how to use Numba to speed up code.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5865654,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] NumPy will be fast enough  \\n for most of your needs,  \\n especially when you avoid for-loops  \\n and use vectorized code or ufuncs.  \\n However, there are cases when you can't vectorize your code  \\n or that you like to squeeze even more performance.  \\n The common approach in Python is to write  \\n soft, performance-sensitive code in Cython  \\n or C and use it from Python.  \\n However, before going that route  \\n let me show you another approach that is less painful.  \\n Numba is a JIT compiler for Python.  \\n What a JIT compiler does,  \\n it is going to create a specific machine code  \\n for a given function.  \\n So the first time you decorate a function  \\n with the number ngit or JIT decorator  \\n it is going to do nothing.  \\n The first time you call the function,  \\n Numba is going to check, is there a compile code  \\n for this function with a couple of integers.  \\n If no, it is going to generate a compiled machine code  \\n which is highly optimized for that specific type of numbers  \\n and then use it.  \\n Next time you call a function,  \\n it is going to see that the compiled version  \\n is already there and use this version.  \\n Under the hood, Numba uses the LLVM compiler infrastructure.  \\n It is a mature compiler with a lot of optimization built in.  \\n Let's have a look.  \\n So here's our code.  \\n This is polyn.  \\n It gets N as an argument, set a total to zero  \\n and then for something in range of N,  \\n it is going to calculate again and again  \\n seven times N squared minus three times N  \\n plus 42, and return the total.  \\n And we're going to use the timeit module  \\n with 10,000 iteration to see how much time it takes.  \\n So let's run the cell.  \\n And we see that it takes about 155 microseconds.  \\n Now let's use Numba, so import Numba  \\n and then we decorate the function with numba.jit  \\n or numba.njit, and then we define the same function.  \\n I just called it with another name  \\n but the coding side is exactly the same.  \\n And again, we're going to run timeit on the function.  \\n So right now we are at 20 microseconds.  \\n So let's do the calculation.  \\n 155 divided by 20.3.  \\n And we see this is about 7.6 times faster  \\n just by adding the Numba.jit decorator.  \\n Numba shines when we run non vectorized code  \\n like a follow in Python.  \\n There are options to squeeze even more optimization  \\n from the JIT compiler.  \\n Do read the documentation.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4363238\",\"duration\":296,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand deep learning\",\"fileName\":\"3084641_en_US_08_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Deep learning has revolutionized machine learning. In this video, learn how to use Keras to create and use deep learning models.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12422156,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Voice Over] Deep learning is a hot subject.  \\n In its base, Deep learning uses neural networks  \\n which has been around for a while.  \\n However, it uses some new ways of utilizing hardware such  \\n as GPUs to gain some very impressive results.  \\n There are several deep learning libraries available.  \\n Lucky for you,  \\n most of them are available in Python.  \\n You can choose from TensorFlow,  \\n PyTorch,  \\n MXNet,  \\n and others.  \\n We're going to work with Keras,  \\n which is part of TensorFlow and it's easier to start with.  \\n We're going to identify handwritten digits.  \\n We use a data set that comes with Scikit-Learn.  \\n So from Scikit-Learn data sets  \\n we import \\\"load-digits\\\" and the digits is \\\"load-digits\\\"  \\n and we are going to run the cell.  \\n Let's have a look at the digit at location 37.  \\n And this is the image.  \\n If you look at the target at the same location  \\n we see that this is the number 9.  \\n The images are constructed of 8 on 8 images  \\n but the data is flattened out to vectors of 64.  \\n Each one containing a pixel.  \\n We are going to build a network.  \\n They're going to have input units,  \\n and for us the input units are going to be the 64 pixels.  \\n So this one  \\n on the left is going to be 64.  \\n Then hidden units.  \\n And finally the output units,  \\n with the probability for each of them.  \\n We are going to guess 1 of 10 digits.  \\n So we are going to have 10 output units.  \\n First, we're going to split and train the data  \\n but we'll need to do one extra step.  \\n The target is one dimensional.  \\n With values from 0 to 9.  \\n You will need to transform each value  \\n to an array with 10 elements.  \\n All of them 0 except the one with the digits.  \\n Since this is a common task  \\n Keras has the utility to do just that.  \\n Right, so if you take \\\"to_categorical\\\"  \\n from \\\"tensorflow.keras.utils\\\"  \\n and do \\\"to_categorical\\\" from 0, 1, 2, 0, 1  \\n and we are going to run it.  \\n And do ignore the warnings on my machine,  \\n it is not set up to use GPU.  \\n We see now that we get the output as vectors of 3.  \\n Each row has 3 elements: either 0, 1, or 2  \\n and 1 only in the place for the right value.  \\n So our first value is 0,  \\n we have 1 here and 0 is here.  \\n The second value is 1,  \\n So we have 0 on the first location,  \\n 1 in the second location,  \\n and 0 at the rest,  \\n et cetera, et cetera.  \\n So now X is Y of data and Y we're going to convert it  \\n to \\\"to_categorical\\\".  \\n So we'll get the right shape.  \\n And now from Scikit model selections,  \\n we're going to do the train test split  \\n and split the data to training and testing data.  \\n Now we're going to build our model.  \\n So from Keras we import sequential  \\n and from the layers we import  \\n the Dense layer and Activation layer.  \\n The input dimension is the shape of the X at location 1  \\n and the output dimension is the shape of Y at location 1.  \\n So to create a sequential model,  \\n start with a Dense of 128  \\n and the input shape is the input dimension  \\n in our case 64.  \\n Then an Activation layer,  \\n then another Dense with an output dimension  \\n in our case 10  \\n and then another Activation, this time with \\\"sigmoid\\\".  \\n And we're going to compile the model  \\n telling it what is the loss, what is the optimizer,  \\n and what are the metrics that we are interested in?  \\n Let's run this one.  \\n And now Keras generated a TensorFlow model.  \\n Now we can call fit on the model  \\n and we're going to say we want it for 10 epochs.  \\n Now that the model is trained, we can call evaluate.  \\n This is slightly different  \\n than the score method of Scikit-Learn, but roughly the same.  \\n So we're going to get the accuracy  \\n which is the second parameter.  \\n So let's run this cell and we can see  \\n that we are at around 0.95 accuracy, not bad.  \\n Now let's predict.  \\n So we'll get X test, the first three arguments  \\n and we see we get an array with probability for each number  \\n which is what the neural network clearing.  \\n But for us, this is not that helpful.  \\n What we're going to do is we're going to call argmax  \\n on this input, meaning I want the location  \\n of the biggest argument and axis=1  \\n meaning I want to work it on every row.  \\n Once we run this one,  \\n we're going to see that it gets 5, 0, and 4.  \\n And if you're going to do the same thing for the Y test,  \\n we're going to see it is actually 5, 0, and 4.  \\n We can save our model to disk and load it later.  \\n So model.save('digits.h5')  \\n We're saving it to a format known as aGF5.  \\n Later on we can load the model from the disk  \\n and run it again, predicting the test.  \\n And you see we get the same results.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361163\",\"duration\":154,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Work with image processing\",\"fileName\":\"3084641_en_US_08_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Images are a unique kind of data. In this video, learn how to do image processing with OpenCV.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6258528,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] When it comes to image processing  \\n you have many options in Python.  \\n Let's have a look at the few most common ones.  \\n Inside matplotlib  \\n you have some utilities to display images.  \\n There are some filters  \\n in the scipy.ndimage module.  \\n Second image has many algorithms for working with images.  \\n Pillow can be used to reshape images and paint on them but  \\n by far the biggest and most comprehensive library is OpenCV.  \\n Open CV is written in C++ and has great bindings to Python.  \\n It'll perform most of the tasks you'd like to do  \\n on images and then some.  \\n Let's have a look.  \\n Say we have this image of a sign.  \\n By the way, I highly recommend hiking this route.  \\n Let's start.  \\n So I'm going to import cv2  \\n and then the image is cv2.imread for image read,  \\n the sign at CSV,  \\n and let's run the cell  \\n and we see that we have 1700 on 2300.  \\n This is the image size and three  \\n and this is for every pixel the colors in it.  \\n Let's have a look.  \\n So we're going to use matplotlib show the image.  \\n Matplotlib does show the image, but the colors seem off.  \\n The reason is that OpenCV uses BGR encoding and not RGB.  \\n So we can convert using the cv2.cvtColor the image  \\n and we tell it cv2.Color_BGR2RBG,  \\n and if you're going to show this one  \\n now it'll have the right color.  \\n A common task is to convert images to gray scale  \\n and this task can be done again  \\n with the cvtColor functions  \\n and this time we're going to use Color_BGR2GRAY  \\n and going to show the image.  \\n This time the image does not look like gray but  \\n the reason now is the color map that matplotlib is using.  \\n If you're going to tell matplotlib to show the image  \\n but use the gray color map, it is going to look just fine.  \\n Let's find the edges of the image.  \\n We're going to use the Canny algorithm on the gray image  \\n and we're going to give it two thresholds, 80 and 150,  \\n and again we're going to show the edges image  \\n with the gray column map,  \\n and now we see that we've detected most of the edges.  \\n OpenVC can do much more  \\n and there are many examples and tutorials on how to use it.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4360144\",\"duration\":182,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand NLP: NLTK\",\"fileName\":\"3084641_en_US_08_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Text is a very common data format. In this video, learn how to use NLTK to do natural language processing on text.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7223307,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - NLP stands for Natural Language Processing.  \\n NLP deals both with understanding text,  \\n and to some extent generating text.  \\n Most of the tasks are in the understanding side  \\n but with the rise  \\n of personal assistance and the rise of chat bots,  \\n text and speech generation is on the rise as well.  \\n In Python, we have several libraries that work with text.  \\n Scikit-Learn has some text processing capabilities.  \\n Deep learning has been used successfully in NLP, so Keras,  \\n TensorFlow and others are options as well.  \\n NLTK, which stands for Natural Language Toolkit  \\n was originally designed for teaching NLP,  \\n and during the time gained a lot of practical tools.  \\n And Spacy, which is an industrial strength NLP package  \\n with many practical tools built over a nice API.  \\n And this is the one we're going to have a look at.  \\n Once you install Spacy, you will also need to  \\n download a pre-trained model.  \\n This is done with Python, -m spacy download,  \\n and then the model name.  \\n In our case, it's the English one,  \\n or the English core Web sm for Small.  \\n This is already done for you in the code space  \\n so you don't have to run this command.  \\n Let's have a look.  \\n So we are going to take from Scikit-Learn data sets  \\n the Fetch 20 news groups, and we're going to  \\n get just a space one and remove the headers and the footers.  \\n We'll take one single text and print it out.  \\n And here is a text that we have.  \\n Now we're going to import Spacy and we're going to  \\n load the core web small that we downloaded.  \\n Then say the doc is NLP of the text  \\n and look at the doc.  \\n Spacy is going to complain about missing  \\n libraries for working with GPUs.  \\n You don't have to worry about that.  \\n It is still going to work.  \\n And if you look the output document looks very  \\n much like the original text.  \\n But if you look at the type of the document  \\n we are going to see that this is something else.  \\n This document contains a lot of information about the text.  \\n For example, we can list the document sentences  \\n and get the first sentence.  \\n Splitting text to sentences is surprisingly hard task.  \\n We can iterate over the sentences and look at the tokens.  \\n So for every sentence, for every token in the sentence  \\n we're going to print the token  \\n with a text and the tag of the token, right?  \\n And these tags, VB for verb, et cetera  \\n is something that you will need to learn about.  \\n The convention in Spacy is that attributes  \\n without underscore at the end are numeric  \\n and one that ends with an underscore  \\n like the tag here are textual.  \\n Document itself also has entities or ents  \\n which are the name entities it can find.  \\n So for every entity in the document  \\n you print the text and the label of it.  \\n And you see that \\\"Army\\\" is an organization  \\n \\\"Intelligent\\\" is an organization, and \\\"luck\\\" was a person.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4364135\",\"duration\":303,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Working with bigger data\",\"fileName\":\"3084641_en_US_08_06_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"NLP has seen some novel advances in recent years. In this video, learn how to use spaCy to analyze text.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11573602,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Pandas, NumPy,  \\n and most of the data science library in Python  \\n works best when the data fits in memory.  \\n Moving outside a single machine is hard.  \\n Try as much as you can  \\n to stay inside the limit of one machine.  \\n If you lack memory,  \\n before you start with a big data solution,  \\n remember that you can rent a cloud machine  \\n with several terabytes of memory.  \\n But sometimes you will need to deal with data  \\n that is too big to fit in memory.  \\n Before you start  \\n the painful and expensive route to big data systems,  \\n there are some options you can try.  \\n Most of the data is stored in databases,  \\n and Pandas can read and right to databases.  \\n Before we start, you need to run the download data script  \\n so you'll have the taxi.csv file in this directory.  \\n We're going to use SQLite.  \\n SQLite is an embedded database, and it's easy to work with.  \\n So what we're going to do  \\n is we're going to take the data from CSV,  \\n and insert it to a database.  \\n This process is known as ETL.  \\n ETL stands for extract, transform, and then load.  \\n So we're going to import SQLite, Pandas,  \\n and tqdm which is a nice library for showing progress bars.  \\n We're going to use SQLite to connect to the database.  \\n And this is going to create a database if it does not exist.  \\n Now we are going to read the CSV, but in chunks.  \\n So we're going to say \\\"chunksize=!00_000\\\".  \\n Pandas is going to load  \\n up to 100,000 lines in memory at once.  \\n And this is chunks, which is an iterable.  \\n We can iterate over it,  \\n so we can do four chunk in tqdm of chunks,  \\n which is basically like four chunk in chunks,  \\n but it's going to show a progress bar.  \\n And then we're going to do to_sql.  \\n This is the data frame, to_sql method.  \\n The SQL table is called \\\"rides\\\".  \\n Using the connection  \\n we tell it that we don't want to store data frame \\\"index\\\"  \\n in the database,  \\n and if the table exists, append the data into it.  \\n Let's run this one.  \\n And now we are done.  \\n If we're going to look at the files again,  \\n we are going to see now  \\n that we have the taxi.db on the disc.  \\n And now we can use SQL to query the data.  \\n So, from \\\"SELECT\\\", \\\"passenger_count\\\",  \\n and the \\\"COUNT of vendorID as count\\\" from the rides  \\n where the passenger_count is bigger than one,  \\n group by the passenger_count.  \\n And we are going to do with SQL,  \\n with this SQL statement and the connection to the database,  \\n and show the data frame.  \\n And we see the passenger count,  \\n and how many rides with these passengers we had.  \\n And we have very few rides with big parties.  \\n Most databases run on strong machines  \\n and have a lot of computation power and storage capacity.  \\n It's common practice to use the database  \\n to run some filtering and aggregation  \\n to make the data small enough to fit in memory  \\n and then continue with Pandas from there.  \\n Which means you will need to learn SQL,  \\n which is another language.  \\n But it is well worth the effort.  \\n A lot of data out there is in relational databases.  \\n Apart from SQL, you can also look at the HDF5 format  \\n which is designed specifically to store big matrices.  \\n If you do need to go  \\n outside of a boundary of a single machine,  \\n you can have a look at Dask first.  \\n Dask provides an API very similar to NumPy and Pandas.  \\n We have an array, data frame, a bag, delayed, and futures.  \\n But most of the time you're going to work  \\n with array or data frame.  \\n When you run a calculation,  \\n Dask will automatically distribute the work among workers.  \\n And these workers can be either  \\n threads on processors on the same machine,  \\n or by using Dask distributed on a pool of machines.  \\n Let's have a look.  \\n We import Dask data frame as dd,  \\n and now we do dd.read_csv, again, our taxi,  \\n and I'm going to help it and say  \\n that datatype for the vendorID is float64.  \\n And we see that this cell runs very fast  \\n comparing to Pandas.  \\n This is because Dask didn't actually load  \\n the whole data to memory.  \\n Now we can do, for example, vendorID value_counts.  \\n And we are going to run this one.  \\n Again, it is going to be very fast  \\n because there's no calculation happening.  \\n Dask is just registering,  \\n what are the calculations that need to be done?  \\n But until we call compute, nothing actually happens.  \\n So now let's do value_counts with compute.  \\n And now you see it's going to take more time,  \\n but it's going to give us a result.  \\n The reason Dask works in this format  \\n is that we can chain computation,  \\n letting Dask decide how to make them parallel  \\n and only at the end run them.  \\n \\n\\n\"}],\"name\":\"8. Other Packages\",\"size\":46403148,\"urn\":\"urn:li:learningContentChapter:4359136\"},{\"duration\":522,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4361164\",\"duration\":45,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Development process overview\",\"fileName\":\"3084641_en_US_09_01_3006561_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2388328,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We work in teams.  \\n Teams allows us to do work that individuals can't do.  \\n And in good teams,  \\n the whole is much greater than the sum of its parts.  \\n However, working in teams  \\n means we need to do some adjustments.  \\n There are many variants of the development process,  \\n and almost every team is a bit unique in the way it works.  \\n I'll try to show you some of the things  \\n I think strike good balance  \\n between helping the team and letting do your thing.  \\n Process is sometimes referred to as accidental complexity.  \\n It's the things you need to do  \\n and not strictly data science,  \\n but are essential to get the product out of the door.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4362142\",\"duration\":72,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand source control\",\"fileName\":\"3084641_en_US_09_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Source control helps with keeping history and parallel work. In this video, explore how to use Git to track your work.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3378015,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - There is a poem going around.  \\n \\\"99 little bugs in my code.  \\n Take one down, patch it around, 117 bugs in my code.\\\"  \\n We do make mistakes, and sometimes we need to get back  \\n to an earlier version of the code we wrote.  \\n This is a job for source control.  \\n The other major job of source control is to  \\n allow several people to work on the same code.  \\n There are many systems for source control,  \\n but Git is by far the most widely used one.  \\n Most IDs have support for Git, and you can run Git command  \\n from inside your ID, such as visuals to the code.  \\n Learning Git is outside the scope of this course.  \\n Do check our offering on the subject.  \\n Sites such as GitHub are offering  \\n more than just version control.  \\n They have support for the full software life cycle.  \\n One problem that is more specific to scientific  \\n application is data files, which tend to be large.  \\n It's not a good idea to commit large files into Git.  \\n Take a look at Git large file support,  \\n known as LFS,  \\n or even at DVC,  \\n which is built for Machine Learning Projects.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4361165\",\"duration\":131,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Learn code review\",\"fileName\":\"3084641_en_US_09_03_3006561_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Code review is used to flush out design flaws and bugs. In this video, learn how to do effective code reviews.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6709609,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Linus's law states  \\n that \\\"given enough eyeballs, all bugs are shallow.\\\"  \\n It means that the more people look at your code,  \\n more bugs are likely to be found.  \\n I teach a lot of classes,  \\n and my students catch my mistakes every time  \\n before I have a chance to find them myself.  \\n The process of letting people look at your code  \\n and comment on it is called code review.  \\n I highly recommend it will be a central part  \\n of your development process.  \\n You'd be amazed how many bugs are caught  \\n and how many improvements people come up with.  \\n There are many ways to do code reviews,  \\n from sitting together in a room and going over your code  \\n to online tools where people comment  \\n on the changes you make to the code.  \\n No matter the way you're doing it,  \\n it's important to remember  \\n that people are commenting on your code and not on you.  \\n Some people get very defensive  \\n when people comment on their code,  \\n and it makes the code review process very painful.  \\n Getting everyone to understand  \\n you're just trying to make the code better and fix issues  \\n and keeping everything technical and professional  \\n is very important step in making good code reviews.  \\n Some companies have checklists, such as:  \\n did you run the tests,  \\n is the code easy to understand,  \\n are there any new dependencies, and more.  \\n Try to think of such a list from your experience,  \\n which means from the mistakes you made.  \\n I think you'll find it useful.  \\n In the world of hosted Git,  \\n it's very common to create a pull request.  \\n This means that you work on a feature branch,  \\n and once you're happy with your code,  \\n you create a pull request that people can comment on.  \\n Here's an example from the pandas project.  \\n We have a pull request that Jeff submitted,  \\n and then they started a discussion about the pull request  \\n with comments from various people.  \\n And every time you see  \\n that the response is a new change to the code,  \\n and people can comment that, etcetera, etcetera,  \\n until finally, when the code is deemed good enough,  \\n it can be merged back to the main branch.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4365168\",\"duration\":153,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Testing overview\",\"fileName\":\"3084641_en_US_09_04_3006561_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Testing makes sure your code is correct. In this video, learn about pytest and how to use it to test your code.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6693908,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] An essential part of your process  \\n should be testing.  \\n Testing ensures that code behaves the way it's supposed to,  \\n and that we didn't break one thing  \\n while fixing another or adding a new feature.  \\n There are many kinds of tests, unit tests, regression tests,  \\n acceptance tests, functional tests, stress tests,  \\n fuzzers, and many more.  \\n You don't have to be a testing expert.  \\n Pick the areas in your code  \\n you think are most important and test them.  \\n Tests also have a downside.  \\n It takes time to write them,  \\n and every time we change our code,  \\n we also need to change some tests.  \\n We say tests add math to your code  \\n and make changes more difficult.  \\n I'd say the testing is a balance between pain and gain.  \\n The gain is how much test you write.  \\n The pain is the cost of an error in your code.  \\n If you're writing an internal application for your company,  \\n bugs, as pleasant as they are, are not that critical.  \\n However, if you write software that lives depend on it,  \\n or software that will run a device on Mars,  \\n bugs are really painful.  \\n Note that no matter how much you test,  \\n bugs will get into production.  \\n You need to be ready with a way  \\n to monitor your program for bugs,  \\n and a way to quickly fix and ship it to production.  \\n The method will vary a lot depending on the system you use.  \\n NASA has one of the most strict development process  \\n in the world and they still manage  \\n to ship bugs to outer space.  \\n However, NASA will manage to fix bugs on Mars.  \\n I'm sure your situation is not that difficult.  \\n Another thing you might want to consider  \\n is continuous integration, or CI system.  \\n These systems will monitor your source control,  \\n and every time they detect a change to your source control,  \\n they will pull the code and run all the tests.  \\n This helps avoid the problem of,  \\n \\\"I don't care it runs on your machine;  \\n we don't sell your machine to customers,\\\"  \\n meaning it'll find integration problems quickly.  \\n When a test fails in continuous integration,  \\n the system will alert the team,  \\n maybe with a message on a chatroom, maybe with an email.  \\n Now the whole team know we have an integration problem  \\n and will work or fix it.  \\n Continuous integration system  \\n also used to run tests that take long time  \\n or require complicated set up.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4360145\",\"duration\":121,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Testing example\",\"fileName\":\"3084641_en_US_09_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Writing effective tests saves you time in the future. In this video, explore an example of how to test a scientific application.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3880458,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Python has a built in test-with.  \\n However, most projects and companies prefer to use pytest,  \\n which is more Pythonic and is packed with features.  \\n Let's see an example.  \\n As soon I have my function, which scales a vector.  \\n It gets a vector in an N and return the vector times N.  \\n And now we're going to write a test.  \\n Test files should either be  \\n test underscore something .py or end with _test.py.  \\n In the code, we import numpy.  \\n We import our scale function and then write a test.  \\n We create an array, set the N,  \\n and this is the expected result,  \\n color code to get the output,  \\n And finally call numpy all close on the expected,  \\n the output, specifying that nans are equal.  \\n And we use the built-in assert to make sure  \\n that this is true.  \\n Pytest is a discovery based system.  \\n It's going to discover all the test files  \\n and then inside the test files all the functions  \\n that start with test and run them.  \\n At first, you need to configure  \\n Visual Studio Code to run pytest.  \\n You do this by clicking on the hamburger menu,  \\n going to view, and then the command palette.  \\n Here find Python: Configure Tests, pick pytest,  \\n and then say that the Root directory is dot.  \\n And give it a minute.  \\n Eventually you're going to see  \\n either a triangle or a check mark here.  \\n And now you can click on it to run the test.  \\n You are going to move to the test view.  \\n And here you see that we have a single test  \\n inside chapter 9 0 9 0 5, and this test is passing.  \\n Pytest has much more to offer  \\n such as parameterize tests, fixtures, and more.  \\n Apart from testing that your code is correct,  \\n you can also run regression tests that make sure  \\n that the accuracy of your models is not degrading.  \\n Check out my course  \\n about testing scientific applications for more.  \\n \\n\\n\"}],\"name\":\"9. Development Process\",\"size\":23050318,\"urn\":\"urn:li:learningContentChapter:4363240\"},{\"duration\":23,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4361166\",\"duration\":23,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"3084641_en_US_10_01_3006561_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1322343,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Congratulations on reaching the finish line.  \\n Now you need to practice.  \\n Grab some data and start asking questions.  \\n A good place to start is Kaggle,  \\n which offers a variety of tasks with many levels.  \\n Keep hacking, and stay curious.  \\n \\n\\n\"}],\"name\":\"Conclusion\",\"size\":1322343,\"urn\":\"urn:li:learningContentChapter:4361169\"}],\"size\":344931897,\"duration\":8738,\"zeroBased\":false},{\"course_title\":\"Python Statistics Essential Training\",\"course_admin_id\":4433355,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":4433355,\"Project ID\":null,\"Course Name\":\"Python Statistics Essential Training\",\"Course Name EN\":\"Python Statistics Essential Training\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"The field of statistics has become increasingly dependent on data analysis and interpretation using Python. With the rise of big data and data science, the demand for professionals who can effectively analyze and interpret data using Python has skyrocketed. In this course, Matt Harrison teaches you how to collect, clean, analyze, and visualize data using the powerful tools of the Python programming language. Join Matt as he gives into the various techniques that form the backbone of statistics and helps you understand the data with summary statistics and visualizations. He explains how to create predictive models using both linear regression andXGBoost, and wraps up the course with a look at hypothesis testing.If you\u00e2\u20ac\u2122re interested in exploring statistics using a code-first approach, join Matt in this course as he shows you how to use Python to unlock the power of data.\",\"Course Short Description\":\"Learn to use Python to unlock the power of data and use it to inform decisions.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":9254436,\"Instructor Name\":\"Matt Harrison\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Python and Data Science Corporate Trainer, Author, Speaker, Consultant\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2023-08-17T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/python-statistics-essential-training-19258005,https://www.linkedin.com/learning/python-statistics-essential-training-2023-reboot\",\"Series\":\"Essential Training\",\"Limited Series\":null,\"Manager Level\":\"General\",\"LI Level\":\"Advanced\",\"LI Level EN\":\"Advanced\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Data Science\",\"Primary Software\":\"Python\",\"Media Type\":\"Video\",\"Has CEU\":\"Yes\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":9565.0,\"Visible Video Count\":31.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":277,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4509084\",\"duration\":50,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Being a Python statistics MVP\",\"fileName\":\"4433355_en_US_00_01_WL30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"In this video, explore a compelling example of the topic in action and learn how to use this space to grab the browsing learner. The TOC and course description exist on the course's home page to communicate what's in the course and who it's for.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3187642,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Have you ever wanted to explore statistics  \\n using a code-first approach?  \\n You've come to the right place.  \\n This Python-based course will dive into various techniques  \\n that form the backbone of statistics.  \\n We'll understand the data  \\n with summary statistics and visualizations.  \\n Then we'll create predictive models  \\n using both linear regression and XGBoost.  \\n Finally, we'll explore hypothesis testing.  \\n Because we use Jupyter Notebooks and GitHub Codespaces,  \\n you can easily run the same code  \\n and follow along in a web-based environment  \\n without any complicated installation.  \\n Hi, I'm Matt Harrison,  \\n a Python and data science corporate trainer.  \\n I'm excited to join you  \\n on this incredible statistics adventure together  \\n using a code-first approach.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4509083\",\"duration\":41,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"4433355_en_US_00_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, discover any use of GitHub or other project tools employed in the course.\",\"captionsStatus\":\"NOT_AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":962095,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":null},{\"urn\":\"urn:li:learningContentVideo:4505089\",\"duration\":186,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Using GitHub Codespaces with this course\",\"fileName\":\"4433355_en_US_00_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to use Codespaces with the course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7492655,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson,  \\n I'm going to show you how to configure codespaces  \\n for this course.  \\n You want to navigate to the course repository  \\n and then click on this green Code button.  \\n You'll see that there's a local tab and a codespaces tab.  \\n If you wanted to run this locally, you could,  \\n if you know how to get a Python environment set up,  \\n there's a requirements.txt  \\n that you can use PIP to install the requirements  \\n if you want to run locally.  \\n Alternatively, you can use codespaces, which is really easy.  \\n Click on this codespaces tab  \\n and then come over here and say create codespace on Main.  \\n That's going to open up a new tab here,  \\n which is going to set up your codespace.  \\n This is going to take a while to run.  \\n We'll let it do its thing,  \\n and then we'll come back when it's finished.  \\n Okay, this has launched codespaces.  \\n If you're not familiar with codespaces,  \\n basically this is a web environment  \\n where you can work on a project.  \\n What it's done is it's launched a virtual machine  \\n and we have a web environment  \\n that lets us talk to the machine.  \\n You can see that I'm in a terminal right here.  \\n I can say LS and I have access  \\n to all of my favorite Linux commands here.  \\n We're also launched in VS Code, so for this course,  \\n all we have to do is click on this Pystats.ipynb.  \\n This is the notebook for the course.  \\n There is a solutions notebook as well  \\n that says -Solutions.  \\n Don't open that one.  \\n You're going to want to open the one that doesn't have  \\n the -Solutions.  \\n So double click on this  \\n and it will open the notebook in VS Code.  \\n If you want to get rid of the explorer over here,  \\n you can click on that to remove that.  \\n Okay, so here is our notebook,  \\n and what I want you to do is just click on  \\n this first cell that has code on it.  \\n I'm just clicking on the left hand side of this rectangle.  \\n Hold down Control and hit Enter.  \\n That's going to run this cell,  \\n and you can see it has some options pop up  \\n when we do that, it says type to choose a kernel source.  \\n It says Python Environments  \\n and it says Existing Python Server.  \\n So we have a Python environment configured here.  \\n Let's just click on that, which is this Python 3.10.8.  \\n Just click on that.  \\n We'll let it do its thing.  \\n You can see in the lower right hand corner,  \\n it says it's connecting to a kernel.  \\n Now you can see it's running and it has finished running,  \\n at this point, you can see it says 2.0.2.  \\n In this lesson,  \\n we showed you how to open a project in GitHub,  \\n create a codespace, and then connect a Jupyter Notebook  \\n to that codespace and execute it through codespaces.  \\n Codespaces is a nice tool that you find  \\n on GitHub that lets you easily start projects  \\n without having to worry about configuring environments.  \\n I recommend using them if you're not familiar  \\n with virtual environments in Python.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":11642392,\"urn\":\"urn:li:learningContentChapter:4509085\"},{\"duration\":2954,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4502196\",\"duration\":328,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Loading data\",\"fileName\":\"4433355_en_US_01_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"With the skill of loading data with Pandas, you can easily read data from various file formats such as CSV, Excel, SQL databases, and more, manipulate and clean data using Pandas' powerful tools, perform data analysis and exploration, and prepare data for modeling and visualization. Pandas is widely used in the data science community, making it a valuable skill for anyone working with data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12963459,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson, we are going to look at  \\n loading the data.  \\n I've opened the notebook in Codespaces.  \\n If you want to run this locally,  \\n you can run this locally as well.  \\n I'm running the PyStats, IPy and B notebook  \\n and I'm just going to hide this  \\n so we have a little bit more screen real estate.  \\n So we're going to be looking at loading the data.  \\n We're going to be using the Ames, Iowa Housing Data set.  \\n This is a nice little data set.  \\n It's got a couple thousand lines of data  \\n about housing sales in Iowa, and it's got 80 plus columns  \\n of information around those cells.  \\n So this is going to be a great data set for us to look at  \\n and examine some statistics using it.  \\n Let's go down and run some of these cells here.  \\n The first cell I'm going to run is this import cell,  \\n and to run that, I'm going to hold down control  \\n and hit enter.  \\n What that does is it tells Jupiter we want to run the cell.  \\n I like control and enter because it leaves the focus  \\n on the cell.  \\n What that allows me to do is hit enter  \\n and I can go in and edit this cell if I want to after that  \\n and then I can rerun that by holding down control  \\n and hitting enter again.  \\n So we're checking the version of Pandas pd.__version__,  \\n we call that Dunderversion  \\n and we see that we are using Panda's version 2.02.  \\n So one thing to be aware of in this course  \\n is that we are using the latest version of Pandas  \\n as of the recording, Pandas Two  \\n and Pandas Two has a few features that allow it  \\n to do things more efficiently  \\n and with less memory than Pandas One.  \\n So I'm going to be using some Pandas Two functionality  \\n in this course.  \\n If you don't have Pandas Two, some of the code might not  \\n work for you, but hopefully I can convince you  \\n that you probably want to upgrade to Pandas Two  \\n to take advantage of the new features there.  \\n Let's load the data set in this cell.  \\n Again, I'm going to hold down control and hit enter  \\n and this should run.  \\n At the bottom here, you see I'm using the read_CSV function  \\n to load the data.  \\n I'm passing in a URL.  \\n We've got this locally on our code space.  \\n I also have a variable pointing to where you can get this  \\n on the internet if you want to as well.  \\n When you look at this read_CSV function, this probably  \\n looks different if you're used to Pandas One  \\n with the two parameters, engine and dtype_backend.  \\n Both of those we are specifying as pyarrow.  \\n What engine equals pyarrow does  \\n is it uses the arrow library to load the CSV file.  \\n This can be a lot quicker than using the standard  \\n Pandas logic to do that.  \\n And what dtype_backend does is it allows you  \\n to store the data types using the arrow library instead  \\n of NumPy, and that gives you a lot of memory benefits  \\n as well.  \\n Let's inspect the shape of this dataset.  \\n I'm going to go down and execute this next cell.  \\n Again, holding down, control and hitting enter to run that.  \\n You can see that this returns a tupple.  \\n There's 2,930 rows and 82 columns.  \\n Let's look at the first few rows of data.  \\n I'm going to use this head method to do that  \\n and you can see the first five rows.  \\n There are 82 columns in here.  \\n You can scroll over if you want to and see some of it.  \\n Note that we are getting some of the columns truncated here.  \\n You can see that there's an ellipses.  \\n We're only showing you the first 10 columns  \\n and the last 10 columns, but we've got 82 columns of, again,  \\n housing sales data from Ames, Iowa.  \\n One of the things I like to do is run the describe method  \\n and what the describe method does is gives us  \\n summary statistics for each of the numeric columns  \\n in the data frame.  \\n In the index now you can see on the left hand side  \\n where it says count, mean, STD, min,  \\n that is the index of this data frame  \\n and indexes don't have to be numeric.  \\n Pandas will let them be non-numeric, and in this case,  \\n we can see that we have non-numeric,  \\n we have string entries for the index  \\n representing the statistical results  \\n of each of these numeric columns.  \\n So for order, this is the order of cells,  \\n and you can see in the count row, 2,930.  \\n Now, one thing to be aware of with Pandas  \\n is that count probably doesn't mean what you think it means.  \\n Count in Pandas means the number of non missing values.  \\n So, if we look over a little bit at the lot frontage count,  \\n it says that that is 2,440.  \\n So what that means is that there are 2,440 entries there  \\n that had values and that there are a few of them  \\n that are missing values.  \\n We also have the mean, the standard deviation,  \\n the minimum value, and the maximum value,  \\n and then the inner quartiles there.  \\n I like to use this for a gut check to understand  \\n what the data looks like.  \\n You can see that a lot of these numeric values  \\n don't go very high and they're also non-negative.  \\n By default, Pandas is going to use 64 bits of information  \\n to store this data.  \\n We can probably save more memory by changing those types  \\n and I'll show how to do that in a future lesson.  \\n In this lesson, we loaded the data  \\n for the Ames, Iowa dataset  \\n and then we did some basic inspection and sanity checks  \\n to make sure that we have the data loaded.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4504093\",\"duration\":959,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Strings and categories\",\"fileName\":\"4433355_en_US_01_02_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"With the skill of cleaning categorical data, you can ensure that the data is accurate and consistent, which is critical for making meaningful analyses and predictions. You can also effectively handle missing values, outliers, and other anomalies in the data, which can improve the accuracy of your models and prevent biased results.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":34827803,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson,  \\n we're going to look at strings and categories  \\n and understand these types of columns  \\n and how to deal with them in Pandas.  \\n I've listed out some of the goals here.  \\n We want to be able to use the dtypes attribute.  \\n We want to look at using the select_dtypes method.  \\n We're also going to do summary statistics  \\n using the describe method.  \\n And we're going to look at memory usage  \\n and converting string columns to category types.  \\n One of the attributes on a data frame  \\n is this dtypes attribute.  \\n And when you run this,  \\n what this is going to give you is a Pandas series.  \\n In the index of this series,  \\n we have the columns.  \\n Remember a Pandas index does not need to be numeric.  \\n In this case, Pandas is sticking the columns into the index  \\n and then the values for the index,  \\n we can see that says int64,  \\n and then in brackets, pyarrow,  \\n that is the values of this series.  \\n And we can see in brackets it says pyarrow  \\n indicating that we are using Pandas 2  \\n and we are using that pyarrow backend.  \\n Again, because we're using the pyarrow backend,  \\n we're going to get some speed improvements,  \\n we're also going to get some memory improvements as well.  \\n One of the big features of Pandas 2  \\n is that it has a native string type.  \\n We can see for MS zoning that the data type is a string,  \\n and in brackets, pyarrow.  \\n In Pandas 1,  \\n if we wanted to select the columns that were strings,  \\n we would say select_dtypes objects.  \\n And this doesn't work in Pandas 2  \\n because the strings don't have the object type.  \\n This is what we need to do in Pandas 2.  \\n We're going to say select_dtypes and say string,  \\n and we're going to pass that in as a string.  \\n If you look at the output of this,  \\n at the bottom it says you have 2,930 rows and 43 columns.  \\n So this is selecting only the strings column.  \\n You can see in the comment up there I said,  \\n alternatively you can say strings,  \\n and in bracket, pyarrow,  \\n if you'd like the type.  \\n Now, one of the things you can do once you have this  \\n is you can throw on this describe method.  \\n So I'm going to actually throw on a describe  \\n and throw on the transpose on that as well.  \\n I like to sometimes transpose my data  \\n just to see a little bit more.  \\n Let's run this without the transpose,  \\n that's the capital T there.  \\n And you can see that we have 43 columns  \\n and I have to scroll over a little bit to see that.  \\n You can also see that there's an ellipsis here,  \\n so it's not showing all of that.  \\n Sometimes transposing the data,  \\n again, that's flipping the rows and the columns,  \\n lets us see just a little bit more.  \\n Remember we ran describe previously on our data frame  \\n and it gave us summary statistics for the numeric columns.  \\n In this case, we don't have any numeric columns  \\n because we said select_dtypes string.  \\n We just have string columns.  \\n And so when we run describe  \\n on a data frame composed of only string columns,  \\n we get summary statistics for the strings.  \\n In this case, we get the count of non-missing values.  \\n We get the number of unique entries for that.  \\n We also get the most common entry,  \\n so that's the entry with the column top,  \\n and the frequency of that.  \\n So for the MS zoning column,  \\n there are 2,930 values in the rows.  \\n There are seven different values.  \\n The most common value is RL  \\n and it occurred 2,273 times.  \\n You can see for most of these,  \\n that unique value is not very high.  \\n Statisticians sometimes call that cardinality.  \\n So a low cardinality might be a prime candidate  \\n for us to convert this into a categorical type in Pandas,  \\n allowing us to save even more memory.  \\n So in this cell, I'm going to run select_dtypes string  \\n and then I'm going to say memory_usage, deep is equal to true,  \\n and then I'm going to do sum.  \\n And what this is going to give me is the number of bytes  \\n used by the string type columns.  \\n This code is written as a chain.  \\n You can see that we have highlighted the parentheses here.  \\n There's a parenthesis above and below it.  \\n I like to write my code that way.  \\n What it allows me to do is put each operation  \\n on a single line  \\n and it makes your code read like a recipe.  \\n So let me just comment these out  \\n and we'll walk through what's going on in our chain.  \\n Here's our original data frame,  \\n and you can see that there's 2,930 rows here and 83 columns.  \\n If I select just the object columns,  \\n you can see that that goes down to 43 columns.  \\n And then I'm going to say on that,  \\n memory_usage deep is equal to true.  \\n So this is going to return a series  \\n and it's going to tell us how many bytes each column is using.  \\n And then I'm going to sum up this series  \\n and get the total of that.  \\n So it looks like this is using 957 kilobytes of data.  \\n In this next cell,  \\n I'm going to do a similar operation,  \\n but I'm going to insert this astype category.  \\n And when I run that,  \\n you can see that this uses quite a bit less memory.  \\n And what's going on under the covers  \\n is that Pandas is encoding all of the unique types  \\n into a number  \\n and then it's using basically an array  \\n with those unique values pointing back  \\n to the original values.  \\n So it doesn't have to store strings for every entry,  \\n it can just hold a numeric value.  \\n And you save quite a bit of memory if you do that.  \\n You can see that this is almost seven times  \\n more memory-efficient  \\n by converting that to a categorical type.  \\n Okay, so remember we have 2,930 rows.  \\n So one of the things I might want to do  \\n is look at the number of rows that are missing.  \\n And for numeric data,  \\n I would do a chain that looks something like this.  \\n Let me just walk through what's going on here.  \\n I'll comment this out  \\n and explain the code as we're going along.  \\n So the first operation I'm going to do is this isna method.  \\n What that's going to give me back is a data frame,  \\n but rather than having the original values,  \\n it has true/false values.  \\n And then what I'm going to do  \\n is I'm going to do the mean operation on that.  \\n One thing to be aware of in Python,  \\n and also in Pandas because Pandas is in Python,  \\n is that Python treats true and false values as one and zero,  \\n respectively.  \\n So if I take the mean of this,  \\n this is going to essentially be taking the mean  \\n of a data frame with zeros and ones in it.  \\n And what that's going to give me  \\n is the fraction of the values that are missing.  \\n So you can see, for example,  \\n it says that lot frontage,  \\n 0.167 of those are missing.  \\n So if I multiply this by 100,  \\n I get the percentage that is missing.  \\n So 16% of the lot frontage values are missing.  \\n Now what I'm going to do is I'm going to use the pipe method here  \\n and filter out the values that are zero.  \\n What pipe allows us to do is pass in a function into it.  \\n In this case, I'm using a lambda function  \\n that takes the series that is passed in,  \\n the series that you're seeing on the screen here.  \\n And then I'm just going to use a Pandas operation  \\n to filter out the values  \\n where the series is greater than zero.  \\n And here's the entries for that.  \\n You can see that a lot  \\n of the lot frontage values are missing  \\n and the basement year built values are missing,  \\n and then a few of the other values are missing as well.  \\n One of the things that stands out to me  \\n when I look at this is, for example, this 0.03413.  \\n This just kind of makes my spidey-sense tingle a little bit  \\n because that same value is repeated in multiple places.  \\n So that probably means that there's probably a row  \\n or a few rows where these values are missing  \\n for all of those.  \\n That's probably what's happening there  \\n when you see those repeated values.  \\n That previous code cell that we ran,  \\n if you think about it,  \\n is actually not containing any string columns in it,  \\n these are all numeric columns.  \\n So this is a difference in Pandas 2.  \\n In Pandas 2, if a string column is missing a value,  \\n Pandas will actually set that to an empty string  \\n rather than the NA value.  \\n So if we want to find the missing string values,  \\n we actually need to,  \\n instead of making this isna data frame,  \\n we're going to check  \\n whether the data frame is equal to the empty string.  \\n Again, let me just run through this quickly.  \\n I'm going to comment this out.  \\n The chain makes it really easy to comment this out.  \\n So here's our data frame with the string columns  \\n and we're going to say,  \\n are these values equal to the empty string?  \\n And this gives us back our Boolean array,  \\n and then we can just repeat the process  \\n that we did there before  \\n and get the percentage of values that is missing.  \\n In this cell, what I'm going to do is I'm going to select  \\n any of the rows where the values are missing.  \\n So I'm going to pull this out here  \\n and I'm actually going to walk through what's going on here.  \\n Because this is a useful piece of code,  \\n let's walk through what's going on here.  \\n So I have it written in a single line.  \\n It's easier for me to show you what's going on  \\n if I write it in this chain style like this.  \\n And I'm going to comment this out  \\n and we'll just walk through this.  \\n Okay, so here is my string columns.  \\n Where are they equal to the empty string?  \\n And this is a data frame of true/false values.  \\n Now I'm going to do the any along the axis is equal to columns.  \\n So this is going to keep the same index.  \\n So if you look at this, you'll see the same index,  \\n but this is going to return back a series  \\n whether any value in the row was true.  \\n And you can see in this case, there are a bunch of falses,  \\n but presumably some of those are true.  \\n And that's what I'm passing in here,  \\n but I'm also passing in this tilde at the front here,  \\n which negates that.  \\n So what this is going to give me  \\n is it's going to give me the rows  \\n where there are missing values.  \\n And it looks something like this.  \\n We can scroll over a little bit,  \\n and it doesn't look like we're seeing  \\n very many missing values here.  \\n One thing to note  \\n is that we are seeing a bunch of NA values as well.  \\n This dataset is a little messy  \\n in that it has missing values that aren't in there at all,  \\n but it also has NA that is encoding missing values as well.  \\n So that's something to be aware of.  \\n Let's just repeat our operation here,  \\n but instead of looking for the empty string,  \\n looking for the NA string.  \\n And you can see, for example,  \\n that 93% of the alley values are missing  \\n and 48% of the fireplace quality values are missing.  \\n Okay, let's inspect this a little bit more.  \\n I'm going to inspect, where is the pool QC,  \\n the pool quality missing?  \\n Let's run that.  \\n And it looks like there aren't any values  \\n where it's missing.  \\n Again, remember that this Ames dataset  \\n encodes some of the missing values as NA.  \\n Let's run this with isna instead.  \\n And these are all the values where the pool QC  \\n or the quality of the pool is missing.  \\n Presumably, these houses don't have pools  \\n and hence this value is missing.  \\n So that's probably an appropriate encoding there for that.  \\n So one thing I might want to do is I might want to go in  \\n and clean up this data set a little bit.  \\n And so here's one of the ways that I would do that.  \\n I would use the assign method,  \\n and here I'm passing in a chain inside of assign.  \\n So I'm passing in select the dtypes of string,  \\n and with all of the string types,  \\n if there's any missing values,  \\n just fill that in with not applicable.  \\n And then what I'm going to do is I'm going to pass it into assign.  \\n Assign generally takes a parameter,  \\n which is the column name,  \\n and a value for the column name to make a new column.  \\n But if we pass in a data frame  \\n and we put these two stars in front of it,  \\n it will say, take in this data frame  \\n and just update the values with this data frame.  \\n So essentially, what this is giving us back  \\n is a new data frame with the empty string values replaced  \\n by the not applicable values.  \\n Okay, let's go a little bit further here.  \\n Let's look at electrical.  \\n There were some missing values in electrical,  \\n so let's look at that.  \\n And this is how I like to look at string columns.  \\n I like to use this value_counts method.  \\n What that gives me back is a Pandas series,  \\n and it gives me in the index the unique values,  \\n and in the values of the series, the counts of those.  \\n You can see, for example, this row right here  \\n is the empty string values there.  \\n So there's one that is missing, the electrical is missing.  \\n So it'd be good to check, why is the electrical missing?  \\n Is this a house that did not have an electrical system?  \\n Or did this data get dropped?  \\n We probably don't want to leave this as an empty string,  \\n we probably want to figure out what's going on there  \\n and clean that up.  \\n Let's look at the row where it actually is missing.  \\n This is how I would do that.  \\n I would use the query method and say a string, electrical,  \\n equals equals the empty string.  \\n And here's the row where that is missing.  \\n We can inspect what's going on here.  \\n It looks like this was a house that was sold  \\n in May of 2008 for $167,500.  \\n So let's look at another example.  \\n I'm going to do the same thing with fireplace quality.  \\n And in this case,  \\n you can see that there aren't empty strings,  \\n but we do have quite a few NA values,  \\n that's the most common value.  \\n Let's do the same thing with basement condition.  \\n Basement condition has both NA values  \\n and a missing value as well.  \\n So you can see that this dataset is not maybe as clean  \\n as we would like.  \\n Maybe we would want to do something like this  \\n where we say, okay, let's take this data frame  \\n and I'm going to update all of the string columns,  \\n replacing the empty values with not applicable,  \\n and then converting those to a category.  \\n This would be my operation that would do that.  \\n I can just test that out here and check that that does work.  \\n It looks like that was successful.  \\n And this returns 2,930 rows.  \\n Let's check how much memory usage that's going to use.  \\n That's going to give us a data frame  \\n that it looks like is using one meg of memory.  \\n Let's compare that with our original data frame  \\n that is not converted to a category,  \\n and that's using 1.8 megs of memory.  \\n So you can see by converting two categories,  \\n we will save quite a bit of memory.  \\n In this lesson, we looked at some of our string columns  \\n and we looked at various operations  \\n that we would do with string columns.  \\n We did summary statistics on them,  \\n but we also explored some of the missing values.  \\n We did that by using some Pandas manipulation  \\n to find the percentage of values that are missing  \\n and then we dove into those using value_counts  \\n and also using query to examine those.  \\n Understanding what is in your data is an important step  \\n and I highly recommend that you do this step  \\n and don't skip it.  \\n The more you understand your data,  \\n the better you'll be able to explain the data  \\n and share insights with others.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4507091\",\"duration\":957,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Cleaning numbers\",\"fileName\":\"4433355_en_US_01_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"The goal of this video is to learn how to clean numbers by handling missing values, handling outliers, and addressing inconsistencies in the data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":40337860,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson we're going to discuss  \\n cleaning numbers.  \\n We're looking at the PyStats notebook  \\n in the 0 1 0 3 section.  \\n Our goals are going to be using select D types,  \\n using describe, finding missing values,  \\n and I'll also show you how to view a little bit more data  \\n than the default values and style the output.  \\n In pandas one, there would be a lot of entries  \\n for the Float D type here.  \\n That's because in pandas one,  \\n when pandas encountered a integer value  \\n that had missing values, it converted it to a float.  \\n You can see that there aren't any missing values here.  \\n That's because we're using pandas two.  \\n Remember that PyArrow backend  \\n actually has support from missing values.  \\n And so if we say select D types is int  \\n we get 39 columns here.  \\n So there are no floating point columns  \\n there's just integer columns in here.  \\n So let's explore some of these columns  \\n and see what's going on.  \\n So again, one thing that we can do,  \\n is run the describe method  \\n to get summary statistics on those.  \\n One of the things I like to do  \\n is look at the count,  \\n just to see how many values are missing.  \\n For example, you can see that lot frontage is missing  \\n quite a few values.  \\n I also like to look at the minimum values  \\n and the maximum values to see what the range is.  \\n You can also look at the mean,  \\n which a lot of people would refer to as the average,  \\n though a statistician would probably get upset  \\n at you if you said that.  \\n And you can look at the 50th percentile  \\n which a statistician would call the median.  \\n If the mean and the median are around the same,  \\n you might have data that's somewhat normal,  \\n though I also like to look at visualizations  \\n to understand what the lay of the land looks like  \\n or the distribution of that data looks like as well.  \\n Okay, so remember,  \\n we've got 2,930 columns there.  \\n So a lot of these columns aren't missing any values  \\n but some of them are.  \\n So let's look at this lot frontage one.  \\n This is how I would explore it.  \\n I'm going to run the query method and say,  \\n I want to look at where the lot frontage is missing.  \\n Let's dive into what's going on inside of query here.  \\n So query expects you to pass in a string that  \\n takes a column and some operation on the column.  \\n In this case, the column is lot frontage  \\n and, because lot frontage has a space in it,  \\n it is not a valid Python attribute.  \\n So if you have a column name that has characters  \\n that aren't valid Python attributes,  \\n you need to surround them by backticks.  \\n That's what we see here.  \\n And then I'm going to say, okay,  \\n we've got that lot frontage column,  \\n let's just look where it is missing.  \\n And you can see that there are 490 rows  \\n where the lot frontage is missing.  \\n You can see it's represented here by NA  \\n in less than and greater than.  \\n One of the questions I frequently get when I teach people  \\n about missing values is how to deal with them.  \\n My typical response is, it depends,  \\n but probably the best way to deal with them  \\n is to find a subject matter expert,  \\n someone who knows why this data is missing,  \\n and they can often point you in the right direction.  \\n For example, lot frontage.  \\n Does NA mean that there is not a front area  \\n of this house or does this mean that lot frontage  \\n is missing that we don't have that information?  \\n That would be something that would be useful to find out.  \\n If there is not a front of the house,  \\n I would probably encode that as zero rather than NA.  \\n So again, it's important to talk to a subject matter expert  \\n and find out why exactly this data might be missing.  \\n Okay, remember I promised I would show you how to  \\n see more data and this is the secret to doing that.  \\n Pandas, by default, is not going to show you all of the columns  \\n and all the rows if you have a large data set.  \\n And, I think this is useful and a lot of people,  \\n as soon as I tell them this,  \\n ask me how do I get around this?  \\n I would caution you to refrain from trying  \\n to get around this as soon as you can.  \\n Humans aren't really meant  \\n for looking at large tables of data.  \\n We're not optimized for doing that  \\n and it's not an effective use of our time, really.  \\n What you want to do is you want to  \\n use something that is effective  \\n for pulling out data that is interesting.  \\n So my recommendation is when you feel that urge to look  \\n at more columns or more rows,  \\n think, can I use a query or a filter,  \\n to pull out the data that is interesting to me.  \\n Alternatively, can I visualize this,  \\n and might that give me more insight into the data?  \\n You can tell a really good story by visualization  \\n that's a lot stronger than presenting someone  \\n with a table of a bunch of numbers.  \\n So how we're going to do this in pandas  \\n is we're going to use the context manager,  \\n that's the with statement.  \\n And we're going to use this option context.  \\n Context manager is a construct in Python that allows you to  \\n enter into a context and exit out of that.  \\n In this case, when we use option context,  \\n in the indented portion here, we're calling display,  \\n while we are indented, what ever we pass  \\n into option context is going to overwrite the values.  \\n When we un-indent from the context manager  \\n it will revert back to what it was before.  \\n So we're going to change the minimum number  \\n of rows that we view to 30 and we're going to change  \\n the maximum columns to 82.  \\n So you can see that we're now seeing quite a bit more data  \\n and if we scroll over  \\n you can see that we are seeing a lot more columns  \\n than we were previously.  \\n If you're not familiar with this feature in Jupiter,  \\n one of the things you can do is put a question mark  \\n after a method or a function in Jupiter  \\n and it will pull up the documentation.  \\n So off of the style attribute,  \\n there are various methods on there.  \\n One of those is set sticky.  \\n What this allows us to do is set the column  \\n or the index to sticky so when we scroll  \\n we can see the index.  \\n This often comes in very handy.  \\n In this example I'm going to show a lot more data  \\n than I normally show.  \\n And then I'm going to run this query  \\n where I'm showing the missing values of lot frontage  \\n and then I'm going to set  \\n the sticky columns  \\n and the sticky index.  \\n The output looks like this, and if I scroll over,  \\n you can see that that index there  \\n stays on the left hand side.  \\n So this comes in useful when you need to find  \\n out what row is going on.  \\n You'll note that I do have a comment here  \\n at the top,  \\n that columns is broken.  \\n I think this is a pandas two bug, where,  \\n the columns are not being sticky here when I scroll down.  \\n Hopefully they fix that.  \\n Okay, let's look at where garage year built is missing.  \\n So here are the rows where garage year built is missing.  \\n It looks like there's 159 of those rows.  \\n Again, we could scroll through this data  \\n and try and determine what a missing garage  \\n year built means.  \\n I am assuming that it probably means  \\n there is not a garage on here.  \\n Hence, the missing value.  \\n In this case because year built is a year,  \\n putting in a default value of a zero to encode missing  \\n probably wouldn't be the right choice  \\n because if you wanted to look at  \\n the average of the year built that's going to throw that off.  \\n So in this case, garage year built,  \\n leaving those values as missing, might be useful.  \\n However, if you want to do some operations  \\n such as running linear regression on this,  \\n that might be problematic,  \\n because, linear regression does not like to  \\n have missing values in the data.  \\n You might need to find a way to encode that,  \\n such that you can run some of these other tools  \\n with columns that have missing data in them.  \\n I'm going to take that year built column  \\n and I'm going to run summary statistics  \\n on that just to see what's going on there.  \\n And, you can see that the count is demonstrating  \\n that we are missing quite a few values.  \\n I like to look at the minimum value and the maximum value.  \\n Now this actually stands out to me, the maximum value here.  \\n 2 2 0 7, remember this is a year,  \\n and as of this recording we're recording this in 2023.  \\n So this garage is being built a few years into the future.  \\n That's somewhat problematic to me  \\n and that probably indicates that there is  \\n some sort of a typo going on here.  \\n This was probably 2007  \\n but it got fat fingered on data entry.  \\n Let's look at that row where that is missing.  \\n And again, I'm going to use my option context  \\n just to display a few more of those columns.  \\n And, I'm going to put in as my query,  \\n garage year built,  \\n remember I'm going to use back ticks around that  \\n because there are spaces ,  \\n and then greater than 2020.  \\n And maybe I'll scroll over here.  \\n You can see that the year remodel or add  \\n is 2006,  \\n and the garage year built is 2,207.  \\n And the year sold is 2007.  \\n So again, I'm going to assume that this is a typo  \\n and we might want to go in and clean that up.  \\n Now, because I've found this by going through the steps  \\n of manually looking at the data,  \\n looking for missing values and also looking at describes,  \\n this makes me want to look at maybe  \\n some of the other year columns,  \\n maybe some of those are  \\n fat fingered as well.  \\n So, one method that comes in handy is the filter method,  \\n and it allows me to pull off various columns.  \\n So I'm just going to pull up the documentation there.  \\n You can see that this says it subsets the data frame rows  \\n or columns according to a specified index.  \\n So there are various ways to do this  \\n using items like or regex.  \\n We can scroll down here  \\n and see that items means keep labels,  \\n which are in items.  \\n So in this case we're passing in a sequence of labels  \\n and we keep the labels which are in that sequence.  \\n You can pass in like which is a string.  \\n It says keep labels from the axis  \\n for which like in label is equal to true.  \\n That would allow us to do SUBSTRING matching,  \\n but apparently it would only allow us to do one substring,  \\n whereas items allows us to match multiple strings.  \\n Doesn't look like there's an option to match  \\n multiple substrings.  \\n However, we could possibly also use this next option,  \\n which is a regular expression,  \\n and we could probably formulate a regular expression  \\n that would allow us to match multiple substrings as well.  \\n Here's an example of using that.  \\n I say filter like is equal to yr.  \\n You can see there are two columns that have yr in them.  \\n If I want to check which of those has  \\n a column greater than 2023,  \\n which is the current year,  \\n I'm going to do this chain here.  \\n So again, here's our first column here,  \\n and let's throw in this pipe.  \\n What is this pipe doing?  \\n It's saying, with the data frame we see on the screen,  \\n we are going to do an index operation  \\n with this result right here.  \\n So we saw something similar to this before,  \\n we said any along columns is equal to axis.  \\n What that's going to give us is a Boolean array.  \\n It's going to have the same index as the data frame  \\n but it's going to have true false values.  \\n The true values will be where it's greater than 2023.  \\n Let's run that.  \\n And we only see that one column,  \\n that index 2, 2, 6, O, so there aren't any other values  \\n for year sold that are greater than 2023.  \\n What about the other columns?  \\n There are some columns that have year in them.  \\n So this would be my process to filter those out.  \\n I'd probably do something like that,  \\n where I'd say, okay let's take our data frame,  \\n rename our columns that are yr and make those year  \\n and then with those, let's do a filter,  \\n and say, take the columns that have year in them  \\n and then with those, let's add in our pipe here  \\n and see which ones are above our limit.  \\n It looks like, in this case,  \\n it was only that one row that was fat fingered.  \\n In this next chain, I'm showing an example of clipping.  \\n So let's take garage year built.  \\n This is just the series here.  \\n And then I'm going to use the clip method from pandas  \\n to say I want to limit this and have the upper values  \\n just be the year built max.  \\n And if we do that,  \\n that gives us a new series that has been clipped.  \\n And if we look at the value counts from that,  \\n we should see that there aren't any values  \\n that are greater than 2023.  \\n Now, this is sorted by the frequency.  \\n We could come in here and say sort index,  \\n and that looks like the highest value in  \\n the garage year built is 2010.  \\n At this point,  \\n I might want to update my data frame with this information.  \\n Now remember, in our previous lesson,  \\n we looked at replacing strings  \\n that had values that were missing  \\n and converting those to categoricals.  \\n So I'm going to leverage that.  \\n I'm going to make a chain here,  \\n and, I'm going to put that operation  \\n to clean up my strings into my chain  \\n and then I'm going to follow that with this other operation  \\n to clean up the garage year built.  \\n Now let's examine this line right here a little bit.  \\n What's going on with this line?  \\n There's quite a bit of syntax here.  \\n I am using what's called dictionary unpacking.  \\n I have a dictionary literal.  \\n and here's my dictionary.  \\n Why am I using a dictionary?  \\n Why don't I just say garage year built  \\n is equal to the current value of garage year built  \\n that's clipped.  \\n I'm using a dictionary because there are spaces in here.  \\n Because there are spaces,  \\n I can't say garage space year built is equal to that,  \\n so I can't use a parameter to do that,  \\n however, I can stick this into a dictionary  \\n and then use dictionary unpacking and that will update  \\n the column with a non-valid attribute name  \\n using pandas.  \\n And then maybe let's just look at our D types  \\n value counts after doing that.  \\n Looks like there are 39 int columns, or integer columns.  \\n And then it looks like there are categorical columns  \\n but, it doesn't sum these up.  \\n Why doesn't it sum these up?  \\n Because presumably, there are four categorical columns  \\n that have the same entries in them.  \\n There are three that have the same entries  \\n and then all of these other ones down here are one-offs.  \\n In this lesson, we looked at examining our numeric columns.  \\n I showed you how to do summary statistics on them.  \\n I showed you how to look for missing values  \\n and I showed you an example of when I looked  \\n at those summary statistics,  \\n I found something off and then I examined it  \\n a little bit more and then I used that to actually feed back  \\n into a chain to clean up the data.  \\n This is a process that you're going to want to go through  \\n when you load your data.  \\n You want to make sure that your data is clean,  \\n so that when you start doing analysis of it,  \\n the analysis makes sense.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4509082\",\"duration\":350,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Shrinking numbers\",\"fileName\":\"4433355_en_US_01_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"With the skill of cleaning integer data, you can ensure that the data is accurate and consistent, which is critical for making meaningful analyses and predictions. Also, learn how to lower the memory requirements if possible.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14071253,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson, we're going to look  \\n at changing the size of our numbers to save memory.  \\n One thing is I will do is create a function  \\n that will automatically convert integer values  \\n to smaller types.  \\n And then I'm going to show  \\n how to apply this function to our dataframe.  \\n You can actually take this function  \\n and apply it to your data as well.  \\n We're also going to make another function.  \\n I'm going to call that clean_housing that combine  \\n some of the steps that we've been looking at  \\n to clean up our data.  \\n This is something that I recommend you do as well  \\n as you're going through your data, cleaning it up,  \\n throw that into a chain  \\n and then take that chain and put it into a function.  \\n This is going to be a practice that, if you use it,  \\n will make your life a lot easier.  \\n We're going to continue where we left off.  \\n In our last lesson, we saw this assign method  \\n where we converted strings into categoricals  \\n and we also cleaned up our Garage Yr Blt.  \\n Here's our summary statistics of that.  \\n Looks like we haven't lost any fidelity by doing that.  \\n We should be good.  \\n We've actually cleaned up that Garage Yr Blt.  \\n Now, remember I said  \\n that I like to go through the maximum values here,  \\n and for example, the Overall Qual column.  \\n You can see that right here has minimum values of one  \\n and maximum values of 10.  \\n Remember that Pandas is using a 64-bit integer  \\n to store that information which is actually a waste.  \\n So we could use a different data type  \\n and save memory without losing any information.  \\n Let me show you how we might want to do that.  \\n In this cell, I'm going to loop through NumPy,  \\n and NumPy has a function in it called, iinfo, integer info.  \\n I'm just going to loop through some NumPy types.  \\n Uint8 stands for an 8-bit unsigned integer.  \\n So if we say np.iinfo(uint8), it tells us  \\n that the minimum value for that is zero  \\n and the maximum value is 255.  \\n This is a type that we could use for that overall quality  \\n because zero and 10 certainly lie within this range.  \\n Now, sometimes you have positive numbers  \\n that go a little bit bigger.  \\n You might need to use 16-bit or 32-bit  \\n before you go up to 64.  \\n One thing to be aware of, these are NumPy types.  \\n These are the types that were used in Pandas 1.  \\n In Pandas 2, you can optionally use that PyArrow backend.  \\n However, these type ranges actually work  \\n for the PyArrow types as well.  \\n So in this cell, I've created a function.  \\n Again, you can use this function  \\n and leverage it for PyArrow-backed dataframes.  \\n What it's going to do is it's going to accept a dataframe  \\n as input, and then it's making a mapping dictionary.  \\n It's going to loop over all of the columns  \\n that are PyArrow 64-bit integers.  \\n And then it's going to look at the minimum  \\n and maximum values for those.  \\n And if the minimum is less than zero,  \\n it's going to continue.  \\n This is only dealing with unsigned integers.  \\n Otherwise, it's going to look at the maximum values  \\n and determine the appropriate type for that.  \\n Once you have this function, all you have to do is use it  \\n in combination with the pipe method.  \\n So if you're not familiar with that pipe method  \\n in Pandas, this is a super handy method,  \\n it allows you to do whatever you want to a dataframe.  \\n All you have to do is pass in a function into it.  \\n In this case, we're passing in the shrink_ints function.  \\n Your function should accept a dataframe  \\n if you're using pipe with a dataframe.  \\n If you're using pipe with a series,  \\n a series also has a pipe method,  \\n your function needs to accept a series.  \\n And then the function can return whatever it wants.  \\n In this case, we're actually returning a dataframe  \\n using that mapping to change the types there.  \\n And with the output of that, we can actually run  \\n a describe on that after we've done that.  \\n So let's run this and test this out.  \\n You'll note that I've still included  \\n what we've done previously here.  \\n So I want to impress upon you  \\n that when you see a chain that's a little bit longer,  \\n I don't create these chains from scratch.  \\n I am building them up as I'm going through my process.  \\n And so let's look at the output from that.  \\n The output looks good.  \\n Looks like we didn't lose any information by doing that.  \\n In this next cell, let's see how much memory this is using.  \\n And it looks like this is using 360 kilobytes of memory.  \\n Let's compare that with our default amount of memory.  \\n So shrinking our integers in combination  \\n with changing our types to categoricals has saved us  \\n five times the amount of memory by doing that.  \\n The next thing I would want to do after doing this  \\n is convert this into a function.  \\n I'm keeping my shrink_ints function that I defined above,  \\n but I'm also making a function called clean_housing.  \\n This is my chain.  \\n I'm just moving this chain into a function.  \\n Let's run the function and make sure that it works.  \\n I'm just going to inspect the dtypes after doing that.  \\n It looks like this is working successfully.  \\n In this lesson, I presented a function that you can use  \\n to shrink PyArrow types without losing data.  \\n We also looked at some of the benefits of that  \\n such as memory usage going down drastically by doing this.  \\n This is one of the ways that you can load  \\n much larger data sets and analyze them  \\n by using the correct types.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4503101\",\"duration\":97,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Clean Ames\",\"fileName\":\"4433355_en_US_01_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3154431,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] We're at our first challenge.  \\n These challenges are a great way for you to practice  \\n and I highly encourage you, in addition to following  \\n along with me in the code space, to try this challenge out.  \\n The science tells us that if you do the challenge,  \\n you're going to use a different portion of your brain  \\n and you will actually remember the content better  \\n if you do the challenge  \\n in addition to following along with the content.  \\n So I highly recommend that you do this challenge  \\n before proceeding to the next video.  \\n Let me explain what the challenge looks like.  \\n I want you to create a cell containing all  \\n of the imports necessary  \\n to run this notebook up to the current state,  \\n and then I want you to create a cell with the clean housing  \\n and the shrink functions that I showed you up above.  \\n I want you to also to add code to load the raw data  \\n and to create a housing variable from calling clean housing.  \\n Then I want you to take these cells that you just created  \\n and move them to the top of the notebook,  \\n restart your notebook, and make sure that these cells work.  \\n What this is going to do is give you a process  \\n that you can use in your own notebooks.  \\n I recommend following this process,  \\n once you've made a chain that is useful,  \\n put it in a function  \\n and then move that to the top of your notebook.  \\n This will allow you to come back  \\n to your notebook and quickly pick up where you left off.  \\n You don't have to scroll through a bunch  \\n of cells to figure out where you were.  \\n You can just have that at the very top,  \\n and then you can just keep working on your data analysis.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4504092\",\"duration\":263,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Clean Ames\",\"fileName\":\"4433355_en_US_01_06_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14240562,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] In this lesson, I'm going to show you  \\n how to do the first challenge.  \\n So here is my cell.  \\n I'm going to get my imports.  \\n My imports were up at the top already,  \\n so these are my imports right here.  \\n We used NumPy to inspect the types  \\n but we didn't really use it in processing our data  \\n but I will keep the numpy import there as well.  \\n That will allow me, if I'm using this notebook,  \\n to not have to worry about importing NumPy.  \\n Okay, the next thing that it wants me to do  \\n is use the clean_housing and shrink_ints function.  \\n So I'll just come up here and I'm going to copy these  \\n and we'll paste those in here as well.  \\n I'll just put them in the same cell.  \\n I think that's fine.  \\n And then the next thing it says is add code  \\n to load the raw data  \\n and create a housing variable by calling clean_housing.  \\n So I'm going to do that,  \\n and that's this code right here.  \\n And I'll put that at the bottom here.  \\n Maybe I'll just call this raw.  \\n Let me explain why I like to use the raw data  \\n and make a chain.  \\n Invariably, what I've found in my experience working  \\n and consulting, it's that when I make a report  \\n or I make a model, someone asks, \\\"What's going on?  \\n Why is it doing this?\\\"  \\n And if I have the raw data and a chain,  \\n it's really easy to trace back through that  \\n and understand what's going on.  \\n So I'm going to keep that raw data there.  \\n And this exercise asked me  \\n to create a variable called housing.  \\n So I'm going to make a variable called housing  \\n and say that is equal to clean_housing  \\n and passing in raw into it.  \\n At this point, I will have the original data  \\n and my cleaned up data if I need to compare those two.  \\n The next thing it asks me to do is move this cell  \\n to the top of the notebook.  \\n So I'm just going to click on the side here.  \\n I'm going to keep this cell here, but I'm going to also click  \\n on the side so I'm not in edit mode in my notebook  \\n and hit C to copy this.  \\n And then I'm going to scroll up  \\n to the very top of the notebook here.  \\n I'll actually put it below this heading section down here  \\n and hit V to paste it right here.  \\n So the next time I came to this notebook,  \\n I wouldn't have to scroll through all  \\n of those cells to find out what's going on here.  \\n It's just for me right at the top,  \\n the code to get my data so I can be off and running.  \\n And finally, it says restart the notebook  \\n and make sure that those cells work.  \\n You'll notice that as I've been going through my notebook,  \\n I've been trying out my code, commenting it out,  \\n going through it, making sure that it works  \\n and you will want to do that as you're using your code.  \\n I'm going to hit this restart button to restart my kernel.  \\n It's going to pop up this warning.  \\n I'll hit restart.  \\n Note that when you do restart,  \\n you will lose all variables that you've created.  \\n I'm okay with that in this case.  \\n And then I'm going to run the cell  \\n and make sure that it works.  \\n Okay, it did not complain.  \\n I'll just make a new cell below this and look at housing.  \\n And it looks like it worked.  \\n In this lesson, I've given you a nice hint  \\n for how to make your notebooks a lot easier to use.  \\n And that is make a chain that cleans up your code,  \\n stick that into a function, and then pull that up  \\n to the top of your notebook  \\n so that when you come back to your notebook,  \\n you can easily start going and not have to scroll through  \\n and figure out what's going on with your code.  \\n \\n\\n\"}],\"name\":\"1. Loading and Cleaning Data\",\"size\":119595368,\"urn\":\"urn:li:learningContentChapter:4503102\"},{\"duration\":3034,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4504091\",\"duration\":242,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Categorical exploration\",\"fileName\":\"4433355_en_US_02_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"With the skill of calculating and interpreting summary statistics, you can gain valuable insights into the central tendency, variability, and distribution of your data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7604335,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Narrator] In this lesson, we're going to look at exploring  \\n our categories a little bit more.  \\n We're going to show the unique values of that  \\n but we're also going to visualize what's going on with that.  \\n Remember, visualization is a powerful tool  \\n you can use to tell a story very quickly  \\n and we're going to show how to do that for categoricals.  \\n I've got my cell that loads my data.  \\n So I've got the raw data and I've got the housing data.  \\n Remember, we can use describe to get summary statistics.  \\n By default, this is going to give us  \\n numerical summary statistics.  \\n It's not going to include the categoricals,  \\n or strings by default.  \\n Let's pull off a categorical.  \\n I'm going to use the zoning value.  \\n One of the things that's interesting about this  \\n is if you look at the dtype at the bottom here,  \\n it says that it is category  \\n and then here are the unique types for that category.  \\n Generally, when I have a category or a string column,  \\n I like to use the value counts method.  \\n We'll tack that on here.  \\n And this gives us a series.  \\n Pandas uses series and data frames all over the place  \\n so it's good to get comfortable with those  \\n and also understand that these operations that we're doing  \\n with series and data frames are like tools  \\n in your tool belt and once you understand  \\n these basic operations, you can start leveraging them,  \\n combining them with other ones.  \\n So one of the things I might want to do with this  \\n is visualize what's going on here.  \\n This is a nice summary.  \\n We can see that the RL Zoning has 2,273 entries  \\n but maybe want to visualize that  \\n and let's see how we can do that.  \\n The common visualization that we would use for this  \\n is a bar plot.  \\n Now look how easy it is to go from this value count  \\n to a plot.  \\n All I need to do is tack on the next operation in our chain  \\n which is .plot.bar.  \\n Let me explain how bar plots work in Pandas.  \\n Once you understand how they work,  \\n they're very easy to leverage.  \\n What a bar plot is going to do is it's going to take  \\n the index and put the index in the X axis.  \\n Note that the index here is not numeric.  \\n It is the unique entries from the MS Zoning column.  \\n That's perfectly fine.  \\n In fact, Pandas converts any index when you do a bar plot  \\n into a categorical or a string in the plot,  \\n which sometimes comes to bite us  \\n if you have dates in the index and you do a bar plot  \\n of that, it actually converts those dates into categoricals.  \\n For this case, it's fine.  \\n So the index is going to go into the X axis,  \\n and then in this case, we only have a single series.  \\n If you have a data frame, each column will be a bar  \\n and those bars will be located above the index.  \\n Let's run this and see what happens.  \\n This allows us to clearly see the RL  \\n is the most common entry for MS zoning, followed by RM  \\n and telling off very quickly.  \\n One of the things I don't like about this bar plot  \\n is that the labels are rotated 90 degrees.  \\n In fact, a common thing to do with a categorical count plot  \\n like this is to convert it to a horizontal bar plot,  \\n and this again, is easy in Pandas.  \\n All we need to do is change the bar  \\n to barh, standing for bar horizontal.  \\n And we get something that looks like this.  \\n Again, telling the same story  \\n but a little bit easier for us to read.  \\n We don't have to tilt our head as much.  \\n In this lesson,  \\n I showed you how to summarize categorical columns.  \\n Again, value counts is going to be your friend  \\n and then once you know how bar plots work,  \\n it's easy to go from value counts to making a bar plot.  \\n And then generally you're going to want to make  \\n a horizontal bar plot.  \\n That is easy as well by using .barh instead of .bar.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4503100\",\"duration\":262,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Histograms and distributions\",\"fileName\":\"4433355_en_US_02_02_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"The goal of this video is to learn how to create and interpret histograms, which are graphical representations of the distribution of numerical data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7903762,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson, we're going to look  \\n at histograms and distributions for numerical columns.  \\n Again, we can start off  \\n with the describe method to get summary statistics  \\n but oftentimes, we want to visualize this as well.  \\n I'll show you some ways to visualize these.  \\n Let's take the SalePrice column.  \\n Note that we can run describe on a single column  \\n or a series in addition to a data frame.  \\n When we run describe on a series,  \\n we get back a series.  \\n Again, in the index of this series,  \\n we have the descriptions  \\n of the statistical summaries that are coming back  \\n and the values,  \\n we have the corresponding values for each statistic.  \\n I'm just going to quickly go through these.  \\n Look at the minimum value.  \\n It looks like there's a house that sold for $12,000.  \\n That seems pretty cheap.  \\n I might want to check out that row  \\n or check out those low values.  \\n Also might want to go on that high side.  \\n We have a house that sold for 755,000.  \\n Make sure that that looks about right.  \\n Also, I could look at the mean and the median.  \\n Those look pretty close to each other and remember,  \\n our count is not the count of rows  \\n but the count of non-missing rows, but it looks  \\n like this is, in this case, the same as the number of rows.  \\n We don't have any SalePrice values that are missing.  \\n One of the ways to visualize a numeric column  \\n is to do a histogram, and again,  \\n this is very easy in pandas.  \\n We just say .hist, and here's the histogram.  \\n You can see that this is somewhat skewed.  \\n It rises quickly to some value  \\n around $150,000 and then tails off  \\n and there's a long tail going out to around $700,000.  \\n We know that it goes up to $700,000 at least.  \\n If it didn't go up to that value,  \\n it wouldn't be on this chart.  \\n However, there aren't very many entries at that end.  \\n Now, you need to be careful with histograms  \\n because you can tell different stories with them.  \\n By default, pandas is going to give you 10 bins.  \\n People often ask, \\\"Is 10 bins the correct number of bins?\\\"  \\n I would say the answer is it depends, but it might not be.  \\n This is one of those things that people  \\n would write dissertations about the correct number of bins.  \\n I'm going to show you how you can change this.  \\n We can say bins is equal to 30,  \\n and we can get a little bit finer granularity in there.  \\n We could also come down here and say bins  \\n is equal to three.  \\n This probably isn't a story that we want to tell  \\n using bins is equal to three.  \\n From inspecting this data, it looks like somewhere  \\n around 150 is the most common value or the mode.  \\n When we do describe, we don't get a mode reported on there,  \\n but we did get a median and a mean,  \\n which were both around that value as well.  \\n You can see that for whatever reason,  \\n there's a little bump here around 550  \\n and there's another bump here around 600.  \\n Those are kind of interesting to me.  \\n They probably indicate that these houses sold  \\n at complete numbers like 600,000  \\n or 550 rather than some value in between there.  \\n It's interesting also that we're not seeing that happen  \\n at other values where we're not seeing like big spikes  \\n at 300,000 or 200,000 or 100,000, and it also doesn't look  \\n like we're seeing bumps at the 50,000 level as well.  \\n We can bump up the number of bins and look  \\n at the finer granularity.  \\n That might yield useful information.  \\n It might just be noise.  \\n In this lesson, we took the SalePrice column  \\n and we did summary statistics on that.  \\n We also showed how to look at a very common visualization  \\n of that, which is the histogram.  \\n We showed how to change the number  \\n of bins in a histogram to tell a different story as well.  \\n Leveraging a histogram is a great way to get a feel  \\n for how the data is distributed.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4502195\",\"duration\":412,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Outliers and Z-scores\",\"fileName\":\"4433355_en_US_02_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover how you can objectively identify data points that are significantly different from the majority of the data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":18691501,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson, we are going to look  \\n at how we might find outliers  \\n or values that are at the extremes  \\n and I'll show two different ways to do that.  \\n I'll show one using our classic Z score and another one  \\n by using the inner quartile range, and we'll look  \\n at how we can use the pandas functionality to do that  \\n and make functions that you can use  \\n and leverage in your own code if you need  \\n to calculate values that tend to be on the extremes.  \\n So here is a function right here, calc Z  \\n and you'll notice that the input to calc Z,  \\n the first parameter is a data frame  \\n so a function that has the first parameter is a data frame  \\n is a candidate to use with the pipe method.  \\n This lets us leverage this in a chain.  \\n Let's run this and see what happens when we do this  \\n and I'll walk through the code.  \\n Okay, so what this is returning is a series  \\n and this is the Z-score of the sell price column.  \\n Let's just look at what these lines are doing.  \\n In this case, you'll note, even though I like chaining,  \\n I'm not using chaining all over the place.  \\n This is a case where I'm not using chaining,  \\n I'm calculating the mean  \\n of this column that I'm passing in here.  \\n So the DF that is coming in is whatever is passed  \\n in from the chain.  \\n In this case, this is the original housing,  \\n just coming in as the DF here  \\n and sale price is coming in as the column value.  \\n So we first calculate the mean of sale price,  \\n then we calculate the standard deviation of sale price  \\n and then the Z score is calculated  \\n by taking that column, subtracting the mean,  \\n and dividing by the standard deviation.  \\n We can leverage pandas to make this a vectorized operation  \\n and to make it work relatively easily.  \\n The Z-score gives us a quantification  \\n how many standard deviations we are away from the mean  \\n and generally an outlier is a value  \\n that has an absolute magnitude above three  \\n for data that has a normal distribution.  \\n One of the nice things about making this  \\n as a function is we can also use it in assign.  \\n Let me show you how this is going to work here.  \\n I've got this little chain.  \\n I'm going to comment out this  \\n and if you look at the column at the end here,  \\n you can see that there is now a Z-score column.  \\n This Z-score column is the Z-score of the sale price.  \\n If I want to find outliers,  \\n I can combine this with the query method.  \\n So now that I have a Z-score column,  \\n I can use this query saying,  \\n take the absolute value of the Z-score column  \\n and find out where that is greater than  \\n or equal to three.  \\n You can see the values that we're looking  \\n at here on the screen, none of those are equal to three.  \\n Let's run this and see what happens.  \\n You can see that this row 15 and this row 44,  \\n if we were to scroll over, presumably would have a Z score  \\n with an absolute magnitude greater  \\n than or equal to three, and indeed they do.  \\n Let's look at these values.  \\n These values all look like high-end houses  \\n or houses that are very expensive.  \\n These values all look like they're greater than three.  \\n It's possible that there are values less than three.  \\n Let's just quickly check and see if we can get those.  \\n I'm going to copy this  \\n and I'm going to just change it a little bit and say,  \\n where are we less than or equal to negative three?  \\n So it looks like there aren't any values  \\n that are outliers on the low end.  \\n Again, you need to remember that this is assuming  \\n that we have a normal distribution of the sale price.  \\n I'll show later on in the lesson how to calculate  \\n if that distribution is normal.  \\n Another mechanism for determining outliers is  \\n to calculate the inner quartile range  \\n and then values that are outside  \\n of that are classified as outliers by some statisticians.  \\n Here I'm making a function called calc IQR outlier.  \\n Again, this takes a data frame and a column.  \\n In this case, what it's doing is it's pulling  \\n off the series from that column  \\n and then we're saying calculate the 75th percent quantile  \\n and subtract the 25th percent quantile from that.  \\n That will give us a series of the inner quartile range.  \\n We can also calculate the median  \\n which is the 50th percentile, and the definition  \\n for an outlier using the inner quartile range is  \\n to take the median  \\n and subtract three times the inner quartile  \\n or the median and add three times that.  \\n So I'm making a variable called small mask and large mask  \\n and then I'm using the pipe to combine those together.  \\n What this returns is a boolean array,  \\n let me just show you that.  \\n So boolean array is a series that has true false values.  \\n Where these are true means that either this is true,  \\n it's low or it's high, and when we throw that  \\n into our index operation here,  \\n this gives us back the rows where this is true.  \\n And you can see  \\n by this calculation that 15 and 44 are both outliers.  \\n Let's show how to throw this into a column.  \\n In this example,  \\n I'm going to make a new column called IQR outlier  \\n and that is going to be the result of calling our function,  \\n passing in our data frame  \\n and indicating the sell price column  \\n as the column that we want to figure  \\n out whether those values are outliers or not.  \\n Now let's look at that result here.  \\n I'll scroll down and scroll over.  \\n You see this is true false value,  \\n so this is our boolean array.  \\n What I'm doing in this next part  \\n of the chain is just saying, filter out the roads  \\n where this is true in the boolean array  \\n and all of these IQR outliers should be true in this case.  \\n In this lesson, we made functions that allow us  \\n to calculate the outlier in two different ways.  \\n One by using the Z-score and another  \\n by calculating the inner quartile range  \\n and using that to determine whether values are outliers.  \\n These are functions that you can leverage in your own code.  \\n Feel free to take them  \\n and use them for outlier calculations.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4503099\",\"duration\":422,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Correlations\",\"fileName\":\"4433355_en_US_02_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to identify relationships between variables, determine the strength and direction of those relationships, and make informed decisions based on the data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":24607809,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson,  \\n we're going to look at calculating correlations.  \\n We'll look at the Pearson and Spearman correlation.  \\n I'll also show you how to make a heat map  \\n and give you some tips for formatting that,  \\n so you can quickly see where your correlations are.  \\n This is one  \\n of those calculations that's really easy to do in Pandas.  \\n All you have to do is say .corr.  \\n Now, as I said that it was easy to do,  \\n it looked like it failed.  \\n Turns out that in this case, this is a Pandas 2 update.  \\n Pandas 2 is a little bit more strict  \\n about running operations on columns that aren't numerics.  \\n Let's scroll down to the bottom  \\n and you can see that the error is,  \\n that it says it \\\"could not convert string to float: RL.\\\"  \\n That's a little hard to understand  \\n but basically what this is saying is  \\n that we passed in string columns  \\n and it didn't like doing the calculation on those.  \\n To get around this,  \\n what we're going to do is specify this parameter,  \\n numeric_only=True.  \\n What this returns is a Dataframe.  \\n In the index is all the numeric columns  \\n and in the columns is also all of the numeric columns.  \\n So we see the correlation between each numeric column  \\n and all the other numeric columns.  \\n This is using the Pearson correlation coefficient.  \\n That is a value between -1 and 1.  \\n The closer those values are to 1,  \\n the stronger the positive correlation,  \\n meaning as one value goes up, the other value goes up.  \\n The closer that value is to -1  \\n the more negative the correlation is  \\n meaning as one value goes up, the other value goes down.  \\n If those values are around zero,  \\n that means that there is no correlation.  \\n As one value goes up, the other value might go up or down.  \\n They're not related.  \\n This is another one  \\n of those things that humans aren't optimized for looking at.  \\n I could give you this big table of data and ask you  \\n to find the biggest correlation and the smallest correlation  \\n and it would take a bit of time for you to do that.  \\n I'm going to give you some hints on how to get around that.  \\n Also, note that this is  \\n calculating the Pearson correlation coefficient,  \\n which assumes that there's a linear relationship  \\n between two values,  \\n ie. if you were to do a scatterplot of this,  \\n you should see a line.  \\n It's not always the case that there's linear relationships.  \\n Sometimes you have non-linear relationships.  \\n And you might even have a monotonic relationship  \\n meaning when one value goes up, the other value might go up,  \\n but it might be in a shape that is not a line.  \\n It might be like a logistic curve  \\n where it looks like a stretched out S.  \\n In that case,  \\n we might want to use the Spearman correlation coefficient.  \\n To do that, we specify the parameter, method='spearman'.  \\n And again, I'm going to say, numeric_only=True.  \\n And I'm also going to tack on this background gradient.  \\n Remember, we want to be able  \\n to see what's going on here easily,  \\n and one of the ways we can do that  \\n is leveraging visualization and color.  \\n So by default, when you do the background gradient,  \\n it puts on this blue background gradient  \\n that immediately draws our eyes to the diagonal.  \\n And you can see  \\n that along the diagonal is a correlation of one.  \\n If you look at the diagonal, those are actually  \\n the correlations of one value versus the same value,  \\n meaning that as that value goes up, it goes up.  \\n Which in itself is not a particularly interesting insight  \\n and actually we kind of want to ignore the diagonal values  \\n when we're looking at this,  \\n but you could look off diagonal  \\n and see where the largest correlation is.  \\n Remember, this is a Spearman correlation coefficient  \\n so it's not necessarily  \\n there would be a straight line between those,  \\n but as one value's going up the other value's going up  \\n this makes it relatively easy to see  \\n that, for example, right here or right here,  \\n these are actually the same correlation.  \\n Year remodel or added to against year built.  \\n So as the year built goes up,  \\n also the year that it was remodeled goes up,  \\n and we can scroll through here  \\n to see if we see any others pop up.  \\n Here's one that has a particularly high correlation,  \\n garage year built,  \\n and year built.  \\n The default coloring used by background gradient,  \\n this going from white to blue,  \\n is okay for looking at positive correlations,  \\n but it's actually not great  \\n for looking at negative correlations.  \\n Why is that?  \\n This color map starts at a white value  \\n and it's going to pin the white value, so to speak,  \\n at the smallest negative value.  \\n It looks like, for example, here  \\n it looks like pretty white and it's -0.4.  \\n We actually wouldn't want the smallest value to be -0.4.  \\n We would want it to be -1.  \\n And we actually don't want to use this color map  \\n that goes from white to blue.  \\n We'd want to use a different color map.  \\n So let's see if we can change this  \\n and use a better color map.  \\n I'm going to use this RdBu color map here.  \\n This is a red to blue color map.  \\n It's actually what's called a diverging color map.  \\n It goes from red through white to blue.  \\n And now we can look  \\n at the bluest things and the reddest things.  \\n The bluest things are the most positive  \\n and the reddest are the most negative.  \\n Again, we kind of want to ignore the diagonal.  \\n The diagonal by definition is going to be 1.  \\n So you'd want to look at the bluest off diagonal values  \\n and the reddest off diagonal values.  \\n Again, if you look at these,  \\n the reddest values look to be around -0.4,  \\n which is not what we want.  \\n So I'm going to give you one more hint.  \\n When you use a background gradient  \\n with a correlation heat map,  \\n you're going to want to pin the negative value.  \\n Let me show you how to do that.  \\n You specify vmin as -1.  \\n We can also put in vmax as 1,  \\n although by definition the diagonal will have a value of 1.  \\n And this tells a completely different story.  \\n If we look at this, you can see  \\n that those values around zero are indeed white  \\n which is what we want to see.  \\n Those are the values that don't have a correlation.  \\n We kind of want to ignore those.  \\n In the other examples, those were distracting us.  \\n And we can look  \\n for the reddest values now and the bluest values.  \\n We can still see that that -0.42  \\n looks like it is probably one of the reddest values.  \\n Let's just scroll through this and see.  \\n Here's another one that looks like it's pretty red,  \\n and here's one that is actually very red.  \\n This is an example  \\n of leveraging the power of color and visualization  \\n to help us read a table more effectively.  \\n In this lesson, we looked at correlations.  \\n We saw how we can easily calculate these in Pandas,  \\n and we saw how we can leverage color  \\n to help us understand these correlations  \\n and pull out the interesting values rapidly.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4502194\",\"duration\":457,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Scatter plots\",\"fileName\":\"4433355_en_US_02_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to create and interpret scatter plots, which are graphical representations of the relationship between two variables.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15216488,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson,  \\n we're going to look at scatter plots.  \\n Scatter plots are an excellent way  \\n to take two numeric columns  \\n and look at the relationship between those.  \\n I'm also going to give you some tips  \\n on how to make more effective scatter plots.  \\n Again, I think the key to understanding scatter plots  \\n is understanding the interface.  \\n The interface here is a little bit different  \\n than what we saw with bar plots or histograms.  \\n In the case of scatter plots, we are going to call .plot.  \\n And .plot has various methods for plotting on it.  \\n One of them is scatter,  \\n and the parameters that we specify for scatter  \\n include the column that we want in the X-axis  \\n and the column that we want in the Y-axis.  \\n Let's run that, and here's our plot.  \\n So this is the year built versus the overall condition.  \\n We want to see what's going on between these two.  \\n I'm going to calculate the correlation between those  \\n so you get a feel for what this looks like.  \\n We'll take housing.  \\n And I'm going to say pull off year built,  \\n and do a correlation with that with housing,  \\n and then look at overall condition.  \\n So that has a -0.36 correlation, meaning  \\n as one value goes up, the other value goes down slightly.  \\n We can also change this  \\n to a Spearman correlation coefficient  \\n and see if that changes that at all,  \\n and it actually makes it slightly more negative.  \\n So just from looking at this, I wouldn't necessarily say  \\n that as one value goes up, the other value goes down.  \\n So let's see if we can tease apart this plot a little bit.  \\n Let me tell you what I'm seeing when I look at this.  \\n I'm seeing a bunch of layers.  \\n What those layers indicate to me is  \\n that our values have been encoded at a certain granularity.  \\n In this case, this is the overall condition  \\n and these are whole numbers between one and nine.  \\n You can kind of consider these to be categorical in nature  \\n though they're encoded as numbers.  \\n I'm also seeing a bunch of dark values,  \\n and when I see dark values in scatter plots,  \\n that gives me pause.  \\n I'll generally want to use transparency  \\n to see what's going on there.  \\n So let's apply this alpha transparency.  \\n I'm going to pass in transparency of 0.1,  \\n and this tells a different story, I believe,  \\n than what we saw above.  \\n This is telling me that we have a lot of values at five  \\n and some values at six and seven.  \\n We could also look at a histogram of overall condition  \\n to see where those distributions lie,  \\n but we're kind of seeing that here.  \\n Again, compare that to what we saw here.  \\n I would not say that we had the most values  \\n at five just by looking at this,  \\n but when we change that alpha, we're seeing that.  \\n Let me give you a hint on how I like to set alpha.  \\n Let's maybe put alpha at 0.5,  \\n and when I see that there are a bunch of dark values,  \\n and there's still a bunch of dark values,  \\n I'm not seeing a transition around those.  \\n I kind of want to keep going down.  \\n So maybe I'll go down to 0.3.  \\n I can see it, five and six and seven.  \\n There are still a bunch of dark values,  \\n and let's put it down to 0.1.  \\n There I'm seeing that six and seven start to get faded out,  \\n so I think that's okay.  \\n I'm also seeing  \\n So I think this is an okay value for alpha.  \\n Now I still am seeing those flat layers.  \\n How would I deal with that?  \\n I like to use jittering to deal with that.  \\n What is jittering?  \\n Jittering is shifting those values up by a random amount  \\n and I'm going to give you the code to do that right here.  \\n So this example, I'm taking my housing Dataframe  \\n and I'm overriding the overall condition column  \\n by taking the value of the overall condition  \\n and using NumPy to randomly add a value  \\n between zero and 0.8.  \\n And then I'm subtracting 0.4 to that, so I'm centering that.  \\n So this should give us a value  \\n between -0.4 and 0.4 on the positive side, but random.  \\n Now I'm going to plot this new overall condition.  \\n And this is what that looks like.  \\n You might ask why did I choose the value of 0.8 and 0.4?  \\n Well, if you look at the overall condition  \\n the overall condition is a whole numeric value,  \\n and I kind of want to see some segmentation  \\n between the five values and the six values.  \\n So if I would put that value instead of 0.8,  \\n put it up to one, then it would stretch all the way  \\n between one value and the next.  \\n So that leaves a little bit of breathing room between those,  \\n and then I subtract four so that it's centered at the value.  \\n Now, if you look at this,  \\n I think this tells a completely different story  \\n than what we're seeing above.  \\n This is telling me that we have a lot of values  \\n around the year 2000 in the condition five.  \\n Again, let's scroll back up to our original scatter plot  \\n and I don't know that you would've been able to tell that  \\n from looking at the original scatter plot,  \\n but by using some of these tricks  \\n such as adjusting the alpha and jittering,  \\n we are able to have some pretty big insight  \\n into what's going on here.  \\n This also helps us understand  \\n that correlation number that we looked at before.  \\n It looks like if you go from the left to the right  \\n the values are going slightly down, and that's easier to see  \\n after we've applied some of these tricks to our plot.  \\n Again, I'm going to go back  \\n to these traditional software engineering things that I do.  \\n If I want to use jitter in other places,  \\n I'm going to make it into a function  \\n so I can take advantage of it.  \\n In this case,  \\n I'm going to pass in a data frame as the first parameter,  \\n a column that I want to jitter, and the amount.  \\n I'm going to default the amount to 0.5,  \\n but we can bump that up to 0.8,  \\n and then I'll just test that function out here.  \\n Instead of using my code that I've commented out right here,  \\n I'm going to use my jitter function,  \\n and let's see if that works.  \\n Indeed, it looks like it does.  \\n One more way to visualize this  \\n that you get for free in Pandas is to use a hex bin plot.  \\n You can see that this plot's little six-sided figures  \\n and then colors those based  \\n on the number of values that fall into those.  \\n In this case, you can see around the year 2000  \\n the overall condition of five has a concentration of values.  \\n I personally don't like this plot as much  \\n as the scatter plot that I did above,  \\n but this gives you similar insights with very little code.  \\n In this lesson, we looked at creating scatter plots.  \\n Remember, scatter plots are a mechanism  \\n to look at the relationship between two numeric columns.  \\n We also looked at the correlation  \\n which is a quantification of that relationship  \\n and saw that we can explore that quantification  \\n by using a scatter plot,  \\n and adding some tricks on top of those.  \\n So the tricks that I like to use  \\n if I'm seeing that there's just a lot of dark spots,  \\n I'm going to lower the alpha  \\n until I start to see a transition between those spots.  \\n And then if I see things that are stacked  \\n up on top of each other in either columns or rows,  \\n I'm going to use jittering to spread those apart.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4506093\",\"duration\":642,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualizing categorical and numerical values\",\"fileName\":\"4433355_en_US_02_06_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to calculate and interpret numerical summaries of data based on categorical variables, such as means or medians of groups.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":24933557,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In our previous lesson, we looked  \\n at comparing two numeric columns to each other,  \\n what happens if you have a numeric column  \\n and a categorical column?  \\n Let's look at how we can visualize and understand those.  \\n We're going to take two columns.  \\n We're going to take year built and overall condition.  \\n Now, both of these are numeric, but I'm going to say  \\n that both of them are also somewhat categorical as well.  \\n Some columns can be both numeric and categorical.  \\n You can think of a year  \\n as a numeric sequence that increases, but you can also think  \\n of events that happened in a year that would be categorical.  \\n Similar with overall condition.  \\n Overall condition is a number that represents the condition  \\n of a house.  \\n You could also think of the condition,  \\n one, being a house that is  \\n in very poor condition and a house that has a condition  \\n of nine to be very good condition.  \\n In this example, let's assume  \\n that year built is a categorical value  \\n and overall condition is numeric.  \\n How could we look at the relationship between those two?  \\n One of the things that we can do is we can use  \\n what's called a box plot.  \\n I'm going to say in the X axis, use the year built column  \\n and in the Y axis use the overall condition column.  \\n We get something that looks like this.  \\n It's a little bit unclear what's going on here.  \\n It says overall condition in the bottom  \\n and then on the left it looks  \\n like we have the condition values from zero to nine.  \\n What is happening?  \\n This is basically taking all of the overall condition values  \\n and plotting a box plot for them.  \\n This isn't doing what we wanted to do, which was look  \\n at the conditions over the different categories of years.  \\n Let's see if we can figure that out here.  \\n I'm going to use some pandas code to manipulate my data.  \\n First of all, let's use what's called a pivot.  \\n You can see that I'm putting the year built  \\n into the column along the top, we have all the year built.  \\n I'm putting the overall condition column into the values  \\n but this is giving us as a data frame  \\n with the same number of rows as the original data  \\n but we see a bunch of missing values here.  \\n That's because this is a sparse data frame.  \\n What I want to do is I want to take those values  \\n for each year and basically push them up along the top  \\n and that's what this next line is going to do.  \\n I'm going to say, let's apply a function here.  \\n The function is going to take the values  \\n that are not missing and keep those in each column  \\n and then I'm going to reset the index and that'll gimme  \\n for every column, the values that are in there.  \\n That's what that looks like.  \\n You can see that all of the values are along the top.  \\n We still have missing values  \\n but those are just padding the bottom  \\n for the values that appear to be the longest,  \\n it looks like 2005 at least has 141 entries.  \\n In this case, when we call .plot.box,  \\n it is going to take each column  \\n and it's going to stick each column in the index  \\n and for each column it's going to take the values  \\n in that column and do a box plot.  \\n In my opinion, the key  \\n to plotting in pandas is understanding how pandas moves  \\n from a data frame into a plot and it's not always the same.  \\n Here's the output of that.  \\n I don't think this plot is particularly useful.  \\n There's too much information going on here.  \\n It's hard to even read the X axis.  \\n What would I do to this?  \\n I would limit it to a certain number of years  \\n or I would bin years maybe into decades.  \\n Here's a chain.  \\n Let's walk through this chain and see what's going on here.  \\n Here's our pivot, which looks like what we had before.  \\n I'm going to do my apply, which is going to again,  \\n move the values up to the top, and instead  \\n of having almost 3000 rows, now we have 142 rows  \\n and now I'm going to use loc to filter out rows and columns.  \\n Loc allows us to specify rows  \\n that we want to pull out and columns that we want to pull out.  \\n In this case, this colon here is the row selector.  \\n If you just put a bare colon in the row selector,  \\n it is going to take all of the rows.  \\n So then we have a comma,  \\n that comma separates the row selector  \\n from the column selector.  \\n Here is the column selector.  \\n It looks like we're just going to take the years  \\n 1900, 1920, 1940, 1960, 1980, and 2000.  \\n so at this point we can do our box plot on that  \\n and there are the box plots for those specific years.  \\n To me, it looks like this is saying the condition  \\n of the median house tends to go down over time slightly.  \\n Recall that this is only looking  \\n at the values from those specific years.  \\n Maybe we want to group by decade.  \\n Let's see how we could do that.  \\n I've got a chain that will do that.  \\n Let's walk through this chain.  \\n I've said this previously, but once you understand  \\n that pandas is basically a toolbox for manipulating data,  \\n if you can get your data into the specified forms  \\n by making these chains, that will give you super powers.  \\n Let's look at what this chain is doing.  \\n It is making a new column called decade.  \\n How are we calculating that?  \\n We're taking the year built column  \\n and we're doing an integer or a floor division by 10  \\n and then we're multiplying that by 10,  \\n so if I do a floor division,  \\n let me show you an example of that,  \\n I'm going to do 2006 and then I'm going to do double divide 10  \\n and that gives me 200.  \\n If I just do divide 10, that's going to gimme 200.6,  \\n so this is called floor division or integer division.  \\n If I did 1993, 10, that should give me 199  \\n and then I'm just multiplying  \\n so this is giving us every decade.  \\n Again, we can scroll over and just validate that that works.  \\n Can see that the decades are all multiples of 10.  \\n Now that we have that we can group by the decade  \\n and get summary statistics for each of those decades.  \\n Now that we have that, we can pivot by decade.  \\n I'm going to put the decade into the columns  \\n and we get values that look like this.  \\n This is what we had before.  \\n We're going to push those values up using the same code  \\n that we had before  \\n and now we're going to do our box plot with that,  \\n there we go  \\n and it looks like we're seeing that same thing  \\n that the condition of the house goes down over time.  \\n I'm not a subject matter expert in this dataset.  \\n This might indicate that people  \\n with old houses take good care of them.  \\n Maybe they're antiques.  \\n Maybe the new houses are not up  \\n to par compared to ones that are built in the past.  \\n It is interesting to see this.  \\n I would naively expect the opposite,  \\n that the overall condition  \\n of a newer house would be better than an older house.  \\n I've given you a bunch of tools leveraging pandas  \\n to group by category and look at numeric values.  \\n Let's see if we can use that using some other tools.  \\n I'm going to use the Seaborn library.  \\n If you're not familiar with the Seaborne library,  \\n it is a library that is built on top  \\n of mapplotlib and pandas for doing statistical plotting  \\n and we can say, let's do a box plot.  \\n Here's our data and here's the column, in the X,  \\n we want year built in Y, we want the overall condition.  \\n This allows us to get that plot that we got earlier  \\n but in one line of code.  \\n Again, is this plot useful?  \\n I don't think it's particularly useful.  \\n There's too much going on here.  \\n You can pull the documentation for this  \\n by just putting a question mark on here.  \\n There's actually a lot of documentation  \\n and we're going to use the order here  \\n to filter what is going on with the years.  \\n You can come down here and look at the output.  \\n Right now it's truncated  \\n but the order allows you to specify which things you want  \\n in the X axis.  \\n I'm going to specify a subset of those years.  \\n Let's run this again  \\n and now you can see a box plot for each of those years.  \\n Note that this is not doing any grouping.  \\n This is just pulling  \\n off the values that happened at those years.  \\n A trick that Seaborn has up its sleeve is it  \\n has other plots that you can do as well.  \\n One of those is the violin plot  \\n And this has the same interface as our box plot above.  \\n This is basically doing a kernel density estimation  \\n and flipping it on its side  \\n and putting another one that's mirrored on the other side  \\n of it, so you can see that, for example, there are a bunch  \\n of values here around six or seven for 1940.  \\n For 1900, there are a bunch of values around eight.  \\n Another plot that you might want to make is a boxen plot.  \\n This is a more modern box plot.  \\n In this case, it is showing you a big rectangle.  \\n The big rectangle is between the 25th percentile  \\n at the bottom and the 75th percentile at the top.  \\n The line in the middle is the 50th percentile or the median.  \\n This next rectangle down below here is  \\n from the 25th percentile to halfway between that and zero  \\n so this would be the 12.5 percentile  \\n and basically these diamonds are outliers, above that.  \\n Oftentimes you'll see another rectangle  \\n and another rectangle beyond that.  \\n In this case, it looks like the 12.5  \\n and in this case this is the 75th, and this is halfway  \\n between the 75th and the 100th, so that would be 87.5.  \\n Most of the values in this year, 1920,  \\n are between 12.5 and 87.5.  \\n You can see for 1900, most of the values lie  \\n between the 25th percentile and the 75th percentile.  \\n In this lesson,  \\n I gave you a bunch of tools that you can use  \\n to compare numeric values by category.  \\n Again, once you start understanding how to leverage pandas  \\n and these techniques in pandas,  \\n those are going to be nice tools  \\n in your tool belt to do some very powerful operations.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4506092\",\"duration\":362,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Comparing two categoricals\",\"fileName\":\"4433355_en_US_02_07_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13220169,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson, we are going to look  \\n at how you might take two categorical columns  \\n and compare them and understand  \\n the relationship between them.  \\n Generally we're going to want to use  \\n some sort of cross-tabulation.  \\n We can also do a visualization, a bar plot,  \\n or a stacked bar plot On top of that.  \\n Let's look at some of our dtypes here.  \\n We can see that we have a bunch of categorical values.  \\n Let's pick some of them.  \\n So I'm going to pick the Overall Quality  \\n and the Basement Condition.  \\n Generally when you compare these,  \\n you want to make what's called a cross-tabulation.  \\n For each category in one column,  \\n you want to count  \\n the number of categories from another column  \\n and you make a matrix of the result.  \\n This is how we do this in Pandas.  \\n Here's our original data.  \\n We're going to group by the two columns  \\n that we want the tabulation for.  \\n Pandas is lazy, when you do a groupby,  \\n it does not calculate anything.  \\n So let's do an aggregation to make it run a calculation.  \\n We're going to say size.  \\n Let's look at the output of this.  \\n This gives us a series  \\n and this is a series that has what's called a multi-index  \\n or a hierarchical index.  \\n In the left-hand side,  \\n we see Overall Qual and Basement Condition.  \\n Those are both in the index,  \\n so there are actually two indexes here.  \\n And then zero, zero, zero, three on the right,  \\n that's the values in the series.  \\n What I'm going to do now is a pretty powerful  \\n and sometimes confusing operation,  \\n I'm going to call unstack.  \\n What this is going to do is it's going to take  \\n that Basement Condition index.  \\n It's going to rotate it, pull it out,  \\n and put it in the columns  \\n so every value from Basement Condition  \\n will be a unique column.  \\n Let's run that and see what happens.  \\n Look at that, and here is our cross-tabulation.  \\n We have all of the Overall Quality values in the index  \\n and we have all of the Basement Conditions in the column.  \\n Because this is a common operation,  \\n Pandas actually gives this to us for free  \\n with the crosstab function.  \\n We can call crosstab  \\n and say in the index I want the Overall Quality column  \\n and in the columns I want the Basement Condition column.  \\n You can see that this is the same result  \\n that we see up above,  \\n but it's one line of code  \\n versus those four lines of code up there.  \\n I show you this because again, I think it's useful  \\n to have these powers to understand grouping,  \\n to understand size and unstack.  \\n Even though crosstab will give all of these to you for free,  \\n sometimes you do want to do these grouped by operations.  \\n Going back to what we've said previously,  \\n humans aren't great at looking at tables of data.  \\n This is a table of data.  \\n So generally if you have a table of data,  \\n you want to visualize it if you can  \\n or filter out the pieces of information  \\n that are useful in it.  \\n In this case, we're going to use style  \\n and background gradient to color it.  \\n I'm going to use a color map called Viridis  \\n which is a transitional color map.  \\n It goes from purple to yellow,  \\n passing through blue and green along the way.  \\n This tells us that we have a bunch of entries  \\n with Overall Quality of five  \\n and Basement Condition TA.  \\n So again, if we want to pull up the documentation,  \\n we can stick a question mark after background gradient  \\n and one of the things that you might want to do  \\n is figure out how it's doing this coloring here,  \\n and there is an access parameter here  \\n which determines how it calculates the color.  \\n Is it doing it by row, by column,  \\n or by everything in there?  \\n So you can pull up the documentation for that.  \\n Basically if you say axis is None,  \\n that's going to color by everything in the data frame.  \\n I'm also going to use loc here to reorder my columns.  \\n Note that these condition columns have a meaning.  \\n Ex means excellent, Gd means good.  \\n TA means typical, FA means fair,  \\n and PO means poor.  \\n So I'm going to order these from best to worst.  \\n I also have Missing and NA at the end here.  \\n Let's run this now  \\n and this tells us a little bit different story,  \\n that we have a bunch of values here  \\n that are in this typical value in the middle here.  \\n Another way to visualize this is to do a bar plot.  \\n I'm going to do a stacked bar plot.  \\n This is taking the results of my cross-tabulation  \\n and I'm going to say do a bar plot  \\n but stacked is equal to true.  \\n So let me walk you through what Pandas does  \\n when you do a bar plot.  \\n Again, it's going to take the index  \\n and it's going to put the index in the x-axis.  \\n In this case, the overall condition.  \\n We have multiple columns here  \\n so generally when we do a plot,  \\n it's going to plot each of those values above the index.  \\n So let's look at what  \\n it would normally look like, something like that.  \\n I'm going to say stack this  \\n and it's going to stack those values on top of each other.  \\n In this case, this is allowing us to see  \\n that for Overall Quality five, six, seven, and eight,  \\n we have a lot of TA entries.  \\n We can easily visualize that by looking at this plot.  \\n In this lesson, we looked at taking two categorical values  \\n and understanding how we can summarize those.  \\n Generally if we do that with a cross-tabulation,  \\n we said that a cross-tabulation is a table of data.  \\n We can use our techniques for visualization  \\n to understand that table better  \\n by either coloring the values in there  \\n or actually making a bar plot.  \\n Using a stacked bar plot  \\n is one way to understand a cross-tabulation.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4503098\",\"duration\":27,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Explore Ames\",\"fileName\":\"4433355_en_US_02_08_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":753160,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (electronic music)  \\n - [Instructor] It's time for our next challenge.  \\n In this challenge,  \\n of the first floor square footage against the sell price.  \\n We want to see what that relationship is between the size  \\n of the house and how much the house sold for.  \\n You might consider using some  \\n of the tricks that we talked about  \\n as we visualize the scatter plot.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4507090\",\"duration\":208,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Explore Ames\",\"fileName\":\"4433355_en_US_02_09_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6756332,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Let's look at the solution for this.  \\n I'm going to take my housing data set.  \\n I'm going to call .plot.scatter,  \\n and let's choose to put SalePrice in the X axis,  \\n and we're going to put first floor square footage  \\n in the Y axis.  \\n Let's run that and see what it looks like.  \\n We get something that looks like this.  \\n So my first thought when seeing this is it looks like  \\n as sell price goes up, first floor square footage goes up.  \\n Also, it looks like there's a hard line here along  \\n the bottom edge, which looks like  \\n there's some relationship going on there  \\n that you can't have a house  \\n that is very small going for a high price.  \\n It looks like we're not really seeing that.  \\n There are some outliers here  \\n where we are seeing large houses that are cheap.  \\n It'd be interesting to know what's going on  \\n with these outliers that are sort of out here.  \\n It looks like there's a cone where most of the values lie.  \\n Now, another thought that I have, when I'm looking at this,  \\n I'm not seeing columns or rows necessarily,  \\n but I am seeing a big dark patch,  \\n and I don't like to see big dark patches in a scatterplot.  \\n Remember, one of the things I said you could do  \\n is adjust the alpha.  \\n Another thing you can do is if you have too much data,  \\n and you just keep lowering that alpha,  \\n but you're not seeing transparency  \\n is to come in here and do samples.  \\n We could say, let's look at maybe 300  \\n of those entries and see what's going on here.  \\n That might be another way to see where the majority  \\n of the data is lying.  \\n However, I'm going to go and use our alpha,  \\n and I'll start off by setting that .5.  \\n That looks okay.  \\n It's telling a different story,  \\n but you can see down here it's just completely dark.  \\n I don't like to see it completely dark.  \\n I like to see some transition in there.  \\n Let's go down to .2.  \\n I think that's looking better.  \\n I might even go down to .1,  \\n and I think that tells a different story.  \\n This is actually fascinating, if you look at this.  \\n I'm maybe going to try a little bit lower here.  \\n Okay, and I'm seeing like a cluster of data here,  \\n but I'm also seeing a cluster down here below that,  \\n which is kind of interesting.  \\n It looks like there's a pattern up here,  \\n and there's another pattern up here.  \\n This looks to me like there is a bunch of houses  \\n that for some given size are less expensive,  \\n and then there's a cluster of houses  \\n for the same size that are more expensive.  \\n Might be interesting to see what's going on there.  \\n Is that a different neighborhood?  \\n Are these built in different years?  \\n These are the the sorts  \\n of ideas I have when I see a scatterplot like this,  \\n and note that if I don't adjust that alpha,  \\n I'm not going to have that insight here.  \\n I don't see that at all  \\n if I leave the alpha as the default value.  \\n I hope this was useful to you.  \\n In this example, I showed you how to plot a scatter plot  \\n and how to adjust the alpha to see some patterns  \\n that aren't apparent with the default scatter plot.  \\n \\n\\n\"}],\"name\":\"2. Exploring and Visualizing\",\"size\":119687113,\"urn\":\"urn:li:learningContentChapter:4507092\"},{\"duration\":1880,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4503097\",\"duration\":441,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Linear regression\",\"fileName\":\"4433355_en_US_03_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Linear regression is commonly used in data analysis and can help you identify significant predictors of a target variable, as well as their effect size and direction.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16203969,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson,  \\n we're going to look at linear regression.  \\n The goal of linear regression is to take data  \\n and predict numeric values from that.  \\n So we're going to take our housing data  \\n and we're going to see if we can predict the sales price  \\n given the other numeric values from that.  \\n I'm going to show you how to do that using Python  \\n and we're going to talk about splitting your data  \\n into a training set and testing set  \\n and evaluating how your model performs.  \\n We're going to be using the scikit-learn library.  \\n The scikit-learn library is a library  \\n that's very popular in academia,  \\n but it's also popular in industry.  \\n One of the nice things about the scikit-learn model  \\n is that it has a common interface.  \\n That common interface makes it very easy  \\n to try out different models.  \\n We're going to be looking at linear regression here,  \\n but we'll also be looking at some other types  \\n of models that can perform regression analysis.  \\n And because they conform to the scikit-learn API,  \\n it's very easy to swap them out  \\n and see how they perform.  \\n In order to run a model,  \\n we have to make two datasets.  \\n One is typically referred to as capital X  \\n and that is a matrix of data.  \\n Generally, it can be a Pandas DataFrame.  \\n And if you look at our X here,  \\n we are saying, okay, take our housing data  \\n and then pull off the numeric columns  \\n and also drop the sales price column.  \\n Remember, we're going to try  \\n This X is the features or the columns  \\n that we're going to try  \\n and use to predict sales price.  \\n So keeping sales price in there would be cheating  \\n if we're training the model with the sales price  \\n and asking it to predict the sales price.  \\n Then the other data set that we need  \\n is the target that we want.  \\n In this case, it is the SalePrice column.  \\n I'm going to use a function  \\n from scikit-learn called train_test_split.  \\n And what that's going to do is,  \\n it's going to take our X  \\n and our Y data and split it up.  \\n We're going to split it up  \\n into a training set and a testing set.  \\n This is very important to do  \\n when you're using machine learning.  \\n The reason is you want to understand  \\n how your model performs.  \\n And if you evaluate your model on data  \\n that's seen before, it's pretty trivial  \\n to make a model that can perform very well  \\n on data that's seen before.  \\n All it has to do is memorize that data.  \\n So we're going to hold out some of our data  \\n and that's what train_test_split does.  \\n The output of train_test_split is four things.  \\n It's the X training data, the X testing data,  \\n the Y training data, and the Y testing data.  \\n Let's run this code and make sure that it works.  \\n Looks like it did work.  \\n Let's look at what X_train looks like after running that.  \\n Here is X_train.  \\n If you look at the index on the left side,  \\n you can see that it's not sequential.  \\n What this has done is randomly pulled out  \\n different rows to train our model on.  \\n If we looked at the X_test data,  \\n it would be the data  \\n that is not in the training data.  \\n The remaining rows in there.  \\n Here's the Y training data.  \\n And you can see that the index for y_train  \\n is actually the same as the index for X_train.  \\n However, this is a series.  \\n Remember, in scikit-learn,  \\n our X is going to be a data frame  \\n and you can have a data frame with one column.  \\n But generally, data frames have multiple columns,  \\n but a series is only a single column.  \\n These are the values that we want to predict.  \\n Here is how we make our model.  \\n We're first going to say linear_model.LinearRegression.  \\n This makes an instance of the model.  \\n And then training the model is one line of code.  \\n It's calling lr.fit.  \\n We pass in the training data with the training label.  \\n In this case, it's y_train.  \\n And then the next line is a mechanism  \\n to score our data and see how it scores.  \\n And we're going to pass in X_test and y_test.  \\n And when we run this, it actually does not work.  \\n Let's look at our error and see what happens.  \\n And we got a value error.  \\n It says the \\\"Input X contains NaN.\\\"  \\n And if you remember,  \\n we had a bunch of missing data in there.  \\n It turns out that linear regression  \\n does not like missing data.  \\n In fact, if you have missing values  \\n in your data, it will not work.  \\n Let's just double-check to see  \\n if there is missing data in there.  \\n And it looks like, for example,  \\n in Lot Frontage, there's missing data  \\n and a bunch of other columns have missing data as well.  \\n We talked a little bit  \\n about handling missing data previously.  \\n Again, my recommendation is to find an expert  \\n to understand why the data might be missing.  \\n That expert might be able to guide you  \\n into appropriate actions to deal with the missing data.  \\n In our case, I'm going to make  \\n this new function called clean_housing_no_na.  \\n And if you look at that,  \\n what that's going to do is,  \\n it's going to add in this extra line  \\n into our chain here that's going to say,  \\n take the current state of the data frame  \\n after shrinking the integers,  \\n and pull off the numeric types,  \\n fill in the missing values with zero,  \\n and stick that back into your data frame.  \\n So housing2 after running this should have a bunch  \\n of zeros instead of missing numbers.  \\n Now, let me make an assign here.  \\n Is this the right thing to do?  \\n I'll say, probably not.  \\n We could have a whole course  \\n that talks about filling in missing values.  \\n For the purposes of this course,  \\n I'm introducing linear regression  \\n and not talking about all the ins and outs  \\n about filling in missing values.  \\n Let's, again, split our data  \\n and now let's run our model.  \\n And it looks like we get a score of 0.84.  \\n So the score that comes out of this  \\n is often called the R2 or R squared score  \\n or the coefficient of determination.  \\n This is a value between zero, typically, and one.  \\n The closer this is to one,  \\n the better the model is.  \\n What this represents is the amount of variance  \\n in the label or target that is explained  \\n by the features or columns in the data.  \\n So 84% of the variance in the label  \\n that we're trying to predict  \\n or that sales price is predicted  \\n by the data that we're passing into this.  \\n If we had another model that had a score of 0.9,  \\n that would be a better model.  \\n If we had a model that scored 0.8,  \\n that would be a worse performing model.  \\n Question people often asks,  \\n is 0.84 good enough or is your score good enough?  \\n And the answer to this is, typically,  \\n an unsatisfying it depends.  \\n It might be good enough, it might not be.  \\n Further analysis and understanding  \\n what the predictions often mean  \\n and how far off they are can often tell you  \\n from a business context  \\n whether it would make sense to deploy a model or not.  \\n We can have a whole class on linear regression.  \\n For the purposes of this class, we're introducing it.  \\n In this lesson, we created a linear regression model.  \\n We were able to predict the sales price  \\n from the training data  \\n from the other numeric columns in our dataset  \\n and we saw that we got a score  \\n of 0.84 when we did that.  \\n We also saw that linear regression  \\n did not like it if we had missing values.  \\n So we took a shortcut.  \\n We filled in those missing values with zero.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4504090\",\"duration\":260,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Interpreting linear regression models\",\"fileName\":\"4433355_en_US_03_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"By interpreting the coefficients, you can understand the magnitude and direction of the effect of each independent variable on the dependent variable.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9957878,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We just made a linear regression model.  \\n Now we want to try and understand what's going on  \\n with the model.  \\n Let me tell you about a convention in scikit-learn.  \\n Again, scikit-learn is full of conventions.  \\n Once you understand these conventions,  \\n it makes it really easy to use the model,  \\n and it also includes a bunch of different models,  \\n so it makes it easy to try out different models  \\n and see how they perform.  \\n One of the conventions in scikit-learn  \\n is that if an attribute ends in underscore,  \\n it was an attribute that was discovered  \\n or learned by fitting the model.  \\n In this case, our linear regression model  \\n has an attribute called coef_.  \\n This attribute represents the coefficients.  \\n If you have taken a math class,  \\n you might have learned about the formula for a line,  \\n y is equal to mx+b,  \\n where m is the slope, and b is the intercept.  \\n It turns out that linear regression has the same thing.  \\n The coefficient is called the slope,  \\n and the intercept is the intercept.  \\n And this coefficient represents the weights  \\n that are multiplied by each of the values in the columns  \\n to make the prediction.  \\n We also have lr.intercept_,  \\n and this is the base value.  \\n In this case, this is saying that our housing value  \\n starts at it looks like $15 million.  \\n To create a prediction,  \\n we add up the intercept value  \\n with the linear combination of the coefficient,  \\n with the values from each corresponding column.  \\n That will give us what the prediction is.  \\n (typing)  \\n Here are the column names  \\n that correspond with those coefficients.  \\n We can stick those into a panda series.  \\n At this point, we can take this panda series and plot it.  \\n So we're going to do a bar plot with this.  \\n And if you look at this bar plot,  \\n what this is telling us  \\n is this is telling us the weights  \\n that impact the model the most.  \\n In this case, it is saying overall quality  \\n pushes the model towards a more positive price,  \\n and kitchen above ground pushes the model  \\n towards a more negative price, as that increases.  \\n You can see in the middle here,  \\n there are a bunch of the features or columns  \\n that have a small value.  \\n These columns are features that don't have a large impact  \\n on the output of the model.  \\n What I'm going to do is use some pandas code  \\n to just do a little bit of filtering here.  \\n So we're going to start off with our series  \\n of the coefficients,  \\n and then I'm going to use this pipe here  \\n to just get the values that have an absolute value  \\n of greater than 100, and we'll do a bar plot of that.  \\n So these are the coefficients  \\n that tend to have the most impact on our model.  \\n One of the nice things about coefficients  \\n is that they have both a magnitude and a sign.  \\n So again, if that sign is positive,  \\n if that value goes up,  \\n it tends to push the value of the final prediction up.  \\n If the sign is negative, if that value would go up,  \\n it would push the result down.  \\n In this cell, I'm pulling off the features  \\n that have the most impact on the model.  \\n It turns out that we might want to use these later on,  \\n and pull those out.  \\n You just pull off the index from the series.  \\n In this lesson, we looked at understanding a model.  \\n One of the nice things about a linear regression model  \\n is that it's completely explainable.  \\n What do I mean by that?  \\n I mean that you can take this intercept value,  \\n and you can take the labels  \\n that are passed into the model to make a prediction,  \\n multiply them by these coefficients,  \\n and if you sum those up,  \\n you get the prediction from the model.  \\n This turns out to be a nice feature  \\n that you can interpret the model.  \\n You can understand why it's making the predictions it has.  \\n Also, you can look at the magnitudes of these coefficients.  \\n The larger the magnitude of them,  \\n the more impact they have on the model.  \\n The coefficients that have a small magnitude  \\n mean that the feature is not having as much impact  \\n on the final prediction.  \\n In future lesson, we'll look at different models  \\n and compare and contrast those with linear regression.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4508090\",\"duration\":382,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Standardizing values\",\"fileName\":\"4433355_en_US_03_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"The goal of this video is to learn how to standardize variables, which involves transforming variables to have a mean of zero and a standard deviation of one.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":18358621,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson, we're going to look at  \\n standardizing values.  \\n We're going to talk first about what this concept  \\n of standardization means,  \\n and then we'll learn how to do this from scikit-learn.  \\n Again, once you understand the conventions of scikit-learn,  \\n this is not particularly hard.  \\n And then we'll look and see if this impacts our score.  \\n We're also going to reexamine our coefficients  \\n after doing that and see if those change as well.  \\n So standardization has a specific meaning to statisticians.  \\n And oftentimes, people say normalize,  \\n and when they say normalize, they mean standardization.  \\n What standardization means  \\n is that we're going to take our X or our dataframe  \\n and we're going to basically collapse the values  \\n or shrink them into a given range  \\n or maybe expand them into a given range and center them.  \\n More specifically,  \\n it means we give each column a mean value of zero  \\n and the standard deviation of one.  \\n Why is that?  \\n You could imagine that certain models  \\n are impacted by the scale of the data.  \\n And if you have a housing model that's predicting price,  \\n it might have one column  \\n that represents the number of bathrooms in a house,  \\n which might be like from one to five bathrooms,  \\n might be the maximum in a house.  \\n You might have another column  \\n that represents the square footage,  \\n which might be from like 200 for a mini house  \\n up to like 20,000 for a mansion.  \\n The scale of the 20,000,  \\n that's a couple order of magnitudes bigger  \\n than the number of bathrooms.  \\n And certain algorithms  \\n are impacted by the relationship  \\n between the magnitude of the columns.  \\n One of the ways to do that is using standardization.  \\n Let's look at how to do this using scikit-learn.  \\n There is a pre-processing module in scikit-learn.  \\n And you can make an instance of the pre-processing module.  \\n A StandardScaler is a transformer in scikit-learn.  \\n That means that it has at least two methods.  \\n One is fit and the other is transform.  \\n In the case of the training data,  \\n we're actually going to call fit_transform,  \\n which combines fitting and transforming.  \\n So fit trains the model,  \\n and then transform takes input  \\n and transforms the input to a given output.  \\n In the case of a StandardScaler,  \\n it takes data after it's been trained,  \\n the training tells the model, \\\"Hey,  \\n this column should have a mean value of this  \\n and this column should have a mean value of this,\\\"  \\n and it knows how to shrink and move those means around.  \\n And then the transform actually does that transform for you.  \\n So here we're going to call fit_transform  \\n on the training data.  \\n This gives us a new X_train.  \\n And then on the testing data, we have to be careful here.  \\n We don't want to call fit again on the testing data,  \\n we just want to call transform on the testing data.  \\n So, once we've made a model,  \\n we want to evaluate it on data that it hasn't seen before.  \\n If we retrain the model,  \\n call fit on data that it hasn't seen before,  \\n we're kind of cheating.  \\n So we want to make sure that we only call transform  \\n on that testing data.  \\n Let's run that and make sure that it works.  \\n It looks like it did.  \\n Let's run our model now  \\n with our new data that's been standardized.  \\n And it looks like in this case, we get the same score.  \\n So our score did not particularly change by doing that.  \\n Let's look at our coefficients.  \\n And it looks like our coefficients  \\n are actually different now.  \\n I'm going to sort the values of our coefficients.  \\n You can see that we have some coefficients  \\n that are pretty negative  \\n and some other coefficients that are pretty positive.  \\n A couple of orders of magnitude above the others.  \\n Let's do a bar plot there with some filtering  \\n just to zoom in on those.  \\n And it looks like we have basement square footage,  \\n basement unfinished square footage,  \\n ground living area, basement square footage 2.  \\n It looks like a lot of the features that impact our model  \\n are square footage features, or how big the house is.  \\n And intuitively, it makes sense that square footage  \\n would be one factor that determines  \\n how much a house sells for.  \\n And if we want to pull those values off of the index,  \\n we can.  \\n In this, I'm looking at the union of the features  \\n from our model that we had not standardized  \\n and the data that we did standardize here below.  \\n Let's look at the correlations between those  \\n and just see if there happened to be some correlations  \\n between all of these features  \\n that tended to be important in these two models.  \\n Again, I'm going to set a background gradient  \\n to this diverging color map.  \\n Red going through white to blue.  \\n And I'm also going to pin that negative value to -1.  \\n So we can look at the redest values  \\n and the bluest values here.  \\n So we do find some blue values that are off diagonal.  \\n That's because I filtered the columns here.  \\n So the diagonal here is not necessarily  \\n going to be the values that are one.  \\n But we can see the overall quality  \\n has a pretty high correlation with sell price,  \\n as does year built, year remodeled.  \\n Here I'll just scroll this over  \\n and look at the correlations there.  \\n You can see the garage area, wooden deck square footage,  \\n open porch square footage  \\n have a slightly positive correlation with our sell price.  \\n In this lesson, we looked at standardizing our data.  \\n Generally, you will want to standardize your data  \\n when you're doing linear regression.  \\n We saw in our model that performing standardization  \\n did not impact this model, but that's not always the case.  \\n It is possible that standardizing your variables  \\n can have a significant impact  \\n on how your model performs.  \\n We did see that it actually changed the coefficients.  \\n And we had different coefficients emerging  \\n as more important when we standardized our data.  \\n In the next lesson, we'll look at using other models,  \\n and we'll see how other models perform  \\n to our linear regression model.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4509081\",\"duration\":522,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Regression with XGBoost\",\"fileName\":\"4433355_en_US_03_04_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"The goal of this video is to learn how to build and interpret a regression model using XGBoost, which is a popular algorithm for gradient boosting.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":19400016,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson,  \\n we're going to use a very popular library called XGBoost  \\n to make a regression model as well.  \\n We're going to compare and contrast the performance  \\n and the interpretability of XGBoost with linear regression.  \\n Let me quickly explain what XGBoost does.  \\n It makes a decision tree to make predictions  \\n and those predictions, in this case,  \\n the price of the house are probably going to be off  \\n from the actual values.  \\n So the boosting comes from making subsequent trees  \\n and the subsequent trees take that same input  \\n and try and fix the error.  \\n The error is called the residual.  \\n And you can add as many trees as you want.  \\n Each of those trees keeps fixing that error.  \\n I like to compare this to golfing.  \\n When you golf generally you don't just hit the ball once.  \\n You have multiple times to hit the ball  \\n and that's what this is doing.  \\n The first tree hits the ball, so to speak.  \\n It's not going to go in the hole probably.  \\n It'll be away from the hole by some amount,  \\n there'll be some residual  \\n and then the next tree will basically take a new club  \\n and hit the ball from there and try and fix the error,  \\n get it closer to the hole, so to speak.  \\n So this is like golfing and having multiple times  \\n to hit the ball to get it in the hole.  \\n That's a good analogy for XGBoost.  \\n Now, one of the nice things about XGBoost  \\n is that it uses the same interface,  \\n that scikit-learn interface.  \\n So if you have data that works in scikit-learn  \\n you can easily use XGBoost and make a model.  \\n One of the downsides of XGBoost  \\n is that it is not completely interpretable.  \\n That is to say with linear regression,  \\n we could look at the intercept  \\n and we could look at that coefficient  \\n and we could explain how the model is working.  \\n With XGBoost, it's a little bit more difficult.  \\n We have a bunch of decision trees  \\n and you could have hundreds of decision trees  \\n and going through each of those decision trees  \\n and looking at how they sum up at the end  \\n could be a little tedious.  \\n You could do it if you wanted to, but in practice  \\n no one does because it's too much work.  \\n Okay, one more thing that's useful about XGBoost  \\n is that you don't have to clean up missing values.  \\n In fact, it will work with columns that have missing values.  \\n In addition, XGBoost will also work with columns  \\n that are not numeric.  \\n So this gives it a lot of power.  \\n You can take your data and without doing a ton  \\n of pre-processing, you can throw it into an XGBoost model.  \\n It also doesn't even require your data to be standardized.  \\n So this makes XGBoost a nice model to try out  \\n and see how it performs.  \\n Here, I'm going to make x and y.  \\n Note that I am not filling in my missing values with zero,  \\n I'm just leaving those missing,  \\n and then I'm going to split my data.  \\n In this case, I will standardize it.  \\n It turns out that standardization does not  \\n impact XGBoost model one way or the other.  \\n It will perform the same because  \\n if you think about how a decision tree is working,  \\n it's looking at a single column,  \\n it's not comparing one column to another,  \\n and so that relative size of the columns  \\n is not really important for a tree based model.  \\n Let's run that.  \\n Okay, it looks like that worked  \\n and we're going to come down here and make our model.  \\n Now look at this.  \\n This looks almost identical to what we had above  \\n when we made our linear regression model.  \\n We made an instance of the model.  \\n In this case, we're making an XGBoost regressor  \\n and then we called our fit with the training data  \\n and we called score with the testing data.  \\n Again, one of the nice things  \\n about scikit-learn is that it has a common interface  \\n and there are many models that have that interface.  \\n Look at this.  \\n Our score here of our XGBoost model is 0.89.  \\n So this is a better model,  \\n or it performs better than our linear regression model.  \\n A question you might ask yourself  \\n is 0.89 a sufficiently good model?  \\n And that's a different answer.  \\n The answer again, is this unsatisfying, it depends.  \\n It is a better model than linear regression  \\n in that it performs better,  \\n but we don't know if that performance  \\n is sufficiently good enough  \\n to impact our business.  \\n That would require us diving into how far  \\n off the predictions are and if we would make  \\n or save money by leveraging that model.  \\n Note that XGBoost has an attribute called  \\n feature_importances_ that is learned by doing the fit here.  \\n This is like the coefficients  \\n but it's a little bit different.  \\n It doesn't have a direction, it only has a size.  \\n So in this case, it is saying that the feature  \\n that impacted our model the most was the overall quality.  \\n After that, it was the garage cars.  \\n And at a high level what's going on here,  \\n you can think we have a bunch of trees.  \\n The features that are at the top of the trees  \\n where it makes those decisions  \\n tend to be the most important features.  \\n So this is basically a way of measuring which features  \\n come at the top of the trees.  \\n You can see that there are some of these features  \\n at the bottom that have very low scores,  \\n meaning that those features don't really impact  \\n or drive the model too much.  \\n In fact, we could probably make a semi-decent model  \\n just by looking at the overall quality,  \\n how big the garage is,  \\n the number of baths, and the living area.  \\n In this example, I'm going to use categories  \\n in my XGBoost model.  \\n One of the things about our XGBoost model  \\n is that our XGBoost model doesn't like the pyarrow types.  \\n So I've got a chain here.  \\n I am selecting the numeric values.  \\n These are the pyarrow numbers  \\n and I'm converting these to N64s.  \\n These are the pandas values that XGBoost is okay with  \\n and I'm also dropping the sales price column.  \\n Note that I did not remove the categorical values,  \\n like I did with the other examples here.  \\n Then I'm going to split my data again  \\n and I'm going to say XGBoost Regressor.  \\n I'm going to say enable categorical is true,  \\n telling it I want categoricals to work in here.  \\n And then if I want categoricals to work  \\n I also have to say that the tree method  \\n which is the way that XGBoost makes the trees  \\n is this string hist.  \\n And let's just run that.  \\n When you run that calling fit and score  \\n we see that we got a model of 0.91.  \\n So remember we went from 0.84 with linear regression  \\n to 0.89 with XGBoost with the numbers  \\n to 0.91 with XGBoost with the categories.  \\n So we do get a boost from using those categories  \\n indicating that those categories might be important.  \\n So let's ask XGBoost  \\n what are the important features of the categorical model?  \\n And it looks like the first categorical model  \\n that I'm seeing is neighborhood.  \\n If you remember back in our scatter plot  \\n we had it looks like two clusters.  \\n The neighborhood might have explained that.  \\n It might have been a fancier neighborhood  \\n that costs more for the same size house.  \\n So we are probably seeing that there is value  \\n including that neighborhood categorical into our model.  \\n Further exploration would be useful  \\n in validating that theory.  \\n In this lesson, we looked at using XGBoost,  \\n a popular library, to make our regression model.  \\n We saw that it performs slightly better out of the box.  \\n We saw that if we add categoricals, it performs even better.  \\n Again, one of the downsides of this XGBoost model  \\n relative to a linear regression model,  \\n is that the explainability suffers a little bit.  \\n A linear regression model is completely explainable.  \\n An XGBoost model is kind of explainable  \\n but it's tedious to explain.  \\n That might be a trade-off.  \\n The performance versus the explainability  \\n might be a business decision that you need to consider  \\n whether you use a better performing model  \\n if you can't explain it.  \\n Think of a situation like a business loan.  \\n You might want to offer someone a loan,  \\n but if you can't explain why you're denying them a loan  \\n that might make the customer upset, causing them to churn.  \\n Finally, remember, because we're using  \\n that scikit-learn interface  \\n it makes it easy to try out these different models.  \\n I highly encourage you to not just try one model.  \\n Try out a few models and see how they perform.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4509080\",\"duration\":40,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Predict Ames\",\"fileName\":\"4433355_en_US_03_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1159627,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] In this challenge,  \\n you're going to predict the sales price  \\n of the Ames housing data.  \\n What I would like you to do is make  \\n a linear regression model,  \\n but I want you to only use the top five features  \\n from the XG Boost model that was not using categorical data.  \\n I want you to pull off those features.  \\n We saw how to do that in a previous lesson,  \\n and then I want you to train a linear regression model  \\n with just those features from the training set  \\n and then evaluate the model.  \\n What is the score or coefficient of determination  \\n or our squared value of that model?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4503096\",\"duration\":235,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Predict Ames\",\"fileName\":\"4433355_en_US_03_06_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8494356,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Let's explore a solution to this challenge.  \\n The first thing I want to do is inspect my XG boost model  \\n and see if I can get those features from that.  \\n So here's my XG boost model.  \\n Remember, we discussed that it has an attribute  \\n called feature_importances_ that ends in underscore,  \\n again that underscore meaning  \\n that attribute was learned when we fit the model.  \\n So here's our feature_importances_,  \\n this is a NumPy array of the importance of each feature.  \\n This corresponds with the training data.  \\n So we should be able to stick this into a Panda series  \\n and get the names of that.  \\n So I'm going to say  \\n let's stick those values into a Panda series.  \\n And for the index  \\n I'm going to use the columns from the training data.  \\n Let's look at what that looks like.  \\n That's looking pretty good.  \\n I'm going to stick this into a chain.  \\n Now I want to sort these values.  \\n Let's just look at what that looks like when we sort that.  \\n It looks like we're going to want to take the values  \\n at the bottom.  \\n Those have the highest score.  \\n So to take the values at the bottom,  \\n I'm going to pull off the index.  \\n Let's just evaluate that  \\n and make sure that that looks correct.  \\n Okay, yeah, we want those values at the end  \\n and we want the last five values at the end.  \\n So to get those, I'm going to use a slice syntax  \\n and I'm going to say minus five colon.  \\n If you use that negative indexing, it's going to go back five.  \\n And then the colon says go to,  \\n and because we didn't put anything on the right side  \\n of the colon, it's just going to go to the end.  \\n So it's going to take the last five values.  \\n Let's just run that and evaluate that.  \\n It looks like that did work.  \\n Okay, these are the features that we want to use.  \\n So I'm going to come up here  \\n and I'm going to store these features in a list.  \\n I'm going to say top five is equal to,  \\n and I'll stick those into a list.  \\n And then what I'm going to do is  \\n I'm going to make a new model down here.  \\n I'm going to call this the linear regression model.  \\n I'm going to say it's the top five features  \\n and then I'm going to use my linear model.  \\n And I'm going to say linear regression.  \\n We'll make an instance of that.  \\n And then I need to fit my model.  \\n So I'm going to say linear regression, top five.  \\n We're going to call fit.  \\n I'm going to call it with X_train.  \\n And I'm going to use loc here to specify a subset of this.  \\n Now remember, loc takes a row selector  \\n and a column selector.  \\n I'm going to specify just a colon here  \\n for the row selector, that's a slice,  \\n that means take all of the rows.  \\n And then for my column selector, I'm going to pass in top five.  \\n Let's just run this right here  \\n and make sure that that works.  \\n And I got an error.  \\n It says it is missing a positional argument, y,  \\n so it did not work.  \\n Why didn't it work?  \\n 'Cause I need to pass in y_train as well.  \\n Let's try it again.  \\n The challenge is asking us what is the score of this model?  \\n To get the score of this model  \\n we need to call the score method.  \\n This looks very similar to what we have above.  \\n I'm going to say lr top five score.  \\n I'm going to say X_test.  \\n And then again, I need to subset what I'm selecting.  \\n So I'm going to say take all of the rows  \\n but just the top five columns.  \\n And then we need to pass in the actual true values  \\n so it can evaluate how it did on its performance.  \\n And we got a score of 79.  \\n So our score dropped a little bit when we used a subset  \\n of these features, but we're only using five features.  \\n This might be a trade-off that you're willing to make  \\n to have a very simple model  \\n even if it performs slightly less.  \\n \\n\\n\"}],\"name\":\"3. Linear Regressions\",\"size\":73574467,\"urn\":\"urn:li:learningContentChapter:4509086\"},{\"duration\":1342,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4507089\",\"duration\":160,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Exploring data\",\"fileName\":\"4433355_en_US_04_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"The goal of this video is to understand the data to perform statistical tests.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6301362,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We're going to start looking  \\n at doing some hypothesis testing using Python.  \\n In this lesson, we're going to look at the data  \\n that we're going to explore.  \\n Let's run this first cell here.  \\n We are going to use the stats module from the SciPi library.  \\n Let's make sure that we import that,  \\n and we're going to be looking at neighborhoods  \\n and comparing neighborhoods.  \\n Because neighborhood is a categorical value  \\n I'm going to use my favorite method on that,  \\n value counts, to look and see what we have in here.  \\n It looks like the two most popular neighborhoods are NAmes  \\n and it looks like probably college, CR College Creek,  \\n College something.  \\n So let's limit our analysis to those two neighborhoods.  \\n Another way to look at these categoricals  \\n is to use a group by,  \\n and then for our aggregation, do a describe.  \\n Look at what the output of this looks like.  \\n This is kind of cool.  \\n In the index, that because we grouped by neighborhood  \\n we have all the neighborhoods  \\n and in the columns, now we have hierarchical  \\n or multi-index in the columns,  \\n we actually have two levels of columns.  \\n For each numeric value, we have the outer column  \\n and then in the inner columns,  \\n under that, we have the summary statistics for those.  \\n Let's limit those to the two neighborhoods  \\n that I'm interested in looking at.  \\n I'm going to call one College Creek  \\n and the other one North Ames.  \\n So let's look at those two rows  \\n and let's just pull out the sale price.  \\n Here we can see the summary statistics  \\n for those two neighborhoods.  \\n College Creek looks like it has 267 entries.  \\n The mean sales price for College Creek is around $200,000,  \\n whereas North Ames has a mean sales price around $145,000.  \\n We might want to ask ourselves,  \\n is this data different or is it the same?  \\n It looks like it's somewhat different  \\n in that the mean is different, let's look at the median,  \\n the median actually looks different as well.  \\n The maximum values are different  \\n and the minimum values are different.  \\n It looks like from just looking  \\n at these summary statistics here  \\n that College Creek is a more expensive neighborhood  \\n than North Ames.  \\n Let me just show you an option that you can do  \\n sometimes when you do a describe that gets a little wide,  \\n if you want to often see a little bit more data  \\n you can use T there to transpose the data.  \\n Makes it a little bit more compact sometimes  \\n and fits in the screen a little better.  \\n We want to explore two neighborhoods, so we're going to look  \\n at this North Ames in this College Creek neighborhood.  \\n In this lesson, we did some summary statistics  \\n to compare the sales price of those two neighborhoods.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4504089\",\"duration\":363,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualizing distributions\",\"fileName\":\"4433355_en_US_04_02_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"The goal of this video is to visualize distributions of data before performing a statistical test.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12359122,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In the last lesson,  \\n we looked at summary statistics  \\n to understand these two neighborhoods.  \\n Let's use some visualization now to understand them as well.  \\n I'm going to pull out the North Ames  \\n and College Creek data into their own series  \\n so that we can look at them a little easier.  \\n I'll just use some pandas to do that.  \\n And the next thing I'm going to do  \\n is I'm going to make a histogram of both of those.  \\n So I want to plot these histograms on top of each other.  \\n What I'm going to do is call hist on our North Ames series.  \\n I'm going to give that a label as well,  \\n so a legend will appear in that,  \\n and that will return a map plot lib axis.  \\n I'm going to take that axis  \\n and pass it into the College Creek histogram call  \\n so that College Creek is plotted  \\n on the same axis or same plot as we have.  \\n I'm also giving that a label,  \\n and then after I do both of those histograms,  \\n I'm going to tell map plot lib to throw a legend on there  \\n with AX dot legend.  \\n This is the result of that.  \\n Now, note that both of these are fully opaque  \\n and because we plotted College Creek  \\n after we plotted North Ames, it is occluding some of that.  \\n It might be the case, probably isn't,  \\n but it might be the case  \\n that what College Creek is occluding,  \\n North Ames matches directly,  \\n but it's hard to tell because we don't know  \\n what's going on underneath College Creek there.  \\n To deal with this, I'm going to adjust the transparency,  \\n and we do that by changing the alpha parameter.  \\n So here I'm going to set the alpha to 0.7.  \\n Remember, alpha is how transparent something is.  \\n This should let us understand what's going on below that.  \\n Let's look at this.  \\n From a quick look at the histogram,  \\n these look like different distributions.  \\n We already looked at the summary statistics.  \\n We thought that they were different distributions as well.  \\n It looks like this histogram is confirming that.  \\n Let's look at one more plot  \\n that's useful for looking at distributions,  \\n that's continuous distribution function.  \\n I'm going to show how to do that with pandas.  \\n Let me just walk through this chain here  \\n because we have a few steps of that.  \\n So we have our North Ames series.  \\n I'm going to convert this to a data frame by saying two-frame.  \\n Now it's a data frame with a single column.  \\n That's fine.  \\n A data frame is two dimensions, but it is possible  \\n to have a data frame with a single column.  \\n Now that I have a data frame, I can add a new column to it.  \\n I'm going to make a column called CDF,  \\n and that is going to be taking these values  \\n and ranking them.  \\n And the CDF column is going to be the result  \\n of calling the rank method on our data  \\n and converting that into a percentage.  \\n So it looks like the first entry there  \\n is around the 96 percentile.  \\n The next entry is around the six percentile of entries.  \\n The next thing I'm going to do  \\n and we should see when we do this, the index should change.  \\n And indeed it does.  \\n The next thing that I'm going to do  \\n is I'm going to plot this.  \\n I've said multiple times in this course,  \\n one of the keys to plotting in pandas  \\n is understanding how pandas makes plots.  \\n In this case, I'm just going to call the plot method,  \\n which will make a line plot.  \\n Now with a line plot, I can specify X and Y.  \\n If I don't specify X, it will use the index for X.  \\n I don't want to use the index for X,  \\n what I want in the X axis is the cell price  \\n and in the Y axis I want that percentage or that CDF.  \\n Let's un-comment that and run that, see what it looks like.  \\n We get something that looks like this.  \\n So if we created a CDF of College Creek and North Ames,  \\n and they had the same distributions,  \\n we would expect these plots to overlap each other,  \\n to trace each other, essentially.  \\n Let's see if they do.  \\n I'm going to create a function called plot CDF  \\n and it's going to take a series as the input  \\n and an optional map plot lib axis and a label.  \\n It's going to do the logic that I just showed above,  \\n converting a series to a frame, making a CDF column,  \\n sorting it, and then doing the plot,  \\n but it's going to return the series as the output.  \\n Let's run that and see what happens,  \\n just make sure that it works.  \\n You can see that this returned the series.  \\n You can see at the bottom, there is a series there,  \\n and then below that  \\n it had a side effect of making that plot.  \\n With that function in hand,  \\n let's now call that on both of our data sets.  \\n I'm going to call that with North Ames and College Creek,  \\n passing in the same map plot lib axis for both,  \\n and it should plot them on the same plot.  \\n I got an error, it says map plot lib is not defined.  \\n I ran into this because I restarted my code space  \\n and my map plot lib library was not installed.  \\n So if I were to do this in the real world,  \\n I would come up here to the top,  \\n where I've put my imports here,  \\n and note that I don't have map plot lib here,  \\n I would come in here and say,  \\n import map plot lib dot py plot as PLT.  \\n That will make it so, in the future,  \\n when I run this, I'm not going to have that issue.  \\n Let's run this again.  \\n You can see that it prints out a series here  \\n because plot CDF returns a series.  \\n Below that we should see our plot,  \\n and we can see that these CDFs do not overlap,  \\n giving us further evidence  \\n that these do not have the same distribution.  \\n In this lesson,  \\n we looked at comparing distributions by looking at plots.  \\n We looked at histograms  \\n and continuous distribution functions.  \\n We saw that,  \\n for these two neighborhoods that we're looking at,  \\n these don't appear to overlap,  \\n suggesting that they are not the same distribution.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4507088\",\"duration\":264,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Running statistical tests\",\"fileName\":\"4433355_en_US_04_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"By performing statistical tests, you can understand if two data sets have the same distribution.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12107398,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson,  \\n we're going to use the scipy.stats module  \\n to run a statistical test to give us a numeric value  \\n as to whether these two distributions are the same.  \\n Imported that scipy.stats module up above.  \\n Scipy is a collection of different utilities  \\n for manipulating numerical data.  \\n One of those modules is the stats library.  \\n Let's just look and see what's in there.  \\n There are a bunch of things that you can do  \\n with this stats module.  \\n One of them is to run statistical tests,  \\n another one is to create distributions,  \\n there are also some plotting capabilities as well.  \\n In this case, we're going to run this ks_2samp function.  \\n Let's pull up the documentation for that.  \\n It says, it performs a two sample Kolmogorv-Smirnov test  \\n for goodness of fit.  \\n Basically what this is doing is it's testing  \\n whether these two distributions are the same.  \\n Let's look at what we need to pass into it.  \\n We need to pass in data1 and data2  \\n and we already have those two series,  \\n so we should be good with that.  \\n What does this return?  \\n This says it returns a p-value.  \\n I'm going to view as a scrollable element here  \\n and let's scroll this up.  \\n This says, it returns an object containing two attributes,  \\n the KS statistic and a p-value as a result.  \\n And if we scroll down a little bit,  \\n we should see some examples.  \\n One of the nice things  \\n about a lot of the Python numeric libraries  \\n such as Pandas, Matplotlib, Scikit-learn and Scipy  \\n is they have really good documentation.  \\n You can see an example of using it right here.  \\n We just create the two distributions  \\n and then we call ks_2samp with our two samples  \\n and this gives us this KstestResult.  \\n You can see that the first part here is a statistic  \\n and the next part is a p-value.  \\n In this case, we're mostly interested with the p-value.  \\n You can see that says, if the p-value is lower  \\n than a threshold of 0.05, we reject the null hypothesis  \\n in favor of the two-sided alternative,  \\n which is the data were not drawn from the same distribution.  \\n Here's another example.  \\n You can see that these two samples, the p-value is 0.5  \\n and so because the p-value is 0.5,  \\n it's not below that threshold.  \\n We cannot reject the null hypothesis  \\n or we fail to reject that.  \\n The null hypothesis in this case would be  \\n that the two samples were from the same distribution,  \\n so we cannot reject that,  \\n in essence saying that we cannot prove  \\n that these were not from the same distribution,  \\n i.e. we think they are from the same distribution.  \\n Let's do this with our data now.  \\n I'm just going to pass in n_ames and college_cr into that  \\n and I'll print out the results there.  \\n We have the statistic and the p-value.  \\n The p-value is very low.  \\n Down here I have an IF statement  \\n that lets us check the p-value.  \\n If the p-value is greater than 0.05,  \\n we fail to reject the null hypothesis,  \\n i.e. these two are from the same distribution.  \\n If it is less than 0.05, we reject the null hypothesis,  \\n meaning that they are not from the same distribution.  \\n In this case, this is a very small number,  \\n it is certainly less than 0.05  \\n and we are going to reject the null hypothesis  \\n that these two data sets are from a different distribution.  \\n This confirms what we saw  \\n with both our statistical summaries  \\n and our visualizations of these two distributions.  \\n In this lesson, we looked at using the scipy.stats library  \\n to do a statistical test to compare  \\n whether two samples were from the same distribution.  \\n The scipy.stats Library makes this really easy  \\n to run statistical tests such as these  \\n and you can leverage the scipy.stats library  \\n to confirm what you see when you do visualizations  \\n or summary statistics.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4508089\",\"duration\":268,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Testing for normality\",\"fileName\":\"4433355_en_US_04_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"The goal of this video is to test whether a set of values has a normal distribution.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9502184,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson we're going to use  \\n the SciPi library to test  \\n whether a distribution is normal.  \\n We're also going to create a visualization  \\n to let us evaluate that visually as well.  \\n Typical test for testing for normality  \\n is called the Shapiro-Wilks Test,  \\n and it turns out that SciPi stats module  \\n has a function that will do that.  \\n And similar to the function  \\n that we looked at in the previous lesson,  \\n this will give us back a statistic and a P value.  \\n So let's see if northern aims is normal,  \\n We're going to run that  \\n and then we'll look at the P value there.  \\n It says that the distribution  \\n of the series is likely not normal,  \\n so presumably that P value was low enough,  \\n so we are going to reject the hypothesis  \\n which is that it is a normal distribution.  \\n Just print out that P value so you can see what it is,  \\n and it is indeed a small number.  \\n Another way to evaluate normality is to  \\n use a probability plot.  \\n It turns out that this module has that capability as well.  \\n You can pull up the documentation if you want to.  \\n The key thing here is that you pass in the data  \\n and I'm also going to pass in a parameter  \\n for this plot to tell us to use map plot lib  \\n to plot that.  \\n Here's my code.  \\n I am going to create a figure in an axis using map plot lib  \\n and then I'm going to call probplot and pass in the axes.  \\n Here's our probability plot.  \\n Let me explain how we interpret this.  \\n We don't have a hard numeric value to say  \\n whether or not something is normal.  \\n By looking at this, this is a visual interpretation.  \\n We would like to see the blue dots track that red line,  \\n and if they do track that red line,  \\n that would be a strong indication that this is normal.  \\n You can see that it looks like it's overlapping the red line  \\n at a few points,  \\n but it's a little bit above on the small end.  \\n Then it goes below the red line  \\n and then it goes back up above the red line.  \\n From looking at this probability plot,  \\n I would say that this is not normal.  \\n It's close to normal on the left hand side,  \\n but on the right hand side it is not.  \\n Let me just plot a histogram of this so we can see what  \\n that histogram looks like and understand what's going on  \\n with this probability plot.  \\n Again, we're going to look at the north aims blue histogram,  \\n and we said that the left hand side of this  \\n looks somewhat normal,  \\n but the right hand side does not.  \\n Indeed, the right hand side has a tail where it's dragging  \\n out a little bit, and you can see that that tail  \\n is represented by this going up and above.  \\n Now, on the left hand side, it's going above as well.  \\n That indicates that it's actually a little bit short here.  \\n We would like to see that drag out a little bit more,  \\n so if this had a long tail  \\n on the left hand side,  \\n we would actually see these blue lines  \\n go below that red line.  \\n So that's the interpretation there.  \\n Let's actually repeat this with the College Creek data.  \\n This doesn't look normal either.  \\n Let's look at what the probability plot looks like for that.  \\n So here's the probability plot for our College Creek.  \\n You can see that it's quite a bit above  \\n on the left-hand side.  \\n What that indicates is that it's not tailing off  \\n as much as we would like for it to be normal.  \\n You can see that it then goes below here.  \\n I am interpreting that as this bump right here.  \\n We would like that to go up a little bit more,  \\n but it's not, looks like along the middle there we're okay.  \\n Then we're going back down below,  \\n and then it's going back up above,  \\n indicating that this tail is a little bit too long.  \\n In this lesson,  \\n we looked at the ability to do a probability plot.  \\n A probability plot is a visualization that lets us  \\n understand whether our data looks normal.  \\n It does not have a numeric yes or no answer,  \\n but this is somewhat fuzzy.  \\n We want to see the blue dots tracking that red line,  \\n and I showed you how you can interpret when  \\n these values are off by comparing that to the histogram.  \\n This is a nice tool to have in your tool belt  \\n if you need to check if your data is normal.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4507087\",\"duration\":20,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Checking square footage distributions\",\"fileName\":\"4433355_en_US_04_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":601364,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat electronic music)  \\n - [Instructor] In this challenge,  \\n you're going to get a chance  \\n to play around with statistic tests.  \\n What I want you to do is get the first floor square footage  \\n from North Ames and College Creek  \\n and compare those distributions.  \\n Are they the same or not?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4508088\",\"duration\":267,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Checking square footage distributions\",\"fileName\":\"4433355_en_US_04_06_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8992336,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Let's look at a solution for this challenge.  \\n The first thing that I want to do  \\n is I want to get the square footage  \\n from both of these neighborhoods.  \\n Let's use our pandas code to do that.  \\n Say from housing, let's do a query  \\n and I'm going to say neighborhood is equal to,  \\n and I'll put in double quotes here  \\n because my string has a single quote and NAmes.  \\n Close my double quote there, close my single quote  \\n and then I need another parentheses at the end for my chain.  \\n Let's run that and just scroll down  \\n and look at how many rows.  \\n Okay, there's 400 rows, so it looks  \\n like this is getting the Northern Ames  \\n or North Ames neighborhood.  \\n Let's pull off the first floor square footage from that.  \\n I'm going to use .loc to do that.  \\n So remember, loc takes a row selector.  \\n I'm going to take all of the rows from this  \\n and then a column selector.  \\n Now, if I want to get a series from this,  \\n I need to just put in a scaler value from this.  \\n That should give me a series.  \\n Let's run that and make sure.  \\n Okay, there's my series.  \\n If I wanted to get a data frame  \\n with just one dimension in that, I could put in a list  \\n of the columns and just have a single column in there  \\n and that should give me a data frame.  \\n Okay, so there's my data frame.  \\n I don't want a data frame. I want a series.  \\n That looks like that's pretty good.  \\n Okay, so this is going to be my NAmes  \\n and then I'll call this _sf.  \\n Okay, now I'm going to just copy this  \\n and I'm going to leverage this  \\n for making the College Creek value here.  \\n Let's change this to college_cr,  \\n and I'm going to come down here and change NAmes to CollgCr.  \\n Let's run that and see if it works.  \\n It looks like it does work.  \\n Let's just look at the output of that.  \\n Okay, it looks like that did work as well.  \\n Okay, now I have my two datasets here.  \\n I want to see if these distributions are the same.  \\n So before I run my statistical test,  \\n I'm going to visualize these.  \\n So let's make a histogram to visualize these.  \\n I'm going to say ax is equal to n_ames_sf  \\n and then I'll do a hist here.  \\n There's the histogram  \\n of the square footage of the first floor.  \\n And let's plot on top of that  \\n my College Creek square footage.  \\n I'm going to say hist, and then I'm going to say ax is equal  \\n to that ax that I just created from North Ames.  \\n I'm going to get something that looks like this.  \\n In this case, I can see that I don't really need to play  \\n with the alpha here, but I can tweak it a little bit.  \\n I'll just change it to 0.7 just on College Creek.  \\n These do not look like the same distribution  \\n but let's quantify that using our summary statistics.  \\n Okay, so we're going to use the Kolmogorov-Smirnov test  \\n and we're going to get a statistic from that  \\n and we're going to get a P-value from that.  \\n And we're going to say stats.  \\n And this is the ks_2sample  \\n and we're going to pass in n_ames_sf  \\n and College Creek sf  \\n and we'll just print the P-value from this.  \\n Okay, remember, our null hypothesis  \\n is that the values have the same distribution.  \\n The P-value is less than 0.05, giving us an indication  \\n that these do not have the same distribution.  \\n In this lesson, we looked  \\n at how we can apply a Kolmogorov-Smirnov test  \\n to two samples.  \\n We also verified that by looking  \\n at the histogram and understanding  \\n that these two distributions graphically also did not look  \\n like they were the same distribution.  \\n \\n\\n\"}],\"name\":\"4. Hypothesis Tests\",\"size\":49863766,\"urn\":\"urn:li:learningContentChapter:4503103\"},{\"duration\":78,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4503095\",\"duration\":78,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"4433355_en_US_05_01_LA30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5259035,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Congratulations on completing  \\n the Statistics with Python course.  \\n Now that you've acquired a solid statistics foundation,  \\n you might wonder what are the next steps?  \\n I highly recommend practicing the concepts  \\n and techniques covered in this course.  \\n Like any skill, the more you practice,  \\n the more proficient you become.  \\n Consider working on improving your data manipulation skills.  \\n As we've seen, many libraries are easy to use  \\n if the data is in the appropriate format.  \\n One valuable resource for this is my book,  \\n \\\"Effective Pandas.\\\"  \\n This book provides comprehensive examples  \\n and snippets for data manipulation using Pandas.  \\n It offers real world examples as well  \\n as exercises to reinforce your learning.  \\n Finally, apply the knowledge and techniques you gained  \\n in this course to your own projects and data sets.  \\n Whether you're analyzing survey data,  \\n conducting AB testing,  \\n or exploring customer behavior,  \\n using statistics and Python will enable  \\n you to extract valuable insights  \\n and make data-driven decisions.  \\n Completing a project will impress your boss  \\n and is also helpful  \\n Thanks for completing this course  \\n and I wish you continued success  \\n in your statistical journey.  \\n \\n\\n\"}],\"name\":\"Conclusion\",\"size\":5259035,\"urn\":\"urn:li:learningContentChapter:4503104\"}],\"size\":379622141,\"duration\":9565,\"zeroBased\":false},{\"course_title\":\"Advanced Python: Working With Data\",\"course_admin_id\":4312001,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":4312001,\"Project ID\":null,\"Course Name\":\"Advanced Python: Working With Data\",\"Course Name EN\":\"Advanced Python: Working With Data\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"&lt;p&gt;Data science is one of the fastest-growing areas of technology today. And whether you work with large data sets or just need to process spread sheet files, Python is a great language to use when working with data-intensive applications. In this course, Joe Marini gets you started on working with data in Python, highlighting some of the most useful built-in features of the language. Using a real data set from the United States Geological Survey that tracks earthquake information, Joe shows you how to perform data operations like sorting and filtering, determining basic information like minimum and maximum values, and reading and writing data to and from other formats like CSV and JSON. Practice what you learn with hands-on challenges. Check out this course with Joe to see how Python can help you make sense of your data.&lt;/p&gt;&lt;p&gt;This course is integrated with GitHub Codespaces, an instant cloud developer environment that offers all the functionality of your favorite IDE without the need for any local machine setup. With GitHub Codespaces, you can get hands-on practice from any machine, at any time\u00e2\u20ac\u201dall while using a tool that you\u00e2\u20ac\u2122ll likely encounter in the workplace.&lt;/p&gt;\",\"Course Short Description\":\"Learn about the features of Python that can help you make sense of your data.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":85,\"Instructor Name\":\"Joe Marini\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Technology Industry Veteran\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2023-03-02T00:00:00\",\"Course Updated Date\":\"2024-08-15T00:00:00\",\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/advanced-python-working-with-data,https://www.linkedin.com/learning/advanced-python-working-with-data-potential-mojo\",\"Series\":\"Advanced\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Advanced\",\"LI Level EN\":\"Advanced\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Programming Languages\",\"Primary Software\":\"Python\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":7527.0,\"Visible Video Count\":27.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":677,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4410701\",\"duration\":49,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Python and Data: Made for each other\",\"fileName\":\"4312001_en_US_00_01_WX30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":774540,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Joe] One of the most popular, fastest growing disciplines\\nin the world of technology today is data science,\\nand the Python programming language\\nis at the center of it all.\\nHi, I'm Joe Marini.\\nI've been building software professionally\\nfor some of the best known companies in Silicon Valley\\nfor more than 30 years.\\nIn this course, we're going to start learning\\nabout some of the built-in features\\nthat make Python a great language to use\\nwhen working with data-intensive applications.\\nWe'll see how to perform common data operations\\nlike sorting and filtering,\\ndetermining basic information\\nlike minimum and maximum values,\\nand how to read and write data to and from other formats\\nlike CSV files used by spreadsheets\\nand JSON for interchange with other systems and programs.\\n\\nCome join me in my course\\nand take a look at what Python can already do\\nto help you make sense of your data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4409281\",\"duration\":208,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Getting Set Up\",\"fileName\":\"4312001_en_US_00_02_XR30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3121347,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] This course is intended for developers\\nwho are already familiar with the basics of Python\\nand want to learn about some of the features\\nrelated to working with data.\\nYou don't need to be a deep expert on the language\\nin order to take this course,\\nbut you do need to have an understanding of the basics\\nlike how to write functions, use variables, and so on.\\nThere are a couple of different ways\\nyou can work with the example code in this course.\\nI've set up a GitHub repository with the examples\\nand you can find it at this link.\\nThe course is organized into two separate folders.\\n\\nThe Finished folder contains all of the code examples\\nin their finished state,\\nso you can compare your code against them\\nas you work through the course.\\nThe Start folder contains the code examples\\nin their beginning state,\\nand this is the folder I will be working in\\nthroughout the course as we build towards the Finished state\\nfor each example.\\nIf you want to download the examples\\nand work with them locally on your computer,\\nthen that's easy enough to do.\\nYou can just go over here to the code menu in the local tab\\nand you can clone the repository or download a ZIP file\\nand use your favorite code editor to work on them.\\n\\nYou'll just need to make sure that you have Python\\ninstalled on your computer, at least version 3.10\\nin order to use the examples.\\nBut this repository has also been set up\\nwith a GitHub Codespace,\\nso you can just work directly online\\nwith nothing to install.\\nSo all you need to do is fork a copy of the repository\\ninto your own GitHub account\\nand then fire up a codespace here in the code menu.\\nThere's nothing to install.\\nYou'll get a complete environment and you'll be ready to go.\\n\\nNow, you can see I've already done that, right?\\nI've already got my codespace set up.\\nWhat you'll do is you'll click on this plus menu here\\nand create your own codespace.\\nSo I'm going to be using the codespaces in this course.\\nEither way works fine,\\nbut I'm going to be using codespaces throughout the course.\\nAll right, so let me open up my codespace\\nand show you what it looks like.\\nAnd when you create your own codespace,\\nit's going to take a few moments to spin up,\\nokay, so just be patient.\\nSo when the codespace starts,\\nyou'll see a browser-based version of Visual Studio Code.\\n\\nAnd here in the files list are all the files\\nthat you're going to need in the course.\\nYou also have a built-in terminal,\\nwhich if I go to the menu over here and the view menu,\\nI'll bring up the terminal,\\nand the terminal is how we'll run some of the examples.\\nAnd I can check to see if Python is already installed\\nand sure enough, there it is, all right.\\nYou're also going to want to make sure\\nthat you have the Python and Pylance extensions installed.\\nSo over here, this little four boxes is the extensions.\\n\\nAnd if I scroll down in my list,\\nyou'll see that here's Python and Pylance.\\nNow, this is my own codespace\\nso I have a bunch of extensions installed\\nand you don't need to worry about these.\\nAll you need to worry about are Python and Pylance\\nand you can find them in the marketplace\\nif you don't already have them.\\nJust go up to this search box, type in Python.\\nIt should be the first result that comes up.\\nIt's this one here from Microsoft.\\nYou can see it's got a ton of downloads.\\nAnd if you want to install this one,\\nin fact, this is the only thing that you'll need to install\\nbecause it will automatically install Pylance and Jupyter\\nand a couple of other things\\nto give you a great Python development experience.\\n\\nSo once you've done that, you're pretty much ready to go.\\nAll you need to do is open Start menu,\\ngo into chapter one and you're ready to start coding.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4409282\",\"duration\":209,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"The sample data set\",\"fileName\":\"4312001_en_US_00_03_XR30\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3135001,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] I've included a sample data set\\nto work with in this course.\\nNow, not all of the examples are going to use it,\\nbut it's a significant set of data\\nthat represents a real world use case\\nof working with data using Python.\\nSo the data set that we'll be working with\\ncomes from the US Geological Survey,\\nand contains information\\nabout all of the measured earthquakes around the world.\\nAnd this here is the homepage\\nfor the USGS Earthquake Information Service.\\nSo I'm going to scroll down here\\nand I'm going to click on this little\\nreal-time notifications link.\\n\\nAnd when I do that, that takes me to the data feed page.\\nIf I scroll down a little bit, and if, by the way,\\nyou can just go right to this link if you want to.\\nSo you can see here that there are some real-time data feeds\\nabout earthquake information in different formats.\\nSo there's an ATOM data feed\\nand we've got some spreadsheet data over here.\\nBut the one that we're going to be using is this one,\\nthe GeoJSON Summary Feed.\\nSo let's go ahead and click on that link.\\nAnd that will take us to this page,\\nwhich is the information page\\nthat describes the GeoJSON data format\\nand provides several links to the actual earthquake data.\\n\\nAnd you can see that the data is available\\nin several different feeds.\\nSo we have for the past hour, right?\\nWe have the past day, here's the past week,\\nand the past month.\\nAnd it's several subsets organized\\nby the size of the earthquake.\\nSo we've got significant earthquakes\\nall the way down to everything, right?\\nSo I'm going to click on one of these data feeds\\nand you can see it's just raw JSON data.\\nOkay, so let's click back.\\nAll right, so now if we scroll down a little bit,\\nthis section right here describes the format of the data.\\n\\nAnd it starts with a description of the data\\nin this metadata field.\\nSo when it was generated, here's the URL to the data,\\nthe title of the data set, the number of earthquakes,\\nand so on.\\nAnd so we're going to skip over this B box item\\n'cause we're not going to be using it.\\nAnd then there's an array of data,\\nsomewhat confusingly called \\\"features\\\",\\nwhich is where the detailed earthquake data itself\\nis contained.\\nNow we're not going to be using all of these fields.\\n\\nBut that's okay.\\nto explore on your own.\\nAnd you can click each of these links\\nto get more information about that particular data field.\\nAll right, so let's go take a look at the actual data set\\nthat I've included with the course.\\nAnd you can see here in the list,\\nI've got this 30DayQuakes.json file.\\nSo I'm going to go ahead and click on this.\\nAnd this data, I've downloaded it from the website,\\nso we can use it locally.\\nIt represents all of the measured earthquakes\\nfor the past 30 days.\\n\\nAnd remember, if you go look at this link now,\\nthe data is obviously going to be different for you\\nbecause I captured this at a specific point in time.\\nAnd you can see it's a pretty large data file.\\nIt has almost 12,000 earthquakes in it.\\nAnd if we look at the first item,\\nwe can see it had a magnitude of 1.54.\\nIt was someplace in California.\\nAnd let's see if we see the felt reports.\\nLooks like nobody reported feeling it.\\nDown here, we have the geometry of where the quake happened.\\nSo we're going to be using this data throughout the course\\nand we'll be looking at a variety of ways to use Python\\nto parse this data set and get some insights out of it.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5959202\",\"duration\":211,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Tour of CoderPad\",\"fileName\":\"4312001_en_US_00_04_FY25Q1_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"updateType\":\"ADDED\",\"solutionVideo\":false,\"challengeVideo\":false,\"exerciseFileUrl\":\"CoderPad URL for Advanced Python: Working With Data\u00a0\\nhttps://screen.coderpad.io/work/dashboard/campaign/1078248/questions/\",\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":215,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6680180,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] This course includes\\nautomated code challenges that appear\\nwhen you click on the challenge links\\nin the course's table of contents.\\nEach challenge includes instructions\\nand a couple of code editors you can use\\nto create and test your own solution to the challenge.\\nThese challenges are hosted by CoderPad,\\nand they appear in the same area of the course page\\nwhere you watch the course's videos.\\nWe recommend using a desktop browser\\nfor the best experience with code challenges,\\nbut you can use the LinkedIn Learning mobile app\\nif you prefer.\\n\\nThe code challenge has four areas.\\nIn the top left, you'll see instructions.\\nThere's a code editor here in the top right\\nfor you to write your answer.\\nThere's another code editor down here in the bottom right\\nwhere you see how your code will be used.\\nAnd there's a console for output in the bottom left.\\nYou can use these drag handles\\nto allocate space as you like\\nas you're working with the challenge.\\nTo get even more horizontal space for the code editors,\\nyou can collapse the course's table of contents on the left.\\n\\nEach challenge has instructions\\nthat include a description of the challenge\\nand the challenge's parameters\\nand the desired result.\\nDepending on the challenge,\\nyou will see some additional information\\nin the instructions,\\nsuch as an explanation of the parameters\\nthat your code will be given,\\nalong with some examples\\nof what the expected output might look like.\\nSo create your answer\\nin the top right code editor.\\nThere are comments in the starting code\\nshowing where to put your solution.\\n\\nAnd when you click test my code,\\nyou'll see a message indicating\\nwhether your code returned a correct result.\\nSo let's go ahead and try that.\\nI'll click on test my code,\\nand you can see that I'm clearly returning the wrong answer.\\nSo I get a message saying,\\n\\\"Hey, something isn't working.\\\"\\nIt shows you what my code returned.\\nNow if your code isn't successful,\\nyou can ask for help.\\nAnd the way that you do that\\nis by modifying these two variables,\\nshow expected result and show hints.\\nSo I'm going to change each of these to true.\\n\\nAll right, and then I'll execute my code again.\\nAll right, in this case,\\nyou can see that my code is still wrong,\\nand it shows that my code returns zero,\\nbut the expected result was 19.\\nAnd then, I'm given a hint\\nto help me along with the challenge.\\nNow, the code editor in the lower right\\nshows you how your code is going to be used.\\nAnd in some cases, you can change that code\\nto experiment with different cases.\\nLet's go ahead and return the right answer.\\n\\nSo I'm going to return max(numbers).\\nAll right, and now when I run this code,\\nright, we can see that the answer is now correct,\\nand 19 sure is the largest.\\nI'll make the largest 25,\\nand I'll change some of these other numbers.\\nI'll make this one a six.\\nAnd let's run it again.\\nAnd now we can see that my code\\nis, again, returning the right answer.\\nNow, regardless of whether your answer is successful,\\nyou're going to see messages here\\nin the console output in the lower left.\\n\\nAnd if any of the messages\\nare too long to fit in that area,\\nyou can scroll sideways to see all of the text.\\nWhen you've finished each code challenge,\\nreturn to the course's table of contents\\nand click the next video to see my solution\\nso you can compare it with your own.\\n\"}],\"name\":\"Introduction\",\"size\":13711068,\"urn\":\"urn:li:learningContentChapter:4408350\"},{\"duration\":2445,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4408347\",\"duration\":80,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Using built-in functions\",\"fileName\":\"4312001_en_US_01_01_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to articulate the basic Python functions used for working with data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1238208,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We are going to start off by taking a look\\nat some of the built-in functions\\nthat Python already has for working with data.\\nAnd these are functions that you can just use\\nout-of-the-box, without having\\nto install any additional libraries or modules.\\nPython provides basic functions\\nfor some very common operations related\\nto working with data, such as the min and max functions,\\nwhich identify the smallest\\nand largest value in a given data set;\\nthe any and all functions,\\nwhich can be used to determine whether any\\nor all of the items in a data set match a certain criteria.\\n\\nThen there's the sum function,\\nwhich as you've probably guessed,\\nis a quick way to add up a set of data values.\\nThen of course, there are the sorted and sort functions,\\nwhich are used to sort data in a particular order.\\nThere's the filter function, which is used\\nto selectively determine which values to remove\\nand which ones to keep in a data set.\\nAnd of course, there's the map function,\\nwhich is used to transform values\\nfrom a dataset into different kinds of values.\\n\\nSo we'll take a closer look at each of these functions\\nthroughout this chapter and how they can be used\\nto operate on data sets.\\nAnd as you can see,\\nthere's quite a bit that Python can do right away\\njust using some of the built-in functions in the language.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4414262\",\"duration\":483,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Finding min and max values\",\"fileName\":\"4312001_en_US_01_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to use Python functions for determining minimum and maximum values in a data set.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7249236,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this example,\\nwe'll see how to use the min and max functions\\nto find the smallest and largest data values\\nin a given data set.\\nAnd so here I am in my GitHub repository.\\nAnd remember, you're going to want\\nto make a fork of this in your own GitHub.\\nAnd I've already got a codespace up and running\\nso you'll need to make one of your own codespaces.\\nSo I'm going to bring up my codespace.\\nAll right.\\nI'm going to go into the Start folder.\\nAnd in chapter one, I'm going to open the minmax file.\\n\\nSo here at the top of my code, I have two lists, right?\\nOne is a set of numbers and the other is a set of string.\\nSo let's use the min function\\nto find the smallest value in each of these lists.\\nSo I'll print out the result,\\nand I'm going to use an f-string to do this.\\nAnd I'll just simply write, \\\"The minimum value is,\\\"\\nand I'll call the min function on the values list.\\n\\nAnd I'll do the same thing,\\nmake a copy of that line,\\nand I'll call the min value on the strings list.\\nOkay?\\nAnd then let's do the same thing for the max function.\\nSo I'll just make a copy of these two lines and paste,\\nand just call max in both cases.\\nAll right.\\nAnd let's go ahead and run what we have.\\nNow remember, we can do this a couple of ways.\\nI can open up the built-in terminal here\\nin Visual Studio Code,\\nand I can go into my Start folder,\\nand I can go into chapter one,\\nand then I'm going to run using the python command,\\nthe python command, I'll call minmax.py.\\n\\nAnd there you can see the output.\\nOf course, you can also,\\nif you have the Python extensions installed in VS Code,\\nyou can just right click\\nand choose Run Python File in Terminal.\\nThat also works.\\nBut here in the results, you can see\\nthat the smallest number in the list is 1.6\\nand the large number is 6.1.\\nAnd then we got these two string results.\\nNow, what does it mean\\nto have a small and large string value?\\nIn this case, the min and max functions\\nare using the alphabetic order of each word\\nto determine what is small and what is large.\\n\\nBut what if we wanted\\nto use a different property of the strings\\nto measure what small and large mean?\\nTo do this, both functions,\\nlet us define what's called a key function.\\nAnd this function's job is to return a value\\nthat can be used in the min or max calculation.\\nSo for example,\\nif we wanted to find the shortest and longest strings,\\nwe would specify a key function\\nthat returned the length of each string.\\nSo let's go ahead and close this terminal.\\nAnd what I'm going to do\\nis I'm going to copy these two lines,\\nand I'll paste it down here in the key area.\\n\\nAnd I'll need to use strings on both of these.\\nAnd I want min and max for this example.\\nAll right, so then what I'm going to do\\nis I want to key off the length of each string.\\nSo what I'm going to do is specify the key function,\\nand I'm going to call the built-in len function in Python,\\nso that returns the length of each string.\\nAnd I'll do the same thing here.\\nOkay, so now let's run this again, right?\\nAnd now you can see that the smallest length string is 1,\\nand the longest length string is 18.\\n\\nSo that's different than the results we got here.\\nThese were alphabetical.\\nThese are an order of their length, okay?\\nSo now let's take what we've learned\\nand apply it to our earthquake data set.\\nSo close this.\\nAll right, first I'm going\\nto uncomment these two lines that open the JSON file,\\nand load the JSON code.\\nNow, if we go back and look at the data, right?\\nSo remember that inside the features list,\\neach one of these seismic events\\nhas a magnitude property, okay?\\nSo let's find the smallest and largest earthquakes\\nas measured by magnitude.\\n\\nLet's go back to my code.\\nSo I'm going to do a couple of things.\\nFirst, I'm going to print out the title of the data set.\\nAnd so you can see here\\nthat I've loaded the JSON code into this data variable,\\nand that gives me this dictionary, right?\\nSo inside here I'm going to operate\\non the metadata and the title properties.\\nSo I have to index into that dictionary using metadata,\\nand then title.\\nAnd then I'm going to follow that up\\nwith the number of earthquakes.\\n\\nSo I'm going to print the length\\nof the features list, right?\\nBecause remember,\\nAll right.\\nNow I'm going to define a function called getmag.\\nAnd this will take a specific earthquake\\nand return the magnitude.\\nSo I'm going to write some code that says\\nthat the magnitude is equal to the passed in data item,\\nand I'm going to get the properties,\\nand I'm going to look\\nat the magnitude field of each one, right?\\nAnd again, if we look here,\\nso here in each one of these features, there's a properties,\\nand then the first properties is magnitude.\\n\\nSo that gives me the magnitude value.\\nNow, that might be null,\\nso I have to check to see if magnitude is None,\\nbecause if it is,\\nthen I'm going to set magnitude equal to 0.\\nOtherwise, I'm going to return a floating point value.\\nDo it that way.\\nI'm going to convert magnitude to a floating point value, okay?\\nSo now I just need to use the min and max functions\\nto find each value using getmag as the key provider.\\n\\nAll right?\\nSo I'll print out the minimum,\\nand I'm going to look at the data,\\nand I'm going to look at the features list.\\nAnd my key function\\nis going to be the getmag function, right?\\nAnd then I'll do the same thing for max.\\nSo this will give us the min and max seismic items.\\n\\nAnd then I'll just go back up here,\\nand I'm going to comment out the previous examples,\\nso that they don't get in the way.\\nAll right.\\nSo now let's run our updated code.\\nOkay?\\nAnd we can see that here,\\nwe've got the title, all right?\\nAnd here we have the number of events.\\nSo that's these two lines right here.\\nPrint the title and print the length of the data set.\\n\\nAnd then we get the minimum value, right?\\nSo here we have 1,\\nthis one is the magnitude of -1.31, okay?\\nSo that's the smallest magnitude.\\nAnd then we look at the next JSON output\\nand we see here that the magnitude is 6.3, all right?\\nSo we've got our smallest and largest earthquakes.\\nSo you can see\\nthat the min and max functions are pretty flexible\\nin that you can use them for basic data types,\\nbut they can also be used with more complex data structures\\nby using the key function\\nto provide a value to measure them with.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4415164\",\"duration\":446,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data utility functions\",\"fileName\":\"4312001_en_US_01_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to apply several Python functions to perform a variety of operations on data sets.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6665625,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now let's take a look at some\\nof the useful utility functions that Python provides.\\nSpecifically the any, all and sum functions.\\nAnd so here in chapter one,\\nI'm going to open up the utility.py file.\\nAnd once again, here are my sample code.\\nI have a list of integer values already defined.\\nThe any function can be used to see if any value\\nin a given sequence evaluates to a Boolean true value.\\nAnd remember that for integers,\\nany non-zero value evaluates to a true Boolean.\\n\\nThe all function is similar except that it determines\\nwhether all of the values in a sequence are true.\\nAnd then finally, the sum function will add\\nup all the values in a sequence.\\nOkay? So let's give each of these a try.\\nI'm going to print out any\\nand I'm just going to call it on the values array.\\nAnd then I'll do the same thing.\\nI'm just going to paste this down here\\nand call all instead, right?\\nAnd then I'll call sum on the values list.\\n\\nRight?\\nNow in this particular list,\\nit's easy to see that you know, yes,\\nin fact there are values that are Boolean true, right?\\nSo when I run this in the terminal, right,\\nyou can see that yes, there's\\nat least one of the values evaluates the true.\\nNow, the second function, all returns false, right?\\nAnd that's because there's a zero right here\\nat the front of the list, right?\\nSo if I take this zero out and save this\\nand then I run it again, right?\\nWell, now you can see that the result is true\\nbecause now all of these values are true, right?\\nAnd then you can see the result\\nof the sum function down here.\\n\\nSo sum just adds them all\\nup and we get the result of 15, right?\\nOkay.\\nSo we can use these functions\\nwith more complex data structures, but unfortunately,\\nunlike the min and max functions\\nthat we used in a previous video,\\nthey don't let us specify a callback to easily\\nprovide a value from the data.\\nBut that's okay.\\nWe can easily improvise this using Python.\\nSo let me close this terminal.\\nAll right.\\nSo let's take a simple example.\\n\\nAll right?\\nAre there any earthquake reports that were felt\\nand reported by more than 25,000 people?\\nOkay, so let's go back\\nto the data structure here in my 30 day quakes.\\nAnd if we scroll down,\\nyou'll see that there's a felt property, okay?\\nWhich can be null, which means\\nthat nobody felt that particular earthquake.\\nSo we have to take that into account.\\nSo what I'm going to do is write an expression\\nthat will iterate over each\\nof the events and inspect this felt property right here.\\n\\nAll right?\\nSo what I'm going to do is first I'm going\\nto uncomment this code to load our data, all right?\\nAnd so now I'm going to write print\\nand I'm going to call the any function.\\n'Cause we want to know\\nif there were any reports that were more than 25,000.\\nAnd then my expression is going to be,\\nI'm going to have a quake variable\\nand that's going to represent an individual quake.\\nSo I have to look inside the properties,\\nand use double quotes for that.\\n\\nSo I'm going to look inside the properties\\nand I'm going to get the felt property, right?\\nSo if that value is not none, all right?\\nLet me just collapse this down so we have more room.\\nSo if that value is not none\\nand the quake properties felt is greater\\nthan 25,000 for quake in\\nand we're going to call this on the entire features list.\\n\\nAll right?\\nover each one of these items inside this features array.\\nAnd I'm going to get a quake variable each time, right?\\nAnd I'm checking to make sure\\nthat the felt is not null and it's greater than 25,000.\\nOkay?\\nSo behind the scenes this will create a generator that loops\\nover each element in the list and evaluates the expression.\\nAll right?\\nSo let's go ahead and let's comment out these previous guys.\\nAll right?\\nAnd now let's run our example.\\n\\nSo let's go ahead and go into chapter one.\\nAnd I'm going to run the utility.py file, okay?\\nAnd sure enough, we can see that the result is true, yes,\\nthat there was at least one quake that was reported\\nby at least 25,000 people.\\nOkay? All right, let's keep going.\\n\\nLet's answer the question,\\nhow many quakes were felt by more than 500 people?\\nSo let me some room here.\\nAll right.\\nSo in this case, I'm going to use a loop expression\\nand I'm going to print out, in this case,\\nI want to know how many,\\nso I want to hear the term, how many, I think sum, right?\\nSo I'm going to sum up, and I'm going to just copy this.\\nSo quake properties felt is not none\\nand quake properties dot felt.\\n\\nSo I'm going to copy this, paste that in.\\nBut in this case, I'm interested\\nin being greater than or equal to 500, okay?\\nAnd I'm going to write for quake in data features.\\nAll right?\\nSo if this expression evaluates to true, right?\\nThis one right here, then that's going to come back\\nwith a Boolean value of one, right?\\nWhich has an integer value.\\n\\nOtherwise it will be zero, which means false.\\nSo that's how I'm going to be able to sum\\nup each one of these, right?\\nSo I'll add a one for yes and add a zero for no.\\nAll right.\\nHow many quakes had a magnitude of six or larger?\\nAll right, so once again, we're going to use the sum function.\\nSo I'm just going to copy this expression here and paste it.\\nAnd in this case, okay,\\nwe want to know how many quakes had a magnitude.\\nSo I'm going to need to look at the magnitude property.\\n\\nSo I'll change that.\\nAnd then I want to know if it was six or larger.\\nSo I'm going to check to see\\nif the magnitude was greater than or equal two six, okay?\\nSo let's go ahead and run this now.\\nSo let's go back down\\nto our terminal and we'll run this again.\\nAlright, now we can see\\nthat there are 12 quakes that were felt\\nby at least 500 people.\\nAnd there were five quakes that\\nwere magnitude six or larger.\\n\\nAnd here again, we can see that just\\nby using some of the built-in Python functions\\nwe're able to build some useful queries\\non our data to answer some questions that we have.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4408348\",\"duration\":309,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Sorting data\",\"fileName\":\"4312001_en_US_01_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"exerciseFileDisplayText\":\"01_07: 1078258\",\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, the learner will be able to implement sorting of data values using Python.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4631148,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Sorting is probably one\\nof the most common operations performed on data,\\nand we're going to take a closer look\\nat that in this example.\\nSo here in my code space,\\nI'm going to click on the sorting.py file.\\nAnd in my example code, I have two lists, right?\\nOne with integers and one with names.\\nThere are two functions built in for sorting in Python.\\nThere's the sorted function,\\nand then there's the sort function.\\nSo let's take a look at each one\\nand I'm going to collapse this down to get some more room.\\n\\nAll right, the sorted function is a top-level\\nbuilt-in function in Python\\nand you can use it on any sequence of values.\\nIt returns a new list of sorted data\\nand does not touch or change the original list.\\nOkay?\\nSo let's try that out on our numbers.\\nSo I'm going to write result1 is equal to,\\nand I'm going to call sorted on the numbers list\\nand then I'll just print that out.\\n\\nThe list object itself also has a sort function,\\nwhich sorts that list in place.\\nSo it doesn't return a new list,\\nit just changes the actual list in place.\\nSo I'm going to call on the names list.\\nI'm going to call the sort function.\\nAnd in this case, I'm going to use the reverse property,\\nor the reverse argument, and I'm going to set that to true\\n'cause I wanted to go in reverse order\\nand I'm going to print out that result.\\nAll right, so we've got two sorting operations.\\n\\nLet's open up the terminal.\\nRight control back tick,\\nand I'm going to go into my start folder in chapter one.\\nAll right, and let's run this example.\\nI'll run Python sorting.py and sure enough,\\nyou can see in the output we've got two sorted lists.\\nSo the numbers are now sorted in ascending order, right?\\nSmallest number to the largest number.\\nAnd then the names list is in reverse order.\\n\\nIt goes alphabetic last, all the way down\\nto alphabetic first.\\nOkay?\\nAnd of course, we can use the sorting functions\\non more complex data just like we did earlier\\nin the min and max functions by specifying a key function.\\nAnd I think you can see where this is going,\\nwe're going to try this on our earthquake data.\\nSo let's go ahead and close that terminal,\\nand let's comment out these two previous examples.\\nAll right, so first I'm going to uncomment the code\\nthat loads our JSON data.\\n\\nAll right.\\nAnd you can also see\\nthat I have the same function that I defined\\nin an earlier example that retrieves the magnitude value\\nfrom a given earthquake structure.\\nOkay, now I just need to use the sort function\\nto sort the data.\\nAnd in this case, I'm going to sort\\non the magnitude value in descending order.\\nSo I'll write data,\\nand I'm going to sort the features list in there, right,\\nand again, just to refresh the memory,\\nif we look at the quakes, right,\\nthe features is what contains all the earthquakes.\\n\\nOkay?\\nSo I'm going to have to sort this guy in place.\\nAll right?\\nin this case I'm going to specify the key\\nis my get magnitude function.\\nThere we go, get mag.\\nAnd we'll set reverse equal to true.\\nAll right?\\nAnd then I'm going to print out the first 10 entries,\\nwhich obviously are going to be the earthquakes\\nthat had the largest magnitudes.\\n\\nSo I'll write for i in range to 10,\\nand I'm going to print out,\\nI'll print out data features\\nand then I have to index into that list with i.\\nAnd then once I have that item,\\nI have to get the properties of that item\\nand then I have to get the name of the place.\\nAll right, and if we look at the data again,\\nyou can see that there is a place.\\n\\nWhere is that?\\nThere's the tight, that's not it.\\nRight here, so the name of the place,\\nI'm going to print out the name of the place,\\nthe top 10 largest earthquakes ordered by the place name.\\nOkay?\\nSo let's go ahead and save this\\nand let's go back to the terminal,\\nand let's try this one more time.\\nSo I'll run sorting.py and sure enough,\\nyou can see that the result lists the top 10 quakes\\nfrom this data set,\\nand a lot of them appear to be in the Pacific, right?\\nWe've got New Zealand, Indonesia, Vanuatu, and let's see,\\noh, yep, California makes the list.\\n\\nSo you can see how easy it is to sort complex data\\njust by using the built-in sort function.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4412627\",\"duration\":409,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data filtering\",\"fileName\":\"4312001_en_US_01_05_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"exerciseFileDisplayText\":\"02_06: 1078259\",\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will understand how to filter data values using Python.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6121090,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Sometimes when you're working\\nwith a large dataset, you want to be able to focus\\non a subset of the overall data.\\nIn other words, you want to be able to filter out the data\\nthat's not interesting and keep the data that is.\\nSo in this example, we're going to see\\nhow to use the filter function to achieve this.\\nWe're here in Ch_1, let's open up filtering.py.\\nAnd here in my example code, I have a couple of functions,\\nalong with some predefined sequences of data, okay?\\nI've got a couple of sequences, this one's numbers,\\nand this one is random letters,\\nand I'm going to use the filter function\\nto refine each of these datasets.\\n\\nThe filter function works by specifying two arguments.\\nThe first is a function that returns true or false,\\ndepending on whether a particular data item\\nshould be retained or dropped,\\nand then the second argument\\nis the dataset to operate on, right?\\nSo the filter function calls that second argument,\\nthat callback, with each item in the sequence\\nso it can be tested against whatever logic you've specified\\nand whether it should be retained or dropped.\\nSo, for example, if I wanted to filter\\nall of the even numbers out of this dataset,\\nI could use this filterEvens function right here\\nthat I've already defined,\\nwhich returns false if a given number is even,\\nand true if it is odd, okay?\\nSo let's go ahead and write that function.\\n\\nI'm going to write odds =,\\nand I'm going to create a new list by using the list function\\nbecause the filter function actually creates an iterator.\\nSo I'm going to use the list function\\nto just exhaust that iterator and create a new list.\\nSo I'm going to call the filterEvens function\\non the nums list, all right?\\nAnd similarly, suppose I wanted to filter out\\nthe uppercase characters from this string, right?\\nI could then use another function\\nlike this filterUppers function,\\nwhich tests to see if a given character is uppercase or not,\\nand again it returns false if it's uppercase,\\nand true if it's not.\\n\\nSo I can call this lowers.\\nAnd once again I'll create a list,\\nand I'm going to call filter,\\nand I'm going to call filterUppers on my chars list.\\nAll right, and then I'm going to print out each one.\\nSo I'll print out odds, I will print out lowers.\\nAll right, so let's go ahead and run this.\\nSo into the terminal and into Start/Ch_1.\\n\\nAnd let's see what's in here.\\nAll right, it's called filtering.\\nSo I'm going to run python on the filtering example.\\nAnd sure enough, you can can see in the result\\nthat all the odd numbers are now remaining,\\nthe evens are gone.\\nAnd sure enough, all of the uppercase letters\\nare now gone from the characters, all right?\\nSo let's try using this\\non our massive earthquake dataset, all right?\\nSo let's close the terminal\\nand let's comment out the previous example,\\nand let's load our JSON, right?\\nSo this might surprise you, okay?\\nBut not all of the seismic events in the dataset\\nare actually earthquakes.\\n\\nSo if I click on this, right?\\nSome of these seismic events\\nare actually the result of explosions\\nor other events that are picked up\\nby the seismic recording sensors.\\nSo I'm going to use the filter function\\nto see how many of these events are not true earthquakes.\\nSo I'm going to define a filter function, right?\\nI'm going to call it notAQuake, all right?\\nAnd it's going to take a quake as an argument.\\n\\nAnd so I'm going to check to see if the quakes,\\nI'm going to look in the properties\\nand I'm going to look at the type, okay?\\nAnd if we look at the dataset,\\nif I scroll down, you'll see that there's a, where is it?\\nType is right here, right?\\nAnd type is going to usually be earthquake,\\nbut it might not be, right?\\nIt might be something else.\\nSo I'm going to check to see if \\\"type\\\" == \\\"earthquake\\\".\\nAnd if it is equal to earthquake, I'm going to return false,\\notherwise, I'm going to return true.\\n\\nAll right, now I just need to filter this dataset\\nwith the filter function.\\nSo I'll name the result events, and I'll create a new list.\\nAnd I'm going to filter out, I'm going to call notAQuake,\\nand I'm going to call that on,\\nremember, we loaded this into the data variable,\\nso I'll pass in the data,\\nI'm going to pass in the features list, all right?\\nAnd then I'm going to print the results.\\n\\nSo I'm going to print out the total non-quake events.\\nAnd that's going to be the length\\nof the events list that I just created, right?\\nAnd I'm going to print out the first 10.\\nSo I'll make a little loop here\\nand I'll print out the events,\\nand I'm indexing it to each one.\\n\\nAnd each one of those is going to be a quake,\\nso that means that there's a properties,\\nand I'm going to print out the type.\\nAll right, so let's save this.\\nAnd let's go ahead and open up our terminal one more time.\\nAnd let's go into Ch_1,\\nand I'm going to run the filtering example.\\nAnd sure enough, we can see that there are 187 events\\nthat are not earthquakes out of the, what?\\nAlmost 12,000 total, right?\\nAnd as we're printing each one,\\nyou can see we got explosion, we got quarry blast, right?\\nWell, I guess those are different from explosions\\nfor some reason.\\n\\nSo now we know that there are are events in here\\nthat are not necessarily earthquakes, right?\\nThey're seismic events, but they're not earthquakes.\\nSo we're going to come back to this\\na little later in the course,\\nbut for now, you can see just how easy it is\\nto perform data filtering\\nusing just the built-in Python features.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4414263\",\"duration\":530,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data transformation\",\"fileName\":\"4312001_en_US_01_06_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"exerciseFileDisplayText\":\"03_05: 1078260\",\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to use various transform functions to take sets of data and perform calculations that produce new, derived data sets.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7936807,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Data doesn't always come\\nin the form that you would like it to.\\nSo you might find yourself in a situation\\nwhere you need to transform the data\\nfrom one format into another.\\nPython provides a function that can help with this.\\nIt's called the map function.\\nThe map function creates an iterator\\nthat takes one or more sequences of values\\nand produces a new sequence by applying a given function\\nto each value in the original sequences.\\nSo let's try a couple of simple examples\\nand I'm going to open up my transform.py file.\\n\\nAll right, so let's imagine\\nthat I've got a couple of sample data sets here, right.\\nSo down here, if I scroll down\\nI've got a couple of sequences, right.\\nThis one here is numbers, actually.\\nWell, they're both numbers, okay.\\nAnd so let's imagine that I wanted to convert each\\nof the numbers in this nums sequence\\nto the square of the original number that's in the list.\\nAll right. So I can easily do this with the map function.\\nSo I'm going to create a new sequence called squares\\nand I'm going to use the list function to create a new list.\\n\\nAnd I'm going to use the map function to do this.\\nSo the first argument\\nto the map function is the function\\nthat I want to be applied to each item in the list.\\nAnd you can see if I scroll up\\nI already have a function here called squarefunc\\nand it returns the number that's given\\nto the second power, right.\\nSo its square is the number.\\nSo I'm going to call the squarefunc\\nand I'm going to use the nums list\\nand then I'll just print out the result.\\n\\nSo the map function's going to call that squarefunc\\nwith each item\\nand we can go ahead and run this to see what happens.\\nSo I'll save this and I'm just going to right click\\nand choose run in the terminal.\\nAnd the output shows the resulting list, right.\\nIt contains all\\nof the original numbers multiplied by themselves.\\nNow that's kind of cool\\nbut map actually gets a little bit more powerful than this.\\nSo suppose I wanted to convert each of the numbers here\\nin the grades sequence into a letter grade, right.\\n\\nYou know, A, B, C, and so on.\\nSo I have a function here called toGrade\\nand that takes an argument, right, a number,\\nand you can see that it uses a series\\nof if and elif statements to assign a letter based\\nupon the range that the given value falls into.\\nSo more than 90, you get an A, right.\\nBetween 80 and 90 you get a B, and so on.\\nSo let's go back to the code.\\nMake this smaller for a second.\\nSo I'm going to do something similar.\\nIn this case,\\nand actually I'm going to call that lettergrades\\nbecause I already have the grades.\\n\\nSo lettergrades, actually, you know what.\\nHere's what I'm going to do.\\nFirst, I'll just sort it in place.\\nI'll just simply assign grades the result\\nof the sorted function.\\nThere we go. Okay.\\nAnd then I'll make a new list called letters\\nand I'm going to make a list function.\\nAnd then I'm going to call map again.\\nAnd this time I'm going to call the toGrade function\\nand I'll pass in the grades.\\nSo now I have a sorted list of letter grades\\nand I'll just print out the result.\\n\\nSo let's go comment out the previous example\\nand let's go back to our terminal.\\nAll right, and let's run this again.\\nAnd now you can see that I've got a list\\nof sorted letter grades.\\nNow the map function gets even more useful\\nwhen we want to reshape a data set into a different form.\\nSo let's go back and look at the earthquake data.\\n\\nAnd let's close the terminal.\\nAll right, so if we look at the earthquake data\\nwe've been working with,\\nthere's quite a few of these properties\\nand fields that we're not making use of, right.\\nSo we can use the map function to create a data set\\nthat doesn't contain the fields that we don't care about.\\nSo let's go back to the code\\nand let's comment out the previous example\\nand then let's load our data.\\nSo what we're going to do first is,\\nlet's use the filter function to reduce the data set down\\nto just the largest magnitude events, okay,\\nthose that are larger than six.\\n\\nSo uncomment this function.\\nAnd remember we saw how to use the filter function\\nin a previous video.\\nSo that's this code is going to do here.\\nIt's going to filter down the list\\nso only large seismic events are left.\\nSo now we're going to create a function\\nthat simplifies our data structure\\nto just include a few properties, okay,\\nthe place, the magnitude, and the date of each quake.\\nSo what I'm going to do is write a function called simplify\\nand that's going to take a quake as an argument, all right.\\n\\nAnd I'm going to return a new JSON data structure,\\nand that JSON data structure is going to have a place in it.\\nAnd that is going to be the quake's properties.\\nAnd I'll just use the existing place, okay.\\nAnd then I'll have the magnitude\\nand that is going to be the properties\\nthat's going to be the mag property.\\n\\nAnd then there's going to be the date.\\nAnd what I'm going to do is if I scroll back up here\\nyou'll see that I've imported\\nboth the pretty print and the JSON modules.\\nWhat I'm going to do is create a date.\\nIn order to do that\\nI'm going to have to import the datetime module, right.\\nLet's scroll back down.\\nSo for the date\\nwhat I'm going to do is create a string representation\\nof the date.\\n\\nSo let's do that first, right.\\nThen I'm going to call the datetime module.\\nI'm going to use the date class\\nand I'm going to call fromtimestamp.\\nAnd again, if you look inside the quakes, right,\\nwe have, where's the time?\\nSo this right here, this property is the number\\nof milliseconds from the start of the Unix epoch\\nwhich represents the time and date of this quake.\\nSo what I need to do is get that value.\\nSo I'm going to pass into from timestamp,\\nI'm going to pass in the quake's properties\\nand I'm going to call, see, get the time property.\\n\\nAnd I've got to divide that by 1000\\n'cause I don't want to use milliseconds.\\nI want to use seconds.\\nOkay. And so now I'm going to return that.\\nI've already got my filtered results right here,\\nright, in the results list.\\nSo I'm just going to call this again, all right.\\nSo I'm going to type results\\nand I'm going to create a new list\\nand I'm going to map.\\n\\nI'm going to call the simplify function\\non the original results.\\nSo here's where I get all the filtered data\\nand then I'm going to take that result\\nand put it back in the same variable.\\nOkay. And then I'm going to use\\nthis module here called pprint, right.\\nThis will pretty print a JSON structure for me\\nso I don't have to do a whole lot of coding here.\\nI'm just going to call pprint.pp\\nand I'm going to pass it the results JSON.\\n\\nAll right. Okay.\\nSo we've simplified the list, right.\\nWe've gotten only the big magnitude items\\nand then we've simplified the data structure to get rid\\nof all those fields we don't need.\\nAnd then we're going to print the results out.\\nSo let's go into the terminal\\nand this is the transform function.\\nSo I'm going to call python transform.\\nAnd sure enough, here in the output you can see\\nthat the new data structure is much simpler\\nthan the original, right.\\n\\nAll the other fields are now gone.\\nI only have place, magnitude, and date.\\nRight, place, magnitude, and date.\\nAnd the new data set is much easier to read\\nand it only contains the data that I care about.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:66b5285634507dd22a295ca6\",\"duration\":900,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Build a data summary\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1078258\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:5963123\",\"duration\":188,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Build a data summary\",\"fileName\":\"4312001_en_US_01_08_FY25Q1_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"updateType\":\"ADDED\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":194,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6264643,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] All right, let's take a look\\nat my solution for this challenge.\\nAnd remember, it's not important\\nthat your code be exactly the same as mine.\\nThere's usually multiple ways to solve challenges like this.\\nAnd the important thing to do\\nis just learn from other people's examples.\\nThis challenge consists of four parts.\\nSo there were four different results we needed to calculate\\nand we'll go through each one of them.\\nSo the first part was to find the total number\\nof earthquakes or well, seismic events.\\n\\nAnd this was a bit of a trick question\\nbecause that data is contained\\nright within the first part of the file.\\nSo I just used the count data field inside the metadata\\nto get the total number of quakes.\\nOf course, if you wanted to use the length function\\nto get the length of the features array itself,\\nthat's also fine.\\nOkay, so moving on.\\nThe second part was to find out how many events were felt\\nby at least 100 people.\\nSo this part of the solution calls for the use\\nof the filter function\\nand my filter,\\nwhich is defined in this little utility function\\ncalled feltreport, it just gets rid\\nof any seismic events where the value\\nof the felt property is not at least 100.\\n\\nSo after I filtered out all of the events that don't meet\\nthat criteria, I just get the length of the resulting list\\nand that tells me how many events were felt\\nby at least 100 people.\\nAll right, let's move on.\\nSo the third value we had to calculate\\nwas the name of the place\\nthat had the seismic event that was felt\\nby the most number of people.\\nSo when you hear the word most,\\nthat usually means the max function.\\n\\nAnd so I used the max function\\nand I defined a key function named getfelt,\\nwhich is down here.\\nSo that gets the value of the felt property and returns it\\nor returns zero if that value is none.\\nSo that will give me the event that has the most number\\nof feltreports, and then I just have\\nto get the title property, which gives me the name\\nof the place.\\nAnd then the fourth value we had to calculate was related\\nto the third, which was to find the actual number\\nof felt reports for the most felt event.\\n\\nAnd since we already have the most felt event,\\nso all we need to do now is get the actual value\\nfrom the felt property inside the property's array.\\nAnd that gives us the actual number of reports,\\nin addition to the name of the place,\\nwhich we calculated in part three.\\nAll right, well, let's go ahead\\nand run the code to make sure it actually works.\\nSo I'm going to click on test my code,\\nand sure enough, we have 11,745 total events.\\n28 of them were felt by at least 100 people.\\n\\nThis quake here in Utah was the most felt one\\nin this dataset, which was by a little over 33,000 people.\\nOkay, so that's my solution.\\nTake a few moments, compare your code to mine\\nand see how you did.\\n\"}],\"name\":\"1. Built-In Data Functions\",\"size\":40106757,\"urn\":\"urn:li:learningContentChapter:4415169\"},{\"duration\":1471,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4410702\",\"duration\":96,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview of collection classes\",\"fileName\":\"4312001_en_US_02_01_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will have a broader understanding of the kinds of data collection classes in Python.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1454401,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Python ships with a basic set\\nof data types for working with collections of data\\nand you've probably already used some\\nor all of these in your work at some point.\\nIn this chapter, we're going to learn\\nabout some of the more specialized\\nand useful collection types\\nand how they build on the basic collection classes\\nto help solve different kinds of programming problems.\\nAs a quick refresher,\\nrecall that Python has four basic collection data types.\\nThere's the list, which is a sequence of data types\\nand is declared using square brackets\\nwith each data item separated by a comma.\\n\\nA close relative of lists are tuples,\\nwhich are defined with parentheses\\nand are also comma-separated.\\nHowever, lists can be changed once they are created\\nbecause they're mutable, whereas tuples are not.\\nNext is the set, which is an unordered collection\\nof distinct values defined using curly braces\\nand like lists, sets can be changed once they're created,\\nand then finally is the dictionary,\\nwhich is a set of key value pairs.\\nIn addition to these basic collection types,\\nPython's collection modules provide several more\\nand these are the namedtuple, which makes it easier to work\\nwith tuple objects by giving names and meaning\\nto each item's position in the tuple,\\nan improved dictionary class called defaultdict,\\nCounters, which can track the number\\nof distinct values added to them\\nand a somewhat interesting collection class\\nknown as a deque,\\nwhich acts as a double-ended list.\\n\\nAs we go through the chapter, we'll learn more\\nabout each of these types and see useful examples\\nof how they can be applied in your Python projects.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4410703\",\"duration\":289,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Named tuples\",\"fileName\":\"4312001_en_US_02_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to use the list and set data types to represent different kinds of data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4339597,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Let's start by looking at the namedtuple.\\nSuppose I wanted to define a data structure to\\nrepresent a geometric point on a typical X and Y axis.\\nNow, I could easily do this\\nby defining a regular tuple, right,\\nwith two elements, the X and the Y values of the point.\\nAnd to access these values,\\nI can use positional argument indexes to get each one.\\nNow this may seem all fine and good\\nbut as my program becomes more complex, this kind\\nof code easily loses its meaning\\nand becomes harder to read\\nespecially if I don't keep the names\\nof all the point variables clear and meaningful.\\n\\nI could just define a Python class\\nand give it member properties\\nfor X and Y and then write\\ngetter and setter functions for the properties.\\nBut that seems a little much\\nfor a relatively simple data structure.\\nNamedtuple helps solve this problem\\nby assigning meaning to each of the values\\nalong with the tuple itself.\\nAnd they also provide some helpful functions\\nfor working with them.\\nSo let's go over to the Code and see how they work.\\nAll right, so here in VSCode,\\nI'm going to open up namedtuple.py in my code space.\\n\\nAnd you can see I've already imported the\\ncollections module, which is where they are defined.\\nSo let's start with a simple example.\\nI'll create a namedtuple\\nby using the namedtuple factory method to\\ncreate one to represent a Point structure\\nand I'm going to call it Point.\\nAnd to do this,\\nI'm going to use the collections.namedtuple() function.\\nSo I'm going to give it a name for the tuple\\nand I'm going to call it a Point.\\nSo this is kind of\\nlike a lightweight way of defining a class almost.\\n\\nAnd then inside this string, I'm going to give it the names\\nof the fields that I want that tupple to have, right?\\nSo my point is going to have two properties, X and Y.\\nNow to create a new Point object,\\nI just use the name as a regular constructor almost.\\nSo I'll declare a point named p1\\nand I'll construct it using this new\\nnamedtuple that I've got.\\nAnd we'll give that values of 10 and 20\\nand then we'll make another one.\\n\\nAnd this one will have 30 and 40.\\nAnd then I'll print out p1 and p2.\\nAll right, so let's go ahead and run what we have.\\nSo I'm going to open the terminal\\nand let's go into start folder into chapter two.\\nAnd let's run this.\\nAnd you can see that when I run the code, right\\nwe have two points and their data is printed\\nin a nice readable style.\\n\\nBut what's really nice about this is that now I can refer\\nto the members of the point by name, right?\\nSo let's go back up to the code.\\nAnd what I'm going to do is I'm going to print\\nalso, I'm going write p1.x\\nand p1 dot and you'll see as I'm typing the type, right\\nI hit the period and I'm also getting statement completion.\\nSo the Python extension is\\nreading the structure of my namedtuple\\nand it realizes that I've got these two properties.\\n\\nSo that's another benefit of using these.\\nSo let's save, and now let's run again.\\nAnd you can see now that I can access them just\\nusing the names of the properties.\\nSo this makes my code much easier to read and maintain.\\nNamedtuple also have some interesting helper functions.\\nSo there's the replace() function that lets me\\ncreate a new instance of the namedtuple while\\nreplacing specified fields with a new value, right?\\nSo let's go up to the code.\\n\\nWhat I'm going to do is for point one\\nI'm going to assign that to value of p1\\nand I'm going to call the replace() method.\\nAnd I'm going to replace the x property with a new value.\\nIn this case, it's going to be a hundred, right?\\nAnd then I'll just print out p1 again.\\nSo now let's run.\\nAnd now you can see that I've got a new point, right?\\nWith x is equal to 100 now.\\nSo namedtuple can really help make your code\\nmore readable when what you really need is\\nthe lightweight immutable class.\\n\\nNow keep in mind though, they do have limitations, right?\\nSuch as you can't use default argument values.\\nThere's some other limitations for working with these.\\nBut if the data that you're working with\\nhas a large number of optional properties,\\nit might be better to go with a regular class.\\nOtherwise, using a namedtuple is sort of like\\ndefining a lightweight class\\nfor those instances where a regular class would be too much\\nand you just want to make your code more readable\\nand easier to maintain.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4411305\",\"duration\":279,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Default dictionaries\",\"fileName\":\"4312001_en_US_02_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will understand how to use dictionaries to map keys to data values.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4201392,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Just about every non-trivial Python program\\nuses dictionaries in some way.\\nThey are a great way to associate data values\\nwith unique keys for fast lookup.\\nThe collections module provides two interesting\\ndictionary subclasses to help out\\nwith some common scenarios where a regular\\ndictionary would need unnecessary code.\\nAnd one such example is the default dict\\nwhich we'll examine in this example.\\nIt's a fairly common scenario to use dictionaries to\\nkeep track of data such\\nas the result of counting operations.\\n\\nAnd so here in my code\\nyou can see I have a list of different kinds of fruit\\nand I want to be able to count the number\\nof each kind of fruit in the list.\\nSo I have some code, right,\\nthat creates a dictionary and then iterates\\nover the list and uses the dictionary to keep track\\nof the count of each type of fruit.\\nNow, the code that I have right now is going to\\nproduce an error when I try to run it.\\nOkay?\\nSo let's see.\\nI'm going to run this in the terminal, and sure enough\\nthere is my error, right?\\nSo why am I getting this key error?\\nThe reason this is happening is\\nbecause I'm trying to modify the value of this key\\nbefore it's been initially added to the dictionary.\\n\\nNow ordinarily, right, you would just go back\\nand fix this problem and you would do something like this.\\nYou would say, well, if the fruit is already\\nin the fruit counter keys, then you can increment the count\\notherwise you have to initialize it.\\nSo that would be fruit counter, right?\\nFruit equals one, right?\\nAnd so now when I run this again, well, I don't\\nget the error anymore, but I'm not outputting anything.\\n\\nBut you can see that the error is gone.\\nSo everything works, but this initial condition\\nis only going to be executed one time for each fruit\\nand it adds unnecessary visual noise to the code.\\nSo to reduce the code and make it more readable\\nwe can just define a default dict instead.\\nSo let me close this terminal\\nand let's go and change this code.\\nSo in order to use the default dict\\nyou have to import it from the collections module\\nwhich you can see I've already done up here.\\nAnd then I can create the default dict object.\\n\\nNow, when I create a default dictionary,\\nit requires me to specify something\\ncalled a factory function\\nwhich essentially acts as the creator\\nof whatever the default value is.\\nSo I'm going to specify that I'm going to use an integer.\\nSo this code basically says\\nif I try to access a key that doesn't already exist\\njust create a default value for me using this object\\nas the constructor, which in this case is an integer.\\nAnd since creating a new int object initializes the value\\nto zero, I can now just access any key and increment it\\nwithout having to check to see if it's already there.\\n\\nSo let's go ahead\\nand remove the checking code I put in earlier.\\nAll right, and so now I've got my original code back.\\nThe only difference is\\nthat I'm now using a default dict instead of a regular dict.\\nAnd I'm going to go ahead and run this.\\nI'm going to print out the results while I'm at it.\\nSo I'll print out the root counter.\\nAll right, so let's go ahead and run this.\\nAnd sure enough, you can see that it's working now.\\n\\nSo here's my default dictionary, and you can see I've got\\ncounts of all the different fruits that are\\nin the dictionary.\\nSo you can also define your own factory method too.\\nSo let me make this a little bit smaller.\\nSo if I wanted the initial value to start off\\nat a different level, I can just use a Lambda function\\nfor that.\\nSo instead of having my default dictionary start off\\nwith just an integer, I can say I'm going to specify\\na Lambda function and make that a hundred.\\nAll right.\\nAnd now if I run this again,\\nright now, you can see that the count is starting\\noff at a hundred for each of these fruit types, right?\\nSo we've got 103, 101, and so on.\\n\\nNow, this factory object doesn't need to be a built-in class\\nlike an int or a string,\\nit could be any custom object you want.\\nSo default dict is really useful\\nbut you have to be careful when you use it because any key\\nthat you didn't explicitly add to the dictionary\\nwill be assigned a default value when you try to access it.\\nSo if you have a situation where the fact\\nthat a key is missing\\nfrom the dictionary is an important indicator, then\\ndefault dict is probably not the right collection to use.\\nIn other situations, however, it can make your code simpler\\nand easier to read and test.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4412628\",\"duration\":382,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Counters\",\"fileName\":\"4312001_en_US_02_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will understand how the Python counter class can be used to count unique data values.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5727415,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] As I mentioned earlier,\\none of the common uses\\nof dictionaries is to keep track\\nof the count of individual items.\\nThe collections module supplies a counter class\\nwhich is basically a dictionary subclass\\nfor counting hashable objects.\\nNow, you might be saying, well, wait a minute.\\nWe saw how to do this earlier\\nwith the default dict class, right?\\nAnd that's true\\nbut counters have some nice additional features\\nfor working with numbers of items.\\nSo let's open up the counter code here\\nin our start folder in chapter two, and we have two lists\\nof student names for two different classes.\\n\\nNow, counters can be initialized\\nin a variety of ways, and in this case\\nI'll create two of them using the name lists, right?\\nSo I'm going to write C1 and I'm going to create a new counter,\\nand I'm going to initialize it with the values from class one.\\nAnd then I'll do the same thing with C2\\nand that's going to be a counter for class two.\\nSo let's try a few operations.\\nNow, since a counter is a dictionary\\nI can use a key to get a value from the list.\\n\\nSo let's see how many students named James are in class one.\\nSo to do that, I'm going to print out C1,\\nand I'm going to use James as the key.\\nOkay?\\nWe can also see how many students there are\\nin the class by using the sum function to add\\nup all of the values in the counter.\\nSo recall that we learned about the sum function earlier,\\nand I'm going to sum up in class one, all of the values.\\n\\nAll right?\\nAnd so I'm going to print that,\\nand I'm going to print that followed by students in class one.\\nI don't actually need that space.\\nAll right?\\nCounters can also be combined together to have\\ntheir values added to each other.\\nSo for example, if I wanted to combine these two classes\\nI can do that by using the update function\\nand I'll add the second, the students\\nin the second class to the first class.\\n\\nSo to do that, I use the update function\\nand then I'm just going to update class one with class two.\\nAnd then we can print the sum\\nof the values again to see the newly defined totals.\\nLet's copy this line and paste it there.\\nSo let's pause here and run what we already have.\\nSo we've got a couple\\nof initialization statements that make new counters.\\nThen we see how many people named James are\\nin the first class.\\n\\nWe count the number of students in class one.\\nThen we combine the two classes into class one\\nand then we check the count again.\\nSo let's go ahead and run this.\\nSo I'm going to open my terminal\\nand let's go into start chapter two.\\nAnd this is the counter example.\\nSo I'll run Python counter, and you can see\\nthat there are two people named James in the first class.\\nIs that right?\\nYeah, there's one.\\n\\nOkay.\\nin the first class before we do the update.\\nAnd then after we do the update\\nyou can see that there's now 24 people in class one.\\nAll right, let's keep on going.\\nSo go ahead and collapse this down.\\nAll right.\\nThere's a convenience function called most common\\nthat will print out\\nthe top most common items in the counter.\\nSo for example, if I print and class one\\nI'm going to call most common, and I'm going to look\\nfor the three most common names in the two classes.\\n\\nNow, if I leave the argument blank\\nit will print all the keys in the dictionary\\nbut I can limit it here just to the top three keys.\\nSo when I run this again\\nlet's bring up this code and let's run it again.\\nAnd now we can see\\nthat we have the top three most common names.\\nSo when we combine the two classes\\nnow we have three people named James,\\nwe've got two people named Frank and one person named Bob.\\nSo just like I can add two counters together\\nI can also subtract one counter from the other.\\n\\nSo let's change the code to use the subtract function to\\nsubtract off the class two name list.\\nAll right?\\nand I'm going to subtract off class two.\\nOkay, there we go.\\nAnd then once I do that, once again, we'll print out\\nthe most common, and this time we'll just limit it\\nto the first most common thing, and when I run\\nthe code again.\\n\\nAll right, so now you can see that the list has changed,\\nright?\\nSo Frank and Bob are gone.\\nAnd now since we're back down just to class one again\\nremember there were only two Jameses in there\\nand so now we have two people named James in that class.\\nOkay, so let's try one more operation.\\nWe're going to use the ampersand operator to\\ncalculate the intersection of the two counters.\\nSo in other words, this is going to tell us which items\\nare present in both of the counters.\\n\\nSo I'll do that by writing print, and then\\nI'm going to write C1 and then ampersand, C2.\\nThis will do an intersection operation and it will print\\nout the result.\\nAll right, so let's go ahead and run that.\\nAnd we can see that when we run this, what's common\\nbetween the two classes is that the names James and Frank\\nare common to both of the classes.\\nSo if you need a dictionary to help keep track of a number\\nof different items along with a set of operations\\nfor working on the data or multiple sets of data\\nthen the counter class just might fit the bill for you.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4410704\",\"duration\":281,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"The deque class\",\"fileName\":\"4312001_en_US_02_05_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will know how to apply the deque class, which is a queue that operates on both ends of a given sequence of data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4231073,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The last collection class\\nthat we will take a look at in this chapter\\nis called a deque.\\nNow, the name may look like it says dq,\\nbut it's actually pronounced deck,\\nand it's a sort of a hybrid object\\nbetween a stack and a queue.\\nIn fact, the name itself\\nbasically stands for double-ended queue.\\nThat's where the de comes from.\\nAnd you can use them to append or pop data from either side.\\nAnd they're designed to be memory efficient\\nwhen accessing data from either end.\\nDeques can be initialized to either be empty\\nor get their initial data from any existing iterable object.\\n\\nIn this case, I'm using a string.\\nThey can also be specified to have a maximum length.\\nTo add data to a deque, you use the append method\\nto add items on the end or the right side\\nand appendleft to add items to the beginning.\\nAnd similarly, items can be removed\\nusing either pop or popleft.\\nDeques also support a rotate function\\nwhich can operate in either direction.\\nThe rotate function takes a parameter\\nindicating how many items to rotate and defaults to one.\\n\\nPositive numbers rotate to the right.\\nNegative numbers rotate to the left.\\nAll right, so let's exercise the deque a little bit\\nwith a few examples.\\nOkay, so here in my codespace, I'm going to open up the deque\\nand you can see I've already imported the collections\\nand string modules.\\nSo let's create a new deque using all lowercase letters.\\nAnd to do that,\\nI'll just simply create a new collections.deque object\\nand I'm going to initialize that to string.ascii_lowercase.\\n\\nSo that gives me all the lowercase letters.\\nAnd then let's print out the number of items in the deque\\nbecause it supports using the len function on it.\\nSo I'm going to print out that the Item Count is\\nand that's going to be the length of d.\\nDeques can also be iterated over, right?\\nThey support iteration.\\nSo you can loop over each of the items\\nthe same way you would do with any other iterable.\\nSo I can write code that looks like this.\\n\\nI can write for elem in d.\\nAnd in this case,\\nwhat I'll do is just convert each element to an uppercase.\\nThat's a couple good examples to get started.\\nSo let's go ahead and try this out.\\nSo here in my terminal, let's go into start chapter two\\nand let's run the deque example.\\nAll right, so let's scroll back up.\\n\\nSo sure enough, we can see that there's 26 items\\nin the deque, which makes sense, there's 26 letters.\\nAnd you can see that as we're iterating over each letter,\\nwe're converting it to uppercase.\\nOkay, we can also pop items from either side.\\nSo let's try that. Let me clear this.\\nOkay, so let's try popping some elements.\\nSo we'll pop from the right\\nand then we'll also pop from the left.\\n\\nAnd, of course, we can also add items to each side,\\nso let's append and we'll put a number on there.\\nAnd let's also append the left as well\\nand we'll put number one on there.\\nAnd then let's go ahead and print out our deque\\nto see what's in there.\\nSo let's go ahead and run the changes.\\nAnd when we run the changes, right,\\nwe can see that A and Z have been removed\\nbecause of the pop and the popleft\\nand they've been replaced with the numbers one and two\\nbecause of the append and the appendleft.\\n\\nOkay, let's try one more thing.\\nLet's try rotating the deque.\\nFirst, we'll print the deck in its current state,\\nso we'll print d,\\nand then we'll rotate it\\nand let's rotate it one position\\nand then let's print it again.\\nOkay, and so let's run that code.\\nAnd sure enough, you can see that when we run this,\\nnow the number two is on the front of the deque\\nafter we rotate it one space towards the right\\nbecause the deque wraps around\\nand places the end value on the front.\\n\\nSo before we do the rotation right,\\none is here and two is here,\\nand then we rotate everybody to the right.\\nSo that way, two goes to the beginning\\nand now y is at the end.\\nSo the deque object is really versatile.\\nSo if you have a use case where you need to be able\\nto operate on both ends of a list\\nand perform operations like this rotation for example,\\nthen this just might be what you need.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:66b5287f498e422fcc4048f5\",\"duration\":1200,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Classify event types\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1078259\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:5962109\",\"duration\":144,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Classify event types\",\"fileName\":\"4312001_en_US_02_07_FY25Q1_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"updateType\":\"ADDED\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":144,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4358790,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] All right, so for this challenge,\\nwe needed to build a classification\\nof the different event types\\ncontained within our earthquake dataset.\\nAnd for my solution,\\nI decided to use the defaultdict collection class.\\nAnd I made this decision for two reasons.\\nFirst, I needed to count the number of each event,\\nand I wanted an easy way\\nto associate the event type with the count.\\n\\nNow, the dictionary class gives us an easy way to do that,\\nand the defaultdict class makes it even easier\\nby declaring an object that uses integers\\nas the default type.\\nSo my code starts with declaring a new defaultdict\\nthat uses integers as the value type.\\nAnd then I just process each event in a loop\\nand use the event type as the key into the dictionary.\\n\\nAnd I increase the count for each type\\nas I come across them.\\nSo when the loop completes,\\neach key in the dictionary represents the event type,\\nand the associated value is the count for that type.\\nThen I convert the defaultdict into a regular dictionary\\nand return the result\\nbecause the challenge required\\nthat the function return a dictionary object.\\nSo now if you solved this challenge\\nusing a regular dictionary with a little bit more code,\\nthat's totally fine.\\n\\nSo don't worry about that.\\nI just decided to use the defaultdict\\nbecause it makes the code a little bit easier.\\nSo, okay, let's go ahead and test the code\\nand make sure we got the right answer.\\nAll right, and we can see that...\\nLet's see.\\nSo the number of earthquakes was 11,558.\\nThere were 85 explosions,\\n81 quarry blasts.\\nNot really sure why that's different from an explosion,\\nbut I guess it is.\\nAnd then there were 17 ice quakes,\\nwhich just sounds pretty cool.\\n\\nAnd then there were four other events.\\nNot sure what they were,\\nbut they were just called other events.\\nThere were four of those that resulted in seismic readings.\\nAll right, so if you got the same result,\\nthen congratulations, you completed the challenge.\\nTake a few moments,\\nand review your code and compare it to mine.\\n\"}],\"name\":\"2. Collection Classes\",\"size\":24312668,\"urn\":\"urn:li:learningContentChapter:4415170\"},{\"duration\":1806,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4411306\",\"duration\":84,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview of serialization\",\"fileName\":\"4312001_en_US_03_01_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will understand how to use data serialization to save and load Python objects from persistent storage.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1305590,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Lecturer] In order to work on a set of data,\\nyou usually first have to obtain the data from somewhere.\\nAnd then of course,\\nonce you've completed your operations on the data,\\nyou usually need to save it back out again\\nand store the results.\\nThis process is called serialization.\\nIt's the process of reading and writing data\\nfrom and to a datastore.\\nThe datastore can be anything.\\nIt can be a local file, or a database, or a network stream.\\nIt doesn't really matter.\\nIn this chapter, we're going to use\\nsome of the built-in Python methods,\\nor taking a set of data and serializing it\\nto and from a couple of common file formats.\\n\\nIn this case, JSON or JavaScript Object Notation,\\nand Comma Separated Values or CSV files.\\nThere's also a third common format called xml,\\nbut that's beyond the scope of this course,\\nso we're just going to focus on JSON and CSV.\\nComma Separated Value files are a popular way\\nof storing data because it's one of the formats used\\nby spreadsheet programs like Microsoft Excel,\\nand Apple Numbers, and Google Sheets, and so on.\\nJSON is also a popular data storage and interchange format,\\nand it's the format that we've been using\\nthroughout this course.\\n\\nBy the end of this chapter you'll be able to use Python\\nto read and write data from both\\nof these common file formats.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4414265\",\"duration\":455,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Serializing data as CSV\",\"fileName\":\"4312001_en_US_03_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to discuss the various features of the pickle module and what it can be used for.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6815033,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] For this example,\\nwe're going to take a subset of our JSON data\\nand save it to the comma-separated values format.\\nYou can find the documentation for the CSV module\\nhere on the Python website\\nif you're interested in reading it on your own time.\\nBut the process is actually pretty simple\\nand straightforward.\\nSo let's just jump over to the code and see how that works.\\nSo here in my Ch_3 start folder,\\nI'm going to click on the serialize_csv function.\\nAnd what we're going to do is create a CSV file\\nthat contains just a few columns of data\\nout of all the properties\\nthat our massive earthquake structure holds.\\n\\nSo first, to limit the size of this file\\nfor the purposes of this demo, we're going to filter out\\nall of the earthquakes that are smaller than magnitude five.\\nAnd you can see I'm using the filter function here\\nthat we learned about earlier in the course\\nto reduce the size of the dataset, right?\\nSo next, we need to create the column headers\\nand the row structure to hold the data.\\nSo I'm going to create a variable named header,\\nand that's going to contain Place,\\nMagnitude, Link, and Date.\\n\\nAnd I'm also going to make a variable\\nthat's going to hold the rows for the data.\\nThen I'm going to fill out the rows list\\nwith the data from the array that we just built\\nthat only contains the largequakes.\\nAll right, so let's go ahead and do that.\\nSo for each quake in largequakes,\\nfirst I'm going to create a date.\\nAnd the date, what I'm going to use is the datetime module,\\nand I'm going to use the date class,\\nand I'm going to call from timestamp, all right?\\nAnd from timestamp accepts a seconds value,\\nnot a milliseconds value.\\n\\nAnd inside the quakes structure, we have this data.\\nIt is basically, let's see,\\nit's quake and it's in the properties,\\nand it's the time property.\\nAnd, whoops, that's got to be in quotes,\\nand I need to divide that by 1000.\\nAnd I'm actually going to convert this\\nto an integer.\\n\\nAll right.\\nAnd let's go ahead and put on a separate line\\nto make it more readable.\\nAll right, so first I create the date,\\nand then I'm going to append to the rows.\\nI'm going to put in, it's got to be a sequence\\nbecause each row is going to be a sequence of data.\\nSo I'm going to have quake, and I'm going to have properties,\\nand place, right?\\nThat's this column right here, okay?\\nSo properties and place, all right?\\nThen I'm going to have quake,\\nand that's going to be followed by the magnitude.\\n\\nSo properties and magnitude, all right?\\nAnd, oh, that's one extra thing right there.\\nAnd then finally, well not finally,\\nwe have to do the link, right?\\nSo that's going to be quake,\\nand that is going to be in the properties,\\nand that is going to be the url property.\\nAnd then finally, we have the date.\\n\\nAll right, oh, no, it's thedate, not date, thedate.\\nThere we go.\\nAll right, so this will now add all the data\\nin those four columns, right?\\nTo our rows, matching all of our headers.\\nSo we have the place, the magnitude, the URL link,\\nand the date that we just calculated from the timestamp.\\nSo once the data has been put into the rows,\\nnow I can use the CSV module functions\\nto create and write the CSV file.\\n\\nSo what I'm going to do is write with,\\nand then I'm going to open the file,\\nand I'm going to call it largequakes.csv.\\nAnd I'm going to open that in write mode.\\nAnd I'm going to operate on that,\\nI'm going to call it the csvfile variable.\\nAll right, so that will open up the file for writing,\\nand I'm going to create a CSV writer object with my file\\nand then specify that the delimiter is a comma.\\n\\nSo I'll make a variable called writer,\\nand that's going to become a csv.writer object.\\nAnd that's going to take my file,\\nand the delimiter is going to be a comma.\\nThen I just need to use the writer object\\nto save out the data.\\nSo on the writer object, I'm going to call writerow.\\n\\nAnd the first row I'm going to write out\\nis this header row right here,\\nso that we have all the headers at the top of the file.\\nSo I'll write out header.\\nAnd then I'm going to use the writer object,\\nand this time, I'm going to call writerows, multiple,\\nand I'm going to pass in the rows structure,\\nthe list of rows that we've already created, all right?\\nThat's really all there is to it.\\nSo here I list, I filter out my data,\\nI get my header in place, I initialize my rows to be empty.\\n\\nAnd then for each one of the pieces of data,\\nI'm going to add this sequence of values in each row.\\nAnd then once I've done that for all the quakes,\\nI just simply write out all the rows into the CSV file.\\nSo let's go ahead and run this.\\nLet's go into Start/Ch_3.\\nAll right, and there's serialize_csv.\\nSo I'm going to run this.\\nyou're going to see a CSV file appear over here in the list.\\n\\nSo python serialize_csv.py.\\nAnd sure enough, there it is, right?\\nThere's the file just showed up.\\nSo let's go ahead and open that file up.\\nAnd we can see, make this a little bit smaller.\\nAll right, so here's the data in the file, right?\\nSo here's our headers, right?\\nPlace, Magnitude, Link, and Date.\\nAnd you can see that here's the name of the place.\\nAll of the magnitudes are at least magnitude five,\\nso none of the smaller quakes.\\n\\nSo that keeps the data files somewhat manageable, right?\\nAnd the data is in the right format.\\nSo we have the link, right?\\nAnd we have the date here at the end.\\nAnd sure enough, we can just go ahead and select this guy,\\nnot including the date, obviously.\\nSo we'll copy that, and we'll paste it,\\nand put it into a new tab.\\nAnd sure enough, there we have the information\\nabout that particular quake.\\nSo now, we have the ability to work with our data\\nin a spreadsheet if we want to do so.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4415165\",\"duration\":430,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Serializing data as JSON\",\"fileName\":\"4312001_en_US_03_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to use the pickle module to save data from memory to a persistent state.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6448489,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Throughout the course,\\nwe've been using the load function in the JSON module\\nto load our dataset from a JSON file.\\nBut you can also use the JSON module\\nto easily save data in the JSON format,\\nboth into a string and into a file.\\nFor this example,\\nwe're going to take our earthquake dataset\\nand simplify it by getting rid\\nof many of the data fields that we're not using,\\nand then serializing the data out.\\nSo let's go ahead and in the Start folder in chapter three,\\nlet's open up serialize_json.\\n\\nAnd you can see I've already got some code here.\\nSo my code starts off\\nby loading our earthquake data from the data file,\\njust as we've been doing so far in the course.\\nThen to make the dataset easier to use,\\nI'm going to filter out all of the quakes\\nthat are not larger than magnitude 6, right?\\nSo just for the purposes of this demo.\\nThis should only leave us with a handful of data points\\njust to make the example more compact.\\nSo once I have the smaller dataset,\\nI'm going to transform it\\ninto a reduced set of properties, right?\\nBecause again, if you look at this data file,\\nyou can see there all these properties, right?\\nAnd we're not using a lot of these guys.\\n\\nSo let's make this dataset a lot easier to use.\\nRight, so what we're going to do is define a function.\\nAnd I'm going to use the map function\\nwhich we learned about earlier in the course, okay?\\nSo here's what we're going to do.\\nI'm going to write largequakes is equal to,\\nI'm going to make a new list,\\nand I'm going to use the map function.\\nAnd I'm going to map the function.\\n\\nI'm going to call a function called simplequake,\\nand I'm going to operate on the largequake's dataset.\\nSo the simplequake function\\nwill take the complex JSON structure\\nand make a more compact one.\\nWe're only going\\nand we'll use the time property to calculate a date.\\nSo now I have to actually define that function.\\nSo I'll define the function, I'll call it simplequake.\\n\\nAnd it takes one argument,\\nwhich will be an individual quake,\\nand I'm going to return a new JSON object.\\nAnd inside that object, I'm going to have a place property,\\nand that's going to be the properties,\\nplace of the original.\\nAnd then I'm going to have a magnitude.\\nAnd just to make things simple, I'm going to copy this,\\nand that's going to be the magnitude property.\\n\\nAnd then there's going to be the link,\\nand I'll paste.\\nAnd once again, that's going to be the URL property.\\nAnd then I'm going to have a date property.\\nAnd I'm going to calculate this.\\nI'm going to create a string.\\nAnd the string is going to convert,\\nI'm going to use the datetime module and the date class.\\nAnd I'm going to call fromtimestamp.\\nAnd fromtimestamp is going to operate on,\\nI'm going to convert this to an integer,\\nand I need the properties,\\nI need the time property from the quake data.\\n\\nAnd that is going to be divided by 1,000,\\nbecause that's milliseconds, right?\\nAnd I don't want milliseconds,\\nI want seconds for the fromtimestamp function.\\nSo let me go ahead and save that.\\nSo after we've modified the JSON data structure,\\nwe can now serialize it out, okay?\\nAnd there's two ways to do that.\\nFirst, I'm going to use the dumps function,\\nwhich is the opposite of the load function.\\nIt takes an existing JSON structure\\nand places it into a string format.\\n\\nSo I'm going to write str.\\nAnd in the JSON module, I'm going to use the dumps function.\\nAnd the dumps function is going to take the data structure\\nthat I want to operate on, right?\\nWhich is largequakes.\\nI'm going to say that I want to sort the keys,\\nso that's going to be set to True.\\nAnd then I'm going to set the indent level to be 4.\\nAnd then I'm going to print out that string.\\nSo let's go ahead and run what we have.\\n\\nSo what we have here\\nis we filter out anything smaller than a magnitude 6.\\nWe have a function that simplifies the JSON structure down.\\nSo we filter out everything that's not big.\\nAnd then we map the complex structure to the simple one.\\nAnd then we put it into a string, and print out the string.\\nSo let's bring up our terminal,\\nand we'll go into chapter three,\\nand we'll run the serialize_json example.\\n\\nAnd sure enough, let me go ahead and make this bigger.\\nYou can see that when I run this, right?\\nNow in the output we have the resulting JSON structure\\nhas been saved into a string.\\nIt's much simpler\\nthan the original big JSON file that we have.\\nAnd it's being pretty printed out by the dumps function.\\nNow that's putting it into a string.\\nWe can also save the data to a JSON file.\\nSo let's try that next.\\nSo let's go ahead, and I'm going to open a file,\\nand I'm going to call the file largequakes.json.\\n\\nAnd we'll open that in write mode.\\nAnd I'll call that outfile.\\nAnd then I'm going to use the JSON module's dump function.\\nSo dumps dumps to a string.\\nThe dump function goes to a file\\nor some other streamable object.\\nSo I'm going to dump out largequakes.\\nI'm going to use the outfile as the destination.\\nOnce again, I'm going to sort the keys,\\nand I'm going to set the indent equal to 4.\\n\\nAll right?\\nOkay, let's go ahead and comment out that previous example,\\nand let's go down to our terminal,\\nand run this code.\\nAll right, so let's run it.\\nAll right.\\nAnd now you can see\\nhas appeared in the folder here.\\nSo let's go ahead and click on that,\\nand let's shrink down the terminal.\\n\\nAnd you can see that our code has now been output,\\nwell, not the code, it's our data.\\nSo our JSON file has been created,\\nand the content of the JSON file\\nis now our simplified JSON structure, right?\\nSo by combining the data manipulation functions\\nlike filter and map\\nwith the serialization functions in the JSON module,\\nit's pretty straightforward\\nto modify a dataset and save it as JSON.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4408349\",\"duration\":404,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Deserialization of data\",\"fileName\":\"4312001_en_US_03_04_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to use the pickle module to load data from a persistent state into memory.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6072897,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that we've seen\\nhow to save data to a csv file,\\nin this example, we're going to do the opposite.\\nWe're going to load information\\nfrom an existing csv file into a json structure.\\nAnd this process is sometimes called deserialization.\\nWe're going to use one of the CSV files\\nthat was created in an earlier exercise.\\nSo if you haven't already done the exercise\\nwhere we save data to a csv file, go back and do it now,\\nso you'll have the file to work on in this example.\\nSo here I have the output from the previous exercise\\ncalled largequakes.csv.\\n\\nSo let's view that file again, and inside,\\nremember, there's four columns of data, right?\\nThere's the name of the place, the magnitude, date\\nand the link to the event on the usgs website.\\nSo we're going to use the same csv module to read this data\\nas we use to write the data in the first place.\\nSo let's open up the deserialize code,\\nand what we're going to do, you can see in my start code,\\nI've already defined a variable called result\\nwhich is going to hold the results of the deserialized data.\\n\\nSo the first thing we need to do\\nis open the file for reading,\\nand then store that file reference in a variable.\\nSo I'm going to write with open,\\nand we're going to open up largequakes.csv,\\nand we're going to open it up for reading as the csvfile.\\nSo then what we need to do is\\ncreate a reader object that will read the data.\\nSo I'll type reader,\\nand on the csv module, there is a reader class,\\nand I'm going to pass in the csv file that we just opened.\\n\\nSo now that opens the file for reading,\\nand to actually read the data, all we have to do is\\niterate over each row in the reader using a regular loop\\nbecause this csv reader returns an iterable.\\nSo I'm just going to write; for row in reader.\\nAnd then for the moment, let's just print each row,\\njust so we can see that it's working, right?\\nSo let's pause here and just run what we have.\\n\\nSo let's go to the terminal,\\nand in chapter three, we're going to run the deserialize.\\nOkay, and you can see that when I run this,\\nthis is good news, right?\\nWe can see that the data's being printed out.\\nSo this is all the data that's inside the csv file, okay?\\nSo since things seem to be working,\\nlet's keep on going, right?\\nSo let's close this.\\nSo let's comment out this print statement, all right?\\nSo what we're going to do now\\nis create a simple json object to represent each row,\\nand then add it to the result list.\\n\\nSo on my result, I'm going to append,\\nand I'm going to create a new json object each time through.\\nAnd so my json's going to have a place,\\nand that's going to be the data that's in row index zero,\\nand then the magnitude.\\nAnd that's going to be row index one.\\nAnd then we're going to have the date,\\nand that's in row index two.\\n\\nAnd then finally we have the link\\nwhich is in row index three, all right?\\nSo pretty simple and straightforward, right?\\nSo once all of this is finished,\\nwe're going to take advantage of the pretty print module,\\nwhich you can see I've imported up here.\\nAnd we're going to just pretty print out\\nthe resulting data as we've read it in, okay?\\nSo let's go ahead and run this.\\nSo let's go back to the terminal, and let's run our code.\\n\\nAlright, so now, we can see\\nthat all of our data is being read in, okay,\\nhere's all the json data, ah, but there's a problem,\\nif you scroll all the way to the top,\\nyou'll notice that we processed the headers\\nas if they were a data row.\\nNow we don't want to do that, right?\\nWe would like to just skip over the first row,\\nbut I don't want to hard code that because some data files\\nmay not contain headers while others do.\\n\\nSo what's the solution in this case?\\nWell, it turns out\\nthat the csv module has an object called a sniffer\\nthat we can use to detect whether or not\\na file has headers as the first row,\\nso let's go ahead and close down this terminal.\\nSo here's how it works.\\nWhat we're going to do is create a sniffer,\\nand that's going to be a csv sniffer class object, right?\\nThen we need to give it some data to work on\\nto figure out if the file contains headers.\\n\\nSo we're going to read in the first 1k of the file,\\nand we're going to call that a sample.\\nSo we're going to, well, csvfile.read,\\nand we'll read in the first 1k of the file.\\nAnd then we have to seek back to the beginning\\nbecause otherwise the reader's going to start\\nfrom the position where we last read to.\\nSo now I can ask the sniffer,\\nif it thinks that the file contains headers,\\nand it uses an internal algorithm\\nto figure this out by examining the data.\\n\\nSo I can write if sniffer.has_header,\\nand I'll give the sample that we just read in, okay?\\nNow, if the file does have header information,\\nthen I just need to skip over the first row,\\nwhich I can do by using\\nthe built-in python function called next, right,\\nwhich is just reads the next value\\nof any iterator which the reader is.\\nSo it's going to skip over the first row,\\nso now let's go back and run this code again.\\n\\nAll right, so let's go back to the terminal,\\nand let's go ahead and run the code.\\nAll right, let's make the terminal bigger,\\nand let's skip up to the top.\\nAnd sure enough, this time you can see that the first row\\ncontaining the header information has been skipped.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4415166\",\"duration\":205,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Create a CSV file\",\"fileName\":\"4312001_en_US_03_05_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3079897,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- Time for another programming challenge.\\nAll right, for this challenge,\\nwe are going to create a CSV file\\nthat contains the 40 most significant seismic events\\ncontained within our JSON data file, right?\\nSo here, let me open up the starting point.\\nSo here's your challenge.\\nYou're going to create a CSV file\\nwith the following information.\\nIt's going to have the 40 most significant seismic events\\nordered by the most recent.\\n\\nThere's going to be a header row\\nthat contains the Magnitude, the Place, the Felt Reports,\\nthe Date and a Google Map link which you're going to create.\\nAnd the date should be in the format of the four digit year\\nfollowed by two digit month and two digit day. Okay?\\nSo I'm going to show you\\nthe resulting CSV file so you can see what the output\\nis going to look like, right?\\nSo here in my finished chapter,\\nI just ran this, my finished code.\\n\\nSo if we click on this\\nyou can see that I've got my header row right?\\nMagnitude, Place, Felt Reports, Date, and the link\\nand it's ordered by significance, okay?\\nAnd you'll see that there's a Google Map link\\nto where the seismic event happens.\\nLet's go ahead and bring one of these up.\\nSo we have to CTRL+click on it to get it to work.\\nAnd when I do that, you can see that this one happened\\njust near a town called Magna in Utah\\nwhich apparently is near Salt Lake City.\\n\\nSo you can see that it has the location right there\\nof the seismic event. Okay?\\nSo if we go back and we look at the original data,\\nlet's go back. All right?\\nAnd remember, so let's open up the file.\\nSo remember that the URL property\\nis not pointing to a Google Map link, okay?\\nSo you're going to have to figure out how to make\\na Google Map link as part of this challenge.\\nAnd also notice that the 40 events in the result here,\\nit's not by Magnitude, okay?\\nIt's ordered by the Date column.\\n\\nSo you can see here's one, it goes from March, right?\\nAnd you see March 18th.\\nThen we've got, you know, March 17th and then the 14th\\nand it's going all the way back down.\\nAnd then the last one is in February of 2020. Okay?\\nSo order it by the Date column.\\nAnd remember that you what you want to look at\\nis if we go back to the code for the data\\nthat's the significance value right here, okay?\\nSo that's the data you're going to want to look at.\\nAnd if you want a hint on how to do this,\\nthen stick around for a few seconds\\nand I'll give you a hint. All right?\\nOtherwise, just go ahead and stop the video here\\nand give the challenge a try\\nand I will be back in the next video\\nto discuss my solution. Okay?\\nAll right. So do you want the hint?\\nHey, are you sure you really want the hint?\\nI'm going to give it to you.\\n\\nHere's the hint.\\nSo in order to make the Google Map link,\\nyou're going to need to take a look at this page\\non the Google Maps documentation to figure out\\nhow to make a link to a Google Map\\nusing Longitude and Latitude, right?\\nSo now that you have the hint,\\ngo ahead, try the challenge\\nand then we'll take a look at my code.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4413399\",\"duration\":228,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Create a CSV file\",\"fileName\":\"4312001_en_US_03_06_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3430603,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat techno music)\\n- [Instructor] All right, let's examine the code\\nfor my solution.\\nOkay, so our task in this challenge\\nwas to create a CSV file containing the 40\\nmost significant seismic events in our data file,\\nordered by most recent date,\\nalong with certain columns of data.\\nAnd we had to make a Google Map link.\\nAll right, so if we look at my solution,\\nremember we had to like find the most significant events,\\nwhich is not the magnitude, right?\\nWe had to look at the sig property.\\n\\nSo I have a function named getsig,\\nwhich returns the value of the sig field in the data,\\nor zero if that field is null.\\nThen my code uses the sorted function\\nto sort all of the quake data\\nby the significance field\\nfrom largest down to smallest.\\nNow remember, I'm only interested in the top 40 events,\\nso I'm going to use Python's slicing notation here\\njust to get the first 40\\nof all the almost 12,000 events in the data.\\n\\nSo that gives me a much smaller data set to work with.\\nAnd because we want the results to be sorted\\nby the most recent,\\nI then resort the remaining 40 items\\nby using the time property of the data structure.\\nOkay?\\nSo I'm defining an inline lambda function here\\nto just return the value of the time,\\nand then that sorts by time.\\nSo first, we sort by significance, then we sort by the time.\\nSo now I have the 40 most significant events,\\nsorted by most recent events first.\\n\\nThat was first part of the challenge.\\nNow I need to create a CSV output file,\\nand write out all the data.\\nSo I define an array of the column header names, right?\\nThat's another part of the challenge.\\nAnd then I have an array to hold the data rows.\\nNext, I have a loop\\nwhich iterates over each item in the sorted list.\\nSo first, I convert the time property into a date\\nby using the datetime module,\\nand calling the fromtimestamp function.\\n\\nAnd remember, I've got to divide by a thousand\\nbecause the data inside the JSON is in milliseconds,\\nand this function expects seconds, not milliseconds.\\nThen I need to build a Google Map link\\nusing the latitude and longitude properties\\nstored in the geometry section of the event data.\\nSo if we go back and look at the data,\\nyou'll see that down here,\\noutside of the actual earthquake data,\\nthere's a section called geometry,\\nand it gives me the latitude and the longitude,\\nand they're reversed.\\n\\nSo this one comes first, then this one.\\nSo if we go back to the code,\\nwe'll see that what I'm doing is\\nI'm getting the latitude and the longitude,\\nand then I'm using this template string right here\\nto create a Google Map link.\\nAnd if you look at the documentation\\nfor the Google Maps link,\\nif we scroll down,\\nyou'll see that there's a search operation,\\nand this is the URL that you can use to build link with.\\nSo I do that, I plug in the latitude and the longitude,\\nand I realize that was a bit challenging.\\n\\nSo if you didn't get that part, don't worry about it.\\nSo then we built the link, and once I have those values,\\nit's just a matter of inserting the data into the rows.\\nSo for each row, I append,\\nyou know, the magnitude of the place, so on and so forth.\\nI do that for each one of the events.\\nOnce I've got all my data rows, I open up the file,\\nI use the csv.writer object to write out the data,\\nand then I'm done.\\nAll right, so, that's my code.\\nHow does that compare to yours?\\n\"}],\"name\":\"3. Serializing Data in Python\",\"size\":27152509,\"urn\":\"urn:li:learningContentChapter:4415171\"},{\"duration\":1098,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4409284\",\"duration\":111,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview of logging\",\"fileName\":\"4312001_en_US_04_01_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to understand the importance of Python logging and how it helps to diagnose problems in your code.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1697697,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this chapter, we're going to learn\\nabout the logging features available\\nin Python, and why you'd want to use them.\\nSo logging is an important tool in the development process,\\nand particularly when you're working with data\\nbecause it enables your code to record events\\nas the program executes for later analysis.\\nSo, you can think of logging as sort of\\nlike the black box of an airplane, right?\\nIt's always recording data about how things are functioning,\\nand if something unexpected happens,\\nyou can use the log to go back and help diagnose the issue.\\n\\nIt's not always possible, or feasible,\\nto use a regular debugger to step\\nthrough your program's code to help analyze\\nand solve problems in real time.\\nSo for example, the code might be running on a remote server\\nwhich makes it difficult to debug locally,\\nparticularly when you're working with large amounts of data.\\nIt can be useful to have logs\\nof transactions that can be searched through later on\\nin order to trace down the root causes of problems.\\nIn addition, some kinds of errors and issues only happen\\non occasion during the program's lifetime.\\n\\nSo, logging makes it easy to categorize various events\\nthat occur while your program executes,\\nwhich can help when you're trying to track down\\nthe origin of a problem,\\nand even if you don't use logging for debugging,\\nit can provide a useful audit trail\\nof a program's execution for business analysis purposes.\\nIt's also very straightforward to customize the\\noutput format of logging data,\\nso you can record information in\\nas detailed or as simple form as you need.\\nYou know, logging is a little bit like flossing, right?\\nLike everyone knows they should do it,\\nbut not everyone does it as often as they probably should.\\n\\nSo by the end of this chapter,\\nyou'll see how easy logging is in Python,\\nand why you'll never again wonder why you need to use it.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4415167\",\"duration\":445,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Basic logging\",\"fileName\":\"4312001_en_US_04_02_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to implement basic logging features within your own applications.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6652924,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] To use Python's logging features,\\nyou need to import the logging module into your app.\\nTo send information to the log output,\\nthere are individual functions\\nfor each kind or level of message,\\nand there are five different methods to use\\nfor recording log messages.\\nThere's debug, info, warning, error, and critical.\\nEach of these methods corresponds\\nto a particular type of message,\\nwhich are used to indicate different types of status\\nof the application.\\n\\nDebug messages are typically used\\nto provide diagnostic information\\nthat's useful when you're trying to track down a problem.\\nSo information messages are usually used\\nto indicate that a particularly interesting operation\\nwas able to complete normally.\\nWarning messages indicate something unexpected happened\\nor that a more serious problem might be approaching,\\nsuch as running out of storage space\\nor the inability to communicate with a remote server.\\nError messages indicate that a particular operation\\nwas unable to successfully complete.\\n\\nAnd critical messages indicate that the program\\nhas suffered a serious error\\nand might not be able to continue running.\\nNow, by default, the logging module\\nonly outputs warning messages and greater,\\nbut you can configure this using the basicConfig function\\nand setting the level argument\\nto the minimum logging level that you require.\\nSo let's see how all of this works in real code.\\nAll right, so here are my code,\\nI've opened up the basiclog_start file,\\nand I'm going to go ahead and add the logging module.\\n\\nSo let me go ahead and import logging.\\nAll right, so let's try out the logging functions\\nto output some text to the log.\\nSo what I'm going to do is I'm going to do logging.,\\nI'm going to do debug first, and I'll just write,\\n\\\"This is debug-level message.\\\"\\nAnd then I'll just make a few copies.\\nSo we'll try out a few more.\\n\\nSo we'll do info, and we'll do warning,\\nand error, and critical.\\nAnd then we'll just make each of these changes.\\nSo that's info, that's warning, error.\\n\\nAll right, so let's go ahead and just save that\\nand let's try this out.\\nand we'll go into Ch_4, right?\\nAnd this is the basic log example.\\nSo let's go ahead and run basiclog.\\nAnd you can see that right now,\\nthe logging messages are being put out\\nto the terminal right here, right?\\nAnd only the messages for warning, error,\\nand critical were displayed.\\n\\nBut remember, we can change that\\nby using the basicConfig function\\nto set the default logging level.\\nSo let's make that change.\\nRight here in the code, I'm going to write logging.basicConfig,\\nand I'm going to set the level argument\\nequal to logging.DEBUG.\\nOkay, so now let's run the code again.\\nAnd now, you can see that the difference\\nis that all of my messages are being put out, right?\\nSo before, I had warning, and error, and critical,\\nand now, debug and info\\nhave also been output as well, right?\\nIt's worth pointing out, by the way,\\nthat this basicConfig function only gets executed once\\nbefore logging starts.\\n\\nSo once you start logging,\\nthen subsequent calls to basicConfig\\nwon't have an effect, all right?\\nSo you have to set it one time\\nand then it'll just remember that setting, so on.\\nAnd again, notice that the log output\\nis being sent to the standard output here on the terminal.\\nNow, in a production environment,\\nthat's obviously not what we would want.\\nWe would want that log sent to a separate file.\\nAnd again, we can control that\\nwith the basicConfig function.\\nSo what I'm going to do is add a filename parameter.\\n\\nAnd filename, I'm going to specify the file\\nto log information to.\\nSo I'll just simply call that output.log.\\nAnd it's kind of a typical convention\\nto name your log file with .log, all right?\\nSo let's go ahead and run this a couple times\\nand then we'll run it again.\\nAnd you can see here that my little output.log file\\ngot created in the sidebar here, all right?\\nSo each time the program is run, the log output is appended\\nonto the most recent content of the file,\\nso it continues to grow.\\n\\nBut again, you can control that with the basicConfig\\nby specifying a filemode.\\nIf you don't specify the argument,\\nthen the file is opened and appended to by default.\\nBut if I go ahead and specify filemode, right?\\nI can use filemode=\\\"w\\\",\\nand that means that that's going to rewrite the file\\neach time, all right?\\nSo here's what I'm going to do,\\nbefore I run this again,\\nlet me go ahead and click on output.log,\\nand you can see it ran several times, right?\\nIt's growing each time.\\n\\nSo let me close that.\\nAnd now I've specified filemode as w.\\nAnd also, let's write a formatted string to the log\\njust to make sure that we see some changes.\\nSo I'm going to write logging.info, let me put that down here.\\nSo I'm going to write logging.info\\nand I'm going to write an f string,\\n\\\"Here's a variable and and integer,\\\" why not?\\nSure. Okay.\\n\\nAnd I'm going to put in some string.\\nOops, no, it's going to be in here.\\nI'll have some variable is equals \\\"string\\\", and y=10.\\nAnd we'll put in x and we'll put in y.\\nAll right.\\nOkay, so now we've got the filemode\\nset to rewrite each time.\\nSo let's go ahead and close the terminal.\\n\\nAnd, oh, actually, I need the terminal to run it.\\nSo let's go ahead and run.\\nOkay, so we ran it.\\nAnd now you can see that the content was rewritten, right?\\nAnd here's my variables inserted into the string, right?\\nSo the, the content is now going to be,\\neach time I run this, it's going to be entirely replaced.\\nOkay, now you might be looking at this log file\\nand thinking that the information here is pretty basic.\\nAnd you know what?\\nIt kind of is.\\n\\nYou might want to have information,\\nsuch as timestamps, and filenames,\\nand other custom data in your log messages.\\nNow the logging module has you covered.\\nAnd we'll take a look at customizing the logging features\\nin the next video.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4410705\",\"duration\":542,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Custom logging\",\"fileName\":\"4312001_en_US_04_03_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After watching this video, you will be able to make use of the more advanced logging features of Python.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8104539,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The Python logging module is very flexible,\\nand it makes it easy to customize the message output\\ndepending on your needs and this way,\\nyou're not stuck with a single message format.\\nSo the basic config function takes two arguments.\\nThere's format and there's date format.\\nThe format argument specifies a string\\nthat controls the precise formatting\\nof the output message that is sent to the log,\\nand the date format argument is used\\nin conjunction with the format argument.\\nSo if the format argument contains a date specifier,\\nthen the date format argument is used\\nto format the date string using the same kind\\nof date formatting strings that you would pass to\\nsay the strftime function.\\n\\nThe table that I'm showing you here,\\nlists some of the formatting tokens\\nyou can use in the format argument.\\nNow, this is not an exhaustive list, okay?\\nBut I've included the most common ones\\nthat you're likely to use,\\nand they're pretty self-explanatory.\\nSo for example, the top one there,\\nthe asctime token is a human readable time format,\\nthe filename and funcName tokens\\nare for the file, and function names\\nwhere the log message originated and so on.\\nSo let's switch over to our editor,\\nand try some of these out.\\n\\nRight here in my code space,\\nI'm going to open up custom log start.\\nAnd you can see I've already got some code in here\\nthat's specifying an output file,\\nand a logging level of debug.\\nAnd I've got a couple of logging calls already.\\nSo if we just run what we have right now,\\nlet's go ahead and try that.\\nAnd I'm going to run customlog, right?\\nSo when I run that, you can see,\\nsure enough, the output log shows up.\\n\\nAnd let's go ahead and click on it.\\nSo you can see that this is the output format\\nthat we've seen so far, right?\\nThere's the logging level,\\nand then there's this root keyword\\nthat's from the root program,\\nand then my custom messages, okay?\\nSo let's define a custom format.\\nSo I'm going to go back to my code.\\nSo what I'm going to do is\\nI'm going to define a format string, all right?\\nSo I'll write fmtstr is equal to,\\nand I'm going to put in percent asctime, s, and then a colon,\\nand then I'm going to put another one in here\\ncalled percent, levelname, s, another colon;\\nand then I'm going to put in percent, and then funcName\\nfollowed by an S, and then Line colon, and then a percent,\\nand then inside parenthesis I'm going to put lineno with a d,\\nthat's going to be an integer.\\n\\nAnd then finally a percent\\nwith a message and followed by an s, okay?\\nSo this is going to put out a custom format.\\nSo it's going to show the date and time,\\nthe logging level, the function\\nwhere it was called from the line number,\\nand whatever message I'm supplying.\\nAnd now I need to specify this in my call to basic config.\\nSo I'll put in format equals fmtstr.\\n\\nAll right, so now let's run our updated code.\\nSo I'll run this again, okay?\\nAnd let's go ahead and look at our log.\\nSo here's the original information\\nthat we put in the first time, but now, right?\\nThe log output has the date and time,\\nit's got the logging level, right?\\nIt has my module, the line number in the code\\nwhere the message was called from.\\nAnd by the way, it says module\\nbecause I don't have a function that I'm calling it from.\\nIt's just the top level code, and then the message.\\n\\nSo let's get a little bit fancier.\\nI'm going to add another function to my code.\\nLet's go back and do that.\\nSo I'm going to define another function,\\nand it's not going to really do anything except log.\\nSo I'll write logging.debug.\\n\\\"This is debug level log message.\\\"\\nSo I'm also going to define a date formatting string\\nbecause I don't like this date format right here.\\n\\nAll right, I'm going to format it a different way.\\nSo let me add that.\\nI'm going to call it datestr, and I'm going to set that to\\npercent month with a slash, and then percent today,\\nand then a slash and percent capital Y for year,\\nand then a space.\\nAnd then I'm going to do percent capital I slash,\\npercent capital M slash, percent capital S.\\n\\nActually not use, let's not use slashes.\\nLet's use colons for this one, right?\\nAnd then a space, and then a percent P for AM, PM.\\nAnd then I need to add this to my basic config call.\\nSo that is going to be,\\ndate format is going to be datestr.\\nAnd then I need to call my new function.\\nSo I'll just simply call another function down here\\nto make sure that it executes.\\n\\nAll right, so now let's run this updated program,\\nsmaller, so we see it.\\nAlright, so now we have our formatting date string, right?\\nWe're calling another function.\\nThat function is doing some logging output.\\nLet's try this again.\\nLet's go back to the log file.\\nAnd so now you can see that\\nthe date formatting is different.\\nHere's what it looked like before I changed it.\\nHere's my new date format, right?\\nIt's got AM and PM.\\nIt's got, you know, the month, day, and year.\\nAnd it has different information,\\nso here I'm calling it from the top level module at line 20.\\n\\nYou can see that this debug message was called\\nfrom my another function at line number seven.\\nSo now I've got much more detailed information\\nin my output log.\\nSo there's one more thing I want to show.\\nAnd that's how to embed custom data into the log message.\\nSo suppose my program had access to information\\nsuch as the current username or their IP address,\\nand I wanted that data to be part of the log message.\\nSo each one of the logging functions\\ntakes a parameter named extra.\\n\\nLet's close the log here.\\nNow that extra parameter can be set to a dictionary object\\nthat contains key value pairs to be included in the output.\\nSo once again, I'll make another variable,\\nand I'm going to call that extdata.\\nAnd I'm just going to make this a dictionary, right?\\nAnd I'm going to, I'm going to have user,\\nand this could have come from anywhere, right?\\nAnd I'm going to have the user be,\\nyou know me, joemorini@example.com, all right?\\nSo now I'm going to modify my format string\\nto include a placeholder for this data.\\n\\nI'm just going to put that right at the front here.\\nIt's going to say User colon,\\nand then I need you to put in percent, user,\\nand so this parameter right here\\nneeds to match the key of the dictionary, right?\\nSo I can have this dictionary have a lot\\nof different values, and then I just have to\\nembed those values in my format string right here.\\nAnd then now I need to add the extra parameter\\nto each of my login calls.\\n\\nSo for example, I'm going to put in comma, extdata,\\noh no, I'm sorry, extra equals,\\nand that's going to be extdata.\\nSo I'll put that in here, copy that, paste that,\\nand we'll put it into my function up here.\\nAlright, so now I've got my extra data going in.\\nAnd one more time, actually, let's delete the existing logs\\nso that we start from scratch.\\n\\nSo let's go ahead and run this again.\\nRight, there's my log.\\nAnd now, you can see I've got my custom data\\nbeing embedded in my logging output.\\nSo that should give you a sense\\nof how powerful and useful the python logging module is.\\nAnd once you start using it, you'll start to think yourself,\\nwhy have I not been using since the entire time?\\nAnd it's really particularly useful\\nwhen you're working with large amounts of data\\nbecause you know you can log out specific parts of data\\nat different times and dates, you can tag it certain ways,\\nso you can easily search the log later.\\n\\nIt's just really useful when working with data,\\nso if you're not already using the logging feature,\\nyou should really strongly consider using it.\\n\"}],\"name\":\"4. Python Logging\",\"size\":16455160,\"urn\":\"urn:li:learningContentChapter:4413400\"},{\"duration\":30,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4415168\",\"duration\":30,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"4312001_en_US_05_01_XR30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":0,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":482714,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Joe] Well, that brings us to the end of the course.\\nCongratulations on completing the course,\\nand I hope you enjoyed learning\\nabout Python's built-in functions and features\\nfor working with data.\\nWe've only just scratched the surface\\nof what this great language can do\\nwhen it comes to data processing, however,\\nso I hope you take some time to explore\\nsome of the other great courses here in the library\\nthat cover this rapidly growing area of computing.\\nI hope to see you again in another one of my courses soon.\\nUntil then, happy coding.\\n\"}],\"name\":\"Conclusion\",\"size\":482714,\"urn\":\"urn:li:learningContentChapter:4415172\"}],\"size\":122220876,\"duration\":7527,\"zeroBased\":false},{\"course_title\":\"Python Code Challenges for Object-Oriented Programming\",\"course_admin_id\":3982821,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":3982821,\"Project ID\":null,\"Course Name\":\"Python Code Challenges for Object-Oriented Programming\",\"Course Name EN\":\"Python Code Challenges for Object-Oriented Programming\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"In this course, instructor Jonathan Fernandes guides you through a series of challenges designed to boost your skills in data analysis using Python. Learn how to utilize CoderPad for seamless code testing and debugging, within an integrated online learning environment. Discover how to create and employ Python data classes to simplify your code while maintaining robust functionality. Engage with various algorithmic problems, implementing Pythonic solutions and leveraging standard library functions. Extend your class capabilities by learning to use special methods. When you complete this course, you will have gained practical experience in solving complex problems and debugging code effectively. Whether you're a student, a professional, or someone passionate about data science, this course provides the tools and knowledge to elevate your Python programming abilities.\",\"Course Short Description\":\"Build your proficiency with Python in data analysis, as you engage with real-world datasets, solve algorithmic challenges, and enhance your coding skills with hands-on exercises.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":10223359,\"Instructor Name\":\"Jonathan Anand Fernandes\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Expert in Generative AI and Large Language Models\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2025-02-26T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"No\",\"Has Challenge/Solution\":\"Yes\",\"LIL URL\":\"https://www.linkedin.com/learning/python-code-challenges-for-object-oriented-programming,https://www.linkedin.com/learning/python-code-challenges-for-object-oriented-programming-coderpad\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Advanced\",\"LI Level EN\":\"Advanced\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Programming Languages\",\"Primary Software\":\"Python\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":1657.0,\"Visible Video Count\":7.0,\"Learning Objectives\":\"Analyze the data and select a specific format.,Choose the correct python OOP tools.,Apply and work with correct Python objects for a task.,Subdivide the problem into classes where each method performs a single function.\",\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":255,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5233323\",\"duration\":38,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Practice object-oriented programming with Python\",\"fileName\":\"3982821_en_US_00_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":43,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1947334,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Jonathan] Robert Martin, the author of \\\"Clean Code,\\\"\\nsaid the ratio\\nof time spent reading versus writing code is\\nwell over 10 to one.\\nAnd so as a Python developer,\\none of the things you want\\nto do is to reduce the amount of boilerplate code\\nso that your code is easy to read\\nand easy to understand\\nso that it can be modified if required.\\nBelieve me, your fellow developers will thank you for it.\\nNow, one of the best ways to learn is to practice\\nand get input from an experienced Python developer.\\nI'm Jonathan Fernandes with years\\nof Python expertise under my belt.\\n\\nJoin me to work on hands-on exercises\\nwriting clean object-oriented code in Python.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5232330\",\"duration\":217,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"How to practice with CoderPad\",\"fileName\":\"3982821_en_US_00_02_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":279,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6365116,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The challenges in this course,\\nwe'll be using Coderpad, which is integrated\\ninto the LinkedIn Learning course website.\\nNow all you have to do is to click on the challenge,\\nand this makes it really easy to watch videos\\nand write some code and go back and forth\\nand test your solutions all on one site.\\nNow, I recommend that you use CoderPad on a desktop browser,\\nbut you can use the LinkedIn Learning mobile app\\nif you prefer.\\nAs you can see, there are four screens in CoderPad,\\nand you can enlarge them\\nand shrink them as you need\\nwith these little handles over here.\\n\\nNow the first screen you're going to want\\nto look at is the instructions,\\nand that's over on the top left.\\nNow I wrote them for each challenge,\\nso they're really important, and please read them carefully.\\nThis will tell you all about the coding challenge\\nand what the expected output is,\\nand some of the test cases.\\nThe answer screen over on the top right is\\nwhere you're going to actually enter your code.\\nNow, I've tried to make this as easy\\nas possible for each challenge.\\n\\nSo in general you'll see an empty function\\nor some starter code.\\nAnd in this example, for example, it has the function name,\\nwhich is find largest,\\nand you just have to write your code inside of this.\\nNow, just below the answer panel is the test code,\\nand here you can see an actual value that'll be used\\nto test your function and how that code will be used.\\nAnd after you've written some code,\\nclick the test my code button on the bottom right\\nand then take a look at the fourth window,\\nwhich is the console output,\\nwhich is over on the bottom left.\\n\\nSo I select test my code.\\nNow you can see that I have a couple of test cases here,\\nand these numbers over here correspond\\nto these test cases.\\nAnd so that's four test cases over here\\nand four outputs that you can see in the console output.\\nNow, keep an eye on the console output,\\nespecially if your code is failing.\\nYou're going to see all of your program output\\nand the print statements and errors\\nthat'll help you debug your code.\\n\\nNow when you click the test my code button,\\nyou'll see a message indicating whether your code\\nreturned a correct result, right?\\nSo let me go ahead and try and look at the first test case.\\nAnd so you can see that the test case\\nthat I have here is find the largest value for a list,\\nand I only have one entry in my list, which is 5.\\nSo if I just try and return 5\\nand select test my code, you can see\\nthat it passes the first two tests,\\nbut it fails the other two.\\n\\nIf I take a look at the third test case\\nwhere I have a positive number\\nand the rest of the numbers in the list are negative,\\nand I try and return just the value 1,\\nwhich is the highest value in that list, you can see\\nthat I've passed the third test case,\\nbut I failed the other two.\\nAnd so your solution needs to pass all of the test cases.\\nSo let's go ahead and put in the right result.\\nSo to find the largest number, I'm going to have\\nto return the max of that list.\\nSo I use the built-in max function\\nand the max of the list numbers.\\n\\nAnd let me go ahead and test my code.\\nAnd you can see down here in the console\\nthat I have a successful result\\nand it shows all of the values that my code returned.\\nNow when you finished each code challenge, return\\nto the course's table of contents,\\nand click the next video to see my solution.\\n\"}],\"name\":\"Introduction\",\"size\":8312450,\"urn\":\"urn:li:learningContentChapter:5238110\"},{\"duration\":1402,\"entries\":[{\"urn\":\"urn:li:learningContentAssessment:67a157df3450999b9064dbb9\",\"duration\":600,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: The month class\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1231230\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:5238109\",\"duration\":207,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: The month class\",\"fileName\":\"3982821_en_US_01_02_C_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":316,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Choose the right data structure to store the information.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6048559,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, this is exciting.\\nLet's take a look at our first challenge.\\nSo we've got these three test cases here,\\nand we need to ensure\\nthat our code passes all three test cases.\\nSo the first test case is\\nthat we need to have a string representation\\nfor our month object so that it returns the month\\nfollowed by a right slash\\nand then followed by the year.\\nNow we can also see that if you have a month\\nwhich has just a single digit,\\nyou need to have a zero in front of that.\\nThen the second and third requirement\\nis that we need to have some sort of ordering.\\n\\nSo we need to be able to sort the month objects.\\nAnd then if I want to try and compare a month object\\nwith a date/time object, I should get a type error.\\nSo if I go ahead and select test my code,\\nyou can see that I end up with an error\\nfor my first test case.\\nAnd then there seems to be problems with\\nthere being a type error where less than is not supported\\nbetween the instances of month and month.\\nAnd that's because we haven't implemented our solution yet.\\nNow you might be tempted\\nto start writing a regular class like this,\\nand as you can see,\\nthere's going to be a lot of boilerplate code.\\n\\nAnd since you need an initializer, a string representation\\nand a quality, you can get all of this out of the box\\nwith data classes.\\nNow if you're new to data classes, here's a quick primer.\\nSo they're part of the Python standard library,\\nand the reason we use them\\nis to reduce the amount of boilerplate code\\nto define classes, right?\\nSo this means that they automatically will generate methods\\nlike dunder_init and dunder_repr\\nand dunder_equals and others.\\nSo you can import data classes from the data class package,\\nand then you go ahead and decorate the class.\\n\\nSo I've got my month class here,\\nand I decorate it by using data class over at the top.\\nSo let me just go ahead and get rid of this code here.\\nAnd then the next thing you'll want to do\\nis to specify the type of each field,\\nand then that's basically it.\\nNow what we need to do over here\\nis to create our string representation for the month class.\\nSo let's go ahead and do that.\\nAnd so we'll want to define our dunder_str method.\\nSo we define dunder_str,\\nand then I want to go ahead and return,\\nand I'm going to use an S string here.\\n\\nI want to return the month first, so that's self.month.\\nAnd then I need to remember that I should have formatting\\nso that if it's a single digit month,\\nI need to return a zero in front.\\nSo I'm going to do that with a colon and a zero two.\\nI've got my forward slash and then self.year.\\nSo let's go ahead and test our code.\\nAnd you can see that we've passed the first test,\\nwhich is this one.\\n\\nAnd now we need to go ahead and implement\\nsome means of working with comparing the date/time\\nand the month\\nand the different month objects.\\nAnd this is really easy to do in data classes\\nbecause all we need to do is to pass order\\nas one of the parameters to our decorator.\\nAnd so I just say order equals true\\nand select test my code.\\nAnd you can see that it's now passed all three tests.\\n\\nThis is just one way to go about solving this problem.\\nIf you have another solution that passes all three tests,\\nthen that's absolutely fine.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:67a157df3450999b9064dbbb\",\"duration\":600,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: First and last day\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1231231\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:5236180\",\"duration\":242,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: First and last day\",\"fileName\":\"3982821_en_US_01_04_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":313,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Compare and update the Python class to achieve the required result.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7220232,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] All right,\\nlet's take a look at this challenge.\\nNow, we have two test cases here.\\nSo in the first test case,\\nif I provide the Feb, 2030, 2 month object\\nand I want to get the first day,\\nI should return a datetime.date object,\\nwhich has 2030, Feb, and that's the first of Feb.\\nAnd similarly, in the second test case,\\nif I have the Feb of 2030\\nand I want to determine what the last day is,\\nit'll return that the last day is going to be the 28th.\\n\\nSo if I go ahead and select Test my code,\\nyou can see that I've got an attributeError.\\nThe 'Month' object has no attribute 'first_day'\\nand that's because we haven't entered any code in yet.\\nNow, calculating the first day of a month is trivial\\nas it's always going to be one,\\nbut it's not as simple\\nto calculate the last day of a given month.\\nSo let's go ahead and take a look at monthrange.\\nSo I'm going to go ahead and import calendar,\\n(keyboard clacking)\\nand I want to just take a look at monthrange.\\n\\nSo I'm going to go and print out the help\\nfor calendar and monthrange.\\nAnd let me go ahead and select Test my code.\\nAnd so the doc stream that I get for monthrange suggests\\nthat it's going to return the weekday,\\nwhich is of no help to us.\\nBut the second argument that it returns is the number\\nof days for year and month, which is really helpful,\\nbecause this will help us determine what the last day is.\\n\\nNow, one of the ways you might have done this\\nis to put the first day\\nand the last day in your dunder init method.\\nNow, the data class\\nalready created the dunder init method for us,\\nand there are ways to put it in there,\\nbut there's a better way,\\nand that's to create a property attribute\\nfor the first day and the last day.\\nSo let me go ahead and show you.\\nSo just get rid of this,\\nand let's go ahead and create first_day.\\n\\n(keyboard clacking)\\nI want to go ahead and return datetime.date\\nfor the first day.\\n(keyboard clacking)\\ndatetime, and then I can go ahead\\nand return datetime.date\\nfor the year, for the month,\\nand then I'm going to always return one.\\nAnd I want to go ahead and decorate this with property.\\n\\n(keyboard clacking)\\nAnd so this will allow me to return the attribute first_day.\\nSo let me just go ahead and modify that.\\nThat's a typo there, so that's first_day.\\nAnd let me do exactly the same thing.\\nLet me just go ahead and copy this\\n(keyboard clacking)\\nand I'm going to do this for last_day.\\nNow if you remember, we have the monthrange,\\nso calendar.monthrange.\\n\\nAnd if we provide the self.year\\n(keyboard clacking)\\nand the self.month, we'll get the weekday,\\nwhich we don't care about.\\nSo let's just go ahead and discard this.\\nI'm just going to use a underscore here.\\nAnd then we're going to get the last day.\\nSo I'm going to go ahead and include the last_date,\\nand then I can go ahead and return the year,\\nthe month, and then the last_date.\\n\\n(keyboard clacking)\\nOkay, and that should be it.\\nSo let's go ahead and select Test my code.\\nAnd you can see that we've passed both the tests\\nand these two tests correspond to these two test cases.\\nSo we're done.\\nNow, if you created another solution\\nthat passes the two test cases,\\nthen that's absolutely fine.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:67a157df3450999b9064dbbd\",\"duration\":900,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Month delta\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1231240\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:5231384\",\"duration\":542,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Month delta\",\"fileName\":\"3982821_en_US_01_06_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":676,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Combine previous results and get the correct result by creating new classes and methods.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":19340053,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] So let's head over to our test cases\\nand look at what we have here.\\nSo you can see that the first test cases,\\nwe seem to have a new class called MonthOffset\\nand we're adding MonthOffset to another MonthOffset.\\nSo we've got MonthOffset five added to MonthOffset three,\\nand we should get a total of MonthOffset,\\nmonth is equals eight.\\nSo if I just go ahead and select test my code,\\nyou can see that we've got a type error,\\nunsupported operand type for MonthOffset and MonthOffset.\\nAnd that's because we haven't created the code for that yet.\\nSo let's go ahead and update our class MonthOffset,\\nwith a dunder add, right, and so I'm going to create my method.\\n\\nSo def dunder add.\\nAnd I should be able to add a self and an other.\\nAnd the first thing I want to check is make sure\\nthat the instance type is off type MonthOffset.\\nSo if isinstance,\\nand other, MonthOffset.\\nAnd then all I want to do now is just sum up\\nthe months, for self and other.\\n\\nSo I'm going to return a MonthOffset object.\\nAnd it's going to be self.month,\\ntogether with other.month.\\nAnd if the object type that we're trying to add\\nis not of type MonthOffset,\\nthen we haven't developed the code for that yet.\\nSo let's just go ahead and return a 'not implemented'.\\n\\nAnd let's test out our code here.\\nAnd that's excellent, so we've passed the first test\\nwhere we've added a MonthOffset to another MonthOffset\\nand we've got the first test case working.\\nNow let's work our way through the other ones\\nand you can see we've got a type error\\nbecause we're trying to add a month and a MonthOffset\\nand we haven't got the code for that.\\nSo let's go ahead and add,\\na dunder add method, to our month class.\\nAnd this will allow us to support addition here.\\n\\nSo def dunder add,\\nself and other.\\nSo I want to just make sure that I'm adding\\na month to a MonthOffset type.\\nSo I want to make sure that the other is of type MonthOffset.\\nSo if isinstance,\\nother is of type MonthOffset,\\nthen let me convert the years to months.\\nSo self.year times 12\\nplus the number of months in self.\\n\\nAnd then I want to go ahead and\\nadd the number of months in other.\\nAnd we will assign this to the variable months.\\nSo months equals the sum of the self.year times 12\\nplus the self month and the number of months in other.\\nNow in order for me to be able\\nto work between years and months,\\nthe most helpful method here is\\na built-in function called divmod.\\n\\nNow if you haven't come across divmod,\\nlet me just show you really quickly what\\nthis function looks like.\\nSo let's take a look at the doc string for divmod.\\nSo print div,\\nhelp divmod.\\nAnd you can see that according to the doc string,\\nthe divmod function is in Python,\\ntakes in two numbers as arguments\\nand returns a topple containing two values.\\n\\nSo the result of the floor division,\\nwhich is also known as the quotient,\\nand the remainder when dividing\\nthe first number by the second,\\nlet's just take a look at a quick example.\\nSo if I use divmod, I say divmod.\\nAnd let's say I want to have 15 months\\nand I want to know how many years that is.\\nSo 15 comma 12.\\nIs, and then I go ahead and run that through divmod.\\n\\nNow you can see that this is\\ngoing to be one year and three months\\nand so we'd expect it to be one comma three.\\nRight, and this is exactly the result\\nthat we get over here, one comma three.\\nSo one year and three months.\\nSo let's use divmod to help us to be able to work\\nbetween the years and the months here.\\nSo let me just go ahead and clear my screen first\\nand let me just comment these two out.\\n\\nSo if I have a divmod of the number of months\\nand I'm going to do the divmod number of months, 12.\\nThat should give me the number of\\nyears and the number of months.\\nAnd then I can go ahead and return a month object.\\nRight, which will be the number of years\\nand the number of months.\\n\\nSo let's go ahead and test our code out.\\nAnd you can see that we've got,\\nthe test case two which seems to be incorrect.\\nAnd so we are adding Feb of 2030\\nand we're adding 10 months to that.\\nAnd the problem that we have here is that\\nwe have a month zero\\nand this is because of the way divmod works.\\nSo let me give you a quick example.\\nSo if I was to go ahead and...\\n\\nNo, let me comment the other line out.\\nSo if I was to go ahead and do a divmod of 12 and 12,\\nso I've got 12 months right.\\nNow 12 months should be equal to a year.\\nBut if I go ahead and print out this result here.\\nYou can see I got a topple with one comma zero.\\nThis represents one year and zero months.\\nSo if we then pass this to the month object,\\nthat will be month one comma zero,\\nbut zero is a non-existent month in year one.\\n\\nAnd so what we'll need to do is\\nwe'll need to make an adjustment to our code\\nbecause of the way divmod works.\\nAnd so we can go ahead and subtract one from here\\nand then add that back when we're\\nsending it to the month object.\\nAnd this adjustment will ensure that we don't end up\\nwith the case where the month equals zero\\n'cause we want to make sure that the month always starts\\nwith one and that represents January.\\nSo after we make this adjustment,\\nlet's go ahead and test out our code.\\n\\nAnd you can see now we've passed the second test.\\nNow you can see that we still have a type error\\nbecause we have an unsupported operand\\nand we are using a plus between a MonthOffset and a month.\\nAnd the problem that we have here\\nis that when we do a MonthOffset plus month,\\nwe don't have any support for that.\\nWe need to be able to provide support\\nfor month plus MonthOffset, or MonthOffset plus month.\\nSo let me give you an example of that.\\n\\nSo you can see that the test case two\\nand the test case three are identical.\\nSo month 2030 Feb plus 10 months\\nshould be exactly the same as 10 months plus 2030 Feb.\\nAnd the reason that we have this problem is\\nbecause when you create the dunder add method,\\nwhen the instance of the class appears\\nover on the left hand side of the operator,\\nthat's when we are providing support for that plus sign.\\nAnd we need to do exactly the same thing\\nif it's over on the right hand side.\\nAnd it's a very simple way to do that\\nbecause what we need to do is we need\\nto add support for right addition.\\n\\nAnd the right addition method is called\\ndunder R add or right add.\\nAnd since we've already created support for addition,\\nwe can just say dunder R add is equal to dunder add.\\nAnd we need to do exactly the same thing\\nfor our MonthOffset class,\\nbecause we'll end up with the same problem where\\nwe can add MonthOffset five plus MonthOffset three,\\nand that needs to be equal to\\nMonthOffset three plus MonthOffset five.\\n\\nAnd so let's go ahead and test our code.\\nAnd you can see now that we have passed all the five tests.\\nNow this is just one way to go about solving this problem.\\nAnd if you are able to use another solution\\nthat passes all the five tests, then that's absolutely fine.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:67a157df3450999b9064dbbf\",\"duration\":600,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Subtracting months\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1231243\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:5233322\",\"duration\":229,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Subtracting months\",\"fileName\":\"3982821_en_US_01_08_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":273,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Create new methods and classes to capture intermediate results.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8587168,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's go ahead and take a look\\nat our test cases.\\nSo in the first one we're subtracting a month offset\\nof five from a month offset of three,\\nand we get a month offset of two.\\nSo let's go ahead and just select Test my code.\\nAnd it's not surprising that we have a type error,\\nunsupported operand,\\nwe're trying to use a subtract\\nbetween a month offset and a month offset,\\nand we haven't created the code for that yet.\\nSo let's go over to month offset.\\nAnd now fortunately for us,\\nwe've done a lot of the work with the add,\\nand so let's use that as our starting point.\\n\\nAnd so I need to replace the dunder add with the dunder sub.\\nThe instance is going to be MonthOffset,\\nand then all I need to do is replace the plus with a minus.\\nSo let's go ahead and test that out,\\nand you can see that that's passed our first test case.\\nNow let's head over to the second one,\\nwhich is where we're going to be subtracting\\na month from a month\\nand we will get a month offset.\\n\\nSo I'm going to head up here and this is the class month.\\nAnd because I'm going to be ending up with a month offset,\\nI'm actually going to copy this bit of code\\nfrom the MonthOffset class\\nand use that as my starting point.\\nSo I want to check that\\nthe other instance is going to be a month.\\nSo I'm subtracting a month from a month,\\nand then I'm going to return a MonthOffset,\\nwhere I'm going to take the years that I have,\\nmultiply them by 12, and add the number of months.\\n\\nI'm just going to put that in a bracket\\nand I'm going to do exactly the same thing\\nover on the other side.\\nAnd so I'm going to just copy that across.\\nSo I'm going to replace self with other,\\nthat's other.year * 12 + other.month.\\nAnd this should allow us\\nto be able to subtract a Month object\\nfrom another Month object and return a MonthOffset.\\n\\nLet's go ahead and test our code.\\nAnd you can see that we've now passed\\nthe second test over here.\\nNow we're then going to look at combinations\\nwhere we subtract a MonthOffset from a month,\\nand the other way around.\\nSo let's continue to update our code.\\nNow again, we've done a lot of the work previously\\nin the dunder add, so let's go ahead and copy this across.\\nWe're going to want to use the offsets that we created earlier\\nwith the dunder add with the months-1 and the month+1.\\n\\nSo I'm going to replace this code,\\nand so this time round we're going to check\\nif the instance is also a MonthOffset.\\nWe have the self years,\\nwe group all of the years together,\\nand then this time round,\\nwe want to subtract the other.month,\\nand that should be it.\\nSo let's go ahead and test our code,\\nand you can see that we've passed all the five test cases.\\n\\nThis is just one way to go about solving this problem.\\nIf you've got another solution\\nthat passes all the five tests, then that's absolutely fine.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:67a157df3450999b9064dbc1\",\"duration\":300,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Negation and multiple months\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1231244\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:5231383\",\"duration\":182,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Negation and multiple months\",\"fileName\":\"3982821_en_US_01_10_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":243,\"solutionVideo\":false,\"editingNotes\":\"Please insert audio PU at 03:08:\\n3982821_en_US_01_10_neg_PU_C_VT\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Combine the results from previous solutions to solve additional requirements.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5316506,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this final problem,\\nwe're going to be looking at negation and multiplication.\\nAnd as you can see, the test cases,\\nthe first one's about negation\\nand the other two are for multiplication.\\nNow we're only going to be making our changes\\nin the month offset class.\\nAnd so with negation, let's go ahead\\nand define our dunder method.\\nSo it's, let's define dunder neg.\\nAnd it's pretty straightforward\\nbecause all we're going to do is we're going to\\nreturn our month offset and we're going to\\nreturn the self.months, but we're going to change the sign.\\n\\nAnd so we're going to return that with a negative sign.\\nAnd then for the multiplication, right,\\nwe're going to have our dunder mul.\\nSo with the self and other,\\nnow this time we're multiplying our month offset\\nwith an integer.\\nAnd so we need to make sure we check if it's an instance\\nof an integer.\\n\\nAnd if it is, we want to return the month offset object,\\nand we're going to multiply the month.\\nSo that's self.months by other.\\nNow, that's almost it, but dunder mul works when an instance\\nof the month offset class appears on the left hand side of\\nthat multiplication sign.\\nAnd we need to make sure that it works,\\neven if it's on the right hand side,\\njust like we did when we worked with add and dunder radd.\\n\\nSo the dunder rmul, which is what we're going to add here,\\nso the dunder rmul here allows us\\nto do a multiplication like month offset,\\n2 times minus 3\\nor minus 3 times month offset 2.\\nAnd so if we go ahead\\nand test our code, you can see\\nthat it passes all three tests.\\n\\nNow, this is just one way to go about solving this problem.\\nIf you've solved it another way, that's absolutely fine.\\nI hope you've enjoyed these code challenges.\\nAnd if you're looking for more,\\nI've got code challenges on using Python for data analysis\\nand working with data.\\nIf you're looking to dig a little bit deeper into Python,\\nI also have another course\\ncalled \\\"8 Things You Must Know in Python\\\".\\nThese are all available in the LinkedIn Learning Library.\\nNow, before I wrap up,\\nif you've enjoyed these problems in Python,\\nand if you're looking for similar Python problems that are\\nto do with data, then check out my CoderPad series,\\n\\\"Working with Data and Data Analysis\\\".\\n\\n\"}],\"name\":\"1. Python Code Challenges for OOP\",\"size\":46512518,\"urn\":\"urn:li:learningContentChapter:5239124\"}],\"size\":54824968,\"duration\":1657,\"zeroBased\":false},{\"course_title\":\"Python for Data Science and Machine Learning Essential Training Part 1\",\"course_admin_id\":3006708,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":3006708,\"Project ID\":null,\"Course Name\":\"Python for Data Science and Machine Learning Essential Training Part 1\",\"Course Name EN\":\"Python for Data Science and Machine Learning Essential Training Part 1\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"&lt;p&gt;&lt;i&gt;Python for Data Science and Machine Learning Essential Training&lt;/i&gt; is one of the most popular data science courses at LinkedIn Learning. It has now been updated and expanded to two parts-giving you even more hands-on, real-world Python experience. In part one, instructor Lillian Pierson takes you step by step through a data science and machine learning project: a web scraper that downloads and analyzes data from the web. Along the way, she introduces techniques to clean, reformat, transform, and describe raw data; generate visualizations; remove outliers; perform simple data analysis; and generate web-based graphs using Streamlit. By the end of this course, you'll have acquired basic coding experience that you can take to your organization and quickly apply to your own custom data science and machine learning projects.&lt;/p&gt;&lt;p&gt;This course is integrated with GitHub Codespaces, an instant cloud developer environment that offers all the functionality of your favorite IDE without the need for any local machine setup. With GitHub Codespaces, you can get hands-on practice from any machine, at any time-all while using a tool that you'll likely encounter in the workplace. Check out the Using GitHub Codespaces with this course video to learn how to get started.&lt;/p&gt;\",\"Course Short Description\":\"Learn Python programming skills for data science and machine learning. Discover how to clean, transform, analyze, and visualize data, as you build a practical, real-world project.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":8464985,\"Instructor Name\":\"Lillian Pierson\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Engineer, CEO, and Head of Product at Data-Mania\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2024-03-12T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/python-for-data-science-and-machine-learning-essential-training-part-1,https://www.linkedin.com/learning/python-for-data-science-and-machine-learning-essential-training-part-1-2024-revision,https://www.linkedin.com/learning/python-for-data-science-essential-training-part-1-2022-revision\",\"Series\":\"Essential Training\",\"Limited Series\":null,\"Manager Level\":\"General\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Artificial Intelligence for Technology\",\"Primary Software\":\"Python\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":27863.0,\"Visible Video Count\":49.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":285,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2715030\",\"duration\":41,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Data science life hacks\",\"fileName\":\"3006708_en_US_00_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":46,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2176232,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Lillian] Have you ever wanted to be able to copy\\nand paste a bunch of data off a website\\nor just get the gist\\nof all the content without actually having to read\\nthrough line by line?\\nIf so, then great,\\nbecause I'm going to show you how\\nto build a web scraper in Python so that you can have\\nthat data written off of the web for you automatically.\\nAnd I'm also going to be introducing you to a Python library\\nthat you can use to visualize that data in a standalone,\\nshareable web application using just a few lines of code.\\n\\nHi, I'm Lillian Pearson.\\nI'm a data and AI strategist with nearly three decades\\nof experience working with data.\\nLet's get going on Python for Data Science.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4589016\",\"duration\":59,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"3006708_en_US_00_02_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":103,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1295829,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] As far as what\\nyou should know for this course,\\nwe'll be programming in Python,\\nbut you don't need to have\\nany prior Python experience to take this course.\\nWe are going to be using Jupyter Notebooks\\nas our interactive programming environment.\\nIn case you've never worked with Jupyter Notebooks before,\\nthen, you can go check out\\nthe work with Jupyter Notebooks video,\\nin the course Python programming efficiently\\nin the LinkedIn Learning Library.\\n\\nDon't worry too much about the notebooks\\npart of this course, though,\\nbecause you'll be getting the course notebooks\\nwith the Codespaces environment.\\nMore on Codespaces in the next video, by the way.\\nLastly, as for the math and statistics requirements\\nfor this course, there are no\\nreal math or stats prerequisites.\\nI'm going to be taking you through everything\\nyou need to know during the course.\\nSo, let's get started.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4589017\",\"duration\":185,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"How to use Codespaces with this course\",\"fileName\":\"3006708_en_US_00_03_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":332,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6859569,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's look at how to configure code spaces.\\nTo begin head over to the courses repository\\nand then select this green code button.\\nSelect the Code Spaces tab,\\nand then click this plus icon\\nto create a code space on main.\\nThis action launches a new tab to prepare your code space,\\nwhich might take some time to complete,\\nso we'll revisit once it's ready.\\n\\nOkay, so as you can see, the Code Spaces is now launched,\\nand you'll find yourself in a web-based working environment\\nthat looks like this.\\nHow Code Spaces works is\\nthat essentially it initiates a virtual machine providing\\nyou a web interface with which to interact.\\nWithin the terminal here\\nyou're free to execute common Linux commands\\nand Python scripts.\\nWe'll be using the terminal\\nto build Streamlit applications in\\nchapter eight of this course.\\n\\nFor our course,\\nsimply open this folder here called Notebooks.\\nThis is the primary folder for the course,\\nbut just so you know, chapter eight scripts are housed in\\nthis folder here called Streamlit.\\nNow let me show you how to open a Jupyter Notebook.\\nLet's click on the 0403 iPython notebook file.\\n\\nDo a double click, and it opens up a Jupyter Notebook.\\nOnce you're inside the notebook, well,\\nfirst thing I like to do is just close this terminal\\nbecause we don't need it when we're working\\ninside of a notebook.\\nSo let's look at how\\nto run a cell within a Jupyter Notebook.\\nSo just click on this first code cell here\\nand press control, enter to execute it.\\nAnd it's connecting here to, you could see it was connecting\\nto a Python kernel,\\nand it's already run, so that's great.\\n\\nWhen you see this check mark here,\\nyou know that the code is finished running,\\nand this timestamp indicates how long it took\\nfor the program to run.\\nAnd this demo has now guided you\\nthrough opening a project on GitHub, creating a code space,\\nopening a Jupyter Notebook,\\nand executing it via code spaces.\\nI find Code Spaces to be a handy GitHub product.\\nIt simplifies project initiation, removing a lot\\nof the hassle involved in environment configuration.\\n\\nIt's particularly useful for those people\\nwho might be a little less familiar\\nwith Python's virtual environments.\\n\"}],\"name\":\"Introduction\",\"size\":10331630,\"urn\":\"urn:li:learningContentChapter:2714164\"},{\"duration\":1598,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4583158\",\"duration\":823,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to the data professions\",\"fileName\":\"3006708_en_US_01_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":1050,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to orient yourself in the data world. This video covers the definition of data science, data engineering, and data analytics.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":21506363,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] Welcome to Introduction\\nto the Data Professions.\\nThe conversation we're about to have is really important\\nto people who are aspiring to become data scientists\\nbecause it's vital that you understand\\nwhere you will fit in within the data profession spectrum,\\nwhen you actually do level up\\nand become a trained data scientist.\\nThe thing about the data space in general\\nis that there are many, many different types of roles,\\nand although there may be similarities\\nbetween the objectives that these roles fill,\\nthe roles are actually quite different.\\n\\nSo when you're taking a training course in data science\\nand you aspire to become a data scientist,\\nyou really do need to have a fundamental idea\\nabout where you fit in and what your responsibilities\\nand requirements should be\\nwith respect to other data professionals.\\nSo you want to have that in mind\\nwhen you get placed in the field\\nin order that in case your employer\\nis asking you to do something\\nthat is outside the scope of data science per se,\\nyou want to know that actually you transitioning your career\\ninto a similar, yet competing field.\\n\\nSo first, let's just talk about the functions\\nwithin the data space.\\nI've broken these down into data science,\\ndata engineering, data analytics, and business intelligence.\\nThese four functions work together\\nto generate business value from data.\\nWhat I really encourage you to do\\nif you're just getting started on your path\\ntowards becoming a data professional,\\nis to focus on mastering one area\\nand then leaning in on your peers\\nthat are data professionals in the other areas.\\nSo don't try and master all of these areas all at one time\\nbecause it will really impede your progress.\\n\\nIt will make it slow\\nfor you to progress down one career path,\\nand it's also going to make it confusing for you.\\nSo this section of the course\\nis really to help you compartmentalize\\nwhere you fit in\\nand what this course is actually preparing you to do,\\nwhich is data science.\\nIf you really want to learn how to become a professional\\nthat serves more than one of these functions,\\nthen I at least suggest\\nthat you master each function one at a time\\nand not try and master them all at the same time\\nbecause that's just going to make it harder for you\\nand result in you making slower progress\\nin your skill development.\\n\\nNow, let's talk about why we need clarity\\non this subject in and of itself.\\nSee, you're really the one who gets to decide\\nwhere you fit in best within a data profession.\\nSo I want to make sure that you are prepared\\nand that you can be deliberate in choosing\\nwhat you actually want to do\\nand not just what you happen to fall into\\nor what you happen to pick up\\nby basically being forced\\nto be the jack of all trades, right?\\nSo I'm trying to give a choice here\\nand a vision whereabouts\\nyou can be deliberate in the advancement of your career\\nas a data professional.\\n\\nI want you to be fully aware\\nof what you're actually signing up for\\nbefore you decide to move further\\ninto the data science space\\nor become a data scientist.\\nAnd there's lots of good news here.\\nThere's plenty of demands for skills in all of these areas,\\nand there are emerging roles\\nwithin the data space all of the time.\\nSo you're going to have opportunity as a data professional,\\nbut we do want to have clarity here\\nbecause basically less confusion in your mind\\nequals more results.\\n\\nAnd I want to make it easier for you to make this transition\\nto the data professions.\\nThat's why I'm clarifying this topic for you\\nso you can get results faster.\\nThis is a course on data science.\\nSo naturally we'll start with the data science rule.\\nAnd what data science really is\\nit's a systematic study of the structure\\nand behavior of data\\nin order to quantifiably understand past\\nand current occurrences\\nas well as to predict the future behavior of data.\\n\\nLet's look at some of the traits\\nof a typical data scientist.\\nData scientists generally should have a degree\\nin a quantitative field.\\nSo if you are already having a STEM degree,\\nit's going to be a lot easier for you\\nto transition into a data science role\\ncompared to if you are of a liberal arts background\\nor something like that.\\nAnother thing is that data scientists are programmers,\\nso they should really know how to program in R and Python,\\nand also how to write SQL queries, things like that.\\n\\nData scientists, by personality, well they're curious\\nand they're tenacious individuals,\\nso they're willing to really stick it out\\nuntil they get the findings and results that they need.\\nIf you have a background\\nin business intelligence reporting, data analysis,\\nor data-driven decision making,\\nthen you're a good candidate\\nfor looking into maybe stepping up\\ninto a data scientist role.\\nGenerally, data scientists are interested in why over how.\\n\\nSo they're looking for the patterns and correlations\\nand the trends and that basically indicate\\nwhy something is happening, not how it's happening.\\nThe reason for that\\nis that they're looking to make predictions.\\nSo once you understand why something is happening\\nthe way it's happening,\\nyou can then make predictions based on that why.\\nData scientists are known to spend hours and hours,\\nif not days, analyzing complex questions.\\nSo if you're a people person\\nand like to spend most of your time consulting\\nand working with people,\\nthen becoming a data scientist\\nmight not be the most fulfilling thing for you.\\n\\nNow, let's look at some of the typical tasks\\nthat data scientists are required to do.\\nData scientists in general derive insights from data,\\nincluding big data sets.\\nSo if you've been around in the field for a while,\\nthen you probably know what big data is.\\nIt's actually a term that's decreasing in popularity.\\nBut big data just means data of many different structures,\\nmany different velocities and many different varieties.\\nAnd a data scientist's job is to derive insights from data.\\n\\nThat data is probably going\\nto be sitting in a data engineered system.\\nAnd when I say data engineered system,\\nI mean if you're working on data\\nthat's being derived from big data,\\nthen it's probably going to be in a more innovative system\\nthan just the traditional\\nrelational database management system.\\nAlthough you may also be pulling data\\nfrom relational databases,\\njust like statisticians and business intelligence people\\nhave been doing for many, many decades.\\n\\nThe next type of task that data scientists do\\nis that they uncover correlations\\nand causations in business data\\nto support business decision making.\\nSo what this is actually called is decision support,\\nand the name makes sense in light of the task\\nassociated with that requirement.\\nAnother typical task that data scientists do\\nis they generate predictions from data\\nand communicate those predictions\\nthrough data visualization.\\nIn terms of competencies\\nthat are generally required of data scientists,\\nyou should have a STEM degree\\nwith some advanced mathematics.\\n\\nSo if you finish your calculus\\nand have gone into differential equations,\\nthat would be a good setup.\\nYou will also need to take fundamental statistics,\\nbut if you've gone into linear algebra\\nand these types of topics during your college education,\\nthen you're going to be in a good place\\nto try and move into the data science role.\\nData scientists also need to know how to code,\\nso they should know how to code\\nin languages like Python, R, and SQL.\\nThey should know how to use coding\\nto implement machine learning algorithms.\\n\\nSo these are basic typical competencies\\nthat are required of data scientists.\\nAnd just to put things into perspective,\\nthis course is Python for data science.\\nSo we'll be discussing some basic math,\\nand we'll be doing a high level overview\\nof machine learning algorithms\\nand the math that goes into them.\\nYou're going to be learning to use Python\\nto implement these.\\nThis is a coding course that's going to be teaching you\\nhow to implement data science methodologies.\\n\\nNow that you know what a data scientist is,\\nlet's compare that to a data engineer.\\nData engineering is the design,\\nconstruction and maintenance of data systems.\\nAs far as typical traits, data engineers are programmers,\\nthey're good at math, but honestly,\\nthey don't use advanced statistics\\nand really advanced mathematics very often.\\nTheir previous experience is likely to be something\\nlike working as a database architect\\nor an ETL developer,\\nwhich is extract, transform, and load.\\n\\nIt's just a process which is used to move data\\nfrom one storage system to another.\\nOther rules they may have had in the past\\nwould be database developer or security architect.\\nSo people that work as data engineers,\\nthey prefer to design\\nand build IT systems rather than to analyze data.\\nSo it's really a shift in scope.\\nData engineers are generally interested more\\nin how over why,\\nbecause they're actually building the systems\\nthat hold the data,\\nthat data scientists analyze,\\nand so they really need to build reliable systems.\\n\\nThey need to focus on how to actually get that done,\\nand they're not as much interested in uncovering the whys\\nthat are contained within that data.\\nObviously, you can see there's a bit of a scope shift there.\\nIn terms of typical tasks,\\ndata engineers design systems that collect, handle,\\nand store big data sets.\\nThey work to build modular,\\nscalable platforms for data processing,\\nand they also design,\\nbuild and maintain systems that store\\nand move big data and regular types of data as well.\\n\\nIn terms of typical competencies, people that work\\nas data engineers generally should have\\ngood computer science background,\\na background in software engineering.\\nAnd then in terms of coding requirements,\\nthey should generally know languages\\nlike Java, C++, and Python.\\nAnd moving into the data analytics professionals.\\nFirst, let's start off by what is data analytics?\\nData analytics are data products\\nthat describe data and how it behaves.\\n\\nSo these data products are generated\\nfrom data analysis and visualization processes,\\nand the people that are generally functioning\\nwithin a data analytics role would be titled something\\nlike analytics specialist or analytics expert.\\nThe role of analytics professionals\\nhas evolved significantly over the past few years.\\nIn 2023, analytics professionals are expected\\nto have a strong understanding of data science,\\nmachine learning, and artificial intelligence.\\n\\nThey're also expected to be proficient\\nin coding languages like Python and R\\nand be able to use advanced tools and platforms.\\nThey need to have a solid background in math and statistics,\\nand they also need to understand\\nthe business context in which they're working.\\nThey're often responsible\\nfor translating complex data insights\\ninto actionable business tactics and strategies.\\nIn terms of typical tasks,\\nanalytics specialists generate predictive,\\nprescriptive, and descriptive data insights.\\n\\nAnd of course, right now you are probably not exactly clear\\non what those terms actually mean,\\nbut don't worry because we're going to cover that later\\nin this course.\\nJust know that analytics specialists\\nuncover correlations and causations in business data\\nto support business decision making.\\nSo yes, they operate in a decision support function.\\nHowever, they're oftentimes achieving this outcome\\nby using applications.\\n\\nThey're often called to deploy analytics technologies\\nand software applications to generate their findings\\nand then return those insights\\nto business decision makers\\nin order to help them lead their organization\\nand become more data driven.\\nThe challenge for you in this section\\nis that I want you to sit back\\nand really think about yourself\\nand your background and your true passions,\\nand then decide which of these fields\\nreally sounds like the best fit\\nfor what you actually love doing.\\n\\nBecause you're going to thrive\\nif you are operating in a capacity\\nthat is the best fit for your personality,\\nfor your passion, and for your interests, right?\\nSo I want to encourage you to place yourself\\nin one of these three roles that we outlined here,\\nand then go ahead and just pursue that.\\nLet that be your focus for this journey in your career.\\nHopefully you will find\\nthat you are interested in pursuing the data science path\\nbecause this course is Python for data science,\\nand you're going to learn a lot about Python\\nand using it to implement data science\\nover the next several hours.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4588022\",\"duration\":297,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data science careers: Identifying where and how you'll thrive\",\"fileName\":\"3006708_en_US_01_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":502,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about three main types of data science careers, as well as info on how data science is creating remarkable changes in all industries, especially the software industry.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13100151,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] For someone taking\\nan entry-level data science course,\\nit would be easy to assume\\nthat once you've developed data science skills,\\nyou will then become a data analyst or a data scientist.\\nAnd for some people that might be true,\\nbut oftentimes, people with data science skills\\ndon't actually work as data analysts or data scientists.\\nIn fact, by 2023, over 50 types of roles\\nhad begun to require applicants to have data science skills.\\n\\nThe fact of the matter is\\nthere are really three different types\\nof data science professional,\\nand where you fit in with these really depends a lot\\non your passion and unique talents.\\nAs I see it, all data science professionals\\ncan be classified as either a data implementer,\\na data leader or a data entrepreneur.\\nBut the long and the short of it is\\ndespite the surging demand,\\nnot everyone should seek to become a data scientist.\\n\\nIn fact, pursuing that goal\\nwhen the data scientist role is not aligned\\nwith your personality is\\nsetting yourself up for disappointment.\\nSpeaking of data scientists though,\\nlet's see where they fit in\\nwithin the three-tiered ecosystem\\nof data science professionals.\\nAs you can see here, the data scientist position\\nis classified with other implementation focused roles\\nlike machine learning engineer and enterprise architect.\\n\\nData implementers are the people we can thank\\nfor coding up all of these amazing applications\\nthat make our lives today so much easier\\nand so much more interesting.\\nImplementers love to code and focus on details.\\nThey're generally happy to code on their own all day long,\\nso long as that means that they don't have to talk to\\nor interact with other people all that much.\\nA recent market research shows that data implementers\\ngenerally earn anywhere between $60,000 to $120,000 per year\\nin salary in the United States.\\n\\nBut what if you love working with people\\nand you really need to see that big picture impact\\nof your work in order to be satisfied\\nin your professional life?\\nNot to worry.\\nPeople like this are just as needed and wanted\\nin the data science space as the data implementers.\\nThey're called data leaders.\\nData leaders are the people who have data science skills\\nand are responsible for leading teams\\nand project stakeholders through the process\\nof building successful data solutions.\\n\\nData science leaders often hold titles like\\nanalytics program manager, data product manager\\nand continuous improvements manager,\\nand their salaries average from $45,000 to $110,000 per year\\nacross the US in general.\\nAnd if neither of these sounds like you,\\nthen you may be a good, old-fashioned data entrepreneur.\\n\\nOf course, as an entrepreneur,\\nyou can be either a data implementer\\nor a data leader in your own business.\\nAnd how you function within the business depends on\\nwhat you want to do since it's your business.\\nData entrepreneurs are distinguished\\nfrom their counterparts\\nby their desire for creative autonomy,\\nfinancial autonomy and their higher risk tolerance.\\nOf course, entrepreneurs who run their own business\\ngenerally hold the title of founder or CEO,\\nbut for people who are just starting off as entrepreneurs,\\nthey generally need to do services work for a while\\nuntil they can build out a more scalable business model.\\n\\nNew data science freelancers would fall into this category\\nand are often offering machine learning services,\\ndata strategy services, or even just data analysis services.\\nFor new entrepreneurs who are just starting off\\nas data science freelancers,\\nthey're often earning between $45 and $115 per hour\\nin the United States.\\nBut one of the nice things about having a business\\nis that the growth potential is pretty open-ended.\\nI mean, look at Facebook and how much money and impact\\nthat Mark Zuckerberg had with that platform.\\n\\nAt its core, Facebook is a data company.\\nNow that you know about the different types of options\\navailable to you with data analysis\\nand data science skills,\\nlet's look at why Python is such a great\\nprogramming language for analyzing data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2714159\",\"duration\":384,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Why to use Python for analytics\",\"fileName\":\"3006708_en_US_01_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":554,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to justify your choice in using Python for data science. This video covers an intro to Python, Python for analytics, and Python in big data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10046349,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's talk about why it's a good idea\\nto use Python for working with data\\ninstead of any of the other competing coding languages\\nor methodologies you could use to implement data science.\\nFirst of all, let's look at\\nwhat are your alternatives to Python?\\nFor data science, of course, you can use Python,\\nwhich is the focus of this course,\\nbut you could also use R, Julia,\\nand a program called Go, short for Golang.\\n\\nLooking into the definition of Python,\\nwhat is the Python programming language?\\nWell, Python is a high-level interpreted coding language\\nthat's useful for a wide variety of applications,\\nand it's the official programming language of Google.\\nThe benefits to using Python are\\nthat it's extremely easy to learn and it's human readable.\\nIt's got an extensive array\\nof well-supported data science libraries,\\nand it's got the biggest user base\\nof all data science languages.\\n\\nAlso, it's useful in data engineering\\nas well as data science.\\nYou can use it\\nfor building predictive web applications as well.\\nIt's basically quite extensible, and you can use it\\nfor a lot of different applications areas,\\nnot just data science.\\nAlso, another thing that's preferable about Python is\\nthat it's an extremely popular coding language.\\nAs of 2023, Python is the most popular programming language\\nof all programming languages around.\\n\\nIn the TIOBE Index, a measure\\nof the programming language's popularity,\\nPython ranks first,\\nwhereas the ubiquitous querying language SQL\\nis ranked at position nine in popularity.\\nR is another widely used language, especially in the fields\\nof data science and research and statistics.\\nBut as of October 2023, R actually ranks\\nas 17th in the TIOBE Index.\\n\\nAnd Go is ranked at the 11th place.\\nJulia, which is a relatively new language\\nthat came out in 2018, has really been growing\\nin popularity, and as of October, 2023,\\nJulia ranks 28th in the TIOBE Index.\\nSo if you're using Python for data science,\\nyou're just going to be a lot better off,\\nbecause when you get stuck, there's going to be a lot\\nof people out there that are getting stuck\\nin the exact same way as you are, and it'll be easy for you\\nto find solutions on Stack Overflow.\\n\\nAlso, the libraries in Python are super well supported\\nbecause it's a popular language.\\nIt's the most popular language\\nof all coding languages out there.\\nAnd so these are just some of the advantages\\nof learning Python for data science\\nrather than R or its alternatives.\\nHere's a Google Trends screenshot\\nthat shows you the difference between Python\\nfor data science in blue and R for data science in red.\\nAs you can see, R for data science has never been able\\nto keep up in popularity with Python for data science.\\n\\nThe nice thing about Python that makes it so desirable,\\nand this is probably why most people are pursuing Python\\nfor data science, is that it's also popular\\nfor data engineering.\\nAs you can see here in blue, that is the search trends\\nfor Python for data engineering.\\nHonestly, more people are searching Python data engineering\\nthan they are Java data engineering.\\nPython's very, very popular for data science,\\nbut people are also interested in working\\nto build out its functionality\\nwith respect to data engineering as well.\\n\\nSo if you know Python, it's good for you\\nbecause you can do data science\\nand then you can expand into data engineering,\\nand then you could get into machine learning engineering,\\nanother role we'll talk about later in this course,\\nbut basically learning Python first is\\njust a really well-rounded decision\\nand will be better for you and your long-term future.\\nNow, let's talk about why use Python for working with data.\\nPython is useful for data science, data analytics,\\nand data engineering, like we just discussed,\\nbut it's also useful in a professional academic environment.\\n\\nPython is an open-source programming language,\\nand so you can use it for web development,\\napplication development, and heck,\\nyou can even use it to build out your own games.\\nSo it's very open-ended.\\nIf you know how to use Python, then you can function\\nin a lot of different types of tech roles.\\nYou don't necessarily need to stay in the data professions\\nfor the rest of your life and you could branch off\\ninto many, many different types of roles.\\n\\nThe same is not true of other programming languages\\nlike R, Julia and Go.\\nThose languages are more likely to keep you locked down\\nand limited to working in a data science capacity,\\nnot extending much beyond that.\\nAnd that's just another reason\\nthat I love Python for data science.\\nNow, in terms of the main libraries that are used\\nwith Python for data science, for advanced data analysis,\\nyou'd use NumPy, SciPy, and pandas,\\nand then for data visualization,\\nthe most widely used libraries are Matplotlib and Seaborn.\\n\\nFor machine learning, in this course,\\nwe'll be using scikit-learn, but if you venture\\ninto deep learning later on\\nin your data science learning adventures,\\nthen you would probably be using TensorFlow,\\nKeras, or PyTorch.\\nIn this course, you're going to be learning how\\nto use NumPy, SciPy, pandas, Matplotlib,\\nSeaborn, and scikit-learn.\\nWe're not covering TensorFlow, Keras or PyTorch\\nbecause those are deep learning libraries\\nand we're only covering data analysis,\\ndata visualization, and machine learning in this course.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4583159\",\"duration\":94,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"High-level course road map\",\"fileName\":\"3006708_en_US_01_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":116,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Get oriented with this course. This video covers a roadmap for data preparation, data visualization, and the math requirements for analytics.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2545971,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that you know where data science fits in\\nwithin the spectrum of data roles\\nand you understand the basics of the data professions,\\nlet me just give you a quick roadmap\\nfor what we're about to cover\\nin the remainder of this course.\\nSo we are here now.\\nWe've just finished up Introduction to Data Professions,\\nand then in the next section,\\nwe're going to talk about data preparation basics.\\nThen we're going to go into the fundamentals\\nof data visualization\\nin a section that's called Data Visualization 101.\\n\\nAfter that, we'll be moving\\ninto practical data visualization\\nwhere you'll be learning\\nto create data visualizations for yourself.\\nNext, we'll be covering exploratory data analysis,\\nalso called EDA.\\nAnd after you've grasped the EDA basics,\\nwe'll move into Getting Started with Machine Learning.\\nAnd then after that,\\nwe'll be moving into data sourcing via web scraping.\\nSo I'll be teaching you how to generate data sets\\nby basically going out onto the internet and scraping data\\nand generating data sets you can use for making predictions.\\n\\nAnd then lastly in this course,\\nwe'll be talking about\\nbuilding collaborative analytics with Streamlit.\\nSo I'm going to be teaching you\\nhow to create interactive data visualizations\\non the internet that you can share across your organization\\nand also use for business decision support.\\nNow that you understand where we're headed,\\nlet's get started with data preparation basics.\\n\"}],\"name\":\"1. Introduction to the Data Professions\",\"size\":47198834,\"urn\":\"urn:li:learningContentChapter:4590008\"},{\"duration\":3953,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4588023\",\"duration\":200,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Intro to data preparation\",\"fileName\":\"3006708_en_US_02_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":291,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about what role data preparation plays in the data science project lifecycle and its significance.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5190004,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Preparing your data for analysis is\\none of the most resource intensive requirements\\nin data science.\\nIn fact, the general consensus is that data scientists\\nspend 80% of their time on data preparation.\\nThat means the better and more efficient\\nyou become in data preparation,\\nthe more likely it is\\nyou'll be effective as a data scientist.\\nLet's look at where data preparation falls\\nwithin the typical data analytics project life cycle.\\n\\nThe data analytics project life cycle is pretty simple.\\nIt starts off with evaluation,\\nthen you move into data preparation,\\nthen analysis and model building.\\nNext implementation and then communication.\\nThere are six main steps involved in data preparation.\\nThose are importing data, cleaning data,\\ntransforming data, processing data, logging data,\\nand then backing up data.\\nThe first step is always\\nto import the data you want to work with\\ninto your programming environment or application.\\n\\nAnd step two is cleaning data,\\nwhich involves removing duplicates\\nand removing out of range records,\\nremoving stray characters, and standardizing casing.\\nThe third step is transforming data.\\nAnd that involves treating missing values\\nand scaling and normalizing variables.\\nIt's really important to scale your data\\nbecause you need to make sure that\\ndiffering magnitudes among the variables in your data sets\\ndo not produce erroneous or misleading statistics.\\n\\nAnd this is a basic step\\nin preparing your data for machine learning.\\nThere are two ways to scale your data.\\nOne is normalization and the other is standardization.\\nNormalization is where you put each observation\\non a relative scale between values of zero and one.\\nAnd this is where you would divide\\nthe value of the observation\\nby the sum of all observations in a variable.\\nThe other alternative is standardization,\\nwhich involves rescaling data\\nso that it has a zero mean and unit variance.\\n\\nStep four is processing data.\\nAnd that generally involves parsing data,\\nrecoding data, and formatting data.\\nAnd step five is logging your data.\\nAnd this is where you would generate descriptive statistics,\\nlog your variable information,\\nand store your variable information.\\nIn terms of what type of information you need\\nto log about your variables,\\nI would consider detailing your variable name\\nand statistical description,\\nthe format of the data,\\nthe method used to collect the data,\\nthe date of the data collection, the source of the data,\\nthe location, where the data is stored,\\nand any other notes that you feel are relevant\\nto your dataset.\\n\\nLastly, you'd be backing up your data,\\nwhich just involves creating a backup copy of your data\\nand then taking it over to begin data analysis.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4589018\",\"duration\":243,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Numpy and pandas basics\",\"fileName\":\"3006708_en_US_02_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":380,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Get introduced to the two most fundamental Python libraries for data science: numpy and pandas. Learn the numpy basics of arrays, vectorized operations, indexing, types and efficient ufuncs. Learn the pandas basics on series, dataframe, data reading, and saving.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6584805,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] For embarking on any sort of\\ndata analysis journey in Python,\\nyou need to be very clear on NumPy and Pandas basics.\\nThe NumPy library is a third party Python library\\nthat is used ubiquitously across data science\\nand data analysis.\\nNumPy is a numerical library that you can use\\nto create shape and reshape data structures.\\nEach of these data structures are what's called\\nan N-dimensional array or ndarray.\\n\\nNdarrays are faster than and more computationally efficient\\nthan Python's built-in containers,\\nand because of that, almost all data analytics\\nand data science Python libraries require NumPy\\nor are what we call built on top of NumPy.\\nAn ndarray is simply a container for data.\\nEach ndarray has a distinct number of dimensions\\nas well as a size and data type.\\n\\nConsider the two ndarrays you see on the screen here,\\nthey both contain identical data,\\nmeaning technically they are the same data set,\\nbut they're stored in completely different structures.\\nNumPy makes it very easy to shape\\nand reshape your data into the exact structure\\nyou need for analysis.\\nWith just one function call,\\nyou can easily use the NumPy library\\nto arrange and rearrange your data set\\nin order to store it in various structures.\\n\\nThe flat container you see on the screen here\\nis an array structure.\\nMore specifically, a one dimensional array.\\nThe table is an ndarray.\\nIt's a two dimensional array.\\nBoth are matrix structures.\\nTechnically, they're both ndarrays,\\nbut the flat array has only one dimension,\\nso it's more clear to call it an array.\\nThe Pandas library is also a third party Python library.\\nIt's built on top of NumPy\\nand it offers an easy way to work with arrays in matrices.\\n\\nPandas is useful for its fast data cleaning preparation.,\\nits powerful analysis capabilities,\\nIts ease of use for data visualization and machine learning,\\nand also for its deep compatibility with NumPy arrays\\nand matrices because of course, it's built on top of NumPy.\\nArrays and matrices are called series\\nand data frames in Pandas.\\nA series is an array similar\\nto a one dimensional array in NumPy.\\n\\nA data frame is a two dimensional table\\nthat can hold different types of data.\\nIt's similar to a matrix in NumPy.\\nWithin pandas, a series object is either a single row\\nor column, and it's always indexed.\\nA data frame object is pretty much like a spreadsheet\\nof rows and columns.\\nThe rows and columns individually are actually series\\nof objects in pandas library,\\nand data frames are always indexable.\\n\\nIn case you're curious about all this index talk,\\nAn index is a list of integers or labels\\nyou can use to uniquely identify rows or columns.\\nIn this course, we're going to be indexing\\nusing mostly square brackets.\\nIn the following coding demonstration\\nwe're going to be using comparison operators,\\nso I wanted to give you this table.\\nYou can review it on your own j\\njust to re-familiarize yourself\\nwith using comparison operators in Python.\\n\\nThe two I wanted to point out are simply\\nthe greater than symbol and the less than symbol,\\nbecause we'll be using those\\nin the following coding demonstration.\\nThroughout this course, we're going to be consistently\\nusing the NumPy and Pandas library.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2714160\",\"duration\":813,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filtering and selecting\",\"fileName\":\"3006708_en_US_02_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"3 pickups recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1599,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to filter and select data by using indexes of series objects and DataFrame objects.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":31754399,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's talk about filtering\\nand selecting data with pandas.\\nNext up, I'm about to show you how to filter\\nand select data using plain indexing, data slicing,\\nand arithmetic comparisons using Python and Pandas.\\nIn this demonstration, we're going to work with two libraries,\\nNumPy and Pandas.\\nAnd the first thing you need to do is just to make sure\\nthat you have your library installed in your environment.\\nSo let's really quickly run pip install pandas.\\n\\nAnd in order to execute any code\\ninside of a Jupyter Notebook,\\nyou want to just hit Shift + Enter in order to run code.\\nSo that's what I did there.\\nAnd okay, so we have installed Pandas.\\nNow we want to import both NumPy and Pandas.\\nSo to do that we say import numpy as np,\\nimport pandas as pd,\\nand then from pandas\\nwe want to make sure to import DataFrame.\\n\\nAnd then we have all of that.\\nSo we will run this, Shift + Enter.\\nSo what you actually do is importing these libraries\\ninto the IPython environment.\\nThen the next step involves the creation\\nof a DataFrame object, and we'll fill it with some numbers\\nand then I'll show you how we can apply different functions\\non that DataFrame object.\\nSo let's call that object numbers_df\\nand we'll set it equal to,\\nand then we'll call the DataFrame constructor\\nand we'll pass in the np.arrange function.\\n\\nAnd then we'll say we want to generate a series of numbers\\nbetween zero and 90,\\nbut we want only every third number.\\nSo to do that, we say zero is the first number,\\n90 as the end.\\nAnd then define that we want only every third number here.\\nThe next thing we need to do\\nis to define a shape for this object.\\n\\nAnd so to do that, we will call the reshape method\\nand we'll pass in\\nthe values of 10 and three.\\n10 is going to be the number of rows,\\nand then three will be the number of columns.\\nThe next thing we need to do\\nis to set an index for this DataFrame.\\nSo to do that\\nwe'll just say index\\nand then we'll set it equal to a list.\\n\\nAnd then for each index, we just want to create a label.\\nSo we'll start with row one,\\nthen we'll go through\\nand just create a label for each index value.\\nAnd since we have 10 rows,\\nwe need 10 index values.\\nSo I'll go ahead and just copy in all of these labels.\\nAnd then also, let's name the columns here.\\n\\nSo we'll say columns equal to, and then create another list.\\nAnd then we'll just call the columns,\\ncolumns one through three.\\nSo column one, and then just finish this out.\\nI'll copy it in.\\nThis is red here.\\nSo it looks like we probably have a syntax error.\\nSo you can see here I have one too many parentheses.\\nSo let me just delete that\\nand then rerun it.\\n\\nOkay, that cleared out that error.\\nSo now let's print the numbers DataFrame object.\\nIn order to do that, we just need to say numbers_df\\nand then run this.\\nOkay, so great, now you can see\\nwe have a numbers DataFrame object,\\nand each of the index values are labeled\\naccording to the labels we passed\\ninto the DataFrame constructor.\\nSo now let's take a look at indexing and slicing.\\n\\nIndexing means accessing items from a data structure\\nlike arrays, series or DataFrames.\\nAnd there are three types of indexing.\\nSimple indexing, Boolean indexing, and fancy indexing.\\nLet's start first with simple indexing on a DataFrame.\\nTo do that, we can use the iloc indexer\\nto access items within a DataFrame.\\nOne thing you need to know\\nis that indexing starts from zero,\\nwhich means that the first element\\nis placed at the zero index.\\n\\nIn a DataFrame, we have multiple rows and columns.\\nSo in order to access an item in a DataFrame,\\nwe need to pass its row and column number\\nin square brackets separated by a comma.\\nThe value before the comma is the row number\\nand the value after the comma is the column number.\\nAnd if we want to access the second item\\nin the first row of the DataFrame,\\nthen we would just call the numbers_df object.\\n\\nAnd then we would use the iloc method,\\nand we will pass row number zero\\nin column number one into this iloc indexer.\\nAnd then when we run this,\\nwe get back the value of three.\\nSo if we look back up here,\\nwe can see that the value of three\\nis sitting in the row that is of index value zero\\nin the column which has an index value of one,\\n'cause again, indices start from zero,\\nwhich means that the first element\\nis placed at the zero index value.\\n\\nWe can use the equal notation\\nto replace a value in a DataFrame.\\nTo illustrate this better,\\nlet's replace the value at row one and column one\\nwith the number 20, and then run the code block.\\nSo we'll call the numbers,\\nnumbers underscore DataFrame object,\\nthe iloc method, and then we'll define the position here\\nas zero and one.\\nSo the value at row zero in column one,\\nand we'll set that equal to 20.\\n\\nAnd then we will, oops,\\nI missed an S here.\\nAnd then let's print out the object.\\nSo we'll say numbers underscore DataFrame\\nand then run this.\\nAnd you can see here now\\nthat what used to be value of three here\\nhas now been replaced with a value of 20.\\nNow let's look at how fancy indexing works.\\nFancy indexing is like the simple indexing\\nthat we just used, but there's a difference.\\n\\nInstead of passing single scalers,\\nwe're going to pass arrays of indices.\\nIn the DataFrame we can retrieve common items\\nbetween specific rows and columns using fancy indexing.\\nSo as an example, let's access the common items\\nbetween the second, third and fifth row\\nin the second and third column.\\nSo to do that, we call the numbers_df object\\nand we call the iloc method.\\n\\nAnd then let's define the rows\\nthat we want to return,\\nwhich would be one, two, and four,\\nand also the columns,\\nwhich is going to be columns at index position one\\nand index position two.\\nAnd then we run this.\\nAnd now as you can see, we have returned the common items\\nbetween the second, third and fifth row\\nand the second and third column.\\n\\nNow let's dig into Boolean indexing in a DataFrame.\\nBoolean indexing is done through comparison operators,\\nand in case you are not quite sure\\nwhat I mean by comparison operators,\\ncomparison operators are just like\\ngreater than, less than, equal than,\\nbasic arithmetic comparison operators.\\nSo we're going to look at comparison operators and masking.\\nComparison operators compare a single scaler value\\nwith all the values in the DataFrame,\\nand they always return a DataFrame with Boolean values.\\n\\nBoolean values are also called a Boolean mask.\\nLet's implement Boolean indexing on the DataFrame.\\nSo to do that, if we wanted to find values\\ngreater than 30 in the DataFrame,\\nwe could say mask equal to,\\nand then call the numbers_df object\\nand just say we want everything\\nthat's greater than the number 30.\\nAnd then print out the mask\\nand run that.\\n\\nThat was marked down so we need to make sure\\nthat it is actually the correct code formatting here.\\nSo I'll turn that to Python code and then run that.\\nAnd now you can see we have a DataFrame\\nfull of Boolean values\\nwhere if the value is greater than 30,\\nthen the result is returned as true.\\nAnd then if the value is less than 30,\\nthe result is returned as false.\\n\\nWe can use this Boolean mask to retrieve\\na subset of data from the DataFrame.\\nIf we pass the Boolean mask in the DataFrame,\\nit will return to us those values\\nwhich were greater than 30.\\nSo we'll say numbers_df\\nand then we just want to return the mask.\\nAnd if we run this code block,\\nwe can see that only the values\\nwhich are greater than 30 are returned.\\n\\nIf the value is less than 30,\\nthen we get NAN, not a number.\\nSo if the values are less than 30,\\nthen we just get this NaN.\\nSo as you can see,\\nthis just returns all the values from the DataFrame\\nthat are greater than 30 as defined by our mask object.\\nBoolean mask is also used to replace values in a DataFrame.\\nSo let's try that out really quickly.\\n\\nHere let's set all of the values in the DataFrame\\nthat are greater than 30\\nsuch that they are set equal to zero.\\nTo do that, we'll just call the numbers_df object,\\nand then we want to return\\nall of the numbers within the numbers DataFrame\\nthat are greater than 30,\\nand we want to set those values equal to zero.\\n\\nAnd then we'll call the numbers_df object again.\\nRun that, and you can see\\nthat all of the numbers from the DataFrame\\nthat were greater than 30 have now been set to zero.\\nThe last thing I wanted to show you here\\nis how to slice values from the DataFrame.\\nSlicing means pulling a section from the DataFrame.\\nSo we can slice values using the colon notation.\\nAny value that we put before the colon\\nis the starting index of the slice.\\n\\nAnd the value after the colon\\nis the ending index of the slice.\\nLet's slice the values from the third to the sixth row\\nand the second to third column.\\nTo do that, we'll say numbers_df\\nand then we'll call the iloc method.\\nAnd then we'll say we want the rows\\nin index position two through six\\nand column index position\\none through three,\\nand we can run this.\\n\\nAnd then as you can see, it has returned to us the values\\nthat are common between the third, fourth,\\nfifth, and sixth row and the second and third column.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4586141\",\"duration\":1084,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Treating missing values\",\"fileName\":\"3006708_en_US_02_04_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1945,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to treat missing values. This video covers dropping values, approximation, and filtering.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":35498780,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now we're going to talk\\nabout comparison operators and scalar values.\\nJust in case you don't know what a scalar value is,\\nit's basically just a single numerical value.\\nYou can use comparison operators like greater than\\nor less than to return true or false values for all records\\nto indicate how each element compares to a scalar value.\\nIn Python, by default,\\nmissing values are represented with a symbol, NaN,\\nwhich stands for not a number.\\n\\nBe warned, if your data set has zeros, 99s, or 999s,\\nbe sure to either drop or approximate them\\nas you would with missing values.\\nLet me give you an example\\nof where treating missing values is useful.\\nImagine you work in a marketing department\\nof a local car dealership.\\nYou've been tasked with summarizing recent results\\nfrom a customer satisfaction survey.\\nYou get this data set and you can see\\nthat most of the records have been completed,\\nbut Sally and Jim didn't respond with information\\nabout their opinion of quality of work.\\n\\nYou can see that here with the missing values.\\nNonetheless, Sally and Jim have responded\\nto 75% of their request for information,\\nso we wouldn't want to drop them from the survey altogether.\\nThat said, the other respondents,\\nRod, Sam, and Jane, did give information\\nabout what they thought of the quality of work.\\nSo we wouldn't want to drop this variable altogether either.\\nWhat could we do?\\nWell, we could take the average value\\nof the responses we do have, which would be an average\\nof eight, nine, and 10,\\nand then just fill in these missing values,\\nin order to generate an approximation\\nthat gives your boss a pretty good idea\\nof the customer's actual responses.\\n\\nYou'll see later in the coding demonstration,\\nwhy it's important to try and use approximation,\\nrather than just dropping missing values altogether.\\nIn the coding demonstration that's coming up,\\nI'm going to show you how to work\\nwith missing values in Python.\\nYou're going to learn how to discover what's missing,\\nfill in for those missing values, count up missing values,\\nand also filter them out.\\nLet's go ahead and look\\nat how to work with missing data and Pandas.\\nSo as you can see, this notebook is coming preloaded\\nwith our NumPy and Pandas libraries.\\n\\nWe just need to run that.\\nAnd when working in data science,\\nthere are going to be situations that arise all the time\\nwhere you encounter missing values.\\nSometimes it's caused by data entry errors,\\nother times by machine function,\\nbut really, we have a variety of ways\\nto handle missing values and data.\\nAnd I wanted to show you those.\\nBut before I do that, I need to create a dataset\\nthat we can work with.\\n\\nSo let's just call that dataset data,\\nand we'll create a dataset about a group of people\\nthat go by the names,\\nSteve, John, Richard,\\nSarah, Randy, Michael,\\nand lastly, Julie.\\n\\nAnd within this dataset,\\nwe're going to describe their age, gender, and rank.\\nSo we'll set the age equal to 20 for the first person,\\n22, 20, 21, 24, 23, and 22.\\nAs far as the gender,\\nI'm just going to copy and paste these over.\\n\\nOkay, and then let's assign them a rank\\nfor each of these people.\\nSo we'll just say two, one, four, five,\\nthree, seven and six.\\nOkay, so now we have a dataset\\nand everything seems to work fine here,\\nso what I want to do is I want\\nto use the data frame constructor\\nto create a ranking data frame object.\\n\\nSo I'll call it ranking_df.\\nAnd then we'll call the data frame constructor,\\nand we'll pass in our dataset here.\\nAnd then off of that, let's call the iloc method.\\nSo we'll say ranking_df.iloc,\\nand then let's pick the rows\\nin index position two through five.\\n\\nAnd then let's also select the column in index position one.\\nAnd we'll set all of these values equal to a missing value.\\nSo to do that, we will just call np.nan,\\nand that will set the values equal to missing value.\\nAnd then let's do the same thing for rows\\nat index position three through six\\nand column at index position three.\\n\\nAnd then lastly, let's do that\\nfor row at index position three and then all of the columns.\\nSo we can just use a colon operator here,\\nand then we can leave the start and end value undefined,\\nand that will set all\\nof the columns equal to missing values.\\nAnd then let's just print this out and see what we get.\\n\\nAnd as you can see,\\nnow, we have a data frame and it's got missing values\\nfor Richard, for all of row at index position three.\\nAnd at the other locations\\nthat we defined in our iloc indexer above.\\nThe Pandas has several different functions\\nthat are available to us for handling missing values.\\n\\nThe first step for handling missing values is\\nto detect if there are any missing values present\\nin the dataset.\\nIn order to do that, in the data frame,\\nPandas provides us two functions, isnull and notnull.\\nThe isnull function returns true for those values\\nwhich are missing values in a data frame,\\nand the notnull function returns true for the values\\nwhich are not missing.\\n\\nSo let's try both of those out.\\nLet's call our ranking data frame object,\\nand then we will call the isnull function.\\nAnd when we print it out,\\nwe see that we get returned a Boolean data frame\\nwhere true represents missing values\\nand false represent the values which are not missing.\\nNow, let's also try the notnull function.\\n\\nSo we'll say ranking_data frame,\\nand then call the notnull function.\\nAnd then when we print this,\\nyou see that it's absolutely the opposite\\nof the isnull method.\\nSo we are getting back true values\\nwhere the value is not missing,\\nand then where there are missing values,\\nwe should be getting back false,\\nwhich as you can see, we do here.\\n\\nNext, I want to show you how to apply Boolean masking\\nto show only the rows where there is a missing value\\nin a specific column.\\nTo do that, first, we need to find the Boolean mask\\nof the column age with the isnull function,\\nand then we'll pass this Boolean mask into the data frame.\\nSo let's call this whole thing bool_series,\\nand then we'll set it equal to pd.isnull,\\nand within this function,\\nwe will pass our ranking data frame,\\nand we are going to select the age column here.\\n\\nNext, let's pass the Boolean mask into the data frame.\\nSo to do that, we will just call our data frame object,\\nand then we'll pass in the Boolean series object,\\nand then print this out.\\nNow, you can see that we have only returned the rows\\nwhere the age is missing.\\nSo if the age value was not missing\\nin the original dataset,\\nthen those rows did not get returned.\\n\\nWe should also look at\\nhow to fill in missing values using the fill NA function,\\nthe replace function in the interpolate function.\\nHow these data frame functions work is\\nthat they replace missing values\\nwith some value of their own.\\nSo all of these functions help in filling missing values\\nwithin a dataset.\\nThe interpolate function is used to fill missing values\\nin the data frame,\\nbut it uses various interpolation techniques\\nto find the missing values,\\nrather than hard coding the value.\\n\\nLet's start out first by filling a missing value\\nwith a single value.\\nLet's call our ranking_df object,\\nand then we'll call the fillna function,\\nand we will pass zero.\\nAs you can see,\\nall of the missing values\\nin the data frame have been replaced with a zero.\\n\\nLooking at another example, let's fill missing values\\nwith the values that comes prior to the missing value\\nwithin the data frame.\\nIt's a little tricky for me to explain,\\nso I just need to show you.\\nWe'll use the fillna function with the data frame.\\nSo we'll say ranking_df.fillna,\\nand if we pass the method pad here,\\nand print this out.\\n\\nWhat you can see is that all the missing values\\nin the data frame have been replaced with the value\\nthat came prior to the missing value.\\nSo as you recall, all of the values in the row\\nat index position three were previously missing values.\\nAnd now, all of these values have been replaced\\nby the values that were in row at index position two.\\n\\nCan see what I mean here.\\nSo that's how the fillna function works.\\nAnother example is to fill missing values\\nwith the next value, which is not missing in the data frame.\\nSo it would be the opposite approach\\nto filling the missing value.\\nTo make this change, we can still use the fillna function,\\nbut we just need to change the method\\nto be fill for backfill.\\n\\nWe'll print this out.\\nAnd then as you can see here,\\nnow, the row at index position three has been filled\\nwith the values that came after it\\nin the original data frame, it came back up here.\\nYou can see that we had Randy, NaN, male.\\nNow interestingly, we had NaN here.\\nSo all of these NaN missing values were backfilled\\nwith the one prior, which would be six.\\n\\nSo that's why all of these values are now six.\\nThey have been backfilled from the value\\nthat was not missing that came after them.\\nLet's also look at filling missing values\\nin a data frame using the interpolate function\\nwith a linear method.\\nThe linear method ignores the index\\nand treats the values as equally spaced.\\nSo let's just call our ranking data frame object,\\ncalled the interpolate function.\\n\\nAnd here, we'll say that the method is equal to linear,\\nand run that.\\nSo now you can see that for each of the missing values\\nthat was numerical here,\\nit has been filled with a linear interpolation.\\nSo as you recall,\\nif we go back up to the original data frame,\\nwe had here in our rank column,\\nwe had numbers which were not a number,\\nand they ranged between four and six,\\nand there were three of them, right?\\nSo if we were to make a linear interpolation,\\nbetween the values of four and six,\\nthen we would come up with these values.\\n\\nAnd so that's exactly what this method did,\\nis it interpolated between the value\\nthat came before the missing numbers and the value\\nthat came after the missing numbers, column-wise.\\nWe can also drop all of the rows and columns\\nthat contain missing value use using the dropna function.\\nSo if we call the dropna function off\\nof the ranking data frame object,\\nand run this, see that all the rows and columns containing\\nat least one missing were all dropped from the data frame,\\nand all we're getting back are the rows and columns\\nwhich have no missing values whatsoever.\\n\\nNow, let's try to drop all the rows and columns,\\nwhich only contain missing and values,\\nusing the dropna function with the keyword,\\nhow equal to all,\\nand see how that changes things.\\nSo we'll say how, we'll set that equal to all,\\nand we'll run this.\\nAnd now you can see that all the rows and columns\\nthat contain only missing values have been dropped.\\nSo for example,\\nthe row at index position three was all missing values,\\nand now you can see it's been dropped.\\n\\nLet's also drop all the columns,\\nwhich contain at least one missing value.\\nWe can use the dropna function to do that as well.\\nAnd then what we would need to do here though is\\nthat instead of using how equals all,\\nwe would say access equal to one.\\nAnd you can see that each column had a missing value.\\nSo what we're doing here with the access equal to one is\\nwe are telling the dropna function\\nthat we wanted to look in all of the columns.\\n\\nAnd for all of the columns that contain a missing value,\\nthen that column needs to be dropped from the data frame.\\nAnd when we run this,\\nyou can see that we get returned no columns.\\nThat is because all of the columns had a missing value\\nin them in the original data frame.\\nSo we can go back here to the original data frame and look,\\nand there is not a column without a missing value,\\nspecifically because of this row at index position three.\\n\\nSo all of the columns have been dropped.\\nNow, if we wanted to drop all of the rows\\nthat contain a missing value,\\nwe can use the same function,\\nand then we would just change the access here to zero,\\nand run this.\\nAnd now you can see, okay, we back only the rows\\nthat have no missing values.\\nFor real, had a missing value,\\nthen it got dropped.\\n\\nAnd that is how to work with missing values using Pandas.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4586142\",\"duration\":470,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Removing duplicates\",\"fileName\":\"3006708_en_US_02_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":779,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to remove duplicates. This video covers dropping records with pandas.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16426350,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] It's really important\\nto remove duplicates from your dataset\\nin order to preserve the dataset's accuracy\\nand avoid producing incorrect and misleading statistics.\\nFor example, imagine you're analyzing a retail sales table\\nand shopaholic Sally came in three times\\nand used three different credit cards\\nto make purchases but provided the cashier\\nthe same zip code, 3-2-8-0-3, for each sale.\\nJust based on the card number, Sally looks\\nlike three different customers all\\nfrom the 3-2-8-0-3 zip code.\\n\\nIf you fail to examine other attributes of the customer\\nso that you can identify and remove duplicates,\\nshopaholic Sally's results would skew the results\\nof any customer demographic analysis\\nbecause Sally would be counted\\nas three people rather than one.\\nTo market to the 3-2-8-0-3 customers effectively\\nyou need to understand their characteristics.\\nDon't let duplicate records skew your analysis.\\nOkay, now let's look at removing duplicates.\\n\\nThis notebook is coming preloaded with Numpy and Pandas.\\nAnd as you can see here, we're also going to be importing\\nthe series as well as data frame from Pandas library.\\nSo you can just run that.\\nAnd then we need to have data\\nfrom which to remove duplicates.\\nSo let's create a data frame.\\nWe'll call it DF_object.\\nAnd we'll set it equal to,\\ncall the data frame constructor here.\\n\\nAnd then we will create three columns.\\nSo column 1.\\nAnd in column 1 let's pass in a list of values.\\nWe'll say 1, 1, 2, 2, 3, 3, and 3.\\nGreat, so let's look at column 2.\\nFor column 2, we'll just create a list of letters.\\n\\nSay A, and I'm just going to copy these over.\\nSo we'll have it be a, a, b, b, c, c.\\nOkay, and then for column 3,\\njust name this column name\\nand then we will define the values\\nas same thing, A, A, B, B, C, C,\\nbut we'll just make those uppercase instead of lowercase.\\n\\nLet's just print this out.\\nOkay, so now we have a data frame to work from.\\nClearly has duplicate values in here\\nso I wanted to show you the .duplicated method.\\nThis method searches each row in the data frame\\nand returns a true or false value\\nto indicate whether it's a duplicate of another value found\\nin a different row earlier in the data frame.\\n\\nSo let me show you how that works.\\nWe'll say DF_obj and then we'll call the .duplicated method\\nand then we'll run this.\\nAnd looking at the original data frame, you can see\\nthat if there was a duplicate value within a row\\nso, looking at row at index position 0,\\nthere were no duplicate values here.\\n\\nSo we got returned a false.\\nBut then when you look at row at index position 1,\\nwe get returned a value of true and that is because\\nof values at row index position 1 are duplicates\\nof the row prior.\\nAnd then you'll see row index position 2 returns a false\\nbecause they are not duplicates.\\n\\nBut then row at index position 3 returns a true\\nbecause, again, we have another set of duplicates.\\nNow that we have found duplicate records, let's look\\nat how we can drop them.\\nTo do that, we'll use the drop duplicates method.\\nAnd if we wanted to just drop all the duplicate rows\\nwe would simply call the drop duplicates method\\noff of the data frame object.\\n\\nAnd we won't specify anything here\\nand you'll see that for row 2\\nor the row with the series index value at 1 was dropped.\\nAnd that makes sense here because it was a duplicate\\nof the row at index position 0.\\nSo it got dropped and then using similar logic\\nthe row at index position 3 also should have been dropped.\\n\\nAnd as you can see, it was.\\nSo, yes, it looks like absolutely all\\nof our duplicate rows have been dropped from our data frame.\\nBut I also want to show you how to drop records based\\non the column values.\\nIn order to do that, I need to make a small change\\nto our data frame.\\nSo let's just go back and copy this code we used originally\\nhere to create the data frame.\\nAnd what I'm going to do is I'm going to change\\nthis letter here at the end.\\n\\nThis C here, I'm going to change it to a D,\\nwhich is a pretty minor change.\\nBut let's just see how we can use this\\nto drop records based on the column values.\\nSo I'll print this out.\\nHere we see okay, we've got changed this C out to a D.\\nSo to drop the rows that have duplicates\\nin only one column series, you just call\\nthe drop duplicates method off of the data frame\\nand then pass in the label index\\nof the column you want to de-duplicate.\\n\\nSo let's say DF_obj and then say drop duplicates.\\nAnd I'll set column 3 here and then run this.\\nAnd just as we predicted, what this function has done is\\nit has dropped the series\\nthat have index values 1, 3, and 6.\\n\\nBecause we do not have a duplicate\\nin column 3 here, that row did not get dropped.\\nI want to highlight really quick\\nthat it's important, in fact it's really important\\nthat you check your data for duplicates\\nand remove them if you find them.\\nNow it's time to move on\\nto data concatenation and transformation.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4584140\",\"duration\":795,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Concatenating and transforming\",\"fileName\":\"3006708_en_US_02_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1375,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to concatenate and transform. This video covers combining data, converting data, and reformatting data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":26344657,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Voiceover] Knowing how to concatenate and transform data\\nis really important in data analysis.\\nConcatenation and data transformation are useful\\nfor getting your data into the structure\\nand order you need for analysis.\\nFor example, imagine you're mailing out a piece\\nof direct mail advertisement.\\nYou have one table with customer ID and name,\\nand you have another table with customer ID,\\nmailing address, and age.\\nYour mailing address application requires you\\nto supply it only one table that contained\\nonly customer name and address.\\n\\nYou generate this table by concatenating your two tables\\nby customer ID, row wise.\\nConcatenating is simply combining data\\nfrom separate sources.\\nTransformation, on the other hand,\\nis converting and reformatting data\\nto the format necessary for your purposes.\\nWhen you transform your data, you convert it\\ninto the format that's required to facilitate analysis.\\nThis could include dropping data, which is essentially just\\ndropping variables or observations,\\ncan include adding data, which is adding variables\\nand observations,\\nand it also includes sorting data.\\n\\nSo, going back to our example,\\ntransformation would be when you drop the age column\\nin order to get your data into the exact format\\nthat the application would need.\\nOkay, so in this demonstration we're going to look at\\nconcatenating and transforming data.\\nAnd as you can see, the notebook is coming loaded\\nwith both pandas and numpy.\\nSo, you've got everything set up here,\\nall you need to do is just make sure you run that.\\n\\nAnd then we'll start first with concatenating data.\\nBefore we do anything else, we'd need to create\\na data frame object, which I'll call \\\"DF_obj\\\"\\nand then call the data frame constructor.\\nAnd we'll pass the np,arange function,\\nand we will say\\nthat we want to create a series of values.\\n\\nRephrase, and we will say we want to create\\na series of 36 values,\\nand then we want them to come in the shape of a 6x6.\\nSo, we'll say reshape,\\nand then say six rows\\nand six columns.\\nAnd then if we print this out,\\nyou can see, great, we have a series of numbers\\nstarting at zero, going to 35, so it's 36 numbers\\nand it's in a 6x6 shape.\\n\\nNow let's create a second data frame.\\nAnd we'll call it \\\"DF_obj_2\\\"\\nand we'll do the same process here, except for let's just\\ncreate a series of 15 numbers\\nand then with five rows and three columns,\\nand print that out.\\n\\nGreat, perfect.\\nSo, now we have something to work with.\\nLet's look at how to concatenate data.\\nTo do that, we will use the concat method,\\nwhich joins data from separate sources\\nand combines them into one combined data table.\\nIf you want to join objects based on their row index value\\nyou just call the \\\"pd.concat\\\" method\\non the objects you want joined and then pass in\\nthe \\\"axis=1\\\" argument\\nand \\\"axis=1\\\" tells Python to concatenate the data frames\\nby adding columns.\\n\\nIn other words, joining on the row index values.\\nSo, let's just test that out here.\\nSay \\\"pd.concat\\\"\\nand we'll select our data frame object\\nand our data frame object two,\\nand we'll pass in \\\"axis=1\\\".\\nNow, as you can see, we have gone from having\\na data frame with... here has six columns,\\nthis one has three columns,\\nnow we have a data frame with nine columns\\nwhere data frame object two has been appended\\nor concatenated onto the original data frame object.\\n\\nAnd in the case where the shapes didn't match up,\\nlike at row index position five here,\\nwe don't have that row in the second data frame object,\\nand so as you can see, we got missing values returned\\nin that part of the results that were returned to us.\\nNow, if we wanted to do the opposite,\\nand we wanted to join on the column index values,\\nwhat we could do is we can just\\neither say \\\"axis=0\\\" or we can just completely omit\\nthis axis parameter and by default it will\\njoin on the column index values.\\n\\nSo, I run this, and now you see that\\nthe two objects have been concatenated column-wise.\\nLet's look at transforming data really quickly.\\nSo first, let's look at dropping data.\\nYou can easily drop rows from a data frame\\nby calling the drop method and passing in the index values\\nfor the rows you want dropped.\\nSo, say \\\"DF_obj\\\"\\nand then \\\".drop\\\",\\nlet's say we want to drop the rows\\nat index position zero and two.\\n\\nWe can execute the code here.\\nThere was a syntax error where I missed brackets.\\nThere we go.\\nNow you see that the rows at index position zero and two\\nhave been dropped from my results.\\nNow, if we wanted to go ahead and instead drop\\nthe columns at index position zero and two,\\nall you would need to do is pass a parameter\\nthat says \\\"axis=1\\\" and run that,\\nand you can see now it's dropped those columns.\\n\\nLet's look now at adding data.\\nSo, let's create a series object called \\\"series_obj\\\"\\nand then we'll call the series constructor,\\nwe'll pass in the \\\"np.arange\\\" function\\nand we will say we want a series of values\\nfrom zero to five, we'll want six values.\\nAnd then let's give this series the name \\\"added_variable\\\".\\n\\nSo, to do that we'll say \\\"series_obj.name\\\"\\nand we'll set that equal to \\\"added_variable\\\"\\nand then just print this out to see what it looks like.\\nOkay, great, so it's a series of six numbers\\nthat range between zero and five\\nand the series is named \\\"added_variable\\\".\\nNow, you can use the join method\\nto join two data sources into one.\\n\\nBy default, the join method works by joining the two sources\\non their row index values.\\nLet me show you real quick how this works.\\nWe will say \\\"variable_added\\\"\\nis equal to...\\nand we'll call \\\"DataFrame.join\\\" function\\nand we will pass in our data frame object\\nand our series object,\\nand then print it out.\\n\\nAh, look, and then as you can see, our series\\nthat we created here called \\\"added_variable\\\"\\nhas now been added to the data frame.\\nNow let's try adding data using the concat function.\\nWe will create a object called \\\"added_datatable\\\"\\nand we'll set that equal to \\\"pd.concat\\\".\\n\\nWithin the concat function, let's pass\\nthe \\\"variable_added\\\" object twice.\\nSo, we're asking it to concat the \\\"variable_added\\\"\\ndata frame to itself, essentially here.\\nAnd then, let's set the parameter \\\"ignore_index=False\\\"\\nand I'll print this out, and then I'll explain\\nhow this will work.\\n\\n\\\"Added_datatable\\\", okay, good.\\nSo, what you can see here is\\nit's taken our \\\"variable_added\\\" object that we had,\\nand it's concatenated it to itself.\\nBut, because we said this \\\"ignore_index=False\\\" here,\\nwhat it's done is the index now has duplicates\\nbecause the original index values from the original object\\nhave just been concatenated into the results\\nthat were returned.\\n\\nSo, if we wanted to reset the index so that it\\nworks properly, we would want to then just say\\n\\\"ignore_index=True\\\".\\nAnd basically what we have done here\\nis we have told pandas to ignore the index\\nof the input data frame and create a new integer index\\nfor this resulting data frame.\\n\\nAnd so, the new index starts at zero,\\nand it increments by one for each row,\\nregardless of what the original index was\\nin the input data frame.\\nThe last thing I want to look at here with you\\nis sorting data.\\nTo sort rows in the data frame, either in ascending\\nor descending order, you'd need to call\\nthe \\\".sort_values\\\" method off of the data frame\\nand then pass in the by argument to specify the column index\\nupon which the data frame should be sorted.\\n\\nSo, let's create a data frame called \\\"DF_sorted\\\"\\nand we'll set it equal to our data frame object,\\nand off of that object we'll call the sort values method,\\nand then we will say we want to sort by our column\\nthat is in index position five.\\nSo, in order to do that, we would just say \\\"by=[5]\\\"\\nand then if we want it to be in descending order\\nthen we just say \\\"ascending=False\\\".\\n\\nAnd then when we print this out,\\nyou can see that our column...\\nour data from our column index position five\\nhas been sorted in descending order.\\nSo, if we look back at our original data frame,\\nyou can see that it was in ascending order\\nand so now the order has been clipped\\nand with that, all of the other values\\nin all of the other columns have also been sorted\\nin that exact same way.\\n\\nAnd that's about all you need to know about\\nconcatenating and transforming data using pandas.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4586143\",\"duration\":348,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Grouping and aggregation\",\"fileName\":\"3006708_en_US_02_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":449,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to group and aggregate. This video covers subgrouping, describing subgroups, and data aggregation.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12311338,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now, we're going to talk about data grouping\\nand aggregation.\\nGrouping and aggregation are useful for exploring\\nand describing your dataset in its subgroups.\\nImagine you are a merchant that sells fruit\\nand you have a dataset that describes\\nthe different types of fruit you have\\nand where you purchase them from.\\nTo understand what a subgroup is,\\nlook at this example of the data set\\nthat has apple and orange records.\\nIf you were to reduce this data set down\\nto its fundamental subgroups by fruit category,\\nyou would get two records, apple and orange.\\n\\nGrouping is an excellent method to use\\nwhen you want to explore and understand your data\\nand its inherent subgroups.\\nIt's useful for many, many reasons.\\nYou can group data in order to compare subsets\\nand deduce reasons why subgroups differ the way they do.\\nOr you may only be interested\\nin specific subgroups for your analysis.\\nGrouping can help you identify\\nand subset out those subgroups.\\nOkay, let's go ahead\\nand look at how to group data by column index.\\n\\nThis Jupyter notebook is coming preloaded\\nwith numpy and pandas, so just make sure to run that.\\nAnd then for this demonstration,\\nwe're going to use a dataset called mtcars.\\nThe first thing we need to do\\nis define a location for that dataset.\\nSo it's going to be address, I say address is equal to,\\nand what you need to do is actually go\\ninto the data folder here and find mtcars dataset,\\nand then right click.\\nAnd you want to copy the full path here.\\nSo Copy path, and then back to the Jupyter notebook\\nand paste the path in here.\\n\\nAnd now you have defined the address\\nwhere the CSV file sits.\\nTo read it in, you want to use the read CSV function,\\nso we'll call this dataframe cars\\nand we'll say cars is equal to pd.read_csv,\\nand we'll pass in the address.\\nAnd then let's assign names\\nto each of the columns in this dataset.\\nSo to do that, we need to access the cars columns.\\nWe'll say cars.columns and let's have that equal to a list.\\n\\nWithin that list,\\nwe'll pass in the name of each of the columns.\\nSo the first column name is car_names.\\nAnd then I'm going to go ahead\\nand just paste in the rest of these.\\nIn a different coding demonstration,\\nwe will dig into further details\\nabout what these variables mean as needed.\\nBut for now, I'm just trying to show you\\nhow to group data by column index.\\nSo let's just take a look\\nat the first five records in this data set.\\nWe'll say cars.head, call the head method off of that,\\nand then print it out.\\n\\nAnd here we go, we have the first five records\\nfor the mtcars data set.\\nSo we're good to go there.\\nNow, what I want to do is show you how to group this dataframe\\nby a particular column.\\nAnd to do that we'll use the group by method.\\nSo let's create a new dataframe called cars_groups.\\nAnd we'll form that by calling the groupby method\\noff of the cars dataframe.\\nAnd then passing in, we'll be selecting the cylinder column.\\n\\nSo we want to say cars.groupby.\\nAnd then we pass in the cars dataframe,\\nand then we'll select the cylinder column here.\\nAnd so say we want to generate a mean of the values,\\nthe numeric values in the dataframe\\nas grouped by the cylinders column.\\nSo to do that, we would say cars_groups,\\nand then call the mean method off of that.\\n\\nWe need to pass in a parameter\\nthat says numeric only equal to true,\\nbecause we only want to be looking at the numeric values\\nsince we're calculating a mean.\\nAnd then here, we can see there's a little typo.\\nSo I'll fix that and then run this.\\nOkay, I need to add an S here.\\nOkay, great.\\nSo now, just to show you what this is doing,\\nthe groupby method has grouped the entire dataframe\\nby the different categories that were in the cylinder field,\\nwhich the cylinder field only had the values\\nof four, six, and eight.\\n\\nSo the dataframe is now grouped according to those values.\\nAnd then for each numeric field within this dataframe,\\nmean value was generated for each grouping.\\nSo to make this a little more relatable,\\nlet me just explain how.\\nHere, you can see, okay, for our four-cylinder group,\\nthe average, the mean miles per gallon is 26.6.\\nAnd then for our six-cylinder cars, our miles per gallon,\\nthe average miles per gallon for six cylinders cars is 19.7.\\n\\nAnd then for eight-cylinder cars,\\nthe average mile per gallon is 15.1.\\nAs you can see, as the cylinders increase\\nthe average mile per gallon that the cars gets decreases.\\nAnd that is how you group data by column index.\\n\"}],\"name\":\"2. Data Preparation Basics\",\"size\":134110333,\"urn\":\"urn:li:learningContentChapter:4583163\"},{\"duration\":1570,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4583160\",\"duration\":223,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Importance of visualization in data science\",\"fileName\":\"3006708_en_US_03_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":392,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn why visualization is a key aspect of data science. Learn how data scientists effectively communicate data, insights, and predictions.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5117666,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Presenter] And let's talk about the importance\\nof visualization in data science.\\nData visualization is important\\nbecause data insights cannot very easily be converted\\nto business value if they're not communicated effectively.\\nData visualization is the process of transforming data\\nin the form of graphs, charts,\\nor any other form of visual scheme.\\nThe purpose of data visualization is to explore data\\nand to extract useful information from it.\\n\\nVisualization helps in finding patterns,\\nleaning valuable insights,\\nand observing trends in the dataset.\\nDifferent types of graphs are used for data visualization.\\nLet's take a look at some examples of graphs\\nand their utilization for various problems.\\nA line graph graphically displays data\\nthat changes continuously over time,\\nor whatever variable you have mapped out over the X axis.\\nLine graphs have a horizontal axis and a vertical axis.\\n\\nWe use line graphs\\nwhen we want to demonstrate trends in data.\\nSimilarly, we can also use line graphs\\nwhen we want to compare different variables\\nin specific time periods.\\nBar graphs are one of the most popular types of graphs.\\nThey are popular for an obvious reason.\\nThey're easy to understand.\\nGenerally, how bar graphs are set up\\nis that on the horizontal axis,\\nwe represent the name of the categorical data,\\nand on the vertical axis,\\nwe demonstrate the measurable value.\\n\\nBar graphs can be used\\nto visualize the distribution of data,\\nand similarly, we can also use bar graphs\\nwhen we want to compare data of different categories.\\nThere are several other scenarios\\nwhere we would use bar graphs.\\nPie charts use pie slices\\nto display relative sizes of data categories.\\nYou can use pie charts\\nto show the relative sizes of many things.\\nFor example, what type of transport most people use,\\nor how many customers a shop has\\non different days of the week.\\n\\nWith technological advancements\\nand generative AI in the picture,\\nhuge volumes of data are generated on the internet\\non a daily basis.\\nWith machine learning,\\nyou make decisions based on actionable insights\\nthat you predict from datasets.\\nHuge datasets can be very tricky to interpret\\nand extracting information from them is a tedious job.\\nData visualization makes this task much easier.\\nData visualization makes decision-making\\nand problem-solving easier for data scientists.\\n\\nIt can help you understand the next steps\\nthat must be taken to complete a project.\\nSimilarly, it makes it easier for you\\nto document your findings\\nand communicate them visually with stakeholders.\\nLet's take a real-life example\\nwhere data visualization can help\\nin business decision-making.\\nHere's a graph showing the number of sales produced\\nfrom a sports store.\\nThe data is collected each hour\\nfrom the time when the store is open\\nto when the store is closed.\\n\\nThe store manager needs to decide\\nwhich time slot is the busiest at the store.\\nIn other words, which period requires\\nthat maximum staff be present at the store?\\nIf we take a look at the graph,\\nwe can see the spike in sales at the store after 4:00 PM,\\nand this goes on till 1:00 AM.\\nWith this, we can conclude\\nthat 4:00 PM to 1:00 AM is the busiest period at the store\\nand the maximum number of staff are needed\\nat this specific time interval.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4584141\",\"duration\":522,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"The three types of data visualization\",\"fileName\":\"3006708_en_US_03_02_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":767,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to identify three types of data visualization. This video covers data storytelling, data showcasing, and data art.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14070959,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this segment,\\nwe're going to be talking about the three\\ntypes of data visualization.\\nThose data visualization types are data storytelling,\\ndata showcasing, and data art.\\nData storytelling is the type\\nof data visualization you would create\\nfor generating presentations that you would use to present\\nto organizational decision makers.\\nData showcasing is the type\\nof data visualization you would be making\\nfor generating presentations for analysts, scientists,\\nmathematicians, and engineers.\\n\\nLastly, data art is the type\\nof data visualization you would create\\nfor presentations when you're presenting to activists\\nor the general public at large.\\nData art is pretty effective in these cases.\\nIn this section of the course, we're just going to go\\nthrough each of the different areas\\nand get them really well-defined for you so\\nthat you can use them effectively across your career.\\nDiving into data storytelling first, the purpose\\nof data storytelling is to make it easy for your audience\\nto really understand the point you're trying to make\\nwith the data visualization.\\n\\nYou need them to understand the point you're making\\nwith your data visualization,\\nand you want them to be able to understand that within 10\\nto 15 seconds of looking at the graphic.\\nIf they have to study it for longer than that, you're going\\nto lose their attention.\\nThat and your data visualization needs to be clutter-free\\nand highly focused.\\nAgain, you have to think about your intended audience when\\nyou're creating a data visualization.\\nIn these cases, when you are generating data\\nstorytelling pieces, your audience is going\\nto be comprised mostly of non analysts\\nand non-technical business managers,\\ndecision makers and leaders.\\n\\nThey do not want to wade through a bunch of clutter\\nand a bunch of details that they don't need\\nto make a conclusion.\\nThey want you to tell them your findings within the data\\nand basically show that in a visual manner.\\nWhen you're creating a data storytelling product\\nthat would be in the form of something like a static image\\nor a very simple interactive dashboard, maybe something\\nthat you would create in an application like\\nXcelsius by SAP.\\nIn this course, you're going to be learning how\\nto use a library called Streamlink\\nto create interactive data visualizations.\\n\\nThis slide demonstrates a great example\\nof data storytelling.\\nIf you look at this example here, you should be able\\nto understand what the graphic is telling you within about\\nfive seconds.\\nThe headline says, \\\"What's really warming the air?\\\"\\nObviously, they're talking about global warming,\\nand there is a simple line chart with a trend\\nthat's increasing.\\nBasically, without even thinking,\\nyou can tell that whoever produced this data storytelling\\npiece is trying to tell you\\nthat there's an increase in global warming.\\n\\nDo you see how simple that is to analyze and understand?\\nYou want to make sure that your data storytelling products\\nare also simple and easy to understand in this way.\\nNow, let's look at data showcasing.\\nData Showcasing is where you showcase a lot of data so\\nthat your audience members can think for themselves.\\nThis is an opposition to what you do with data storytelling.\\nData storytelling is\\nwhere you present your findings in a very, very clear\\nand easy to understand way.\\n\\nWhen you do data showcasing,\\nthe audience is actually looking\\nto draw conclusions for themselves.\\nTo that end, you need to make sure\\nthat your data visualization is highly contextual,\\nthat you've included all sorts of background information\\nto help people in your audience draw their own conclusions.\\nThe data visualization should be open-ended.\\nIn other words, you are not telling them your findings,\\nbut allowing them to think for themselves.\\nThe intended audience\\nfor data showcasing products would be analysts, quants,\\nengineers, mathematicians, scientists, basically people\\nwith an analytical mindset, background in that,\\nbecause this is the kind of thing that's going to be\\nof value to them.\\n\\nIn terms of product types,\\nwhen you are doing data showcasing,\\nyou'll be producing static images\\nand interactive dashboards,\\nand you're going to learn how to do both\\nof these things throughout the remainder of this course.\\nHere's an example of data showcasing.\\nI actually took this example from my book,\\n\\\"Data Science for Dummies.\\\"\\nYou can see that here,\\nthere's a lot of data that's being showcased on one screen.\\nYou could probably spend 15 minutes visually exploring this\\ndata visualization, looking for trends,\\npatterns, and correlations.\\n\\nYou could come away from this with many,\\nmany different types of hypotheses.\\nData showcasing is a good tool for exploratory data analysis\\nand for allowing other analysts\\nto do exploratory data analysis on a visual basis without\\nthem actually having to get in\\nand generate that data visualization for themselves.\\nIt's a great presentation tool.\\nLastly, let's look at data art.\\nYou can use data art to make a statement.\\nMost data art is used in something like data journalism,\\nor if you're trying to get your community\\nor a community of activists to be engaged in taking action,\\nthen you would benefit by generating a piece of art\\nthat's based on data.\\n\\nSomething that's really riveting\\nand inspiring or provoking to get them to have an opinion\\nand take action based on the data you're presenting them\\nthrough the artwork.\\nYour data visualization should be attention-getting\\nand creative, if not controversial,\\nbecause the more controversial it is,\\nthe more action you're likely to inspire.\\nIntended audiences\\nfor data art tend to be people like idealists, dreamers,\\nartists, and social activists.\\n\\nData art is a tool to educate, inform, and motivate\\nthe public in most cases.\\nIn terms of product types,\\ndata art is almost always produced as static images.\\nIn some cases, it's also produced\\nas an interactive data piece on the internet.\\nStatic data visualizations are generally\\nsufficient here though.\\nHere's an example of data art\\nthat I took from periscopic.com.\\nIf you're like most people, then you look at this example\\nfor about 10 to 15 seconds\\nand you understand exactly what it's trying to say.\\n\\nIt's saying that US gun laws need to change.\\nThis data art is attempting to get people\\nto rally up against the Second Amendment\\nand get gun laws tightened up in America.\\nIt's using data art to achieve that goal.\\nAs you can see here on the left, you've got an account\\nfor the number of people killed,\\nand that's 9,595 people\\nas highlighted in orange.\\nThen on the right side of this example, you see\\nthat they have actually counted up an approximation\\nof the number of years that were lost from those lives,\\nfrom those people that were killed.\\n\\nWhat they're really trying to do in this piece is\\nthat they're trying to numerically exaggerate the\\ndetrimental impact\\nthat US gun laws have upon the population as a whole.\\nIn order to do that, they're breaking down the number\\nof people killed to the number of years of life\\nthat's been lost.\\nIt's clear that they really want to rally up people in a\\npolitical way and get them\\nto take action against a Second Amendment.\\nBy exaggerating impact, they're looking for ways to amplify\\nthat impact as much as possible by using a metric\\nthat's got the highest number possible.\\n\\nIn this case, it's number of years of life lost\\nbecause of the gun laws.\\nThen if you look here under the title, it's not the number\\nof years of life lost.\\nThey call it stolen.\\nThey're really trying to impassion people\\nto take action against gun laws.\\nThey're saying, okay, people's lives are being stolen here.\\nIn fact, that may or may not be true\\nbecause a lot of times death from guns happen as a result\\nof deliberate criminal activity that a person chooses\\nto participate in despite its high risk nature.\\n\\nIf a person dies via a gunshot wound in this case,\\nis that truly a life stolen?\\nIt's up for you to decide.\\nWhat's presented in this visualization is a\\nmatter of perspective.\\nWhoever created this is really trying\\nto push people in their audience to get out\\nand change gun laws.\\nThey're infusing their opinions in order to get that done,\\nbut they're using data art as a way to drive impact.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4590003\",\"duration\":417,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Selecting optimal data graphics\",\"fileName\":\"3006708_en_US_03_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":575,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to select optimal data graphics. This video covers graphics for storytelling, graphics for showcasing, and graphics for data art.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15285627,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] It is time to talk about selecting\\noptimal data graphics.\\nYou're going to use different types of data graphics\\ndepending on the type of data visualization you're creating.\\nAgain, referring back to the last section,\\nyou learned that you can create data storytelling pieces,\\ndata showcasing visualizations, or data art.\\nIf you were to choose graphics for data storytelling,\\nappropriate graphics for this type of data piece\\nwould be things like pie charts, line charts,\\nbar charts, even area charts, point maps,\\nor chloropleth maps.\\n\\nA chloropleth map is a map that shows area boundaries.\\nIf you looked at a map of the United States,\\nyou would see 48 different areas outlined,\\nthat's a chloropleth map.\\nNow, if you were to take a look\\nat a map of the United States\\nto examine major cities within the United States,\\nyou would see points to indicate\\nwhere the major cities are located,\\nthat's a point map.\\nThe reason why these types of graphics are appropriate\\nfor data storytelling\\nis that people understand what they mean.\\n\\nAgain, going back to the purpose of data storytelling,\\nthat purpose is to share your findings\\nwith an audience of people\\nwho are generally not very quantitative\\nor analytical by nature.\\nBecause this type of audience tends not\\nto be very analytical,\\nthey generally want to be given the findings\\nin a way that is comfortable\\nand easier for them to understand.\\nThat's why these are the best type of graphics\\nwhen we are creating data storytelling pieces.\\nNow, in terms of creating data showcasing pieces,\\nyou have a little bit or a lot more options\\nwith respect to the type of graphics you can use here.\\n\\nOf course, you can use the same type of graphics\\nthat you would in data storytelling,\\nbut you also want to add more context\\nto data showcasing pieces.\\nIn addition to pie charts, line charts,\\nand bar charts, you would also want to include\\nor could include things like histograms,\\nscatter plots, scatter plot matrices, and even raster maps.\\nTo add a little bit of clarity here\\non what exactly a raster map is,\\nI brought in a picture of a Doppler weather radar\\non top of a geographic map.\\n\\nBasically, what a raster map is,\\nis it's a raster file that's actually made\\nof an X and Y grid, which is filled\\nwith a variety of numerical values.\\nThe grid is X and Y.\\nThe numerical values are colored according to count.\\nIn this case, the raster is showing you\\nthat where it's red, the value is higher,\\nand then as the value decreases,\\nit goes from yellow to green to blue,\\nand then turquoise.\\n\\nRaster coverage is just a coverage\\non top of a geographic map that provides\\na layer of information about one metric.\\nSince data showcasing is intended\\nfor analytical audiences, STEM grads,\\nanalysts, engineers, quants, these type of people,\\naudience members usually want\\nthat extra bit of information,\\nand these would be an appropriate graphic type\\nto use in the data showcasing piece.\\nOkay, now let's look at data art.\\n\\nIn terms of data art, like we saw in the last section,\\ndata art is intended for the general public\\nor for activists, dreamers, and doers.\\nThey're generally not super analytical people,\\nso we would want to keep it simple for them.\\nYou could use a line chart, a graph network,\\nor a chloropleth map to communicate\\nas the data graphic within data art.\\nBut honestly, most of the time when you see data art,\\nthe data is shown in some sort of weird\\nor artistic representation\\nthat actually isn't a data graphic at all.\\n\\nThe creator often finds some sort of artistic way\\nto represent their data\\nand what they're trying to present about that data.\\nTo try and make it just a little more clear\\nabout what I mean when I say weird\\nor creative data visualization,\\nI brought in this data visualization\\nthat was presented by Mona Challabi and TEDx at NYC.\\nShe was basically educating her audience\\non three ways to spot a bad statistic.\\n\\nBut as you can see here,\\nthe state of the visualization is definitely provocative\\nbecause it's kind of gross.\\nWell, it's really gross,\\nand it's also really, really creative\\nas data visualizations go.\\nIn this case, the graphic represents\\nthe peak month of flu virus and the times per month\\nthat this season's peak since 1982.\\nThe creator is trying to make a point\\nabout when flu viruses peak,\\nand they've created this provocative data visualization\\nto get the point across.\\n\\nThis is a fine example of data art.\\nLet's now move on to the four steps\\nto choosing the right type of data graphic\\nwhen you're creating a data visualization.\\nStep one is that you make a list of questions\\nthat your data visualization is meant to answer.\\nStep two is to consider\\nwhether your data visualization type\\nshould be data storytelling,\\ndata showcasing, or data art.\\nYou would make that determination\\nby thinking about your intended audience,\\nwho is meant to consume the data visualization.\\n\\nStep three is just to answer the question,\\n\\\"What data graphic types are preferable\\nfor this type of data visualization?\\\"\\nWe covered this topic in this lecture here.\\nOnce you've answered that third question,\\nmoving into step four,\\nyou go ahead and you test out\\nthe different types of data graphics with your data\\nand decide which graphic type displays\\nthe most clear and obvious answer to your question.\\nLet me illustrate what I mean\\nbecause it could sound kind of vague\\nif you haven't seen an example.\\n\\nIn step four, we're actually testing out\\nyour data graphic to see\\nwhich is the most effective visual communication tool.\\nJust look at this example.\\nYou see that you have two data graphics.\\nThey both represent the same statistic,\\nbut you notice that the data graphic on the right\\ndoes a much better job of visually emphasizing\\nthe differences in values.\\nOn the left, you basically can't see\\nany difference whatsoever,\\nso it makes it very hard to communicate the data.\\n\\nYou should always test your different data graphics\\nto make sure that you use the one\\nthat is most clear and effective\\nin displaying your data\\nand the trends and patterns within the data.\\nThe graphic on the left below\\nis definitely not the most effective choice.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4588024\",\"duration\":408,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Communicating with color and context\",\"fileName\":\"3006708_en_US_03_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":590,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to communicate with color and context. This video covers color schemes, annotation, and visual context.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11462929,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now let's talk about communicating\\nwith color and context.\\nThe correct choice of color and context can really,\\nreally add a lot of value to your data visualization,\\nso it's an important topic for us to cover here.\\nStarting first with creating context with color.\\nColor should be used strategically,\\nsparingly, and consistently.\\nWe want to use color to draw attention\\nto the most important parts of your data visualization\\nand away from the parts that are not that important.\\n\\nThe first and foremost rule you need\\nto follow when you're choosing a color scheme\\nfor your data visualization is that you need to make sure\\nthat the color stage you choose\\nto use in your data visualization are all from the\\nsame color formula.\\nThere are many different options out there\\nfor color formulas.\\nPopular ones include monochromatic, analogous,\\nand complementary,\\nbut then there's also split complimentary, triadic\\nand tetradic and many other options out there as well.\\n\\nColor formulas are based on extensive color theory\\nthat's been adapted over hundreds of years,\\nand so you want to use them in order to ensure\\nthat you're using a color scheme\\nor a color formula that's going to be harmonious to the eye\\nof the end consumer.\\nYou also want to make sure that you have some shades\\nthat are useful for drawing the attention of the eye\\nand other shades that are more muted so\\nthat you can use your color strategically\\nto attract attention where it's needed.\\n\\nIn case you're looking for a good tool\\nto find good color schemes\\nto use in your data visualizations,\\nthere is an AI driven color matching tool\\ncalled Colormind IO.\\nI'll put a link for you here.\\nI took this color scheme\\nthat you're seeing on this side from Colormind,\\nand it's actually a very good example of something\\nthat could be used in a data visualization quite well.\\nAs you can see, there are two shades that really pop,\\npurple and red, and the other three shades are muted\\nor they don't draw a lot of attention,\\nso you can use the muted shades to basically plot out data\\nthat is of lesser interest.\\n\\nAnd then the purple\\nand red shades would be good\\nfor really showcasing whatever trend\\nor finding that you're demonstrating.\\nIn other words, the part of the data visualization\\nthat you want to stand out the most to the consumer.\\nAll of the color schemes that are generated\\nwith a Colormind tool include shades\\nthat are within the same color formula.\\nThis tool takes care of that for you, which is very nice.\\nNow, moving into annotation,\\nyou can see here an example I got from USA ID\\nwhere they're basically using annotation to add context\\nand tell the story around how the data is performing.\\n\\nUsing annotations as context is useful\\nfor providing your audience some information about why the\\ndata behaves as it does.\\nAnnotation is especially useful in data storytelling\\nand data showcasing because they add a layer of context\\nand meaning to the data visualization\\nthat helps consumers understand\\nwhat it is that you're showing them.\\nAnother way that you can add context\\nto a data visualization is to introduce graphic elements.\\n\\nIn this case, we're looking at a trend line,\\nbut also single value alerts\\nand benchmarks are very useful in helping your audience\\nunderstand the relative significance\\nof the data that you're showing them.\\nLet's talk a bit about the mechanics\\nbehind creating context in a data visualization.\\nYou can create context in your data visualization just\\nby adding data on additional and relevant metrics.\\nFor example, if you were\\nto be featuring some statistics on dropout rates in a\\nparticular region of a city, some data\\nthat might be a good contextual layer\\nto add would possibly be data on the household\\nand the parents' level of education,\\nand if the parents were also dropped out of high school,\\nbecause it would seem that if a child comes from a household\\nwhere both of the parents\\nor one of the parents dropped out of high school\\nand did relatively okay with their life,\\nthat the child would also feel like dropping out of\\nhigh school would be a viable alternative for them.\\n\\nSo that would be an example of adding relevant data sets\\nand additional metrics around\\nwhat you're featuring in your data visualization.\\nAbout why you'd want to create context.\\nBy adding context,\\nyou're giving your audience a deeper perspective\\nand insight into what's happening with the situation\\nthat's represented by your data visualization.\\nOther ways that you can add context are, again,\\nlike I stated, just adding trendlines, single value alerts,\\na strategic use of color and annotations.\\n\\nAdded context is especially useful in data showcasing\\nbecause again, data showcasing pieces are meant\\nfor analytical audiences that really want to put a lot\\nof thought into drawing conclusions from the data\\nthat they're looking at.\\nBut that said, context is also useful in data showcasing,\\nbecause usually in data showcasing,\\nyou would just add a little bit of textual context\\nor maybe a trend line to clarify exactly what you're trying\\nto tell your audience.\\n\\nYou wouldn't expect them to go in\\nand read multiple layers of annotation\\nor to go deep into evaluating the visualization,\\nbut you may want to give them some text\\nor markers in order to make it abundantly clear\\nwhat you're trying to tell them\\nwith your data visualization.\\nHere's an example of a good solid data visualization.\\nWhat I really love about this data visualization is its\\nstrategic use of color.\\nThe color is done right.\\nThis data visualization is taken from Data Lab's agency,\\nand they have really done a great job in using color\\nto draw attention to what matters most in this piece.\\n\\nThey are using more muted shades in order\\nto underlie and support their data visualization\\nfor completeness, but they use the red\\nand orange to really draw attention\\nto the points they're trying\\nto make within the data visualization.\\nNow that you know the basics of data visualization,\\nthe types of data visualizations you can create,\\nhow to select optimal graphics\\nand how to communicate well with color\\nand context, it's time to move into the next section\\nof the course where we'll talk about practical data\\nvisualization and how\\nto build data visualizations using Python.\\n\\n\"}],\"name\":\"3. Data Visualization 101\",\"size\":45937181,\"urn\":\"urn:li:learningContentChapter:4586146\"},{\"duration\":5866,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4590004\",\"duration\":1069,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to the matplotlib and Seaborn libraries\",\"fileName\":\"3006708_en_US_04_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1651,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Get a practical introduction to Python's matplotlib and Seaborn libraries. Learn how to set up an environment for creating data visualizations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":36335610,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this lecture, we'll explore two\\nof Python's most powerful visualization libraries,\\nMatplotlib and Seaborn.\\nWhile both are highly capable,\\nthey have distinct features and use cases.\\nSeaborn stands out for its simplicity and its elegance.\\nIt requires less complex syntax\\nand offers beautifully designed default themes,\\nmaking it ideal for creating sophisticated visualizations\\nwith very little effort.\\n\\nOn the other hand, Matplotlib excels at customizability.\\nIt allows for extensive visual modifications\\nthrough direct axis to its classes.\\nIt suffers greater control if you need\\nto fine tune every aspect of your charts and plots.\\nIn this module, I'm going to give you an overview\\nof how to effectively utilize both libraries\\nfor your data visualization needs.\\nMatplotlib and Seaborn are versatile libraries\\nfor creating a wide range of standard chart graphics.\\n\\nThey're useful for visualizing all types and kinds of data.\\nThis, of course, includes but isn't limited\\nto financial data, operational data,\\nemployee data, engagement and behavioral data.\\nOne thing to note, however, is\\nthat you can display network data\\nusing standard chart graphics,\\nbut you cannot create network graphs when you're working\\nwith only the Matplotlib or Seaborn library.\\n\\nMatplotlib is typically employed for generating basic plots.\\nIts visualization capabilities commonly include\\ncreating bar charts, line charts, scatter plots,\\nand similar straightforward graphical representations.\\nConversely, Seaborn offers an extensive array\\nof visualization options.\\nIt's known for its beautiful themes,\\nand Seaborn allows for the plotting of complex graphics\\nwith relatively minimal coding effort.\\n\\nIt specializes in statistical visualizations,\\nmaking it ideal for summarizing\\nand displaying distributions within datasets.\\nNow let's look at how we can utilize both Matplotlib\\nand Seaborn to create various types of graphics.\\nOkay, in this coding demonstration,\\nI just want to show you how to get started really quickly\\nwith Matplotlib and the Seaborn library.\\nSo we're first going to start\\nby installing the required libraries.\\n\\nSo for that, we'll need to do a pip install.\\nAnd we will pip install Matplotlib,\\nand run this.\\nAnd we'll also need to do a pip install\\nof the Seaborn library.\\nOkay, so we've got all of those installed.\\nNow we need to import them into our IPython environment.\\nSo we'll do that now\\nby just saying import matplotlib.pyplot as plt,\\nand import seaborn as sns.\\n\\nAnd then for this demonstration,\\nwe also need to work with DataFrames.\\nSo we'll say from pandas import DataFrame.\\nOkay, so then we'll run this.\\nOkay, now we have imported both,\\nall of the libraries that we need here.\\nSo let's get started looking\\nat how we can use them to generate plots.\\nI will go ahead\\nand just create a dataset that we can use\\nfor practicing our data visualization.\\n\\nSo we'll just call it data.\\nAnd then let's create a dataset\\nwith the names of individuals\\nalong with their age, gender, and rank.\\nThis is a dataset that we actually\\ndefined in an earlier lecture,\\nso I won't spend too much time typing everything out.\\nThe names here should be steve, john, richard,\\nsarah, and julie.\\n\\nSo we'll pass these strings\\ninto the list.\\nAnd then,\\nwe will define the age of each of these people.\\nSo create a column named age,\\nand we'll set the ages equal to,\\n20, 22, 20, 21, 24, 23, and 22.\\n\\nAnd it looks like we forgot a bracket.\\nSo let's close that.\\nAnd then create the next column, which is going to be gender.\\nWe'll say gender,\\nand set that equal to Male, Male, Male,\\nFemale, Male, Male, and Female.\\nAnd then the next column we need is rank.\\nWe'll create a rank column\\nand then we will assign a rank to each of these people.\\n\\nAnd that is 2, 1, 4, 5, 3, 7, 6.\\nSo now we have a dataset that we can work with.\\nLet's create a DataFrame from this dataset.\\nTo do that, we just use the DataFrame constructor\\nand pass in data,\\nand we'll set this equal to df,\\nand then let's just print it out.\\n\\nOkay, it looks like we have a syntax error,\\nwhich should be pretty easy to fix.\\nThe string just was not closed here for gender.\\nRun this again. Okay, so here is now our DataFrame.\\nAnd let's use this to create a Matplotlib bar chart.\\nAnd we'll just start by creating a simple bar chart.\\nTo do that, we will use the bar function.\\n\\nAnd in the function, we'll pass the DataFrame's names column\\nas the first parameter,\\nand then we'll pass in the DataFrame's age column\\nas a second parameter.\\nSo let's get started with that.\\nWe'll say plt.bar,\\nand then of the df DataFrame,\\nwe want to select names.\\nAnd then we also want\\nto select the age column from the DataFrame.\\n\\nSo we'll say df, and then we'll select here, age.\\nThe values of the first parameter will be shown\\nacross the X-axis, and then the values\\nof the second parameter will be shown across the Y-axis.\\nSo let's go ahead\\nand set some labels for both of these axes.\\nIn order to do that, you would use the xlabel function,\\nand so that looks like plt.xlabel.\\nAnd yeah, so along the X-axis we're going to have Names.\\n\\nAnd so we'll just create a label here for that axis.\\nAnd then for the Y-axis ylabel,\\nwe're going to have Age.\\nSo this is going to be shown along the Y-axis.\\nAnd let's also create a title for this plot.\\nTo do that, we would say plt.title,\\ncalled the title function,\\nand then pass in a string for the title of the plot.\\n\\nLet's call it Comparing Ages.\\nAnd then let's print the plot out.\\nTo do that, you use the show function.\\nSo plt.show,\\nand run this.\\nAnd okay, great.\\nSo now we actually have a bar chart\\nthat shows the ages of each\\nof the people in our DataFrame.\\n\\nNow let's create a similar bar graph using Seaborn.\\nAnd to do that, we'll call Seaborn's bar plot function.\\nIn the bar plot function,\\nwe'll just pass the DataFrame\\nand specify the columns we want to show on the X and Y-axis.\\nSo let's test that out real quick.\\nWe'll say plot, we'll say plot is equal to sns.barplot.\\n\\nAnd then we will say\\nthat our data is equal to the DataFrame function here.\\nOur x value is equal\\nto the names column.\\nSo we're selecting the names column for the X-axis.\\nAnd the Y-axis, we will select the age column.\\nAnd then let's create a title for this plot.\\nTo do that, we use the set_title function.\\n\\nSo this is plot.set_title.\\nAnd again, we'll just call it Comparing Ages.\\nNow to plot out this graph,\\nwe would just say plt.show,\\nand run that.\\nAnd here you can see we have similar object,\\nbut now it's a Seaborn bar graph.\\n\\nAnd as you can see, there are small differences\\nbetween the Matplotlib one and the Seaborn one,\\nbecause this name's variable\\nand this age variable have different casing than above,\\nbut in general, they look pretty darn similar.\\nThe key difference here though, is you can see\\nwith the method we're creating the bar chart\\nusing Matplotlib, it just requires more code\\nto generate the same graph.\\nSo it's pretty much always the case\\nthat Seaborn is more efficient\\nat data visualization than Matplotlib.\\n\\nAnd in general, it's more beautiful.\\nIt comes up with graphs that are more beautiful,\\nalthough in this example we aren't seeing a huge difference.\\nLater on in the course, you will start\\nto see the differences between Seaborn and Matplotlib\\nin terms of aesthetic.\\nDrawing all sorts of other types of graphs\\nwith Matplotlib and Seaborn are also pretty straightforward.\\nSo let's go ahead and create a line plot with Matplotlib.\\n\\nTo create a line plot with Matplotlib,\\nyou just use the plot function, so that's plt.plot.\\nAnd then let's pass in the DataFrame,\\nand let's put out again the names and age\\nof the people in this DataFrame.\\nSo put name.\\nAnd then for our Y-axis,\\nlet's select the age column here.\\n\\nAnd we can name the label names the same\\nas the bar chart here.\\nSo we can actually just copy these labels,\\npaste them down here.\\nAnd then to take a look at it,\\nwe would just say, plt.show,\\nand run this.\\nAnd okay, so here we have the ages\\nof our people plotted out in line chart format.\\n\\nLet's also look\\nat how we can create a line plot using Seaborn.\\nSo again, this is going to be a lot more simple.\\nWe'll create an object called plot,\\nand we'll say that it's equal to the DataFrame.\\nWe need to call the line plot function.\\nSo that's sns.lineplot,\\nand we'll pass in our df DataFrame\\nas our data here.\\n\\nAnd then again, we will just take X and Y-axis.\\nWe can just copy and paste these from above.\\nSo that's all the same as the bar chart.\\nAnd then to plot this out, we'll say plt.show.\\nOkay. And we'll run this.\\nAnd as you can see, it looks pretty much exactly\\nlike the line plot we created with Matplotlib,\\nwhich makes sense since it's exactly the same dataset,\\nbut it's just again, a lot more efficient\\nto use this method to create a line chart\\nthan it is to create a line chart in Matplotlib.\\n\\nLet's really quickly look at how\\nto create a pie chart in Matplotlib and in Seaborn.\\nSo for Matplotlib, we need to use the pie function.\\nIt will say plt.pie.\\nAnd then let's plot out the age column.\\nSo we'll select that from our DataFrame here,\\nand we'll add some labels.\\n\\nTo do that, we create a labels parameter\\nand we set it equal to the names column.\\nSo we'll select df, names column here.\\nAnd then let's create a title for the chart.\\nSo to do that, we use the title function,\\nwe pass in a title that says Age Comparison,\\nand then we can print this out.\\n\\nI have a typo here\\nwhere it should be title, so I'll fix that.\\nAnd then, okay,\\nso here's our Matplotlib pie chart.\\nNow, Seaborn doesn't have a default function\\nto create pie charts,\\nbut we can use the syntax in Matplotlib\\nto create a pie chart\\nand then add the Seaborn color palette.\\nLet me show you how to do that real quick.\\nSo we'll create a variable say, colors,\\nand then we we'll call the color_palette function.\\n\\nSo that's sns.color_palette.\\nLet's take a look and get the pastel color palette.\\nAnd then say if we only want five colors,\\nwe can just slice the color palette from zero to five.\\nAnd then what that will do is\\nit will just extract the color palette list,\\nand the colors that are associated\\nwith position 0, 1, 2, 3, and 4.\\n\\nAnd then now we have a color palette set up.\\nSo let's call the pie function, which is plt.pie,\\nand we'll pass in our DataFrame and select the age column.\\nAnd then let's set our labels equal to the names column.\\nSo to do that, we say labels is equal to df,\\nand select names here.\\n\\nAnd then lastly, we'll define the colors\\nequal to the colors palette\\nthat we just created in the line of code above.\\nAnd then we can run that.\\nSo I just ran this without using the show function.\\nAnd then that's actually what you get,\\nis you get all of this extra information about the plot\\nthat you don't necessarily want or need to see.\\nSo just to clean this up, I'm going to say plt.show\\nand run it again.\\nAnd here now we have a Seaborn chart.\\n\\nSo we have a pie chart that we actually used Matplotlib\\nto help build.\\nAnd we've then applied the color palette\\nfrom Seaborn to this pie chart.\\nAnd those are the basics of just how to get started\\nusing Matplotlib and Seaborn libraries.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4584142\",\"duration\":625,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating standard data graphics\",\"fileName\":\"3006708_en_US_04_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1141,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to create standard data graphics. This video covers line charts, pie charts, and bar charts.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":20015180,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Standard chart graphics are excellent tools\\nfor conveying simple data insights in a way\\nthat anyone can understand.\\nFor example, imagine you are an E-commerce business analyst\\nfor a company that just made some major changes\\nto its website layout.\\nYou are visualizing the site usability insights\\nthat you've discovered to convey findings to your manager.\\nTo do this, you use a line chart,\\na bar chart, and a pie chart.\\nYou use a line chart to show how the total number\\nof items purchased per day has increased since the change.\\n\\nYou'll use the bar chart to show how the number\\nof purchases increased for customers\\nin the 18 to 25-year-old category,\\nbut that it decreased\\nfor customers in the greater than 44-year-old category.\\nLastly, you use a pie chart to show what categories\\nof products generate the greatest proportion of sales\\nand how the site changes affected that proportion,\\nboth before and after.\\nTo summarize, you'd use a line chart\\nto show the changes over time,\\nyou'd use a bar chart to show changes in categorical data,\\nand you'd use a pie chart to visualize categorical data\\nas a proportion of a whole.\\n\\nNow, we have two methods for plot building,\\na functional method and an object-oriented method.\\nIn the demonstration we're about to do,\\nI'm going to show you\\nhow to build plots using the functional method.\\nWith the functional method, you build plots\\nby calling the plotting function on a variable\\nor set of variables.\\nWith object-oriented plotting,\\nyou build a plot by first generating a blank figure object\\nand then populating that object with plot and plot elements.\\nWe'll be doing that in later demonstrations.\\n\\nIn the coding demonstration to come, I'm going\\nto show you the most popular data visualization libraries\\nin Python, which are, of course, Matplotlib and Seaborn.\\nIn this demonstration,\\nI'm going to show you how to use Matplotlib.\\nLet's get started.\\nSo here we have our Jupyter Notebook,\\nand I'm giving you the mtcars dataset preloaded here\\nwith the column names and everything prepared for you,\\njust so we don't have to type that out again.\\n\\nBut first things first, let's go ahead\\nand import our libraries, and in this demonstration,\\nwe're going to use NumPy.\\nSo we'll import numpy as np.\\nWe're also going to be generating random numbers\\nin this demonstration, so within NumPy,\\nwe need to import randn.\\nSo let's say from numpy.random,\\nimport randn,\\nand then we always of course need to import pandas,\\nand we'll import that as pd.\\n\\nAnd then let's bring in our series and data frame.\\nSo we'll import Series and DataFrame,\\nand we're going to be using Matplotlib in this demonstration\\nso we'll import matplotlib.pyplot\\nas plt,\\nand from matplotlib,\\nlet's also import the rcParams.\\n\\nGreat, and so now let's just run this\\nand we'll have our libraries ready for us.\\nNow, the first thing we're going\\nto work on here is that we're going\\nto create some line charts, so the first line chart,\\nlet's just generate two variables, an x and a y variable,\\nand x will be equal to a range\\nof values between one and nine.\\nSo we'll say x equal to range one through 10,\\nand y is equal to a list,\\nand it's just going to be a list of numbers,\\none, two, three, four,\\nzero, four, three, two, one.\\n\\nTo generate the line plot,\\nwhat we need to do is call the plot function,\\nso that's plt.plot,\\nand then let's just pass in the x and y,\\nand run this.\\nOkay, so we have our line chart,\\nand that's pretty straightforward.\\nNow let's work on creating a line chart\\nfrom a Pandas object.\\nWe've already got your data loaded\\nand ready to go inside of the Jupyter environment,\\nbut we need to go ahead and isolate a variable.\\n\\nSo let's isolate the mpg variable,\\nwhich we're doing here, mpg,\\nand we're setting it equal to the cars data frame,\\nand we've selected the mpg column out of that data frame.\\nSo the first thing I want to do is just run this,\\nand then we have our mtcars dataset available to us in here.\\nAnd now, let's just plot out the mpg variable,\\nso to do that, we'll do mpg.plot, and we'll run this.\\n\\nHere we go, we have a line chart\\nthat shows the values within the mpg variable.\\nNow, I want to show you\\nhow to actually plot out three variables in a chart.\\nSo to do that, let's create a data frame called df,\\nsay df is equal to cars,\\nand then let's select the variables we want to plot\\nand let's make those, let's make those cyl,\\nweight, and mpg.\\n\\nAnd then to plot it out,\\nwe just call the plot method off of the DataFrame object\\nand run this.\\nOh, it looks like I need to actually add another set\\nof brackets here.\\nAnd I'll run it again.\\nOkay, cool, so now we have a line chart\\nwith three variables plotted out instead of just one\\nand we've got a nice little legend here\\nso that we can make sense of which variable is which.\\n\\nNow, let's look at how to create bar charts.\\nTo create a bar chart, you would just call the bar function,\\nso that's plt.bar.\\nAnd then let's pass in our x and y variable\\nand run this, and it's easy as that.\\nWe've got a bar chart.\\nLet me show you how to create a bar chart\\nwith a Pandas object.\\nTo do that, you would just call the plot function,\\nso let's plot out our mpg variable as a bar chart.\\n\\nTo do that, we will say mpg.plot,\\nand then we'll pass a parameter\\nthat says kind is equal to bar,\\nand run this.\\nAnd now we have our mpg variable plotted out as a bar chart.\\nIf you wanted to plot this horizontally instead\\nof vertically, all you would need\\nto do is use the same command,\\nbut the parameter would instead be kind equal to barh,\\nand then you would get a horizontal bar chart,\\nso let me show you that real quick.\\n\\nI'll just copy this code,\\nand add a H here for this.\\nNow we have a horizontal bar chart, and that is easy-peasy.\\nThe last thing I want to show you\\nin this demonstration is how to create pie charts\\nand actually print them out.\\nSo for this example, let's create a new variable x,\\nand let's say that x is just a list\\nwith the values one, two, three, four, and 0.5.\\n\\n'Kay.\\nAnd then if we wanted to create a pie chart from this,\\nwe would just call the py function, plt.py,\\nand pass in our variable x,\\nand then to print it out, we would call the show function,\\nwhich is plt.show,\\nrun this.\\nAnd okay, now we have a very, very simple pie chart.\\nBut what I really wanted to do\\nin this demonstration was actually to show you\\nhow to save this pie chart as an image file.\\n\\nSo to do that, first, let's generate the pie chart again.\\nSo we'll say plt.py, and then we'll pass in our x variable.\\nAnd then to save this as an image file, we actually need\\nto use the savefig function,\\nso that's plt.savefig,\\nand we'll pass in a string with the file name,\\nand we'll call that pie_chart.png,\\nand then we can just plot this out\\nusing the show function here,\\nand great, okay, so that's still the same pie chart,\\nbut what this has actually done\\nwith the savefig function is it's saved this pie chart\\nas an image.\\n\\nSo you can see that here in our Notebooks folder,\\ndown at the bottom, you'll see pie_chart.png,\\nand here we go, we have the same image\\nthat is from our notebook\\nthat has been now printed within a separate image file.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2715031\",\"duration\":748,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Defining elements of a plot\",\"fileName\":\"3006708_en_US_04_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1306,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to define elements of a plot. This video covers object-oriented plotting, sub-plots, and axis labels.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":23712323,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now you're going to learn\\nabout defining plot elements using Matplotlib.\\nPlot elements add context to your plot\\nso that the plot effectively conveys meaning to its viewers.\\nYou set axes limits to make sure\\nthat your chart is well fit to your data graphic,\\nyou set axes tick marks and plot grids\\nto make it easier and faster for viewers\\nto interpret your chart at a glance.\\nYou can use subplots to visually compare changes\\nin data values under different conditions,\\nlike in different seasons,\\ndifferent locations, or in different years.\\n\\nThere are two methods for building plots.\\nYou can use the Functional method\\nor the Object-oriented method.\\nAdding plot elements is an essential part\\nof Object-oriented plotting.\\nWe discussed the functional plotting\\nin the last demonstration.\\nSo now it's time to go over Object-oriented plotting.\\nAgain with Object-oriented plotting,\\nyou build a plot by first generating a blank figure object\\nand then populating that object\\nwith plots and plot elements.\\nThere are four steps in Object-oriented plotting.\\n\\nFirst one is to create a blank figure object.\\nThen second, you will add axes to the figure.\\nThird, you'll generate a plot\\nor plots within the figure object.\\nAnd then fourth, you will specify plotting\\nand layout parameters for the plots within your figure.\\nOne last thing I want to touch on here\\nbefore we go into the demonstration is subplots.\\nA Subplot is a plotting figure\\nthat contains more than one plot or subplots.\\n\\nIt's easy to generate a subplot in Matplotlib\\nwith the subplots function.\\nLet's look at how to define elements\\nof a plot using Matplotlib.\\nSo this Jupyter notebook is coming preloaded\\nwith the libraries you'll need,\\nwhich is NumPy, Pandas and Matplotlib.\\nAnd I wanted just to start off this demonstration\\nby showing you how to set the figure size\\nand inline settings for your Matplotlib plots.\\n\\nSo to make sure that your plots print out properly\\nwithin Jupyter notebooks,\\nyou want to use the command matplotlib inline.\\nWhat this does is it tells Python\\nto print your Matplotlib plot in line\\nwith your Jupyter notebook.\\nAnd then you'll also probably want to go ahead\\nand do define a figure size.\\nSo to do that, you're going to use the rcParams function.\\n\\nThen create a list,\\nand you want to define the figure and figure size.\\nSo you'll say fig.figsize.\\nIn manual, just set this equal to the dimensions\\nyou want for your plots.\\nSo in this demonstration,\\nlet's just print plots\\nthat are 5 inches wide and 4 inches tall.\\nSo we'll put 5, 4 here.\\nAnd then we basically have set the parameters\\nfor generating plots within our Jupyter notebook.\\n\\nSo we run this.\\nNow let's go ahead and define axes, ticks and grids.\\nTo do that, we first need to generate some variables.\\nSo let's create an x variable here,\\nand then we'll say that the x variable\\nis a series of numbers between 1 and 9.\\nSo we'll use the range function\\nand we'll pass in the starting value of 1,\\nand then the end value is 10,\\nwhere 10 is actually excluded\\nfrom the numbers that are generated.\\n\\nAnd then another variable will be y\\nand we'll set y equal to a list.\\nThe values in this list will be\\n1, 2, 3, 4, 0, 4, 3, 2, 1.\\nNow let's generate a figure.\\nSo to do that, we will say fig is equal to plt.figure,\\nwe'll call the figure function here.\\n\\nAnd this is basically going to create a blank figure.\\nThe next thing we're going to do\\nis that we're going to add axes to this figure.\\nTo do that, you call the add axes method\\noff of this fig object.\\nSo we say fig.add_axes, axis.\\nAnd this will be our ax object.\\nSo I'll set this whole thing equal to ax.\\n\\nAnd for our axes parameters,\\nlet's just pass a list\\nthat will contain the numbers we want.\\nSo that'll be 0.1, 0.1, 1, 1.\\nAnd now let's plot this out.\\nTo plot it out, all we need to do is call the plot method\\noff of the ax object\\nand pass in our x and y variables.\\n\\nThere is a extra equal sign.\\nSo then we'll run this.\\nAnd there we have it.\\nWe have a simple line chart,\\nbut now we have created tick marks along the axes,\\nand we've created a figure\\nin which the line chart can be displayed.\\nThese are the basics of Object-oriented plotting.\\nNow let's go into some more details.\\n\\nWe're going to add limits\\nand tick marks on the x and y axes.\\nSo we'll start again with our figure and our axes.\\nAnd I'll just go ahead and copy this code\\nso we don't have to type it all out again.\\nNow let's add some limits to our x and y axes.\\nTo do that, you're going to call the xlim function\\nand set the 8 ylim function.\\nAnd basically what we want to do\\nis we want to set the limits of our axes,\\nand then we need to pass a list\\nthat tells Python what we want those limits to be.\\n\\nSo in this case, that will be 1 and 9.\\nSo we'll say ax.set_xlim\\nand then the limit is going to be 1 and 9.\\nAnd then moving into the ylim,\\nwe say set_ylim,\\nand we define those limits.\\n\\nSo those are going to be 0 and 5.\\nAnd then we also want to just practice\\nwith setting some tick marks.\\nSo to do that, we're going to use the xticks function\\nand set yticks function.\\nSo to do that, we're going to use the set xticks function\\nand the set yticks function.\\nSo let me just write those out here\\nand we'll say set_xticks.\\n\\nLet's also just set it up for our y tick marks.\\nI'll copy this over.\\nOkay, great. So now let's pass in a list here\\nand say what we want our tick marks to be.\\nAnd so for the x axes, let's just make that,\\nmake those 0, 1, 2, 3, 4, 5, 6, 8, 9 and 10.\\n\\nAnd then for our y ticks,\\nlet's make those 0, 1, 2, 3, 4, and 5.\\nAll right. And then we'll plot this out\\nusing the plot function.\\nSo now we have another line chart,\\nbut you can see now that we've added tick marks\\nand we've also added limits to our axes.\\nSo if we look back and forth between the prior graph\\nand this one, you can see the changes.\\nSpecifically, you can see that\\nthe 7 tick mark is missing here on the x axis,\\nand the maximum limit of the y axis is now 5 here\\ninstead of 4,\\njust above 4 in the prior graph.\\n\\nAnother thing you can do is that you can add a grid\\nto the background of your chart.\\nSo let me just show you how that works real quick.\\nWhat I'll do is I'll copy in this code from above,\\nand then we'll just add a grid to the plot.\\nAnd to do that, we just call the grid function.\\nSo we'll say here,\\nI'm going to take these tick marks out,\\nand then we'll say ax.grid\\nand then we'll plot it out, ax.plot\\nand then we'll pass in our x and y variables.\\n\\nAnd then we'll run this\\nand we'll get a nice little grid background\\non our line chart so that it makes it more easy to read.\\nYou can also generate multiple plots in one figure\\nwith a subplots function like I mentioned in the lecture.\\nSo let's go ahead and do that.\\nFirst things first, of course, we need to generate a figure.\\nSo in this case, it's going to be fig is equal to plt.figure.\\n\\nThis is the same as last time,\\nbut we're going to adjust the figure\\nto actually include two subplots.\\nSo to do that we need to say fig,\\nand then we need to create two axes here.\\nTo do that, we're going to create a tuple,\\nand we're going to say ax1 and ax2.\\nThen we're going to set these\\nequal to the subplots function.\\nThat's plt.subplots,\\nthen we want to define\\nwhat we want our subplots to look like.\\n\\nSo let's say we have\\n1 row and 2 columns.\\nSo we would just say 1 and 2 here.\\nOkay. And then to plot it out,\\nwhat you need to do\\nis you need to actually plot both axis,\\naxis 1 and axis 2.\\nSo we'll say ax1.plot.\\n\\nThen we'll pass in the x variable.\\nAnd ax2.plot\\nand we'll pass in the x and y variable here\\nso that we have a little bit\\nof a difference in our plots.\\nAnd then when I run this,\\noh before I do that, it's in markdown,\\nso I need to just make sure it's in Python.\\n\\nAnd then when I run this,\\nI'm missing a comma here.\\nSo I'll run this\\nand I need to add the figure subplots, okay.\\nHere we go.\\nSo now we have two subplots.\\nWhat we've actually done though\\nis we have created one figure with two axes,\\nand then we have generated plots within that one figure.\\nPretty cool, huh?\\n\"},{\"urn\":\"urn:li:learningContentVideo:4588025\",\"duration\":901,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Plot formatting\",\"fileName\":\"3006708_en_US_04_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"2 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1357,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to format plots. This video covers custom plot colors, line styles, and marker styles.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":31101182,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this section, I'm going to show you\\nhow to define custom plot colors,\\nline styles, and marker styles.\\nThe reason I think this is important to show you\\nis that enhancing chart colors and markers\\ncan help you convey a point\\nin a way that is visually easier to understand.\\nJust think, if all the slices of a pie chart\\nhad the same color, the chart would be meaningless.\\nAnd if all the lines in a line chart were the same color\\nand use the same markers,\\nyou wouldn't be able to decipher one from the other,\\nand the chart would have very little or no meaning.\\n\\nMatplotlib offers colors, line styles,\\nand marker style options\\nto help you build clarity into your data visualization.\\nMatplotlib has unlimited color options,\\nso you can set the color parameter\\nequal to the name of your color you want,\\nor you can set the color parameter equal to RGB\\nor RGBA color codes for even more customization.\\nMatplotlib also has a variety of line styles.\\n\\nIf you look at this chart here,\\nyou can see some of the codes\\nthat you would use to set those styles.\\nAlso, I'm going to show you how to do this\\nin the coding demonstration.\\nMatplotlib also has a lot of different options\\nin terms of marker styles.\\nI created this handy-dandy little chart\\nfor you to review\\nin case you want to get fancy with your charts,\\nyou can look it over,\\nbut I'm also going to show you\\nhow to use these markers in Python in just a second.\\nYou've already learned to generate your plots,\\nso let's go ahead and start working on customizing them.\\n\\nSo for this coding demonstration,\\nthe Jupyter Notebook is already coming preloaded\\nwith the libraries you need, which are NumPy,\\nPandas, and Matplotlib.\\nAnd also, I've gone ahead and set the parameters\\nin terms of the plot size.\\nWe discussed that in an earlier lecture.\\nSo all of this information is preloaded for you,\\nand we're going to start off by working to define plot color.\\nWe'll just run these.\\n\\nSo the first thing we need to do\\nis just to create some variables,\\nand we'll create an X variable and a Y variable.\\nWe'll set X equal to\\na series of numbers between one and nine.\\nTo do that, we'll call the range function\\nand we'll pass in, starting number one and number 10,\\nwhere the last number is excluded in the output.\\nSo that will generate a series of numbers from one to nine,\\nand then we'll define Y as\\nthe numbers one, two, three, four,\\nzero point five, four, three, two, one.\\n\\nOkay, so we have two variables,\\nand now let's just plot this out as a bar chart.\\nSo to do that, we'll say plt.bar,\\nand we'll pass in our two variables, X and Y,\\nand run this.\\nOkay, so now we have a nice little bar chart.\\nNow let's define some colors here.\\nLet's change things up a bit,\\nbecause the default colors are kind of boring,\\nand we could create something a lot more fun.\\n\\nSo we'll change the width of the bars\\nand we'll also change the color.\\nHow about that?\\nWe can also go ahead and change the alignment a bit.\\nSo first things first,\\nlet's create a list to define the width of our bars.\\nTo do that, we'll just create a variable name,\\nwe'll call it wide,\\nand we'll say wide is equal to a\\nkind of just a mix and match of some sizes here.\\nSo zero point five, zero point five, zero point five,\\nand then maybe we'll make\\na bar that's zero point nine wide,\\nanother one that's zero point nine wide,\\nanother one that's zero point nine wide,\\nand then we'll go back to bars of width zero point five.\\n\\nSo I'll just kind of paste these in here.\\nOkay, cool.\\nSo there we have some widths\\nthat Python is going to use to adjust our default bar chart.\\nSo we'll use the color label here\\nand we'll say, \\\"Color is equal to,\\\"\\nand then we'll create a list,\\nand we'll pass on a string that reads, \\\"Salmon.\\\"\\nNow, to plot this bar chart out,\\nwe again need to use our bar function,\\nso that's plt.bar.\\n\\nAnd then we want to pass in the X variable,\\nthe Y variable,\\nand then we wanted to find the width, color,\\nand alignment parameters.\\nSo to do that, we'll say width,\\nand we'll set width equal to wide,\\nwhich is the variable we just created.\\nWe'll set color equal to color,\\nand then we'll say that we want to align\\nthe bar in the center,\\nso we'll say align equal to center here.\\n\\nNow the color is going to be equal to color,\\nwhich we just defined a sum in,\\nand then we set the alignment to center.\\nAnd alignment refers to\\nwhere the bars actually plot out within the chart,\\nso we want them to be centered when they plot.\\nSo we have set the line parameter equal to center,\\nand so now all we need to do here is print this out.\\nOkay, so, cool.\\nYou can see we have made these outer bars,\\nand they're a lot more narrow than the ones in the center,\\nand the color has been changed to salmon,\\nand you can also see that the alignment of the bars\\nis centered between each of the intervals\\ninstead of kind of being pushed inwards.\\n\\nLooking at the difference between the two,\\nyou can see that, yeah, it's definitely made a difference.\\nSo we've got that there.\\nAnd now let's go ahead and start working with\\nsome data frame objects.\\nThe Jupyter Notebook is coming preloaded\\nwith the NumPy cars data set.\\nOf course, you are going to need to customize your address\\nand specify where that data set is stored on your computer,\\nbut I've named all the columns and everything,\\nso the next thing you'd want to do\\nis just to create a data frame.\\n\\nI'm going to run this.\\nAnd we will call the data frame DF,\\nand we'll set it equal to cars,\\nand then let's just go ahead and\\nselect the cylinder column,\\nthe MPG column,\\nand the weight column,\\nand then plot it out with the plot method,\\nso df.plot, and we'll run this.\\n\\nOkay, so now we have here a little line chart,\\nand that looks decent,\\nbut these are just the standard settings\\nthat you would get with Matplotlib.\\nAnd I want to play with these a little bit,\\nso we'll change the color scheme,\\nwe'll create a new variable called color theme.\\nSo let's do that now, color underscore theme,\\nlet's add an \\\"equal to,\\\"\\nand then we'll create a list of colors\\nwe want to use in the chart.\\nSo we'll say dark gray,\\nlight salmon,\\nneed to change this to a single quote,\\nand then the last color will be powder blue.\\n\\nOkay, and then we'll go ahead and call\\nthe plot method off of the data frame,\\nand we'll pass in the color theme.\\nTo do that, we say df.plot,\\nand then we set the color parameter equal to\\nour color underscore theme, and run this.\\nAnd now you can see we have shifted the colors\\ninto these sort of more fancy-looking colors.\\n\\nI also want to show you how to change colors\\nbased on our GP codes.\\nSo we will create a Z variable\\nand we'll set Z equal to a list,\\nand it'll have the numbers one, two, three, four,\\nand zero point five.\\nAnd now we want to call the pie function\\nand we'll pass in our Z object.\\nSo that's plt.pie.\\n\\nAnd then, we'll go ahead and say plt.show,\\nprint it out,\\nand there we have a pie chart,\\nbut it's coming with our default colors for Matplotlib.\\nSo let's go ahead and adjust these colors using our GP codes\\ninstead of the color labels we have been using.\\nSo we'll create a new color theme.\\nAnd we can just overwrite the previous one,\\nso we'll say \\\"color theme equal to,\\\"\\nand then we'll create a list.\\n\\nAnd this time, instead of passing in the color labels,\\nwe're going to pass in our RGB codes.\\nSo our first RGB code will be #A9A9A9.\\nAnd then I'm going to paste in the next four.\\nThese are just RGB codes to represent\\nwhat the color should look like\\nwhen it gets plotted into the pie chart.\\nNow we've got those.\\nAnd now all we need to do is call the pie function.\\n\\nSo that's plt.pie, and we'll pass in Z.\\nAnd then our color, we'll set our color parameter\\nequal to color theme,\\nand then plot it out for the plt.show.\\nThere is a typo here, so this should be colors.\\nAnd as you can see,\\nwe have now adjusted the colors out a little bit\\nso that there's just a bit more creativity\\nthan the standard Matplotlib printout.\\n\\nNow let's look at how to customize line styles.\\nSo the first thing we would do\\nis just to create two new variables.\\nWe'll call them X1 and Y1.\\nAnd let's say X1 is basically the same variable\\nthat we created here, series of numbers between\\none and nine.\\nSo I'm just going to copy this.\\nAnd then our Y1 variable will be a series of numbers\\nbetween one and 10 in descending order,\\nso I'll just create a list and write them in.\\n\\nOkay, so now let's just plot this thing out.\\nThe plt.plot.\\nWe'll pass in our X and Y.\\nLet's also create a second plot.\\nAnd we'll just plot the X1 and Y1 variables\\nin that second plot.\\nSo just copy this,\\nand add the numbers for the variable names\\nthat we just created, and run this.\\n\\nOkay, so it looks like we have a little bit of a typo\\nbecause X and Y must have the same first dimension,\\nso I messed up in my creating\\none of my variables here, and that would be this variable\\nbecause we need to have nine numbers in X1\\nand nine numbers in Y1,\\nso I wrote in one too many.\\nSo let's run this, and yeah, okay, great.\\nSo here we have now two lines in the plot,\\nand that's exactly what we wanted,\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4586144\",\"duration\":1129,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating labels and annotations\",\"fileName\":\"3006708_en_US_04_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1973,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to create labels and annotations. This video covers labeling pot features, legends, and annotating plot features.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":38717589,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Labels and annotation\\nadd a deeper layer of context to a plot,\\nenabling the plot to convey extra meaning to its viewers.\\nIn this section, I'm going to show you\\nhow to label plot features,\\nadd a legend to your plot,\\nand annotate features on your plot.\\nBut before that,\\nlet me give you an example of where this comes in handy.\\nData journalists often add a lot of context\\nand annotation to their visualizations\\nin order to add and augment the story they're telling.\\nFor example, imagine you're a data journalist\\ncovering a story of tourism in central Florida.\\n\\nYou'd use a simple line chart\\nto show the number of travelers over time.\\nBut if you're telling a story about the success\\nof a grand opening of a new theme park,\\nyou may want to add some text\\nabout how many visitors came to the grand opening.\\nYou then may also want to add a pointer\\nthat ties that text into the date\\nof the park's grand opening.\\nThere are two methods for labeling and annotating.\\nAgain, the functional and object-oriented method.\\n\\nI'm going to show you both of those\\nin the demonstration to come.\\nBut before that, let me give you some extra information\\nabout the methods we'll use.\\nIn order to add a legend,\\nyou'll need to call the legend method\\nand you'll pass in a label and a location.\\nThis will then place a legend on your plot axes.\\nOver here on the right of the page,\\nyou can see all of these different options\\nin terms of where you want to position your legend\\non your plot.\\n\\nAnd then with respect to annotating your plot,\\nyou would call the annotate method,\\nand then you would just pass in a parameter\\nfor the location that's being annotated,\\nas well as the location of the text\\nand any information that you want to add\\nabout how the arrow should be drawn.\\nSo let's get to work on doing this in Python.\\nThe Jupyter Notebook here is already coming preloaded\\nwith the libraries you'll need.\\nSo those are numpy, pandas, matplotlib, and seaborn.\\n\\nI've also added in the parameters\\nfor plotting by matplotlib, which you can see right here.\\nSo everything is all typed out for you.\\nLet's just start by labeling plot features.\\nAnd I'm going to start by showing you\\nhow to do that using the functional method.\\nSo the first thing we need to do\\nis to create some variables.\\nLet's create an x variable using the range function,\\nand we'll just create a series of numbers\\nbetween 1 and 9.\\nAnd then let's create a Y variable.\\n\\nWe'll set that equal to a list,\\nwhich will contain the values, 1,2,3,4,0.5,4,3,2,1.\\nAnd then say we want to generate a bar chart.\\nSo to do that, we'll use the bar function.\\nThat's plt.bar, and we'll pass in our variables x,y.\\nAnd then let's go ahead and create a label.\\nTo do that, we will call the xlabel function.\\n\\nSo that's plt.xlabel.\\nAnd then we'll pass in a string\\nwith the label that we want to be showed on the x-axis.\\nSo we'll say your x-axis label\\njust to make this super clear.\\nAnd then let's also create a label for the y-axis.\\nSo to do that, it's actually very simple\\nbecause the function for that is just ylabel.\\n\\nSo I'm going to copy this code\\nand then just change out the function name here for x for y,\\nand then update the string\\nand then run this.\\nOkay, cool.\\nSo now we have a bar chart\\nand it looks like we actually have two labels,\\none on the x-axis and one on the y-axis.\\nAnd we did that using the functional method.\\nSo now I want to show you how to do the same thing\\nwith something like a pie chart.\\nSo let's create a new variable here and we'll call it z.\\n\\nAnd we'll say z is equal to a list\\nthat contains the values, 1,2,3,4,0.5.\\nAnd then let's also create some labels\\nand we'll label it according to vehicle type.\\nSo we'll call this veh_type,\\nand we'll set it equal to a list\\nthat contains a series of strings\\nwhere the first string is 'bicycle',\\nthe second one is 'motorbike',\\nthird one is 'car',\\nthe fourth one is 'van',\\nand then the fifth one, let it be 'stroller'.\\n\\nNow we want to generate a pie chart.\\nSo we'll call the pie function,\\nit's plt.pie, and we'll pass in our variable Z.\\nAnd then we needed to find the labels parameter.\\nWe'll just say labels=veh_type.\\nAnd then plot it out using the show function, plt.show.\\nYou can see what this has done is it has gone along\\nand nicely created some labels\\non the outside of the pie chart.\\n\\nThis is still the functional method of creating labels.\\nNow I want to show you the object-oriented method.\\nSo for this part of the demonstration,\\nI want to use the mtcar dataset\\nthat we've been using in earlier demonstrations.\\nSo I'm going to set the address for that first.\\nI'm going to go up to the data folder\\nand right click to get the path\\nand then paste that into the string here, right?\\nSo we have the address for our CSV file.\\n\\nAnd then let's create a data frame called cars.\\nAnd then we'll read in the file using the read_csv function.\\nSo that's pd.read_csv,\\npass in address,\\nand then let's assign names to the columns.\\nTo do that, let's first select the columns\\nby saying cars.columns,\\nand then we'll assign values to these column names\\nby just setting this equal to a list of strings\\nwith names for each of the columns.\\n\\nSo the first column is 'car_names',\\nand then I'm just going to copy and paste\\nover the rest of the column names,\\nwhich are 'mpg','cyl','disp',\\n'hp','drat','wt','qsec',\\n'vs','am','gear','carb'.\\nAnd now let's isolate the mpg variable.\\nSo to do that, we're going to say mpg = cars.mpg.\\n\\nAnd since this is object oriented,\\nof course, we'll start first by creating a figure.\\nSo we're going to do that by calling the figure function,\\nwhich is plt.figure,\\nand we'll set this equal to fig.\\nThen we're going to add some axis to the figure.\\nTo do this, we need to call the add_axes method\\noff of the fig object.\\nSo we'll say fig.add_axes,\\nand then let's define what we want for our axes.\\n\\nSo create a list here.\\nI'll say .1,.1,1,1,\\nand we'll set this whole thing equal to ax.\\nNow let's call the plot method.\\nSo to do that we say mpg.plot.\\nBut before actually running the cell,\\nlet's go ahead and add some tick marks and labels\\nalong the axis.\\n\\nSo to do that, we will use the set_xticks method.\\nSo we'll say ax.set_xticks,\\nand then we'll pass in range function\\nand we'll pass in the value 32.\\nOkay, and then let's also set some xticks labels.\\nSo we'll say ax.set_ticklabels.\\n\\nAnd within that function,\\nwe're going to pass the car names,\\nthat's cars.car_names.\\nAnd let's add a little rotation to the label\\nso that they're easier to read.\\nTo do that, we'll pass a perimeter\\nthat says rotation=60.\\nLooks like I need to move this bracket.\\nAnd then we'll also set a fontsize='medium'.\\nSo it's fontsize=, and then create a string\\nthat reads 'medium'.\\n\\nOkay, so we need to make sure\\nthat this says set_xticklabels.\\nSo I need to add an x here.\\nAnd then let's also add a title.\\nTo do that, we'll call the set_title method\\noff of the ax object, set_title,\\nand then we'll pass in a string for the title of the graph.\\nAnd let's call it 'Miles Per Gallon of Cars in mtcars'.\\n\\nLastly, I would like to create some labels for our chart.\\nSo we'll call the set_xlabel method\\nand the set_ylabel method.\\nSo ax.set_xlabel,\\nand then pass in a string that reads 'car names'.\\nAnd then for the ylabel,\\nwe can actually just copy this.\\n\\nThe method is ylabel.\\nSo I changed this here,\\nand then I update this string to read 'miles/gallon'.\\nOkay, I am just looking it over real quick to make sure\\nthat I don't see any syntax errors.\\nAnd then let's run this.\\nAnd here we have our printout.\\nSo look how nice this is.\\nWe've got a little rotation\\nin terms of the labels on the x-axis,\\nand both the x-axis and the y-axis are labeled.\\n\\nThe only thing that's not so nice about this graph\\nis of course the text,\\nwhich is kind of piling on top of itself.\\nAnd that's actually because we set the parameters\\nwith the rcParams at the top.\\nAnd so it's eight inches wide.\\nIf we wanted to fix that,\\nwe could just change that to 12 here and then go back down\\nand run this code block and you'll see that.\\nOkay, now it's much more nicely spaced.\\n\\nAnd we also have a title to the chart.\\nI also wanted to show you how to add a legend to your plot.\\nSo we'll first do that with a functional method\\nand let's call the pie function\\nand we'll pass it in our z variable.\\nSo that's plt.pie, and then pass in our z variable.\\nAnd then let's also create a legend.\\nTo do that, you need the legend function.\\nSo we'll say plt.legend,\\nand then we need to pass in our vehicle type\\nbecause we want to use those as our labels in our legend.\\n\\nSo that's the veh_type.\\nAnd then we want this legend location\\nto be set at the best possible location, right?\\nSo to make that happen,\\nwe will set the loc parameter equal to 'best',\\nand then you just plot it out.\\nTo do that, you call the show function.\\n\\nAnd then, cool.\\nNow we actually have created a nice little legend\\nfor our pie chart,\\nand that's awesome.\\nLet's also look at how we can create a legend\\nwith the object-oriented method.\\nSo what I'm actually going to do\\nis I'm going to go back up here to the top\\nwhere we've already written all of this code\\nand I just want to reuse it and adjust it.\\n\\nSo I'm going to take this chunk here\\nand I'm going to paste it down here.\\nAnd we're still using the mpg variable.\\nThe xticks are the same.\\nAnd the only thing we're actually going to change here\\nis that we're going to add a legend.\\nTo add a legend,\\nwe'll call the legend method off of the ax object.\\nSo we'll say ax.legend.\\n\\nAnd then we also need to set the perimeter for the location.\\nAgain, let's just set that equal to best.\\nSo we'll say loc='best'.\\nRun this, and nice.\\nYou can see up here in the upper right corner,\\nwe now have a legend.\\nAnd the final thing we're going to cover in this demonstration\\nis how to annotate your plot.\\nSo first things first,\\nlet's just look and find out\\nwhat is the max value for our mpg variable.\\n\\nTo do that, we're going to call the max method\\noff of the mpg variable here,\\nso mpg.max.\\nAnd we can see that the max value\\nin the mpg variable is 33.9.\\nSo let's go ahead and label that point on our chart.\\nI'm going to go back up\\nand get the code for this chart\\nagain so we can reuse it.\\n\\nAnd what I want to do\\nis set a limit to this chart for the y-axis.\\nSo to do that, I'll use the set lim method.\\nSo we'll say ax.set_ylim,\\nand then we'll pass in\\na list that contains two values.\\nSo our minimum y limit will be 0,\\nand our maximum y limit will be 45.\\n\\nAnd let's also create some annotation.\\nTo do that, we'll call the annotate method\\noff of the ax object, so ax.annotate.\\nAnd the first thing we need to do is pass in a string,\\nwhich is going to be the text\\nthat should be added as annotation to the graph.\\nWe'll move this up\\nand then create the string that says 'Toyota Corolla'.\\n\\nAnd then we need to give some details\\nabout where this annotation should be placed.\\nSo we're going to say xy=,\\nand then we're going to create a two pull\\nthat describes the position.\\nSo we'll put 19 and 33.9.\\nOkay, and then we also need to go ahead\\nand pass a parameter for the xy text.\\nBasically, where the text needs to be placed.\\nSo we'll say xytext=,\\nand then it's created two pull.\\n\\nAnd we'll say we want the text\\nto be positioned at 21 on the x-axis\\nand 35 on the y-axis.\\nAnd let's also set an arrow, okay?\\nSo to do that, we're going to pass a perimeter\\nthat reads arrowprops,\\nand then we're going to set it equal to a dictionary.\\nAnd then we're going to set a color for our arrow,\\nand that color is going to be black.\\nSo we need to call the dictionary constructor.\\nAnd then we need to say that the facecolor parameter\\nis equal to a string called 'black'.\\n\\nAnd then let's just shrink it up a little bit.\\nSo we'll pass a perimeter that says shrink=0.05.\\nSo I'm going to print this out.\\nAnd look at that,\\nwe have a nice annotation with an arrow pointing\\nto the maximum value for mpg.\\nAnd because we created this nice label,\\nit's really obvious and easy to see\\nthat the Toyota Corolla is the vehicle\\nthat is getting the best mile per gallon\\nout of all of the cars in the mtcar dataset.\\n\\nNow that you know how to add labels and annotation,\\nlet's look at how to visualize time series in Python.\\nThat will be our next lecture.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4579301\",\"duration\":503,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualizing time series\",\"fileName\":\"3006708_en_US_04_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":810,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to visualize time series. This video covers handling time series and plotting time series.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17052908,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Time series plots convey\\nhow an attribute changes over time.\\nUsing statistical methods like\\nautoaggressive integrated moving average,\\nyou can reliably predict or forecast\\nthe demand of a particular retail product\\nbased on historical time series\\non previous sales of that product.\\nBefore forecasting your time series,\\nyou need to know how to handle and plot time series\\nin Python.\\nWorking with time series in Python can get really tricky,\\nbut Pandas makes it simple.\\n\\nBefore showing you how to use time series in Pandas,\\nlet me just show you what a time series looks like.\\nThese four plots all show time series,\\nthe first one is a constant time series.\\nBasically, you're not seeing any trends or changes\\nin the variable over time.\\nTrended time series is like this chart over here\\nin the upper right,\\nthat's where you see a net increase or decrease\\nin the time series variable over time.\\nIn the lower left you'll see an untrended time series.\\n\\nThis is an untrended seasonal time series,\\nso the variable is increasing and decreasing\\naccording to the seasons of the year,\\nbut you're not seeing a net change\\nin the average value of the time series.\\nSo we call that untrended.\\nIn contrast, over in the lower right corner,\\nyou'll see a trended seasonal time series.\\nThis is where the variable increases and decreases\\nwith the season,\\nbut there's a net gain in the variable over time.\\nSo like I said,\\nand your Jupyter Notebook is coming\\nall loaded with the libraries you need,\\nwhich are NumPy, Pandas, and Matplotlib.\\n\\nWe're also going to be using Seaborn here\\nto just set the style.\\nSo I actually prefer to use the alias, sns here,\\nlet me update this.\\nAnd we can just run this\\nand we'll be ready to start creating time series.\\nSo we'll get right to work with just reading in our data\\nand then printing it out as a time series.\\nThe first thing we need to do is set an address.\\nSo let's say address =,\\nand then for this demonstration,\\nI want to use the Superstore Sales data.\\n\\nSo the location of that is over here in the data folder.\\nJust Right Click, copy path,\\nand then paste it into this string here.\\nAnd then I'll close this.\\nThe next thing that we'll do\\nis we'll create a data frame from the CSV file.\\nAnd we need to read the CSV file in\\nusing the read CSV function.\\nSo let's just call this data frame df\\nand we'll set it =pd.read_csv,\\nand then we'll pass in the address.\\n\\nAnd then we also want to set our index.\\nThe index for the data frame\\nshould be set to the order date.\\nSo to do that we'll just say index_col,\\nwhich is the parameter which is used to set the index.\\nAnd let's set that = 'Order Date.'\\nAnd we have to be sure also to encode the data properly.\\nFor this particular demonstration,\\nwe need to set our encoding here = cp1252.\\n\\nAll we need to do is define the encoding parameter\\nequal to a string, which reads cp1252.\\nAnd let's also parse the dates.\\nSo we'll want to pass a parameter that says\\nparse_dates=True.\\nAnd then let's just look at the first five records\\nby calling the head method off of the DF data frame.\\n\\nSo we'll say df.head() and run this.\\nAnd here we've got now a little preview of the data\\nthat is sitting inside of this Superstore Sales data set.\\nAnd as you can see,\\nPerfect, so now let's go ahead\\nand create a time series plot from this.\\n\\nTo do that, we're going to call the plot method\\nand we'll just select the order quantity variable\\nfrom the data frame.\\nSo we'll say df and then select order quantity here,\\nand then call the plot method off of this and run it.\\nAnd it printed out very quickly,\\nbut it's way too much data.\\n\\nWe can't make any sense of this, right?\\nSo what I'm going to do is I'm just going to create\\na small sample of this data\\nso that we can plot something out\\nthat we're able to actually decipher.\\nSo let's create a second data frame\\nand we'll call it df2.\\nAnd then we'll call the sample method\\nin order to take a random sample of data points\\nfrom the dataset.\\nSo we'll say df.sample(),\\nand then let's take 100 data points.\\n\\nTo do that, we need to define the n perimeter = 100.\\nNow this function is going to be doing random sampling\\nand I want you to get the same results on your machine\\nthat I get in here in this demonstration.\\nSo what we need to do is actually set the seed for that.\\nSo to do that,\\nwe'll say random_state and just set it equal to 25.\\nAnd that will ensure that the points that are pulled\\nin this demonstration are the same ones that are pulled\\non your machine when you're doing this\\nat home or in the office.\\n\\nAnd then we also have to pass in a perimeter for the axis\\nand we'll say axis=0.\\nAnd let's also create some labels real quick.\\nSo plt.xlabel on the x-axis.\\nLet's label that order date.\\nAnd then let's create a y-label.\\n\\nChange out the string here, call that order quantity.\\nAnd let's also call the title function,\\nwhich is pt.title.\\nAnd we'll just pass in a string for the title of the chart.\\nSo that's going to be Superstore Sales.\\nAnd then let's just plot out this smaller data frame.\\nSo we're going to call the plot method.\\n\\nLet's select only the order quantity field.\\nSo we'll say df2.\\nAnd then from that data frame,\\nwe want to select only the order quantity.\\nAnd then off of this entire thing, we call the plot method.\\nAnd I'm going to run this.\\nLooks like there was a syntax error.\\nI need to add a letter T here, run this again.\\n\\nOkay, cool.\\nSo this is a lot easier to read than the graph prior.\\nAnd what we're seeing along the x-axis\\nis really the dates that the orders were made\\nand the quantity of orders that happened on those dates.\\nSo this was a really quick and easy way\\nto use Python to generate a time series.\\nIn the next section,\\nI'm going to show you how to create statistical data graphs.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2714161\",\"duration\":891,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating statistical data graphics in Seaborn\",\"fileName\":\"3006708_en_US_04_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1398,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to create statistical data graphics. This video covers histograms, box plots, and scatter plots.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":29803818,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Statistical plots allow viewers\\nto identify outliers, visualize distributions,\\ndeduce variable types, and discover relationships\\nand core relations between variables in a dataset.\\nIn this course, I'm going to show you\\nhow to use statistical plots to visually detect outliers,\\ndeduce variable distribution and type, and uncover\\nrelationships and core relations between variables.\\nNow, histograms are very simple plots\\nthat are used to show variable distribution.\\nScatter plots, on the other hand,\\nare used to show relationships between variables.\\n\\nScatter plot matrices show core relations between variables\\nand box plots show variable spread\\nand are useful for outlier detection.\\nLet me show you how to create these in Python.\\nSo your Jupyter Notebook is coming loaded with the libraries\\nthat you will need, or at least most of them.\\nSo we have our standard libraries, which are num, pi,\\npandas, matplotlib and seaborn.\\n\\nI've also gone ahead here\\nand set the plotting parameters for matplotlib.\\nIn this demonstration, we're actually going to be working\\nwith seaborn, and I showed you\\nhow to import that in earlier demonstrations.\\nA nice thing about seaborn is that it provides you options\\nfor style sets and there are a lot of different options.\\nSo what I did is I set a seaborn style equal to whitegrid.\\nThere are many different options,\\nbut I like the whitegrid option, so we'll use that.\\n\\nAnd yeah, I'm just going to run all of this\\nto have it imported into our IPython environment.\\nAnd then let's get started\\nwith creating statistical data graphics.\\nThe first thing we're going to do in this demonstration is just\\nto start eyeballing dataset distributions with histograms.\\nAnd this Jupyter Notebook is coming loaded with the data\\nthat you should need, although you'll want to change\\nthis file extension for the file extension at your setup.\\n\\nSo you would just go over here to the Explorer\\nand then go to the data folder and right-click.\\nIn this demonstration,\\nwe're going to be using the empty cars dataset.\\nSo then you will just right-click here,\\nget the empty car dataset,\\nand then change out the string for the address.\\nAnd for this demonstration, let's just set the index\\nfor the data frame equal to the car names column\\nin the cars data frame.\\n\\nAnd I've already preloaded this into your notebook\\nso you don't have to type all of this out again,\\nbecause this is very similar to prior demonstrations.\\nSo what I want to do here first\\nis just isolate the mpg variable.\\nSo we'll say mpg variable is equal to cars.\\nAnd then we'll use the indexer\\nto select the mpg column here.\\nAnd to plot it out, we'll call the plot method\\noff of the mpg variable,\\nso mpg.plot, and since we are creating a histogram,\\nwe need to set that perimeter inside of the plot method.\\n\\nTo do that, we're going to say kind=,\\nand then create a string that reads hist\\nand that will instruct Python to create a histogram.\\nAnd then we run this,\\nand as you can see, we have a nice little histogram\\nthat shows the distribution of data in our mpg variable.\\nAnother way to create a histogram\\nwould be to just call the hist function\\nand then pass in the mpg variable.\\n\\nLet me show you how to do that,\\nplt.hist and then pass in mpg\\nand then call the plot function,\\nplt.plot, and run this.\\nAnd now we have just a different way\\nof creating a histogram inside of Python.\\nNow let me show you how to create a histogram using seaborn.\\nTo do that, you would use the disc plot function,\\nso that's going to be snsdisplot.\\n\\nAnd then we need to pass in the mpg variable.\\nAnd this is one of the two ways\\nI want to show you how to do it.\\nThis is the simplest way\\nof creating a histogram using seaborn.\\nSo I'm going to run this.\\nAnd there you can see we have a nice,\\nkind of more beautiful, more styled plot\\nthat was actually created more simply\\nusing the seaborn method.\\nNow let me show you another way to use seaborn\\nto create scatter plots.\\n\\nBasically, I messed up with our script.\\nSo I said something about\\nhow I'm going to show two different ways to do this,\\nbut that wasn't proper.\\nThat wasn't correct.\\nSo let's just actually start over again from the part\\nwhere we're going to create this from scratch\\njust 'cause it's super simple.\\nSo now let me show you how to use seaborn\\nto create a histogram.\\nTo do that, you would use the displot rephrase.\\n\\nTo do that, you would use the displot function,\\nso that would be sns.dissplot,\\nand then pass in the mpg variable and run this.\\nAnd as you can see, you know,\\nwe've got a nice styled histogram here\\nthat was extremely simple to create,\\nand that's the basics of how\\nto create a histogram using seaborn.\\nI'm going to show you two different ways of doing this,\\nand the first one is just via the plot method.\\n\\nSo we'll call plot off of our cars data frame,\\nand then we'll pass in a parameter\\nto say what kind of plot we want.\\nSo we'll say kind is equal to scatter.\\nAnd then we want to set our variables.\\nSo X should be equal to hp\\nand then let's set our Y variable equal to mpg.\\n\\nAnd let's also go ahead and set a color for this plot.\\nTo do that, we can just say C is equal to,\\nand then we'll select the color dark gray\\nby writing a string that reads dark gray.\\nAnd lastly, let's set a size\\nfor each of our dots in our scatterplot.\\nSo to do that, we would say S is equal to 150.\\n\\nAnd I'm just checking this over\\nfor any obvious syntax errors.\\nOkay, and then run this.\\nAnd there we go, we have a nice scatter plot.\\nBut I also want to show you\\nhow to create a scatter plot using seaborn.\\nSo with seaborne, you just use the reg plot function,\\nwhich it's going to be sns.regplot,\\nand you pass in the variables that you want plotted out\\nfor the X and Y axis.\\n\\nSo for X, we will say x should be equal to the hp variable,\\nand then y should be equal to the mpg variable.\\nAnd then we also need to define\\nwhere we're actually pulling this data from.\\nSo for that, we would need to create a perimeter\\nthat says data and then set the equal to cars,\\nour cars data frame.\\nAnd then to make sure that this comes out as a scatterplot,\\nwe need to say scatter equal to true.\\n\\nAnd this just tells seaborne yes, create a scatterplot.\\nAnd then when we run this, we see seaborn's version\\nof the very same scatterplot.\\nAs you can see, it's a lot more detailed\\nand helpful than the generic version\\nwe created with the simple plot method above.\\nMoving on, I want to show you how to use seaborn\\nto generate a scatter plot matrix.\\nAnd this is actually really, really easy.\\nSo you would just call the pair plot function.\\n\\nSo this is going to be sns.pairplot\\nand we pass in our data frame cars and then run this.\\nAnd of course this is awesome,\\nbut it's also a lot of information to take in at a glance.\\nSo what I think we should do is let's just create a subset\\nand then we'll plot that out instead.\\nWe'll call it car subset.\\nSo we'll say cars_subset,\\nand we'll set that equal to cars.\\n\\nAnd then we'll use the indexer\\nto select the variables we want here.\\nLet's make those mpg,\\ndisp, hp and wt.\\nAnd then again, we will use our pairplot function.\\nSo sns.pairplot,\\nand we'll pass in this time our car subset.\\n\\nAnd then to generate the plot, we just need\\nto say plt.show, run this.\\nAh, okay, so this is a bit easier to read,\\nbut the nice thing about having a scatterplot is\\nat a glance, you can really get an idea about the type\\nof relationship that is occurring between the variables.\\nAnd also you can see things like possible outliers.\\nLike this point right here is possibly an outlier\\nor over here, this could be an outlier.\\n\\nAnd then you can also tell at a glance\\nwhat type of variables you have.\\nFor example, these variables plotted here are\\nall continuous variables, which you can tell by looking at\\nthe distribution of points that are plotted.\\nSo scatterplot matrices are just really nice to have\\nto get a fast understanding of your variables\\nand your variable pairs.\\nNow let's look at how to build a box plot.\\nWe're going to use the box plot method to do this.\\n\\nSo we're going to say cars.boxplot,\\nand then we'll pass in the perimeter to say\\nwhat we want plotted in our box plot.\\nSo our first box plot, let's just plot out a column mpg.\\nSo we'll say column equal to mpg.\\nAnd then we want to plot the mpg variable\\nagainst the automatic manual transmission variable.\\nSo to do that, we'll say by is equal to am\\nfor automatic and manual transmission variables.\\n\\nAnd then let's also just create a second block box plot.\\nAnd this time we'll plot weight against transmission type.\\nSo we'll say cars.boxplot,\\nand we'll say our first column should be weight.\\nAnd then we want to plot this\\nagainst or by the automatic manual transmission variable.\\n\\nSo am here and then we run this.\\nI would say it's a little cramped.\\nI'm going to go up and just change this variable\\nto make it a little taller.\\nI'm going to say six inches, six inches tall for our chart\\nhere, just so we don't have any labels scrunched up.\\nOkay, so this is better.\\nAnd this is showing us how our data is distributed\\nacross mpg and automatic and manual transmission.\\n\\nSo what it's saying here is that cars\\nthat get less miles per gallon\\ntend to not have an automatic transmission.\\nIf they do have an automatic transmission,\\nwhich is am equal to one, then they tend\\nto get more miles per gallon.\\n\\n\"}],\"name\":\"4. Practical Data Visualization\",\"size\":196738610,\"urn\":\"urn:li:learningContentChapter:4583164\"},{\"duration\":4779,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4583161\",\"duration\":494,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Simple arithmetic\",\"fileName\":\"3006708_en_US_05_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":749,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to perform simple arithmetic. This video covers standard arithmetic, arithmetic multiplication, and matrix multiplication.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15743431,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this section,\\nwe're going to be talking about how\\nto do simple arithmetic in Python.\\nThe benefit of the NumPy library\\nis that it makes it really easy to do math on data\\nthat's stored in either arrays or matrices.\\nNow, an array is just a one-dimensional container\\nfor elements that are all of the same data type.\\nAnd, matrix is just a two-dimensional container\\nfor elements that are stored in an array.\\nThe basic arithmetic operators in Python\\nare addition, subtraction,\\nmultiplication, and division.\\n\\nLet me give you an example of where NumPy can come in handy.\\nHave you ever tried to use a spreadsheet application\\nto perform mathematical operations on a data set\\nthat has more than 300,000 rows?\\nWhat happened?\\nIf the application didn't crash,\\nthen it took a lot of time\\nand effort to get the program to make the computation.\\nBut with NumPy, on the other hand,\\nyou can quickly and easily do mathematical\\nand statistical operations on data sets\\nwith even millions of records.\\n\\nSimply put, NumPy makes it easy\\nto do math on large data sets.\\nWith this slide here, I just wanted to familiarize you\\nwith the operators that you will use in Python\\nto achieve these arithmetic operations.\\nSo we're starting out the coding demonstration\\nwith having imported NumPy\\nand also imported the random number generator from NumPy.\\nJust run this.\\nAnd,\\nit never looks good to see more than two digits\\nafter a decimal point,\\nso let's go ahead and limit the number\\nof decimal places returned in this demonstration.\\n\\nSo to do that, you would just say,\\nnp.set_printoptions,\\nand then let's set a precision of 2.\\nSo precision equal to 2,\\nand run this.\\nNow the first thing we're going to do\\nis look at math with arrays.\\nSo we of course need to create some arrays to do that math.\\nAnd our first array will be array a,\\nand we'll set that equal to an array of six values.\\n\\nSo what we need to do is call the array function,\\nnp.array,\\nand then we'll pass in a list\\nof values from one to six.\\nAnd then print that out.\\nWe'll just say a and run this,\\nand there you go,\\nwe have an array of six values.\\nAnd then let's create a second object,\\nwhich will be a matrix and we'll call it b.\\n\\nAnd to create this matrix,\\nwe'll use the array function,\\nso we'll say np.array,\\nand this time we need to pass in two lists.\\nSo the first one is going to contain the values,\\n10, 20, and 30.\\nAnd then the second list will contain the values,\\n40, 50, and 60.\\n\\nAnd then we'll print this out\\nand very good, we have a matrix.\\nNow, before we're actually going in and doing math,\\nlet's create an array via assignment.\\nI want to show you how to do that.\\nSo let's this time use a random number generator\\nin NumPy, and what we're going to do\\nis we're going to create six random values.\\nSo to set the seed for a random generator,\\nwe need to say np.random.seed.\\n\\nAnd this just makes so\\nthat the results you get on your computer\\nwill be the same as we,\\nas I show you here in the demonstration.\\nAnd then we need to assign a value to the variable c.\\nSo we'll say c is equal to 36 times,\\nthe random numbers that are generated\\nby the random number generator in NumPy.\\nSo we'll say np.random.randn,\\nand let's pass in the value of six.\\n\\nThis is just saying that we want to have six values.\\nAnd we'll print this out.\\nAnd the one thing that I would like to point out here is\\nthat when we use the randn function,\\nwhich is the random number generator in Python,\\nwhat that is actually doing\\nis it's generating both positive\\nand negative random numbers.\\nNow let's create a fourth array,\\nwhich is going to be called d,\\nand we'll set d equal to a series of sequential numbers\\nbetween 1 and 34.\\n\\nSo we'll use the arange function,\\nnp.arange,\\nand then we will make the starting point 34\\nand the ending point 35,\\nwhere 35 is excluded from the series of numbers\\nthat's output from this function.\\nAnd we can just go ahead and print this out.\\nOkay, great.\\nSo we have our array.\\nNow let's just start performing some math.\\n\\nBefore doing so, I just want to point out one thing here\\nbecause I'm calling all of these objects arrays,\\nbut some of them are actually matrices,\\nand what I want you to keep in mind\\nis that a matrix is actually just a two-dimensional array.\\nThat's why you're hearing me say array\\nwhen I'm actually creating matrices\\nand kind of using them interchangeably in certain points.\\nFirst things first,\\nlet's just multiply the array a by the number 10.\\nSo we'll say a times 10, run this.\\n\\nAnd if we look back here at the a variable,\\nif we multiplied each of these numbers by 10,\\nwe would get this output here.\\nMakes sense, and that was very easy to do,\\nvery straightforward.\\nNow let's try an addition operation.\\nSo we'll do c plus a,\\nand run this.\\nSo this output array is a result of adding the c array\\nto the a array.\\n\\nJust to do a little back checking here,\\nlet's look at the value of c and a,\\nto make sure it matches up.\\nWe've got a 9.22 and up here, c,\\nc here is 8.22,\\ngo up a,\\nvalue here is 1,\\nso when you add those together, you get 9.22.\\nSo yeah, that makes sense.\\nAs you can see, it's really simple\\nto do straight-out arithmetic\\nwith arrays and matrices in Python.\\n\\nLet's do c minus a.\\nAnd again, makes sense.\\nThis time we're getting 7.22,\\nwhich is 8.22 minus 1.\\nSo yep.\\nNow let's try c times a,\\nwhich of course should be 8.22\\nas the first value in the output array.\\nSo we'll say c times a,\\nand run this.\\nAnd yeah, it's 8.22.\\n\\nMakes sense.\\nAnd if you wanted to do division here,\\nall you'd have to do is say c divided by a.\\nAnd of course, 8.22 divided by 1 is going to be 8.22.\\nSo we're good to go there.\\nAnd now you know how to do simple arithmetic using arrays\\nand matrices in Python.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4589019\",\"duration\":579,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Generating summary statistics\",\"fileName\":\"3006708_en_US_05_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":868,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to create standard data graphics. This video covers line charts, pie charts, and bar charts.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":18762967,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Descriptive statistics describe a variable's\\nvalues and their spread.\\nFor example, imagine you work for a company\\nthat monitors patients' health in real time\\nand you need to build a script\\nthat detects dangerous anomalies.\\nYou could generate summary statistics\\nin micro batch and then calculate the mean, max,\\nand standard deviation of incoming health data\\nthat's generated from the monitoring device.\\nWith these descriptive statistics\\nyou could generate automatic alerts\\nwhen unusually high or low data points are generated\\nby the patient's monitoring dived\\nindicating potentially dangerous health status\\nof the monitored patient.\\n\\nDescriptive statistics provide a quantitative summary\\nof a variable and the data points that comprise it.\\nYou can use them to get an understanding of a variable\\nand the attributes that it represents.\\nThere are two categories of descriptive statistics.\\nOne is descriptive statistics that describe the values\\nof an observation in a variable.\\nAnd the second is the descriptive statistics\\nthat describe a variable's spread.\\nSo to get more specific,\\nif you wanted to generate descriptive statistics\\nthat describe the observations in a variable\\nthen what you would do is you'd go ahead\\nand calculate the sum, median, mean, and max\\nof those observations in the variable.\\n\\nBut again, the second way\\nto generate descriptive statistics is\\nto describe the variable's spread.\\nAnd if you wanted to do that\\nwhat you would to is calculate out standard deviation,\\nvariance, counts or quartiles.\\nIf this doesn't make a lot of sense to you now\\njust hold on, because I'm going to show you an example\\non the coding demonstration and then\\nyou'll really understand what I'm talking about.\\nBut first let's look at some of the uses\\nfor descriptive statistics.\\nYou can use descriptive statistics\\nto easily detect outliers.\\n\\nYou can use it for planning data preparation requirements\\nfor machine learning, and you could use it\\nfor selecting features for use in machine learning.\\nOkay, so now let me show you how\\nto generate summary statistics using pandas and scipy.\\nWe're going to start off, of course,\\nby bringing in our libraries.\\nSo we need to to import numpy, pandas, series and data frame\\nand then also scipy.\\nI want to point out that we are importing stats\\nfrom scipy.\\n\\nAnd I've got these already loaded for you\\nin the Jupiter notebook\\nand your notebook's also coming preloaded\\nwith the MT car's data set\\njust to save you time\\nso you can just go ahead and run these.\\nWe've already covered these in previous lectures.\\nAnd what I'd like to do first is just to print out\\nthe first five records of the cars data frame here.\\nSo we'll say cars.head.\\n\\nRun this.\\nAnd now we have a little preview of the data\\nwe've got inside of the cars data set.\\nNow I want to show you how to use the sum method.\\nSo we'll drop down here\\nand the sum method adds up the total of numbers\\nin a column, or in rows of a data frame.\\nBy default, sum will count up values and provide a total\\nfor each column.\\nBut if you pass an axis equal to one argument\\nthen it will add up the values\\nalong the data row-wise instead.\\n\\nSo let's practice with this one.\\nSo we'll just say cars.sum and run this.\\nSo how this works is, the sum method has gone along\\nand summed up the values in the columns\\nof the cars data frame.\\nBut if you wanted to sum the values along the rows instead\\nthat's easy enough to do.\\nYou would just take the same command\\nand then you would pass in a perimeter\\nthat says axis is equal to one.\\n\\nSo let's try that out here.\\nWe'll say cars.sum\\nand we'll say axis equal to one.\\nAnd of course, we only wanted to sum up numeric values\\nso we'll pass in an argument that says numeric_only\\nequal to true.\\nLooks like I need to change this m to an n.\\n\\nAnd run this.\\nAnd there we have it.\\nSo this is basically the output of the summation,\\nthe values of the dataset, row-wise.\\nThe median method finds and returns a median value\\nwith the middle value from the columns\\nor rows of the data frame.\\nSo let's calculate a median value by saying\\ncars.median and then again we need to pass a perimeter\\nthat says numeric only equal to true\\nand run this.\\n\\nAnd then what Python has done here is it's gone\\ninto each of the variables in the cars data set\\nand it's found the median value for each of those variables.\\nIt's returned those as an output here.\\nAnd to calculate the mean it's very, very simple.\\nYou can just say cars.mean.\\nAnd again pass in the numeric only equal to true.\\n\\nAnd run this.\\nAnd this is the average value for each variable\\nin the cars data frame.\\nIf you wanted to generate some statistics\\nabout the maximum value for each variable\\nwe would just say cars.max\\nand then what this is doing is\\nit's outputting the greatest value in each of the variables.\\nNow if you wanted to be able to identify the row\\nwhere the maximum value came from\\nyou'd just call the id maximum method.\\n\\nTo see the index value of the row\\nthat contains the maximum value.\\nSo let's try that out here.\\nWe'll look at the mpg variable.\\nTo isolate that, we'll say mpg is equal to cars.mpg,\\nselect that variable,\\nand then we'll call the id xmax method off of this\\nso that's mpg.idxmax\\nand then what we're seeing here is that\\n19 is the index number of the row\\nwhere the maximum value was found in the mpg variable.\\n\\nNow let's look at som summary statistics\\nthat describe variable distribution.\\nThe most fundamental summary statistic\\nthat describes distribution would be standard deviation.\\nAnd in order to generate that in Python\\nyou can just use the standard method.\\nSo for our example we would take cars.std and then\\npass in perimeter numeric only equal to true\\nand run this.\\n\\nAnd what this has done is it's gone along\\nfor each of the variables\\nand calculated the standard deviation\\nof the values in that variable.\\nTo calculate the variance you would say cars.var,\\nuse the var method here.\\nAnd set numeric only equal to true.\\nAnd now you're getting the variance\\nfor each of the variables.\\nThere's also the value counts method\\nand this method counts up the unique values\\nin and array or a series object.\\n\\nIt shows you how many unique values are present\\nin a data set.\\nSo let's look at the gears variable.\\nWe'll isolate that.\\nWe'll create and object called gear\\nso we'll say gear is equal to cars.gear\\nand then off of that object\\nwe will call the value underscore counts method\\nand run this.\\nAnd what you're seeing here is that\\nthe gear variable has three unique values.\\n\\nThose are three, four, and five.\\nOn the right side you can see the unique counts\\nfor each of these variables.\\nIf you wanted to take a broader perspective,\\nwhat this is really saying is\\nthat the cars data set has 15 cars with three gears,\\n12 cars with four gears\\nand five cars with five gears.\\nAnd I want to show you really quickly the easy peasy way\\nto get an entire statistical description of a data set.\\nThis is the describe method.\\nSo all you would need to do is say cars.describe\\nand with this you're basically getting\\nall of the descriptive statistics that can be generated\\nfrom each of the variables in the entire data set\\nall at one time.\\n\\nSo it's super efficient and helpful to have this on hand.\\nNow that you know how to summarize numerical variables\\nlet's move on to summarizing categorical ones.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2715032\",\"duration\":619,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Summarizing categorical data\",\"fileName\":\"3006708_en_US_05_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"2 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1018,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to define elements of a plot. This video covers object-oriented plotting, sub-plots, and axis labels.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":20130845,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's talk about how to summarize\\ncategorical data.\\nCategorical data is described by how observations\\nare distributed across a variable's categories.\\nA very simplistic approach to sentiment analysis\\ncould involve web scraping public product reviews,\\nthen classifying certain words found in the scraped data\\nas positive or others as negative.\\nYou could then do a categorical word count\\non the product review data\\nto score product reviews or feedback as either good or bad.\\n\\nCategorical variables only assume a fixed number of values.\\nSo, as you can see here, we have a very simple dataset\\nthat contains apples and oranges.\\nSo this variable here, the fruits variable,\\nis actually a categorical variable.\\nAnd then what you can do\\nis you can group your categorical variable into subgroups\\nbased on the fruit category.\\nSo in this case we could break down the dataset\\ninto apples and oranges\\nbased on this categorical fruits variable.\\n\\nThere are three main ways to describe categorical variables.\\nThose are counts, variable description, and grouping.\\nSo I'm going to show you\\nhow to create each of these three types of descriptions\\nover in the Jupyter Notebook.\\nRight, so we're bringing in numpy and pandas.\\nI've already got those imported for you\\ninto the IPython environment.\\nAnd I've also got the empty cars dataset\\nready for you to go here.\\nSo let's just take a look at the first 15 records.\\n\\nTo do that, we will use the head method.\\nSo we'll say cars.head() and then pass in the number 15.\\nNow, you get a basic idea\\nof the data that's inside this dataset.\\nAnd, of course, we've also now set an index.\\nThat index here is equal to the car_names variable.\\nSo you can see that here.\\nWe set it here.\\n\\nOkay, so the first thing that I want to show you\\nis the value_counts method.\\nAnd this method makes a count of all the unique values\\nin an array or a series object.\\nSo, first, let's just isolate our car variable.\\nThis is for carburetors.\\nIt's a number of count of carburetors that each car has.\\ncarb is equal to cars.carb.\\nAnd then we'll generate the value counts\\nby calling the value_counts method.\\n\\nWe'll say carb.value_counts() and run this.\\nAnd what you're seeing here is that there are 10 cars\\nthat have four carburetors.\\nThere are 10 cars that have two carburetors.\\nSeven cars have one carburetor.\\nThree cars have three carburetors.\\nAnd so that's basically how you need\\nto interpret this result.\\nNow, let's look at the groupby function.\\nAnd to group a dataframe by its values\\nin a particular column,\\nyou just call the groupby method off of a dataframe\\nand then pass in the index value of the column series\\nyou want the dataframe to be group by.\\n\\nSo let's just create a subset of our dataframe,\\nand we'll call that cars_cat.\\nAnd within this subset, we'll include five variables,\\nthe cyl variable,\\nthe vs variable,\\nthe am variable,\\nthe gear variable,\\nand, last one, carb.\\n\\nAdd the single quote here.\\nThen let's just print this out.\\nSo we'll say cars_cat.head().\\nRun this.\\nOkay, so here we have a small subset\\nof our original cars dataframe.\\nSo now let's group by the gear variable\\nand then describe the dataset by that unique grouping.\\nIn order to do this,\\nlet's just create a new variable called gears_group.\\n\\nAnd then we'll set it equal to our cars_cat dataframe.\\nAnd what we'll do for the grouping part of this\\nis that we will call the groupby method\\noff of that dataframe\\nand pass in the label index of the column\\nby which we want the dataframe to be grouped which is gear.\\nThen we want to generate a statistical description\\nof the dataset based on that grouping.\\nSo let's just use the describe method.\\n\\nWe'll say gears_group.describe()\\nand run this.\\nAs you can see, we have three rows by 32 columns.\\nThe reason we have three rows\\nis because there are only three different options\\nor your count for the cars dataset.\\nAnd then we have 32 columns\\nbecause for each of the variables in the small subset,\\nthe cars_cat subset,\\nwe have generated statistical descriptions for each of them.\\n\\nSo, as you can see, we're getting a long output table.\\nThe next thing we need to look at\\nis how to transform variables to categorical data type.\\nNow, to create a series of categorical data type,\\nyou would just call the pd.Series function\\non an array or a series that holds\\nthe data you want the new series object to contain.\\nNow, when you pass in the dtype\\nequal to category argument,\\nthis tells Python to assign\\na new series data type of category.\\n\\nHere, we'll create a new categorical series\\nfrom the gear variable.\\nAnd then we'll assign it to a new column\\nin the cars dataframe called group.\\nSo we'll say cars[],\\nand then we will create a new column here called group.\\nAnd then we're going to say that this is equal\\nto a new series object that we're going to create here\\nwith the series constructor.\\nSo we're going to say pd.Series().\\n\\nAnd we'll pass in cars.gear because we're interested\\nin converting the cars.gear variable to a series.\\nAnd then we want to, of course, assign the dtype\\nequal to category.\\nTake that and then print this out.\\nNow, let's just print out this new variable\\nand look at its data type.\\nSo to do that, we will just copy our variable here,\\ncars['group'],\\nand then we call the dtypes method off of that, .dtypes,\\nand run it.\\n\\nOkay, great, so now you see\\nthat we actually do have a categorical data type,\\nand it's part of our cars dataframe which is cool.\\nWe just created this new variable\\nand added it to our cars dataframe.\\nNow, let's look at the distribution of gear types\\nin this variable.\\nTo do that, what we would need to do\\nis say cars[] and then select our group variable here\\nand then call the value_counts method off of that,\\nvalue_counts().\\n\\nAnd so here we have got cars\\nwith three different counts of gears.\\nSo pretty much makes sense.\\nLast thing I wanted to show you in this demonstration\\nis basically how to create a crosstab\\nor cross-tabulation table.\\nThese are very important.\\nYou need to know how to use these\\nin order to make sense of categorical data in data science.\\nLet's just start by creating our crosstab.\\nTo create a crosstab,\\nyou would just call the pd.crosstab function\\non the variable you want included in the output table.\\n\\nSo let's do that here by saying pd.crosstab().\\nAnd we're going to select our first variable\\nwhich would be the am variable.\\nSo we'll say cars[]\\nand then select the am variable.\\nThen for our second variable,\\nthat's going to be the gears variable.\\nSo we'll say cars[], and we'll select gears.\\n\\nAnd we run this. Looks like there is a typo.\\nYeah, so I need to take this s out,\\nthen run it again.\\nAnd, as you can see, we have a really concise summary\\nof each of these two variables.\\nA crosstab is a cross tabulation of two or more features.\\nBy default, a crosstab shows frequency counts for features.\\nSo the example we just did in Python, we basically selected\\nthis am variable and the gear variable,\\nand Python went ahead and created\\na small cross tabulation of these two variables\\nand described to us basically about how gears are broken up\\nwith respect to automatic and manual transmission.\\n\\nSo the am stands for manual transmission.\\nAnd what this table is telling us\\nis that for cars with a manual transmission,\\nthey mostly have three gears,\\nbut cars that have an automatic transmission\\nare more likely to have five gears.\\nIt's basically showing a distribution of gear counts\\nbased on the type of car transmission.\\nThis is just a very simple example\\nof how you can use crosstabs to generate\\nsummary statistics and descriptions of categorical data.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2714162\",\"duration\":893,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Pearson correlation analysis\",\"fileName\":\"3006708_en_US_05_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"3 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1573,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to conduct parametic correlation analysis via Pearson correlation. This video covers linear correlation and causation.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":32785448,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's talk about\\nparametric correlation analysis.\\nParametric correlation analysis is\\na method you can use\\nto find correlation between\\nlinearly related continuous numeric variables.\\nDon't worry if you don't\\nexactly understand what that means\\nbecause I'm going to show you how\\nto figure this out in just a minute.\\nFirst, I want to explain one\\nimportant point about correlation.\\nCorrelation does not imply causation.\\nLet me explain.\\nImagine you're a doctor studying\\nregional obesity trends.\\n\\nYou have two data sets:\\nOne on store size reported\\nby zip code,\\nand two on national obesity prevalence broken down\\nby zip codes.\\nIn the course of your investigation,\\nyou apply the Pearson correlation method,\\nthat's the method I'm about to show you,\\nand you find that there's\\na very strong positive correlation\\nbetween grocery store size and obesity.\\nThe bigger the grocery stores,\\nthe more obesity there tends to be.\\nOf course, the size of the store doesn't cause obesity,\\nbut they're correlated,\\nand that correlation is quantifiable\\nthrough the Pearson Method.\\n\\nPearson correlation is measured\\nby the correlation coefficient, R.\\nIf you have a Pearson R\\nthat's close to one,\\nthen that's a strong positive relationship.\\nAnd if you had an R value\\nthat is close to negative one,\\nthen you've got a strong negative relationship.\\nIf you have an R value equal to zero,\\nor close to it,\\nthen you're basically seeing\\nthat your variables are not linearly correlated.\\nNow, the Pearson correlation assumes\\nthat your data is normally distributed,\\nthat you have continuous numeric variables,\\nand that your variables are linearly related.\\n\\nA really important note\\nthat I wanted to add here is\\nhow do you use the Pearson correlation?\\nSo it's safe to use Pearson correlation\\nto uncover linear relationships between variables,\\nbut you can not use it\\nto rule out the possibility\\nof non-linear relationships between variables.\\nFor this demonstration,\\nwe're going to be bringing\\nin our standard libraries,\\npandas and numpy,\\nbut also please note that\\nI've imported matplotlib and seaborn,\\nas well as the rcParams.\\n\\nWe're also going to be using scipy\\nin this demonstration.\\nSo all of these are already loaded\\nin our notebook,\\nand I've also preloaded\\nthe empty cars data set\\nthat we've been working with,\\nand set the plotting parameters for matplotlib.\\nWe covered all of these things in previous lectures,\\nbut the one thing I want to point out here is\\nthat we are importing Pearson R\\nfrom the scipy stats package.\\nSo you have to first start off\\njust by running these.\\n\\nAnd like I said,\\nempty cars is ready to go.\\nSo all we need to do is run this\\nto load it into our environment.\\nCool.\\nand let's just go ahead and start\\nby generating a pairplot using\\nthe seaborn library.\\nTo do that, we'll use the pairplot function,\\nso that's sns.pairplot,\\nand we'll pass in cars data frame\\nand run this.\\n\\nMove it up a bit.\\nIt's just thinking for a little while.\\nYou can tell what Python's doing\\njust by seeing this moving blue scroll\\nat the top,\\nand also this icon here saying\\nhow long it's taken for Python\\nto process the request.\\nOkay, wow.\\nSo we have a lot of data here\\nthat's been plotted out for us.\\nAnd as you can see,\\nif you were to count them up,\\nwe actually have 11 numeric variables\\nin the cars data set.\\nThis basically takes up a lot of space.\\n\\nI went ahead and selected\\nsome variables for our analysis,\\nand I'll go ahead and generate\\na scatter plot matrix of those\\nin order to show you\\nwhat about them is desirable\\nfor the Pearson correlation.\\nAnd I'm going to take you over\\ninto another screen\\nto explain really quickly.\\nBut before I do that,\\nlet's just make this second scatter plot.\\nSo we'll call it X,\\nand we'll set X equal to our cars data frame,\\nbut we only want to select four columns,\\nwhich are mpg, hp, qsec, and wt.\\n\\nAdd the single quotes here.\\nAnd then, again, we use the pairplot function,\\nso that's sns.pairplot,\\npass in X, and run this.\\nOkay, so that was a lot faster,\\nand here we have a smaller pairplot.\\nNow let me take you over to\\nthe other screen to explain\\nwhat all this means.\\nSo let's consider the model assumptions\\nfor the Pearson correlation analysis.\\n\\nPearson correlation assumes\\nthat your data is normally distributed,\\nthat variables are linearly related,\\nand that the variables are\\ncontinuous numeric variables.\\nLet's look here at\\nthe normally distributed requirement.\\nA normally distributed requirement is going\\nto give a shape like a bell curve\\nin a histogram.\\nI wouldn't say that all these variables are\\nexactly normally distributed,\\nbut they could possibly be close enough\\nin order to generate\\nsome sort of correlation using\\nthe Pearson correlation method,\\nso I'm going to go with these.\\n\\nNow, let's look at the requirement\\nfor a linear relationship.\\nDo these variables have\\na linear relationship between them?\\nIn other words, does one increase\\nwhile the other decreases?\\nBased on the shape of the distribution\\nof points between the variables,\\nit looks like most of these have\\na distribution that could be\\nat least close to linear,\\nso I'm going to test them out\\nwith the Pearson correlation method.\\nThe last requirement is that\\nthe variables be continuous numeric variables.\\nThe best way for me to show you\\nwhy I think that these are\\ncontinuous numeric variables is\\nto show you what a variable looks like\\nwhen it's not a continuous numeric variable.\\n\\nIf you look over at the scatterplot on the right,\\nthese variables over here are not\\ncontinuous numeric variables.\\nThese are categorical variables\\nbecause they can only assume\\na fixed number of positions,\\nlike we just discussed in the last section.\\nSo this variable can assume one of two values.\\nThat makes it a binomial variable.\\nIn the gear variable,\\nit can assume three values;\\nthree, four, or five.\\nThat makes it a multinomial variable.\\nThese are not continuous numerical variables.\\n\\nWhen you see continuous numerical variables,\\nthe scatterplot of the variables is much\\nmore randomly and evenly distributed.\\nThe end conclusion here is that\\nthe variables that are shown\\non the right would not qualify\\nfor the Pearson R correlation analysis.\\nOkay, great.\\nSo let's get back to our coding demonstration\\nand use scipy to calculate Pearson correlation coefficients.\\nNow, let's look at how to use scipy\\nto calculate the Pearson correlation coefficient.\\n\\nOkay, so let's start by creating\\nsome variables we can use here.\\nSo we'll create an mpg variable,\\nand we'll set that equal to cars.mpg.\\nAnd then let's create an hp variable\\nthat's equal to cars hp.\\nWe'll create a qsec variable\\nand we'll set that equal to cars qsec.\\n\\nAnd then, a wt variable,\\nwhich will be directly\\nfrom our cars data frame,\\nthe weight variable here.\\nOkay.\\nSo let's start first by taking\\nthe Pearson R coefficient\\nof the mpg and hp variable pair.\\nSo to do that,\\nwe're going to say pearsonr_coefficient\\nand P value.\\n\\nWe're going to set these equal to\\nthe Pearson R function,\\nand we'll pass in our mpg\\nand our hp.\\nAnd then, let's print out the label.\\nSo we'll say print,\\nand let our label be Pearson R correlation coefficient.\\n\\nOkay.\\nAnd then, say %0.3f.\\nAnd then, give another percentage sign,\\ncreate a tuple here,\\nand pass in our Pearson R coefficient object.\\nOkay, I'm going to look this over\\nreally quick for any typos.\\nOkay, yeah, so one issue is that\\nI needed to close out\\nthe string here after the F,\\nso I'm going to add a single quote\\nand remove the single quote from there,\\nand then we should be good to go.\\n\\nSo let's run this.\\nAnd then, what I'm going to do\\nto calculate the Pearson R\\nfor the other variable pairs is\\njust to copy this little chunk of code\\nand paste it down here,\\nand just change the variables out.\\nOnce we have the Pearson R coefficients,\\nthen we will discuss.\\nSo the second variable pair is going to be mpg and qsec.\\nAnd the third variable pair will be mpg and weight.\\n\\nSo let me run this.\\nAnd this.\\nOkay, great.\\nSo now we have our Pearson R values.\\nLet's just look at what this means.\\nBased on the Pearson correlation coefficient\\nof these three variable pairs,\\nthe mpg weight variable pair appears\\nto have the strongest linear correlation.\\nThe mpg qsec variable pair has\\na moderate degree of linear correlation.\\n\\nAnd you may be wondering,\\n\\\"Well, what do I do\\n\\\"with this information once I have it?\\\"\\nWhen you're doing machine learning,\\nor other forms of advanced statistical analysis,\\nthese models often have assumptions\\nthat either the features are independent\\nof one another\\nor that they exhibit\\na degree of correlation,\\nand you're going to see\\nthat later in this course.\\nSo you can use the Pearson R correlation coefficient\\nto establish whether or not\\nyour variable pairs meet the requirements\\nof more advanced models.\\n\\nNow, that you've seen the long form way\\nof calculating the Pearson R value,\\nlet me show you some shortcuts.\\nWe will start by using pandas\\nto calculate Pearson R correlation coefficient.\\nSo let me just notate that here.\\nYou can also generate\\nsome Pearson R statistics\\nby using this corr method,\\nso let's do it really quick.\\nUsing pandas, you can also generate Pearson R statistics\\nby using the corr method.\\n\\nSo let's do that real quick.\\nWe'll say that corr here is equal to x.corr,\\ncalled the corr method.\\nAnd then we'll print this out.\\nAnd as you can see,\\nit's really quickly generated\\nall of the Pearson R values\\nfor each of the variable pairs\\nin our smaller subset.\\nThe last way you can do this is using seaborn,\\nand that would be with its heat map function.\\n\\nSo we'll just say sns.heat map,\\nand then we'll pass in our corr variable,\\nand then we'll create some tick labels.\\nSo our xticklabels will be equal to\\nthe column values in our corr data frame.\\nSo we're going to say corr.column.values,\\nand then our ytick labels will be equal to\\nthe columns in the corr data frame.\\n\\nSo corr.columns.values,\\nand then we run this.\\nThat looks nice, but what does it mean?\\nWell, the darker shades of red indicate\\na strong degree of positive correlation,\\nas you can see from the legend.\\nBased on what we see,\\nthe hp weight variable pair has\\nthe highest degree\\nof positive linear correlation.\\nJudging by the darker hues in the grid,\\nthe mpg weight variable pair appears\\nto have the strongest degree\\nof negative linear correlation.\\n\\nYou'll of course see here\\nthat when mpg is plotted against itself,\\nthen it has an absolute value of one.\\nIt correlates 100% with itself,\\nthat's why these are solid cream colors here.\\nAnd then the sort of fuchsia color here,\\nthe weight qsec variable pair is not linearly correlated.\\nKeep in mind, that doesn't mean\\nthere's no correlation between\\nthese variables whatsoever.\\nIn the next video, I'm going to show you\\nsome methods you can use\\nto establish correlation\\nbetween non-linearly related variables.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4588026\",\"duration\":888,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Spearman rank correlation and Chi-square\",\"fileName\":\"3006708_en_US_05_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1465,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to conduct non-parametric correlation analysis. This video covers categorical variables, Spearman rank correlation, and Chi-square tables.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":29738610,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's talk about nonparametric\\ncorrelation analysis.\\nYou can use nonparametric correlation analysis\\nto find correlation between categorical nonlinearly,\\nnon-normally distributed variables.\\nFor an example of where nonparametric correlation analysis\\ncould be useful, imagine that you're a social scientist\\nthat studies smoking habits.\\nYou'll use a non-parametric correlation analysis\\nlike Spearman's rank to test the population\\nfor a correlation between income as a bracket\\nand cigarette consumption of smokers.\\n\\nYou find that higher income individuals are much more likely\\nto smoke cigarettes than lower income people.\\nI'm about to show you how to use Spearman's rank correlation\\nand chi-square tables to establish correlation\\nbetween categorical variables.\\nThe Spearman's rank correlation method\\nworks on ordinal variables.\\nIn case you don't know what that is, an ordinal variable\\nis a numeric variable that is able to be categorized.\\nThe Spearman's rank method converts ordinal variables\\ninto variable pairs and then calculates\\nan R correlation coefficient\\nby which to rank their variable pairs\\naccording to the extent of their correlation.\\n\\nIf that doesn't make much sense for you now,\\ndon't worry at all because I'm going to show you\\nwhat this means in the coding demonstration to come.\\nBut first, let's talk about the R values\\nfor Spearman's rank test.\\nSimilar to Pearson correlation,\\nif you get an R value that is close to one,\\nthen you are seeing a strong positive relationship.\\nWhereas if you get an R value that's close to negative one,\\nthen you're seeing a strong negative relationship.\\nIf your R value is close to zero,\\nyou are seeing that there is either a weak relationship\\nor no relationship whatsoever.\\n\\nIn terms of assumptions, the Spearman's correlation\\nassumes that your variables are ordinal.\\nIn other words, they are numeric\\nbut able to be ranked like categorical variable.\\nIt also assumes that your variables\\nare related nonlinearly.\\nLastly, it assumes that your data\\nis non-normally distributed.\\nDon't worry about these too much right now though,\\nbecause in the coding demonstration,\\nI'm going to show you how to examine your variables\\nand find out whether they meet these assumptions.\\n\\nYou can also use the chi-square test to see\\nif non-linear variables are independent of one another.\\nThe null hypothesis of this test\\nis that the variables are independent of one another.\\nSo if you have a p value of less than 0.05,\\nyou would reject the null hypothesis\\nand conclude that the variables are correlated.\\nIf you had a p value greater than 0.05,\\nyou'd accept the null hypothesis and conclude\\nthat the variables are independent of one another.\\n\\nIn terms of the assumptions of the chi-square test,\\nyou just want to make sure\\nyour variables are categoric or numeric.\\nIf you have numeric variables, then you're going to need\\nto make sure that you have binned them.\\nAnd in case you don't know what binning is,\\nnow is a great time to get familiar with that term.\\nAs an example, imagine you had a variable\\nthat had values between zero and 100.\\nThat's a numeric variable.\\nAs an example of binning, you could break up that variable\\ninto 10 separate groups, 10 groups of 10.\\n\\nAnd then within these 10 groups of 10,\\nyou would just put your data into different categories\\naccording to its numeric values, like this.\\nNow that you know what binning is, let's move on\\nto the coding demonstration portion of this section.\\nOkay, so for this demonstration, we're bringing in\\nour standard libraries, pandas and numpy,\\nbut please note that I also imported matplotlib\\nand seaborn as well as rcParams.\\nAnd we're going to be using scipy in this demonstration.\\n\\nSo all of these are already loaded into our notebook.\\nI've also preloaded the mtcars data set\\nthat we've been working with\\nand set the plotting parameters for matplotlib.\\nWe covered all of these in previous lectures.\\nThe one thing I wanted to point out specifically here\\nis that we are importing spearmanr from scipy.stats package.\\nYou can see that here.\\nSo you just start by running this.\\n(keyboard taps)\\nAnd as I said, the data sets are already loaded for you,\\nbut what you need to do is just run this\\nso we can take a look at the head of the dataframe,\\nthe cars dataframe.\\n\\nOkay, great.\\nSo we have a little preview\\nof what's inside the cars data set.\\nNow, let's just generate a quick pairplot from seaborn.\\nTo do that, we'll say sns.pairplot,\\nand we'll pass in cars, and run this.\\nLet it think for a little while.\\nOkay, great.\\nSo we've got our scatter plot matrix, but as you can see,\\nsince there's so many variables in the dataset,\\nit's pretty hard to visually see what's going on.\\n\\nSo I went ahead and selected some variables\\nfor our demonstration here.\\nSo let's just make a scatter plot matrix of these\\nso I can show you why I chose them.\\nWe'll call the subset x\\nand then we'll just select some variables\\nfrom our cars dataframe.\\nWe'll take the cyl variable, the vs variable,\\nthe am variable and the gear variable.\\n\\nAnd then we will call the pairplot function.\\nSet sns.pairplot, and then we'll pass in our x object,\\nand run this.\\nAll right, so there you have it.\\nNow let me explain why I chose these variables.\\nThe first thing I looked at is are these ordinal variables?\\nWell, if they're numeric\\nbut able to be ranked into categories,\\nthen yes, all of these variables are numeric.\\n\\nAnd they each assume only a set number of possible values.\\nSo yes, these variables are ordinal.\\nAre these variables related nonlinearly?\\nWell, based on this quick glimpse,\\nI don't see any linear relationships between the variables.\\nSo hopefully, yes.\\nLastly, is the data distribution\\nof each variable non-normal.\\nJudging from the histogram here, I'd say yes.\\nBased on this reasoning,\\nI decided to test the variables cylinder, vs, am and gear.\\n\\nSo next steps.\\nLet's just go ahead and isolate each of these variables.\\nSo we'll have cyl, and that's going to be equal\\nto the cyl column of our car dataframe.\\nAnd vs is equal to our vs column.\\nAm is equal to our am column.\\n\\nAnd gear is equal to our gear column.\\nOkay, so we isolated our variables here.\\nSo now, let's go ahead and let's just create some outputs\\nfor our Spearman rank correlation.\\nLet's do that by saying spearmanr_coefficient,\\n(keyboard taps)\\nunderscore coefficient.\\nP_value, and most of these equal to the spearmanr function,\\nand we'll pass in our cyl and vs variable pair here.\\n\\nNeed to change this order here to YL.\\nAnd then we'll also print out a label\\nso that we can really understand\\nwhat our test is telling us.\\nTo do that, we'll just write print,\\nand then we'll create a string which reads,\\nSpearman Rank Correlation Coefficient %0.3f.\\nSpearman Rank Correlation Coefficient %0.3f.\\n\\nAnd then close out the string.\\nAnd then we will write out our results,\\nwhich is going to be this placeholder here,\\nwhich is spearmanr_coefficient.\\nAnd check the syntax real quick.\\nLooks okay.\\nI'm going to run this.\\nOkay, so now what I'm going to do is I'm going to copy\\nthis code here so that we can use it\\nto calculate spearmanr for the other variable pairs.\\n\\n(mouse clicks)\\nSo the second variable pair here should be cyl versus am.\\nAnd then the third variable pair will be cyl versus gear.\\nOkay, so we have these, let's just run them real quick.\\nSo based on the Spearman's rank correlation\\ncoefficient of these three variable pairs,\\nthe cylinder vs variable pair\\nappears to have the strongest correlation.\\n\\nThe other variable pairs do show some correlation,\\nbut only a moderate amount.\\nThat was pretty easy.\\nNow let's look at the chi-square test for independence.\\nTo implement the chi-square test,\\nwe first need to start off by creating a cross tab.\\nWe'll call it table,\\nand we'll say cross tab equal to pd.crosstab.\\nWe'll pass the cylinder and am variables into this function.\\n\\nAnd then what we need to do\\nis we need to import our chi-square function.\\nSo that comes from the scipy library stats module.\\nSo we'll do an import by saying,\\nfrom scipy.stats import chi2_contingency.\\nOkay, now we have what we need\\nto actually implement a chi-square test.\\n\\nLet's write some placeholders for our output.\\nWe'll put chi2 as the first placeholder,\\nand then p and then dof, and lastly, expected.\\nAnd we'll set these equal to the output\\nof our chi contingency function.\\nSo chi2_contingency.\\n\\nAnd then we need to pass in our table.\\nThis is the table we just created,\\nbut we want to pass in only the values.\\nSo we'll say table.values.\\nAnd then let's print this out.\\nWe need a label so that it all makes more sense.\\nSo let that label be Chi-square Statistic %0.3f,\\nclose the string.\\n\\nAnd then we want to print out the p value, p_value label.\\nOkay, so we should actually close the string here.\\nThis is all one label.\\nAnd add a percentage sign.\\nAnd then our actual values that are generated by the task.\\nSo that's going to be chi2 and p placeholders here.\\n\\nOkay, and then we'll run this.\\nOkay, so it looks like there's a typo here.\\nLet's go ahead and take out this comma\\nand then also just fix the spelling here, table.\\nAnd then run this.\\nSo what we're getting here is a chi-square statistic\\nfor cylinder am variable pair.\\nLet's just generate a few more chi-square statistics\\non other variable pairs.\\nTo do that, I'm just going to copy this code over.\\n\\n(keyboard taps)\\nAnd then I'll change out the variable pairs.\\nSo let's change this for vs.\\nAnd then we'll switch out the am\\non the third variable pair for gear.\\nRun this.\\n\\nSo I'll move this up so we can see the results.\\nRemember, with the chi-square test,\\nwe need a p value greater than 0.05\\nin order to conclude that the variables\\nare independent of one another.\\nBased on what I see here, none of the p values\\nare greater than 0.05, so we must reject the null hypothesis\\nand conclude that the variable pairs are correlated.\\nThat's exactly how you would use the chi-square test.\\n\\nNow, let's look at how to do extreme value analysis\\nfor outliers.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4588027\",\"duration\":839,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Extreme value analysis for outliers\",\"fileName\":\"3006708_en_US_05_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"5 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1581,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to conduct extreme value analysis for outliers. This video covers point outliers, contextual outliers, and collective outliers.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":27084894,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now, let's look at extreme value analysis\\nfor outliers.\\nMost machine learning methods assume your data\\nhas been treated for outliers.\\nDetecting outliers can be a data preprocessing task\\nor an analytical method of its own merit.\\nBasically, use outlier detection\\nto uncover anomalies in data.\\nIn this section, we're going\\nto talk about univariate methods.\\nSome use cases for outlier analysis include detecting fraud,\\ndetecting equipment failure,\\nand also cybersecurity event detection.\\n\\nTukey methods are useful\\nfor identifying a variable's outliers.\\nYou can detect unusually high\\nor low data points in a variable\\nby applying the Tukey method for outlier detection.\\nData points identified using the Tukey method\\nshould be treated as potential outliers\\nto be investigated further.\\nThis is a Tukey Boxplot\\nand I wanted to point it out to you to show you\\nhow you can use a Boxplot to detect outliers.\\nBoxplot whiskers are set\\nat 1.5 times the interquartile range.\\n\\nThe interquartile range is really just a distance\\nbetween the lower quartile and the upper quartile.\\nThe upper quartile is where 25%\\nof data points are greater than the value.\\nAnd the lower quartile is where 25%\\nof data points are less than the particular value.\\nAny points beyond 1.5 times the interquartile range\\nare considered outliers.\\nThey'll show up in a boxplot visually as the dots\\nthat extend past the whiskers of the boxplot.\\n\\nThe other way to use Tukey methods to find outliers\\nis to use the Tukey outlier labeling method.\\nAnd this is essentially calculating the Tukey outlier\\nmathematically instead of using the boxplot.\\nSo let's look at how to do all of this in Python.\\nFor this demonstration,\\nwe're bringing in the standard libraries,\\npandas and numpy.\\nPlease note that I also imported matplotlib\\nand cbon as well as rcParams.\\nWe're going to be using scipy in this demonstration.\\n\\nSo all of these are already loaded into your notebook.\\nAnd I've also set the plotting parameters for matplotlib.\\nWe covered all of these in previous lectures.\\nWe're going to be using the iris data set\\nin this demonstration.\\nSo let's go ahead and get that imported.\\nWe'll start by setting the address variable\\nand we'll say that address is equal to\\nand then create a string.\\nAnd I'm going to go over to the notebook.\\n\\nYou'll need to do this for your setup.\\nSo we want iris.data.csv.\\nRight click \\\"Copy Path\\\"\\nand then copy that into the string.\\nOkay, great.\\nSo next, let's create a data frame called \\\"df\\\"\\nand we'll use the read CSV file.\\nSo we'll say, \\\"pd.read_csv\\\"\\nand then say \\\"filepath_or_buffer\\\" here.\\n\\nAnd we'll set that perimeter equal to \\\"address\\\".\\nAnd for header, we will say \\\"none\\\".\\n(keys typing)\\nAnd then it separated with comma.\\nSo for sep, we set that equal to comma\\nsince it's a column delineated file.\\nOkay, so now let's assign names to the columns.\\nSay \\\"df.columns\\\".\\n\\nAnd then we'll set that equal to a dictionary,\\nwhich contains column names.\\nSo the first column name is going to be \\\"Sepal Length\\\".\\nThe second column name is going to be \\\"Sepal Width\\\".\\nNext, \\\"Petal Length\\\".\\nAnd then, the last column...\\nThen, there's two more columns,\\nwhich are \\\"Pedal Width\\\" and \\\"Species\\\".\\n\\nShould be \\\"Species\\\".\\nOkay, now let's just create an X and Y variable.\\nFor X, we want that to be a data frame\\nthat contains the predictor variables.\\nSo in this case, we're going to use the iloc method.\\nSo we're going to call that off of the data frame\\nby saying \\\"df.iloc\\\".\\n\\nAnd then we're going to select only the first four columns.\\nSo we're going to say \\\":,0:4\\\".\\nOkay. And then we actually only want the values.\\nSo we'll just type \\\".values\\\" here.\\nAnd for our target, our Y variable,\\n\\\"y = df.iloc\\\"\\nand we're going to select the column\\nat the column index position four.\\n\\nSo we'll just put \\\":,4\\\".\\nAnd again, we only want the values.\\nSo then we're going to write \\\".values\\\".\\nAnd then let's go ahead\\nand just print out the first five records.\\nSo to do that, we'll say \\\"df\\\"\\nand then we'll select the first five records here.\\nRun this.\\nAh. \\\"PD is not defined.\\\"\\nThat makes sense because I didn't run this at the top here\\nwhen I opened the notebook.\\n\\nSo you always have to remember that even if your notebooks\\ncome preloaded, you, of course,\\nhave to run the cells in order\\nto import your libraries and modules that you need.\\nSo, okay, I ran everything here.\\nAnd then, now,\\nwe have the first five records in our dataset.\\nSo we've already gone ahead\\nand split that data frame into a set of X variables\\nas well as a Y variable.\\nSo now, let's begin looking at the Tukey boxplot.\\nWe can call the boxplot function off\\nof the data frame in order\\nto generate a boxplot automatically.\\n\\nSo to do that, we'll just say \\\"df.boxplot\\\".\\nAnd then we'll pass a parameter\\nthat says, \\\"return_type=dict\\\" for dictionary\\n'cause we want to return a dictionary.\\nAnd then plot this out with a plot function here.\\n\\\"plt.plot\\\". Run this.\\nOkay, so here we have a Tukey boxplot\\nfor our four numeric variables from our data frame.\\n\\nSo I'll go ahead and show you how\\nto actually use this boxplot to detect outliers.\\nSo you see here, we have a boxplot\\nand do you see those points that lay beyond the whiskers?\\nNow, those are our potential outliers.\\nWhat I did was I took a quick note of\\nwhere those outliers were found.\\nThat's the Sepal width column\\nand it's the values that are greater than four\\nor less than 2.5, approximately.\\n\\nLet's look a little closer at these values.\\nI'm going to use filtering in comparison operators\\nto isolate these values from the rest of the data frame.\\nSo let's go back over to our coding demonstration\\nand let's isolate Sepal Width.\\nSo let's get back over into our coding demonstration\\nand then we'll isolate Sepal Width.\\nTo do that, we'll just say \\\"Sepal_width = \\\"\\nand we'll set it equal to our X variable\\nthat we just created.\\n\\nAnd what we want to do is we want\\nto select our Sepal Width variable.\\nSo we're going to say \\\":,1\\\".\\nAnd then in terms of our outliers,\\nlet's create a second variable here called \\\"Iris Outliers\\\".\\nSo this would be \\\"iris_outliers\\\".\\nAnd we're going to say,\\n\\\"iris_outliers = Sepal Width greater than four\\\"\\nSo we'll just set this equal to a tuple,\\nwhere sepal_width is greater than four.\\n\\nAnd what we're actually trying\\nto do here is isolate the records\\nwhere this Sepal width is greater than four,\\nso that we can understand what's really happening\\nwith these data points.\\nNow, let's go ahead and print these out.\\nSo we'll say \\\"df\\\"\\nand then we want to print out the iris outliers.\\n(keys typing)\\nRun this.\\nOkay, it looks like I have a typo.\\nThis should be equals, not minus thing,\\nand I fixed that, rerun it, and great.\\n\\nSo we see that we have three records\\nthat have a Sepal width greater than four,\\nwhich makes sense\\nbecause, as you can see here,\\nthere are actually three small circles, right?\\n1, 2, 3, where they're outside the whiskers\\nthat are located at position of four here\\nand the Sepal Width variable.\\nLet's also isolate this value here that is below 2.05.\\n\\nSo what I'm going to do is I'm actually just going to copy\\nand reuse the code we just created here.\\n(keys typing)\\nOkay. And so all I need to do here is just change this\\nso that it is Sepal width less than 2.05 and then run this.\\nAnd as you can see,\\nwe have this one record here that is lower\\nthan the interquartile range\\nand it corresponds to this point here in the boxplot.\\n\\nLet me try to explain\\nwhat these records are actually telling us.\\nMoving the results over here,\\nI just wanted to point out\\nthat we now have the row index values for each\\nof the records that are coming back in looking suspicious\\nas outliers.\\nNow, I'm going to show you how\\nto do Tukey outlier labeling.\\nLet's go back over to the coding demonstration.\\nSo in this case, what we need to do is we need\\nto get some display settings.\\nSo we're going to say \\\"pd.options.display.float_format\\\"\\nand we're going to set that equal to a string value,\\nwhich contains a dictionary\\nand it's going to include a blank value.\\n\\nSo we'll create a string and then a dictionary.\\nSo \\\":1f\\\"\\nand then we're going to say \\\".format\\\".\\nOkay?\\nSo this is basically just setting up the display settings.\\nAnd let's create a X data frame.\\nWe'll call it \\\"X_df\\\"\\nand we'll set it equal to pd.\\n\\nAnd we're going to call the data frame constructor here\\nand pass in our X variable that we created earlier.\\nAnd then let's just print out a description\\nof these variables\\nthat are in the X data set that we created.\\nSo to do that, we're going to say \\\"print\\\",\\ncall the print function,\\nand then we're going to pass in X_df.\\nAnd off of that, we need to call the describe method.\\n\\nAnd then print this out.\\nNow, we have some descriptive statistics on each\\nof the variables in our data frame.\\nLet me explain to you what these actually mean.\\nLet's see how we can use them to find potential outliers.\\nThe interquartile range is the distance\\nbetween the third quartile and the first quartile.\\n75% is our third quartile, so we'll say\\nthat 3.3 minus 2.8, that's our first quartile.\\nThe difference between them is 0.5.\\nSo we multiply the interquartile range times 1.5\\nand we get a value of 0.75.\\n\\nTo find outliers from our first quartile,\\nwe'll just look at the value from the first quartile,\\nwhich here, it is 2.8,\\nand we will subtract out 0.75.\\nThis gives us a value of 2.05.\\nWe see that our minimum value is even less than that,\\nwhich means that it is suspicious for being an outlier.\\nFinding an outlier from the third quartile\\nuses the same approach.\\nIn this case, you would take the value\\nat the third quartile, which is 3.3,\\nand you'd add 0.75.\\n\\nThat gives us 4.05.\\nSince the max value in the Sepal Width column is greater\\nthan 4.05, we know that the Sepal Width is suspect\\nfor having outliers.\\nThat's it for univariate methods to finding outliers.\\nNext, I'm going to show you some multivariate methods.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4583162\",\"duration\":467,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Multivariate analysis for outliers\",\"fileName\":\"3006708_en_US_05_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"3 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":812,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to conduct multivariate analysis for outliers. This video covers box plots and scatter plot matrices.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15790874,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now let's talk about\\nmultivariate analysis for outliers.\\nUse multivariate methods to find outliers\\nthat only show up within combinations of observations\\nfrom two or more different variables.\\nThere are many different multivariate methods\\nto detect outliers.\\nWe are going to pick up where we left off\\nin the last section with the box plot,\\nand then I'm going to introduce you\\nhow to use scatterplot matrices to find outliers.\\nFor this demonstration,\\nwe're bringing in our standard libraries,\\npandas, matplotlib, and seaborn.\\n\\nSo all of these are already loaded in the notebook,\\nand I've also set the plotting parameters for matplotlib,\\nand I preloaded the iris dataset,\\nwhich we worked with in the previous demonstration.\\nSo all you need to do is run these code blocks.\\nAnd then let's just print out the first five records\\nin this data frame so we can get a look at the data.\\nAnd next, let's generate a box plot.\\nTo do that, we'll use seaborn's box plot function.\\nSo what you need to say is sns.boxplot.\\n\\nAnd for X, that's going to be equal to our species column.\\nSo you'll say X is equal to a string that reads species.\\nFor our Y variable, we're going to set that\\nequal to the sepal length label.\\nSepal length.\\nIn terms of the data we want plotted,\\nthat's going to be our data frame.\\n\\nSo we'll say data is equal to df.\\nAnd then with seaborne it gives us\\na lot of different color options.\\nSo in this case, we want our hue to be set equal to species.\\nAnd we'll set our color palette equal to HLS.\\nSo we'll say palette equal to,\\ncreate a string and write HLS.\\n\\nAnd lastly, we'll just say legend is equal to false.\\nOkay, so checking to see if I made any typos.\\nThe only thing I can see is that\\nthis should be boxplot instead of barplot.\\nSo let me change that.\\nI'm going to run this.\\nAnd great.\\nSo now we have a box plot.\\nAnd there are two things I want to point out\\nabout this box plot here.\\nOne is that we are plotting sepal length against species.\\nSo we're actually plotting two variables in one box plot.\\n\\nWhen we do that, the outlier falls out, as you can see here.\\nIt's passed the whiskers in the virginica species.\\nAnd this would be considered as\\nsuspicious for being an outlier.\\nNow let's look at the scatterplot matrix.\\nIt's really easy to generate\\na scatterplot matrix using seaborn.\\nSo we'll just say sns.pairplot.\\nAnd we're going to plot out our data frame.\\nSo we'll pass in df as the first variable.\\n\\nWe'll set our hue equal to species.\\nAnd again, we'll set our pallet equal to HLS.\\nWe run this.\\nAnd look how beautiful that is.\\nSo now we have a great scatterplot matrix.\\nLet me take you over to the other side\\nto explain what all of this actually means.\\n\\nSo we already know that our sepal width\\nvariable is suspect for outliers.\\nIf you look at each of the scatterplot matrices,\\nthere's an odd red point\\nthat doesn't fit any of the other clusters.\\nAnd so I've added a circle to that\\nand pulled it up from the data table.\\nThat's actually record 41.\\nSo I just jotted that down.\\nAnd I keep that in mind,\\nto investigate whether that's an outlier\\nand whether it needs to be removed.\\nAnd that's it for using\\nmultivariate outlier detection methods.\\n\\nNow let's look at applying Tukey outlier labeling.\\nThis is basically just a manual process for finding outliers\\nif we don't use the box plot.\\nSo in this case, what we need to do\\nis we need to get some display settings.\\nSo we're going to say pd.options.display.float format.\\nAnd we're going to set that equal to\\na string value, which contains a dictionary,\\nand it's going to include a blank value.\\n\\nSo we'll create a string and then a dictionary.\\nSo colon 0.1 F.\\nand then we're going to say dot format.\\nOkay.\\nSo this is basically just setting up the display settings.\\nAnd let's create a X data frame.\\nSo we'll call it X_df,\\nand we'll set it equal to pd.\\n\\nAnd we're going to call the data frame constructor here\\nand pass in our X variable that we created earlier.\\nAnd then let's just print out\\na description of these variables\\nthat are in the X data set that we created.\\nSo to do that we're going to say print,\\ncall the print function, and then we're going to pass in X_df.\\nAnd off of that we need to call the describe method.\\n\\nAnd then print this out.\\nNow we have some descriptive statistics\\non each of the variables in our data frame.\\nLet me explain to you what these actually mean.\\nLet's see how we can use them to find potential outliers.\\nThe interquartile range is the distance between\\nthe third quartile and the first quartile.\\n75% is our third quartile.\\nSo let's say 3.3 here.\\nMinus 2.8.\\n\\nThat's our first quartile.\\nThe difference between them is 0.5.\\nSo we multiply the interquartile range times 1.5,\\nand we get a value of 0.75.\\nTo find outliers from our first quartile,\\nwe would just look at the value from the first quartile,\\nwhich is 2.8, and we would subtract out 0.75,\\nwhich gives a value of 2.05.\\nWe see that our minimum value is even less than that,\\nwhich means that it's suspicious for being an outlier.\\n\\nFinding an outlier from the third quartile\\nuses the same approach.\\nIn this case, you would take\\nthe value from the third quartile, which is 3.3,\\nand you'd add 0.75.\\nThat gives us 4.05.\\nAnd since the max value of the sepal width column\\nis greater than 4.05, we know that the supple width\\nis suspect for having outliers.\\nThat's it for univariate methods to find outliers.\\n\\nAnd next, I'm going to show you\\nmultivariate analysis for outlier detection.\\n\"}],\"name\":\"5. Exploratory Data Analysis\",\"size\":160035610,\"urn\":\"urn:li:learningContentChapter:4579303\"},{\"duration\":1595,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4589020\",\"duration\":648,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Cleaning and treating categorical variables\",\"fileName\":\"3006708_en_US_06_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1236,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about categorical encoding including one-hot, binary, and vectorizations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":23468515,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's take another look\\nat categorical variables and why we might need\\nto treat categorical variables as well\\nas the options we have for treating them.\\nAs you recall, a categorical variable is a type\\nof variable that can take on only a limited\\nor fixed number of possible values.\\nFor example, fruit types is a categorical variable\\nas there are only a limited number of types of fruits.\\nSay for example, apples, oranges, lemons,\\nthere's not an infinite number\\nof fruit types, so it's categorical.\\n\\nIn the field of machine learning,\\nit's common to come across categorical variables\\nwhen addressing data science challenges.\\nTypically, machine learning algorithms are not equipped\\nto directly process categorical data.\\nTherefore, we have to transform this type of data\\ninto numerical formats that are compatible\\nwith machine learning algorithms.\\nThis transformation can be done through various methods,\\nincluding label encoding, one-hot encoding, among others.\\nThe conversion of categorical variables\\ninto numerical forms is known as encoding.\\n\\nIn this coding demonstration,\\nI will demonstrate two common encoding techniques,\\nlabel encoding and one-hot encoding.\\nWe'll be using the scikit-learn library\\nto implement these encodings.\\nDuring the coding demonstration,\\nwe will explore how to transform categorical variables\\ninto formats that are interpretable\\nby machine learning models.\\nLet's get started importing the required libraries\\nfor cleaning and treating categorical variables,\\nI've already imported numpy in Pandas,\\nand now we'll also import the other required libraries.\\n\\nOne thing I wanted to point out here is\\nthat from sklearn, the preprocessing module,\\nwe're importing LabelEncoder and OneHotEncoder.\\nWe will need both of these functions for this demonstration.\\nHere you can see I've already created\\nthe dataset that we will work with.\\nHere are the columns,\\nand as you can see in the gender column,\\nit has some missing values.\\nSo let's convert this dataset\\nto a data frame and then print it out\\nto get started working with it.\\n\\nWe'll call the data frame df\\nand we'll say df is equal to data frame constructor,\\nand then we'll pass in the dataset,\\nwhich is called data, and then print this out.\\nThere are different ways to handle missing data\\nand categorical variables.\\nIf there are just a few missing values,\\nthen we can drop the rows that contain the missing values.\\nOr if there are a lot of missing values in a column,\\nthen we can drop that column altogether.\\nWe can also replace the missing values\\nwith the most frequent value of that column or row.\\n\\nOf course, let's start by adopting the most logical way\\nof handling missing categorical data points.\\nIf you think about it here with this dataset,\\nyou cannot fill the missing values\\nin the gender column with the most frequent values\\nbecause there's a chance of assigning\\nthe wrong gender to a person.\\nSo in this case, we'll have to just drop the gender column.\\nTo do that, we'll say df is equal\\nto df.drop, call the drop method,\\nand then here we'll pass the column name gender\\nthat we want to drop as the first perimeter.\\n\\nAnd in the second perimeter,\\nwe will pass Axis equal to 1,\\nwhich refers to the columns of the data frame.\\nAnd we'll print this out, and as you can see,\\nthe gender column is dropped from the data frame.\\nNext, let's try to represent the information\\nthat's contained within the names field\\nsuch that it's represented by categorical numerical data.\\nThis will require a two-step approach.\\nFirst, we'll use label encoding\\nto create a numerical representation\\nof each value in the names field.\\n\\nAnd then after that, we'll use OneHotEncoder\\nto convert each value of these numerical values\\ninto its own unique categorical column.\\nFor labeling coding, we'll use\\nsklearn's label encoding function\\nto transform the names into categorical numerical values.\\nHow label encoding function works is\\nthat it encodes the target labels,\\nin this case, names, with values between zero\\nand N minus one, where N is the total number\\nof unique values in the variable.\\n\\nTo do this, first, we'll create an object\\nof the label encoder.\\nSo we'll say label_encoder\\nand we'll set this equal to label encoder.\\nThen we'll call the label encoders a fit function\\nand pass column with a categorical data in it.\\nSo we'll say label_encoder.fit\\nand we will pass in df['names']\\nbecause this is the variable\\nthat we want to have transformed.\\n\\nWhat this does is that it's going to create a numerical mapping\\nthat maps the names labels to categorical numerical values.\\nSo I'll run this.\\nNow we will generate the encodings\\nof the categorical variable by calling the transform method\\noff of the label encoding class.\\nSo let's call this label encoded names,\\nlabel_encoded_names\\nand we'll set this equal to our label_encoder\\nand we'll call the transform method off of that,\\nand we'll pass in again, we'll pass in our names column.\\n\\nSo df['names']\\nand print this out.\\nWhat this is going to do is it's\\ngoing to generate their encodings.\\nSo here, you see the output,\\nthe numerical encodings have been\\ngenerated for the names column.\\nNow we need to transform the categorical data\\nusing one-hot encoding.\\nIn one-hot encoding, each categorical value is converted\\ninto a new categorical column\\nand it is assigned a binary value 0 or 1\\nfor whether the data point is true or false for that value.\\n\\nLet's just try it out so you can see how it works.\\nFirst, we'll create the object of class onehot_encoder.\\nSo we'll say OneHotEncoder\\nand we want to pass a parameter\\nthat says sparse_output is equal to false.\\nAnd what we'll do is we'll call this the onehot_encoder\\nand then run this, then we'll just call the fit method\\noff of the class onehot_encoder.\\n\\nSo to do that, we'll say onehot_encoder.fit\\nand we'll pass in our data frame names column.\\nIt looks like I have a typo.\\nI'm missing a set of brackets here,\\nso I'm going to go ahead and add those,\\nand run this again.\\nWhat this has done is that it's performed\\none-hot encoded mapping of the categorical values.\\n\\nNow let's transform the data\\nby calling the transform function\\nand passing the categorical values.\\nWe'll call this onehot_encoded_names.\\nSo onehot_encoded_names\\nand we'll set it equal to our onehot_encoder.\\nWe'll call the transform method\\nand we'll pass in our data frame,\\nnames column, and run this.\\n\\nLastly, let's just save this as a data frame.\\nSo we'll say one-hot encoded data frame,\\nonehot_encoded_df.\\nWe'll call the data frame constructor.\\nFirst, we'll pass the output encodings,\\nso we'll say onehot_encoded_names here.\\n\\nThen we'll pass the mappings\\nof all the columns as column names.\\nSo we'll say columns is equal to\\nonehot_encoder.categories.\\nNext, we'll assign the original column\\nand the data frame as names.\\nSo we'll say onehot_encoded_df\\nand we'll select the names\\nand we'll set this equal to df[['names']] column.\\n\\nAnd we'll print this out\\nby saying onehot_encoded_df,\\nI'm just looking at the syntax real quick\\nbefore running this.\\nAnd okay, so now we have transformed our names\\ninto a set of categorical numerical variables\\nthat are represented in binary format here, as you can see.\\nSo each of the names has been represented\\nas its own categorical variable in the dataset.\\n\\nAnd then for where the value is\\nactually true for that data point,\\nwhich is actually Steve in the original dataset,\\nthat value gets a 1.0.\\nEvery other value in that column gets a 0.0.\\nAnd that's the basics of label encoding\\nand one-hot encoding.\\nNext, let's look at transforming dataset distributions.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4590005\",\"duration\":415,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Transforming data set distributions\",\"fileName\":\"3006708_en_US_06_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":584,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to transform data set distributions. This video covers preparing data for machine learning, normalization, and standardization.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13592207,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The term data transformation\\nrefers to the practice of changing data\\nfrom its original state into a different format.\\nThis often includes turning raw data\\ninto a format that is clean and ready for use.\\nIn this coding demonstration, we're going to explore\\na variety of beneficial data transformations\\nand look into those scenarios in which they're necessary.\\nWe'll focus on two specific data transformation techniques,\\nnormalization and standardization.\\n\\nNormalization, also known as min-max scaling is a method\\nwhere data values are adjusted and scaled\\nto fall within a range of zero to one.\\nThis technique maintains the original distribution of values\\nwithout altering their ranges.\\nOn the other hand, standardization is a technique\\nthat re-scales data so that it has a mean value of zero\\nand a standard deviation of one.\\nThis effectively normalizes the distribution of the data.\\nKeep in mind that in machine learning,\\nnot every data set necessitates normalization.\\n\\nIt's only required when the features\\nwithin the data set have varying ranges.\\nThe decision to use either normalization or standardization\\ndepends on the specific problem at hand\\nand the machine learning algorithm you're using.\\nThere is no strict rule that dictates the use\\nof normalization or standardization.\\nA practical approach is to initially feed\\nthe machine learning model with raw data,\\nas well as both normalized\\nand standardized versions of the data.\\n\\nBy evaluating the performance of the model\\nunder these different conditions,\\none can determine the most suitable type\\nof data transformation for the given scenario.\\nLet's take a look inside of Jupyter.\\nIn this demo, I'm going to show you\\nhow to transform dataset distributions.\\nAs you can see, I've already imported the required libraries\\nthat we're going to be using in this demonstration\\nwhich are numpy, pandas, matplotlib and sklearn.\\nFor pre-processing data,\\nwe're going to need the MinMaxScaler and the scale\\nfrom sklearn's preprocessing module.\\n\\nSo just pointing out that I have set those\\nand imported them here.\\nSo I'm going to run this.\\nAnd we're going to be using the mtcars data set\\nin this lecture.\\nSo I have imported that data set\\nand gotten that ready for us.\\nLet's look at the first five records\\nby calling the head method.\\nSo yeah, we've covered all of these things\\nin previous lectures and it just saves time\\nfor me to include these in the notebooks.\\nSo let's work with the miles per gallon column\\nwhich is represented by mpg.\\n\\nLet's see how we can transform its values\\nusing normalization and standardization.\\nThe first thing I want to do\\nis just plot the values of the mpg columns.\\nSo we'll just call the plot function, plt.plot,\\nand we'll pass in our data set, mpg column here.\\nAnd run this.\\nOkay, and then we have a visualization\\nof the distribution of the data points in the mpg variable.\\n\\nThat's what I wanted to create as a baseline\\nto compare this to as we work to transform\\nand normalize the data.\\nSo let's start first with normalization.\\nWe're going to normalize the values of the mpg column\\nusing sklearn's MinMaxScaler function.\\nSo we'll first create the object MinMaxScaler class.\\nWe'll say minmax_scalar,\\nand we'll set it equal to MinMaxScaler.\\n\\nAnd then what we want to do\\nis call the MinMaxScaler fit function.\\nSo we'll say minmax_scalar.fit,\\nand then we'll pass in the column, mpg.\\nAnd run this.\\nNow, if we call the MinMaxScaler's transform function\\nand then pass in the mpg variable,\\nthis will transform the values of the mpg column\\nso that they are distributed\\nas a series of numbers between zero and one.\\n\\nWe'll call it scaled_data, scaled_data.\\nWe'll set it equal to minmax_scalar.\\nWe'll call the transform method,\\nand we'll pass in our mpg column here.\\n(keyboard taps)\\nAnd then we'll just go ahead and plot this out\\nso you can see how the data has changed.\\nSo to plot it out, we'll use the plot function plt.plot,\\nand we'll pass in our scaled_data.\\n\\n(keyboard taps)\\nAnd run this.\\nNow, on its face, it looks just like the original plot,\\nbut if you look at the y-axis, the values have been rescaled\\nsuch that they fall between the values of zero and one.\\nWhereas in the original plot,\\nthe data points fell between 10 and 35.\\nBut there wasn't any distortion\\nin the range of the data points.\\nIt looks like the same distribution,\\nbut the values of the y-axis have just been scaled.\\n\\nNow, let's standardize the values of the mpg column\\nusing scikit-learn's scale function.\\nTo do that, we'll say standard_scalar is equal to scale,\\nstandard_scalar equal to scale.\\nAnd then let's pass in our mpg column.\\nSo we'll say, dataset mpg.\\nLet's just go ahead and plot this out, plt.plot,\\nand we'll pass in our standard_scalar, and run this.\\n\\nAnd again, the distribution of the data points\\nlooks identical to the previous two charts.\\nBut in this plot, we can see that the data points\\nhave been rescaled such that they have a mean equal to zero\\nand a standard deviation of one.\\nAs with normalization, the data points\\nhave not been skewed or distorted in any way.\\nNow that you know how to transform your data sets,\\nyou should be ready to get started\\nwith basic machine learning algorithms.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4579302\",\"duration\":532,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Applied machine learning: Starter problem\",\"fileName\":\"3006708_en_US_06_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":798,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to use the data in a ML problem by formatting predictors and labels, and then passing them to a simple ML algorithm.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16862354,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's dig into the process\\nof preparing a dataset\\nfor training a machine learning model.\\nOnce we have our dataset ready,\\nwe'll proceed to train\\na fundamental machine learning model\\nusing this data we've prepared.\\nThe preparation of a dataset\\nfor a machine learning models encompasses several steps.\\nMany of which, we've already covered.\\nThese include collecting the data,\\nfiltering out irrelevant features,\\nand managing existing values.\\nBuilding on this foundation, the next crucial step is\\nto format the data in a way that makes it suitable\\nfor input into a machine learning model.\\n\\nThis involves separating the dataset into features,\\nthe inputs and labels, the outputs we want to predict.\\nAdditionally, the dataset needs to be split\\ninto training and validation sets.\\nThis partitioning allows for the training\\nof the machine learning model on one subset of the data\\nwhile the other subset is used for validation.\\nThis is helpful for assessing\\nthe model's performance and effectiveness.\\nIn the coding demonstration I'm about to walk you through,\\nwe'll be utilizing various modules from scikit-learn.\\n\\nInitially we'll apply scikit-learn's\\ntrain test split function to divide our dataset\\ninto a training set and a validation set.\\nFollowing that, we will train a decision tree classifier\\nusing our dataset.\\nLastly, we'll employ scikit-learn's metrics module\\nto assess the performance of our model.\\nLet's get started\\nand see how these steps unfold in practice.\\nI've already imported the required libraries\\nthat we will use in this demo.\\nThose are pandas and sklearn.\\n\\nAlso, please note that\\nI'm importing metrics from sklearn here,\\nas you can see right here,\\nand that I've preloaded the iris dataset for you\\nthat we've been working with earlier demonstrations.\\nIris is a species of flowering plants.\\nIts dataset contains five columns,\\nwhich include petal length, petal width,\\nsepal length, sepal width, and species type.\\nWe'll try to predict the species type\\nusing the other features within the dataset.\\n\\nSo let's just start off here\\nby calling the head method off of this dataset.\\nLet me make sure I run this code block here\\nto import our libraries\\nand then we'll call the head method.\\nGive that a chance to run.\\nOkay, great.\\nSo this is the basic setup of the data\\ninside of the dataset.\\nThese are the first five records.\\nSo let's start off by just taking a look\\nat the unique species of iris flowers\\nthat are represented within this dataset.\\n\\nTo do that, we're going to call the unique method.\\nSo we'll say dataset.Species.unique\\nand run this.\\nAnd what we see here is that there are three types\\nof iris species in this dataset,\\nsetosa, versicolor, and virginica.\\nNext, we'll separate features and labels from the dataset.\\nThe second, third, fourth, and fifth columns\\ncontain features that describe the flower.\\n\\nAnd the sixth column contains the species labels.\\nFirst, let's separate the features\\nand save them as an X variable.\\nSo we'll say X is equal to dataset.iloc,\\nand then we'll tell it to return all of the rows\\nand only the second through fifth columns.\\nSo we'll pass in one through five here and print this out.\\n\\nAnd what we can see is that\\nthe dataset X now contains all\\nof the features from the dataset,\\nbut it no longer contains the label, the species label.\\nNow what we need to do is we need to separate the label\\nand save it as a Y variable.\\nSo we'll say Y is equal to dataset.iloc.\\nAnd in this case we want to select all of the rows,\\nbut only the sixth column.\\n\\nSo we'll pass a five here and then print this out.\\nAnd as you can see now we have all of the labels,\\nall of the species labels,\\nbut then none of the other variables in the dataset.\\nNext, we'll split the features and labels\\ninto training and test sets.\\nFor this, we'll use sklearn's train test split function.\\nAnd the training set will be used\\nto train the machine learning model.\\nThe test set will be used\\nto test the accuracy of our trained model.\\n\\nLet's write the code.\\nWe'll say x_train, x_test,\\ny_train and y_test.\\nThese are our placeholders.\\nAnd then we'll set them equal\\nto the train test split function.\\nSo it's train_test_split.\\nAnd then what we need to do is pass in our variables.\\n\\nSo first we're going to pass in our X dataset.\\nThen we'll pass in our target variable, which is Y.\\nIn the third parameter, we'll pass in the ratio\\nof the test data from the whole dataset.\\nSo here we'll need to say test_size equal to 0.3.\\nAnd what this means is that 30% of the data\\nwill become the test set\\nand 70% of the data will be the training set.\\n\\nAnd then let's set a seed for a random state.\\nWe'll just set random_state equal to zero,\\nand that's so that you get the same results on your screen\\nas we get in the demonstration here, we'll run this.\\nAnd now it's time to train a decision tree classifier\\non the training dataset.\\nFirst we'll create an object\\nof decision tree classifier class.\\n\\nSo to do that, we'll say clf is equal to.\\nAnd then we'll call DecisionTreeClassifier.\\nAnd then second, we're going to call the fit function.\\nSo we'll say clf.fit,\\nand then we'll pass in our x_train and y_train.\\nRun this.\\nOkay, because the Decision tree classifier is now trained,\\nwe can use it to predict labels for our test set.\\n\\nTo do that,\\nwe'll say y_predict,\\nand then we'll set it equal to clf.predict.\\nAnd we will pass in our x_test here\\nand then print this out.\\nAnd what you see as a result is that\\nyou see all of the predicted labels on the test dataset.\\n\\nThe last thing I want to show you how to do is\\nto evaluate our classifiers performance\\nby comparing the predicted labels\\nwith the original labels of the test set.\\nWe'll use the accuracy metric to evaluate the results.\\nAnd the function that we'll use is\\nsklearn's accuracy score.\\nSo we'll say accuracy is equal to metrics.accuracy_score\\nand we'll pass it in our y_test\\nand our y_predict variable.\\n\\nAnd then let's just print this out.\\nAnd when doing so, we'll create a little label here\\ncalled accuracy,\\nand we're just printing out our accuracy.\\nSo just pass that variable in and run.\\nAnd as you can see, the train model\\npredicted the labels of the test set\\nwith more than 90% accuracy.\\n\"}],\"name\":\"6. Getting Started with Machine Learning\",\"size\":53923076,\"urn\":\"urn:li:learningContentChapter:4583165\"},{\"duration\":5391,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4588028\",\"duration\":145,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction of web scraping\",\"fileName\":\"3006708_en_US_07_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":223,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about data scraping from web sources to obtain data from multiple online sources. Learn why web scraping is needed in data science.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3689857,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Web scraping is a cornerstone technique\\nin data science.\\nWeb scraping automates the extraction of data from websites,\\nthus allowing us to gather large amounts\\nof information really quickly.\\nThe tool we use for this purpose is known as a web scraper.\\nToday, we'll dig into how web scrapers transform\\nthe raw data from websites into a format\\nthat's readily available for analysis.\\nYou might wonder why web scraping is so important.\\nThe internet is a treasure trove of data, stock prices,\\nsports statistics, product details, and more.\\n\\nManually collecting this data is a daunting task, though.\\nThat said, it's essential for many business activities\\nincluding market research and analytics.\\nWeb scrapers streamline this process\\nby efficiently gathering data that's crucial\\nfor organizational decision making.\\nWeb scraping offers several advantages.\\nIt enables the automatic extraction of fast data sets\\nwhile also significantly reducing manual efforts\\nand associated costs.\\nThis method is not only time-efficient,\\nbut also cost-effective.\\n\\nDeploying multiple web scrapers simultaneously\\naccelerates data collection while minimizing\\nthe human error, which is common in manual data gathering.\\nSo how does web scraping work?\\nThe process begins with identifying the target website.\\nWeb scrapers send requests to retrieve\\nthe site's HTML content,\\nwhich includes all of its data and code.\\nThe next step involves pinpointing\\nand extracting the necessary data\\nfrom the specific HTML text.\\n\\nThis extracted data is then cleaned, structured,\\nand stored in a database for further use.\\nWeb scraping has a diverse range of applications.\\nIt's a critical part of generative AI, machine learning,\\nand data analytics.\\nIt offers insights for market research\\nand competitive analysis.\\nWeb scraping is also instrumental in website migrations\\nwhere data is transferred from one site to another.\\nAll this said, it's important to recognize legal\\nand ethical boundaries.\\n\\nNot all data is permissible to script,\\nespecially when it involves confidential\\nor personal information.\\nEthical scraping respects privacy\\nand complies with legal standards.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4589021\",\"duration\":577,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Python requests for automating data collection\",\"fileName\":\"3006708_en_US_07_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":834,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the basics of the requests library, how connections are established, response headers, body, content-types, and metadata extraction.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17782415,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now it's time to talk\\nabout the requests library.\\nRequests is a Python library which is used\\nto make all sorts of HTTP requests.\\nIt's a human friendly HTTP library.\\nRequests Library provides a lot of customizable features\\nthat can be used for all sorts of different types of tasks.\\nAnd in this coding demonstration we're going to work on next,\\nI will teach you how to make requests using HTTP methods\\nand how connections are established.\\n\\nWe'll also look at to use requests to get response headers,\\ncontent types, and response content.\\nSo let's get started.\\nLet's start this demonstration\\nby importing the required libraries.\\nSo in this case, it's going to be the requests library.\\nWe'll say import_requests.\\nRun this.\\nAnd one of the most common HTTP methods is get.\\nthe get method gets or retrieves data\\nfrom a specific resource.\\n\\nLet's make a get request using the requests library\\nand check its response.\\nSo we'll say response\\nis equal to requests.get\\nand then we'll pass in the URL\\nhttps://www.python.org\\nto retrieve data from the original Python website's URL.\\n\\nLooks like I missed a Y here, so I'll add that.\\nAnd then print this.\\nThis needs to be a string.\\nSo let me add single quote\\naround each side of the string here.\\nAnd then we run it.\\nAnd when we run this code,\\nwe see that it returned a Response (200).\\nThis 200 response means that the connection\\nwith the website was successful.\\n\\nNow let's dive a little deeper\\ninto the response of that request.\\nWhen we make a request to a server like this,\\nit returns extra information with a response called headers.\\nHeaders contain all of the metadata of the URL\\nfor which we made the request of the server.\\nLet's print the response header.\\nSo we'll say response.headers\\nand run this.\\nAnd what you see is all of the headers information\\nthat is returned in the response.\\n\\nGoes on quite a ways here.\\nYeah, there's quite a bit of information.\\nOne type of information\\nthat's returned in headers is the content type.\\nContent type indicates the media type\\nof the returned content.\\nFor example, if the returned content is a simple HTML page,\\nthen its content type will be text/html.\\nSimilarly, if the returned content is a PDF file,\\nthen its content type will be application/pdf.\\n\\nLet's check the content type of the response object.\\nSo we'll say response.headers\\nand then we want to look here at the content type,\\nContent-Type\\nand run this.\\nAnd now we see that the content type\\nof the response object is text/html\\nwhich indicates that the returned content\\nis a simple HTML page.\\n\\nIn general, we use Python request\\nto fetch content from a server.\\nResponse content is the information\\nabout the server's response that's delivered back to us\\nwhen we send a request.\\nIf we want to return the content of the response in bytes,\\nwe'd say response.content like this.\\nresponse.content\\nAnd what you're seeing here is that we have returned\\nall of the content of the response in bytes.\\n\\nNow let's compare sequences in lines of text.\\nTo do that, we will need to import the diflib module.\\nSo we will say import_diflib\\nand run that.\\nNow let's create two sequences that we can compare.\\nThe first will be called flines\\nand we'll set that equal to a string that reads,\\nHello.\\n\\nHow are you?\\nI am fine.\\nAnd then the second sequence\\nwill be an object called glines,\\nand that will be equal to a string that reads,\\nHow are you, Lillian?\\nI am doing well.\\nOkay, so simple.\\n\\nI'll put a period here at the end\\njust for good grammar, at the end of the word fine.\\nAnd run this.\\nAnd next we need to create a differ object,\\nwhich we can use to compare the sequences of text\\nand find differences between them.\\nWe'll call this differ object d.\\nWe'll set it equal to diflib.Differ class,\\nand to compare the sequences,\\nwe need to call the compare method off of the d object.\\n\\nLet's call this whole thing diff\\nand we'll set diff equal to d.compare.\\nAnd then we want to compare our flines\\nagainst our glines.\\nSo we'll pass those names in.\\nThen we need to iterate over the sequence\\nand print out the differences.\\nOne problem here though,\\nbefore moving forward, I see a typo.\\nI have a stray dot here, so let me get rid of that.\\n\\nAnd then iterating over sequences\\nto print out the difference.\\nTo do that, we're going to use a for loop\\nthat iterates over the generator that's returned by compare.\\nSo we will say for line\\nin diff\\nprint(line)\\nWe'll call the print function\\nand we'll just have it print out the line.\\n\\nI need to add a colon here.\\nAnd then I'll run this.\\nOkay, it looks like I have a typo.\\nAh, okay, so I called it fline.\\nI forgot an S when I was creating these objects.\\nSo I'll just add an S here.\\nSo F line is equal to, \\\"Hello, how are you? I'm fine.\\\"\\nAnd then rerun this code block.\\nAnd there we go. We have our output.\\n\\nAnd let's take a look at what this actually means.\\nFirst off, what you're seeing here\\nis that each iteration has yielded a line\\nthat has been compared and printed to the console.\\nThe lines from the generator can include information\\nsuch as which elements are present in one sequence,\\nbut not the other, which are common to both,\\nand which are present in the second sequence,\\nbut not the first.\\nOn the bottom here you can see a notification\\nthat the output is truncated\\nand we can view it as a scrollable element\\nby clicking on this link here, which I'll do now.\\n\\nAnd then let's examine these results.\\nNow, if you look closely, we can see that the elements\\nthat are present in the first string,\\nbut not in the second string,\\nare marked with a negative sign.\\nSo for example, this word hello.\\nLet's look back.\\nHello is in the first line, but it's not in the second line.\\nHello is in flines, but it's not in glines.\\nAnd that difference is indicated\\nwith a negative symbol here.\\n\\nFor elements that are present in both strings,\\nthey're given no symbol or just a blank space.\\nSo for example, the word how.\\nHow exists in both flines and glines.\\nSo it's assigned the symbol\\nof a blank space in the output.\\nAnd for elements that are in the second string,\\nbut not the first, you could probably guess this,\\nbut those are indicated by a positive sign.\\n\\nSo for example, here the word Lillian has plus signs.\\nAnd as you can see,\\nthe word Lillian is in the glines variable,\\nbut it's not in the flines variable.\\nThat's why it's been assigned a plus sign as an indicator.\\nThat's the basics of what you need\\nto know about Python requests.\\nNow let's look at the beautiful soup object.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4588029\",\"duration\":1144,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"BeautifulSoup object\",\"fileName\":\"3006708_en_US_07_03_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1857,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to work with objects. This video covers the Beautiful Soup library and BeautifulSoup objects.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":42977993,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's look at web scraping with Python.\\nI'm about to show you how to scrape data from the internet.\\nBut before jumping in, let me give you a brief introduction\\nto web scraping by explaining how it's useful.\\nImagine you're a small business owner\\nand you've got a blog.\\nYou decide that you want to create a new resources page,\\nand on that page you want\\nto include every link from your blog.\\nWhat would you do?\\nGo through each page of the blog manually\\nand pull all of the links?\\nThat would take forever.\\n\\nWhat you could do instead is use Python\\nto automatically go through your blog for you\\nand extract every link from every page.\\nThat way you could just copy\\nand paste the links onto your new resource page\\nand it would be a lot more efficient.\\nI've seen environmental engineers\\nwho use Python to scrape web data from weather station pages\\nin order to gather sufficient data for hydrology analysis.\\nI've also seen Amazon vendors\\nwho scrape web data from competing Amazon vendor pages\\nso that they can use that data\\nto populate their product descriptions\\nusing a semi-automated approach.\\n\\nI've seen humanitarian volunteers scrape web data\\nfrom a foreign country's census site\\nso that the data could be used\\nto quickly form a resource allocation plan.\\nAnd last, but of course not least, without web scraping,\\nthere would be no generative AI.\\nScrape content in the form of copy and images\\nis a baseline necessity of training generative AI models.\\nWithout the source data that's scraped from the web\\nThere would be no chatGPT, Midjourney and what have you.\\n\\nIn other words, web scraping is useful\\nfor an almost unlimited number of applications.\\nIn the coding demo that's coming up,\\nI'm going to teach you about objects in Beautiful Soup\\nand how to work with them.\\nLater in the course, I'm going to teach you to work\\nwith parse data, scrape a webpage, and save your results.\\nThere are four main object types in Beautiful Soup.\\nThose are BeautifulSoup object, tag object,\\nNavigableString object and common object.\\n\\nThe BeautifulSoup object is a representation\\nof the document you're scraping as a whole.\\nIt's easily navigable and searchable.\\nTag elements correspond to XML\\nand HTML elements in an original document.\\nYou can navigate the reference data using tag attributes.\\nA NavigableString object is\\nto add a bit of text within tag.\\nBeautiful Soup uses NavigableString class\\nas a container for bits of text.\\n\\nAnd lastly, the comment object.\\nThe comment object is a type of NavigableString object\\nthat you can use for commenting your code.\\nIn the coding demonstration that's coming up,\\nI'm going to teach you about these objects in Beautiful Soup\\nand how to work with them.\\nIn this coding demonstration,\\nI'm going to teach you about objects in Beautiful Soup\\nand how to work with them.\\nLater in the course, I'm going to teach you how to work with\\nparse data, scrape a webpage, and save your results.\\n\\nBefore getting started here, I just want to check\\nto make sure that our version of Python is compatible\\nwith the demonstration we're about to do.\\nSo this notebook was written for Python 3.10\\nand let's just check the version we're running here.\\nSo we'll say import sys\\nand then let's print sys version.\\nSo we'll say sys.version and run this.\\n\\nAnd it looks like we have version 3.10,\\nso we're good to go.\\nNow let's go ahead\\nand import our Beautiful Soup into the Jupyter Notebook.\\nSo we'll say from bs4 import BeautifulSoup\\nand run this.\\nGreat, so now we have Beautiful Soup to work with\\ninside of our Jupyter Notebook.\\n\\nNow what I've done for this demonstration\\nis I'm providing you an HTML document\\nso you don't need to type all of this stuff out of course.\\nAnd so your Jupyter Notebook\\nis coming loaded with this HTML.\\nAnd we're going to use it to begin exploring\\nthe different types of objects within Beautiful Soup.\\nAll you have to do with this is run the block.\\nScroll to the end and then run it.\\n\\nLet's start by looking at the Beautiful Soup constructor.\\nBy default, the constructor will attempt to detect\\nwhat parser type you need based on the document\\nobject you pass.\\nLet's pick a parser for our constructor instead.\\nTo do that, we'll simply call the Beautiful Soup constructor\\nand we're going to pass in our_html_document.\\nThat's what we just created when we ran the block prior.\\n\\nAnd then for the second argument,\\nwe're going to tell Beautiful Soup exactly what type\\nof parser we want it to use to parse our data.\\nSince our data is html,\\nwe'll pass the html parser.\\nSo create a string that reads html.parser.\\nLet's set this whole thing equal to our_soup_object.\\nThis will be our soup object.\\nAnd then let's just print that out.\\n\\nSo we'll say print and then print(our_soup_object).\\nRun this.\\nOkay, so this is the output.\\nIt's all of our HTML.\\nBy default, the Beautiful Soup object\\nis a format of UTF eight,\\nwhich can be sort of difficult to read\\nbecause it doesn't have much formatting.\\nOne great way to make the output easier to read\\nis to prettify the soup object.\\nThe prettify method will turn a Beautiful Soup parse tree\\ninto a nicely formatted unicode string\\nwith each HTML or XML tag on its own line.\\n\\nLet's print out the first 300 characters of our soup object.\\nWe'll call the print function\\nand then we'll pass in our_soup_object\\nand then we'll call the prettify method off of that.\\nAnd we only want the first 300 characters,\\nso we'll just select [0:300]\\nand run this.\\n\\nGreat, so that's actually a lot easier to read\\nthan the output we got earlier.\\nIf you look back up here, it was kind of a big blob\\nand now at least we have some structure.\\nNow let's look at tag objects.\\nFirst, we'll create tag names.\\nSo let's create another Beautiful Soup object\\ncalled soup_object.\\nAnd to generate this object,\\nwe'll call the Beautiful Soup constructor.\\n\\nAnd let's just pass in a tag.\\nSo we'll create a string\\nand we'll say that this is h1 attribute_1\\nequal to heading level one.\\nSo this is actually heading level one tag\\nand we're saying that the attribute_1 = \\\"Heading Level 1\\\".\\n\\nAnd this heading should read\\n>Future Trends for IoT in 2018<.\\nAnd then we will close the </h1> tag.\\nAnd lastly, I want to pass in our html parser.\\nSo we'll say html.parser.\\nAnd let me check this index on this really quickly.\\n\\nShould be, okay.\\nSo now we have an h1 tag\\nand it's got an attribute of heading level one\\nand then it reads Future Trends for IoT in 2018.\\nNow what we need to do is go ahead\\nand create a tag variable.\\nWe'll call it say tag= soup_object.h1.\\nThis essentially tells Beautiful Soup\\nthat the tag's name is h1,\\na reference to the HTML we passed in.\\n\\nSo let's go ahead and print this whole thing out.\\nSo to do that, we'll call the type function\\nand we'll pass in our tag and hit run.\\nAnd so what you can see here is\\nthat our soup_object.h1 is actually a tag.\\nSo we named it tag,\\nbut when we call the type function,\\nit actually prints out here as a tag element.\\nAnd so we do indeed have a tag.\\nNow let's actually print out this tag\\nand see what it looks like.\\n\\nTo do that, we will call the print function\\nand we'll pass in our tag.\\nAnd as you can see, it returns a string that reads h1.\\nAnd that makes sense, right?\\nNow, let's see what happens when we call the tag name.\\nLet's say tag.name, print this out.\\nAnd of course it's also h1.\\nSo the name of our tag is actually h1.\\nAnd if you wanted to replace the tag name h1\\nwith heading one instead, you can do that.\\n\\nYou would just set the tag.name\\nand set that equal to heading one instead.\\nHere I'll show you.\\ntag.name = 'heading 1'.\\nPrint this out\\nand you can see that we've actually changed the tag name.\\nSo instead of it reading h1 as it does up here,\\nit actually reads heading 1.\\n\\nSo that's how you change the name of a tag.\\nLet's just also print this out really quick\\njust for clarity sake.\\nSo we'll say tag.name\\nand we get heading 1 and that's great,\\nwe changed the tag name.\\nNow let's look at tag attributes.\\nA tag can have any variety of attributes.\\nYou can access the tag's attributes\\nby treating the tag like a dictionary.\\nIn our example, the tag's attribute is attribute_1.\\n\\nSo let's just go ahead\\nand create a soup object, soup_object\\nand we'll set it equal to our BeautifulSoup constructor.\\nAnd then let's just take the tag that we created above,\\nI'm going to copy and paste it in.\\nWe need the quotes\\n'cause it should be a string.\\nAnd then we're going to use the same parameter of html.parser.\\n\\nAnd then let's create a tag variable.\\nSo we'll say tag is equal to soup_object.h1\\nand then we'll print this whole thing out.\\nSo we have our tag and it's been printed out here.\\nAnd imagine for example,\\nif you select attribute_1 from the tag object,\\nit returns to string that reads Heading Level 1.\\nThat is directly from the markup we passed into\\nBeautiful Soup constructor.\\nSo let's just try this out.\\n\\nWe'll say tag and then we'll select our attribute_1.\\nLet's see what we get back.\\nOkay, I missed a t here, attribute.\\nOkay, so fix that and then run this.\\nAnd cool, so that's called Heading Level 1.\\nThat's what prints out.\\nTo return a dictionary that contained\\nall of the tag attributes\\nyou'd simply call the attrs method.\\n\\nSo let's try that out here.\\nWe'll say tag.attrs.\\nAs you can see, this tag has only one attribute,\\nso you only get back one key value pair.\\nYou can easily add an attribute to a tag\\nby simply attaching an attribute labeled to a tag object.\\nLet's try that now.\\nSo we'll say tag\\nand then we'll select attribute_2\\nand we'll assign that a value of Heading Level 1,\\nwhich will be a string object, Heading Level 1\\nand I'll put an asterisk here.\\n\\nNow I have a little typo.\\nI got a dot that we don't need, so I'm going to remove that.\\nAnd then let's call the attrs method off of that tag.\\nRun this.\\nAnd then let's just print the tag.\\nSo those attributes both appear as part of the h1 tag.\\nPretty interesting.\\nWe can actually delete an attribute\\nfrom a tag object.\\nYou would just say del tag\\nand then select the attribute you want to delete.\\n\\nSo in this case, we'll delete attribute_2\\nand print this out again.\\nAnd now you can see attribute_2 is missing.\\nVery easy.\\nLet's also go ahead and just delete attribute_1.\\nI'm going to copy this and paste it.\\nAnd then just go ahead and change out the one for two.\\nRun both of these.\\nThen let's call attrs.\\nSo we'll say tag.attrs and run this.\\n\\nAnd now you can see we've deleted all of the attributes\\nand we get back an empty dictionary.\\nNow I want to show you how to navigate a parse tree\\nby using tags.\\nTo navigate a specific portion of the tree,\\nyou'd simply write the name of the tag you're interested in.\\nFirst things first though,\\nwe're going to start by importing our HTML document.\\nAnd this is the same document we used earlier\\nin your Jupyter Notebook.\\nIt's coming preloaded.\\nI'm just going to go back up to the top\\nand I'm going to copy this code block\\nand bring it down so we can reuse it.\\n\\nAnd let's create a soup object.\\nWe'll call it our_soup_object\\nand we'll set that equal to the BeautifulSoup constructor\\nand we'll pass in our_html_document\\nand an argument that reads html.parser.\\nGoing to run this.\\nNow to retrieve certain tags from within the parse tree,\\nall you need to do is write the name of the tag.\\n\\nSo if you want to pull up the title element\\nfrom the HTML document,\\nyou would just say our_soup_object.head,\\nso our_soup_object.head.\\nAnd this returns the head tag\\nthat contains the document title.\\nSo as you can see here, the document title is IoT Articles.\\nYou can also achieve the same outcome\\nby using the title tag.\\nSo let's just check that out.\\n\\nWe'll say our_soup_object.title,\\nrun that and get the same thing,\\nexcept that now we're missing the head tag, right?\\nBut we still get our article title.\\nIf you wanted to pull up this name of the article,\\nwell first let's look and see\\nwhat part of the tree that's actually located in.\\nSo let's go back up to the top here.\\nAnd it's held within the body tag,\\nbut there's a lot of other stuff in the body tag too.\\n\\nSo to narrow and further we can specify\\nthat it's within the body tag of the b tag.\\nSo here's the first b tag.\\nGo back down here and then just try\\nand access the title that way.\\nSo we'll say our_soup_object.body.b\\nand run this.\\nWell as you can see, the title of the article\\nis the same as what's been printed out.\\n\\nSo we were able to access it using tags.\\nJust to show you what we'd get if we were\\nto write in the b element,\\nI'll say our_soup_object.body.\\nAnd then what we're getting here\\nis the entire body of the HTML.\\nIt's a lot of text,\\nand so it's not really that useful\\nfor isolating portions of that text.\\nTo retrieve only the tags that are associated with lists,\\nyou could say our_soup_object.li.\\n\\nAnd then you get only the tags\\nthat are associated with lists.\\nAnd if you wanted to retrieve the first tag\\nthat contains a web link, you could say\\nour_soup_object.a and run this,\\nand then you would get our first link in the article,\\nwhich is a bit.ly link.\\nNow that we finished with tag objects,\\nlet's start looking at the NavigableStream object.\\nWe'll cover that in our next coding demonstration.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2715033\",\"duration\":660,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"NavigableString objects\",\"fileName\":\"3006708_en_US_07_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":928,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to work with objects. This video covers the NavigableString objects.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":26814035,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that we've finished tag objects,\\nlet's start looking at the NavigatableString object.\\nBefore getting started here, I just want to check to make sure\\nthat our version of Python is comparable\\nwith the demonstration we're about to do.\\nSo this notebook was written for Python 3.10.\\nLet's just check the version we are running here.\\nSo we'll say import sys,\\nand then we'll print sys.version.\\n\\nAnd run this.\\nOops.\\nThis needs to be\\na new line here.\\nOkay, run this.\\nOkay, cool.\\nSo we have version 3.10, so we're good to go.\\nNext, let's just go ahead and import BeautifulSoup.\\nSo to do that, we'll say from BS4\\nimport\\nBeautifulSoup.\\n\\nRun this.\\nAnd now we have our BeautifulSoup library\\nimported into our IPython environment.\\nNow in this demonstration I'm going to show you how to work\\nwith NavigatableString objects.\\nNavigatableString objects are used as containers\\nfor chunks of text that are stored inside of tag objects.\\nSo let's just go back to our example from a previous lecture\\nwith our soup object,\\nand we'll create a variable here called soup object.\\nSoup_object.\\n\\nAnd we'll set it equal to\\nBeautifulSoup constructor.\\nAnd then within the constructor, we'll pass in an H1 tag.\\nAnd so to do that we'll create a string\\nand an open tag for an heading one element.\\nSo opening tag H1,\\nattribute_1 is equal to,\\nand then we'll set that equal to heading level one.\\n\\nLet's take that L out here,\\nand then close the string,\\nand then close this.\\nAnd we want this to read future trends for IoT\\nin 2018.\\nAnd then we'll close this H1 tag.\\nAnd then we need to be sure to pass our HTML parser\\njust to tell BeautifulSoup how to interpret\\nthe document we've passed in.\\n\\nIt does look like there's a typo,\\nso I'm missing a single quote here.\\nAnd then that should have cleaned that up.\\nSo, now let's create a tag,\\nand we'll set the tag equal to\\nour soup object\\nH1.\\nAnd then let's use the type function and pass in our tag.\\nAnd this is to verify that we actually indeed\\nhave created a tag object.\\n\\nSo I'll run this.\\nAs you can see,\\nwe have indeed created a tag object here.\\nSo let's verify the name of that tag.\\nIt should be H1,\\nbut let's double check by saying tag.name.\\nAnd yeah, indeed it is H1.\\nWe have an H1 tag,\\nbut if you just wanted to isolate the string object\\nfrom within this tag object,\\nthen what you could do is you could just say tag.string.\\n\\nSo tag.string and run this.\\nUh-oh, typo.\\nTag.string.\\nCool, so here we have it.\\nThat's our string within this tag.\\nIt's right here.\\nIt reads Future Trends for IoT in 2018.\\nI want to show you here how tag string is\\nactually a separate object of its own.\\nSo let's go ahead here and call the type function,\\nand then we'll pass in our tag.string.\\n\\nAnd run this.\\nAnd what you're seeing here is that the tag string\\nis actually a NavigatableString.\\nSo let's play with this a little bit.\\nI'll create a new variable,\\nand let's call that variable Our NavigatableString.\\nOur_Navigateable_String.\\nAnd we'll set it equal to our tag string.\\n\\nAnd then let's print it out.\\nSo basically what this is saying\\nis that our NavigatableString\\nis now this future trends in IoT in 2018.\\nIf you wanted to replace the string object\\nfrom within the NavigatableString,\\nyou can just call the replace\\nwith method off of the NavigatableString\\nand then pass in a replacement string.\\nSo let's try that out.\\nWe'll just replace this future trends for IoT in 2018\\nwith not a number.\\n\\nTo do that,\\nfirst, we'll pull up our NavigatableString object,\\nand then we'll call the replace with method.\\nAnd we'll pass in a string that reads NAN\\nfor not a number.\\nAnd then we'll print this out by saying tag string.\\nAnd as you can see, our string is\\nnow simply just not a number.\\nWe have replaced the future trends in IoT with not a number.\\n\\nOkay, so let's look at\\nhow we can utilize NavigatableStrings.\\nI'm giving you here this HTML document,\\nand we used this in a prior demonstration,\\nbut it's coming preloaded in your notebook,\\nso you don't have to try to type all of this out.\\nBut what we're going to do is we're going to convert this\\ninto a parse tree like we did in the previous section.\\nSo just to start things off, let's just run this cell.\\nWe'll go ahead and create our soup object,\\nand we'll set it equal to our BeautifulSoup constructor.\\n\\nAnd then we'll pass in our HTML document\\nand define our parser as the HTML.parser.\\nAnd then run this whole thing.\\nIf there's one or more string object within a parse tree,\\nyou can easily isolate them.\\nOne way to do this would be calling the\\nstripped string generator to return all of the strings\\nwithin the object where strings consisting entirely\\nof white spaces are ignored,\\nand white space at the beginning\\nand end of the string is removed.\\n\\nSo for this example,\\nfor each string object in the parse tree,\\nthis stripped strings generator passes through,\\nstrips white spaces,\\nand then prints out each string\\nthat contains a printable representation.\\nSo let's try this out here.\\nWe'll say for string\\nin our BeautifulSoup object,\\nour soup object.stripped_strings.\\n\\nWe want to print representation.\\nSo R-E-P-R.\\nAnd then of the string.\\nSo I'll pass in string.\\nThis is going to print a representation of the string,\\nand then we will run this.\\nOkay, so now you can see\\nthat our strings have been pretty much cleaned up.\\nWe just have a list of strings here without the tags\\nor any of the markup within the body\\nof the series of strings.\\n\\nThe last thing I want to show you in this demonstration\\nis how to access parent tag objects within a parse tree.\\nSo let's create a new object called First Link.\\nFirst_Link.\\nAnd then we'll set it equal to the A tag.\\nSo for our soup object,\\nwe'll reference the A tag.\\nOur soup object.A,\\nand then we'll call the print function\\nand pass in our first link.\\n\\nRun this.\\nUh-oh, this was changed to append\\nwith the autopopulater.\\nSo let's just fix that and run it again.\\nOkay, great.\\nSo this is actually the first link in our document\\nand the text that contains it.\\nSo in our document,\\nthis text here would actually be hyperlinked or clickable,\\nand it would redirect to a Bitly link.\\nIf we wanted to access the parent of that first link,\\nwe would just say, first link.parent.\\n\\nParent.\\nAnd we see that now we have the parent tag\\nof this first link.\\nNow, the NavigatableString object\\nof the first link is a string that reads\\nlast month Ericsson Digital invited me.\\nLet me show you.\\nWe'll say first link.string.\\nWe're going to take this string,\\nand we're basically going into this link\\nby pulling only the string from within it, right?\\nSo that is now printed out as the string object.\\n\\nAnd lastly, we can also retrieve the parent\\nof the NavigatableString object.\\nTo do that, we would say first link.parent.\\nSo in this case, the parent of the NavigatableString\\nis the A tag, which is sort of self-evident, right?\\nSo now you know how to work with objects in BeautifulSoup.\\nWe're going to get into using BeautifulSoup\\nfor data parsing.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4584143\",\"duration\":837,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data parsing\",\"fileName\":\"3006708_en_US_07_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded, please mask the bottom deck from 00:00 - 10:13 in pickup\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1527,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to parse data. This video covers parsing data, getting data from a parse tree, and searching and retrieving data from a parse tree.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":34432705,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's look at working with parsed data\\nin beautiful soup.\\nI've broken the demonstration to come into three sections,\\nparsing data, getting data from a parse tree and searching\\nand retrieving data from a parse tree.\\nParsing data is where you'll pass an HTML\\nor XML document to a Beautiful Soup constructor.\\nThe constructor converts the document to Unicode\\nand then parses it with a built-in HTML parser.\\nWell, HTML parser, by default that is.\\n\\nLooking closer at searching and retrieving data.\\nI'm going to show you the find all method.\\nAnd this method searches a tag in its descendants\\nto retrieve tags or strings that match your filters.\\nThere are several methods for searching\\nand filtering a parse tree.\\nThe ones that I'm going to show you now are the name argument,\\nkeyword argument, string argument, lists, Boolean values,\\nstrings, and regular expressions.\\nYou can pass any of these arguments into the find all method\\nto use as filters and return either strings or tags.\\n\\nI'll show you in our demo.\\nData parsing is super simple with Beautiful Soup.\\nYou just pass in an HTML or XML document\\nto the Beautiful Soup constructor.\\nThe constructor converts the document to Unicode\\nand then parses it with a built-in HTML parser.\\nSo the first thing we need to do is just\\nto import our libraries.\\nAnd so your Jupyter notebook is coming preloaded\\nwith Beautiful Soup\\nand the urllib as well\\nas the regular expression library here.\\n\\nSo all you need to do is just run this code block\\nand you have your libraries available.\\nSo let's start by reading in some data.\\nWe're going to use the URL Lib library to do that.\\nSo we're going to say with\\nURL lib.request.URLopen,\\nand within this function, we're going to pass in a string\\nof a web address that has our HTML.\\n\\nSo that's https://Raw GitHub user content.com/bigdatagal/\\ndata-mania-demos/master/IOT2018.html.\\n\\nAnd then close the string.\\nSo again, that's https://raw GitHubusercontent.com/\\nbigdatagal/data-mania-demos/master/IOT- 2018html.\\nAnd we want this to be read as response\\nand we want HTML to be set equal\\nto the response read function.\\n\\nSo we'll say HTML equals response.read.\\nI need to HTML,\\ncheck the syntax really quick.\\nSo we need to have a colon here\\nand then I'm just going to bring this back up\\nand press enter so it's structured properly.\\nAnd then, okay, run this.\\n\\nSo now what we've done is we've loaded data into our\\nJupyter Notebook.\\nSo let's create a soup object, we'll call it soup,\\nand we'll say soup is equal to beautiful soup constructor.\\nAnd we'll pass in our HTML\\nand then we'll pass a perimeter that says it should be read\\nwith the HTML parser.\\nSo create a string that reads HTML.parser.\\n\\nThen let's just print the type out here.\\nSo we'll say type\\nand pass in our soup object and run this.\\nOkay, looks like I have a stray bracket here.\\nSo clean that up, run it again. And here we go.\\nAs you can see, we have created a Beautiful Soup object.\\nNow let's move into data parsing.\\nFirst thing's first, let's go ahead\\nand just prettify it so we can kind of get an idea\\nof what's in there and not have a bunch\\nof HTML that's totally unformatted.\\n\\nSo let's call the print function.\\nAnd we'll pass in soup.prettify and then let's just go\\nahead and print out only the first 100 characters.\\nSo we'll say zero:100\\nand run this.\\nAnd this is just a little preview of what we've got inside\\nof our soup object.\\nLet's practice getting data from a parse tree.\\nLet's start by getting the text out of our HTML\\nand isolating the text from within that HTML code.\\n\\nTo do that, let's just create an object called text_only\\nand we'll call the get text method off of our soup object.\\nSo we'll say soup.get_text.\\nAnd then print it out.\\nPrint our text only.\\nOkay, and then let me just check the syntax really quick.\\nWe've got a stray dot there,\\nso let's just get rid of that and then run this.\\n\\nAnd here's our output. That's nice and pretty easy to read.\\nNow I'm going to show you how to use this data\\nand basically search and retrieve data from a parse tree.\\nI've left some basic information\\nfor you within the notebook about searching\\nand retrieving data from a parse tree.\\nAnd we're going to be using the find all method.\\nThe find all method searches a tag in its descendants\\nto retrieve tags or strings that match your filters.\\nBasically, you can just pass any\\nof these arguments into the find all method to use\\nas filters and then return either strings or tags.\\n\\nSo the first thing I want to show you is how\\nto retrieve tags by filtering the name arguments\\nand name argument search for tags\\nby filtering based on the name tag.\\nSo let's practice really quickly.\\nWe'll just say soup\\nand we'll call the find all method off of that.\\nAnd then we want to pass in the tech LI.\\n\\nAnd run this.\\nHere's our output.\\nWhat this has done is it's gone through our parse tree\\nand it's basically filtered out all of the tags\\nthat have the name LI and printed only those.\\nSo you see we have a bunch of LI tags here.\\nYou can also retrieve tags by filtering\\nwith the keyword argument.\\nSo let's try that out real quick.\\nThis basically is going to search the parse tree for tags\\nby filtering based on tag attribute.\\n\\nSo what we need to do is call the find all method off\\nof our soup object,\\nand we'll go ahead and create a keyword filter.\\nWe'll call that ID equal to\\nlink seven.\\nSo we're basically saying we want all of the tags\\nthat have the identification link seven as a keyword.\\nSo when we run this,\\nwhat we're actually getting is only one tag, which happens\\nto be the one associated with the ID link seven.\\n\\nIt's a link to online courses for IOT.\\nYou can also retrieve tags by filtering\\nwith string arguments.\\nSo this is where you search for tags\\nby filtering based on exact stream.\\nSo again, let's call the find all method off\\nof our soup object.\\nAnd then this time, let's pass a string\\nthat reads OL and run this.\\nAnd so here what it's done is it's passed through our\\nparse tree and it's basically filtered out all of the code\\nthat fell within the OL tags.\\n\\nSo you can see here there's an opening tag here\\nand a closing tag here.\\nSo it's just passed through the entire document\\nand retrieved the tags within the OL tags.\\nNow let's retrieve tags by filtering with lists objects.\\nAgain, we will call the find all method\\noff of our soup object.\\n\\nAnd this time we'll pass in a list of tags.\\nThe first one will be the OL tag.\\nAnd then let's make the next one a B tag\\nand run this.\\nOkay.\\nYou can see now that we have filtered out from our\\nparse tree only the text from within the tags that fall\\nwithin the B tag and the OL tag.\\nYou can retrieve tags by filtering with regular expressions,\\nand what this function does is it searches for tags\\nand strings simply by filtering based on regular expression.\\n\\nSo let's go ahead and let's create a variable called T\\nand we'll say T is equal to\\nre, regular expression.compile,\\nAnd we'll pass in a string called T.\\nAnd then let's create a for loop.\\nWe'll say for tag in,\\nand then we'll call the find all method off\\nof the soup object.\\nSo we'll say soup.find all,\\nand we'll pass in again T.\\n\\nFor each of these tags, we want to print the tag name.\\nSo we'll call the print function\\nand pass in tag name.\\nTag.name,\\nrun this.\\nAnd so now we have retrieve tags by filtering\\nwith regular expression here, which was T.\\nYou can also retrieve tags by filtering\\nwith a Boolean value.\\nSo let's just practice that real quick.\\n\\nI'm going to copy this code that we just wrote here\\nso we can reuse it.\\nAnd then in this case, what we want to do is we want\\nto filter based on the Boolean value true here.\\nSo we're going to change that T to true.\\nAnd then just run this again.\\nAnd here's our output.\\nThat's basically how you filter with Boolean values.\\nYou can retrieve web links by filtering with string objects.\\n\\nLet me show you how to do that real quick.\\nSo for this, let's just create a new for loop\\nand we'll say for link in\\nand all the find all method, off of the soup object.\\nAnd then we'll pass in an A, a string that reads A.\\nBasically what we're doing here is we are retrieving all\\nof the links that are associated with an A tag.\\nFor each of those, we want to print them out.\\n\\nSo we'll call the print function\\nand then we'll say link.get,\\nand then let's just ask it to get the Href,\\nand run this.\\nOkay, here we have a list of links.\\nSo basically what this has done is it's gone\\nthrough our entire soup object and isolated only the links\\nand printed them out in a list.\\nYou can also retrieve strings by filtering\\nwith regular expression.\\n\\nSo let me show you how to do that really quick.\\nAgain, we're going to use the find all methods.\\nSo we'll say soup.find all,\\nand this time we wanted to find where string is equal to.\\nAnd then we want to call the compile function\\nfrom regular expression.\\nSo that's re.compile,\\nand then we'll pass in a string\\nthat reads data and run this.\\n\\nOkay, so that's a bit long as you can see here,\\nbut what it's actually done is it's filtered our parse three\\nby the regular expression data.\\nNow that you've made it this far,\\nyou've basically covered all of the mechanics\\nof scraping web data with Beautiful Soup.\\nSo next I'm going to show you how\\nto use this stuff in action.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4590006\",\"duration\":792,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Web scraping in practice\",\"fileName\":\"3006708_en_US_07_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1615,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to scrape web data. This video covers reading and writing data from the internet.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":32738581,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let me show you web scraping and action.\\nIn the following demonstration,\\nI'm going to show you how to scrape a webpage\\nand then save your results to an external file.\\nLet's get started.\\nSo your Jupyter Notebook is coming\\nwith the standard libraries for beautiful soup\\nalready loaded in it, and you've also got URL Lib,\\nand regular expression library preloaded.\\nSo all you need to do is run this\\nand what we're going to do is we're going to scrape\\na page from analytics.usa.gov.\\n\\nSo to start, we're going to create an R variable,\\nand we're going to say R is equal to urllib.request.urlopen,\\nand then we're going to pass in a string\\nthat is the URL we want to be having data read from.\\nSo that's going to be https://analytics.usa.gov.\\n\\nAnd then we're going to call the read method off\\nof this whole thing.\\nAnd next, we're going to create a soup variable,\\nand we're going to set it equal\\nto the beautiful soup constructor.\\nAnd then we want to pass,\\nour first perimeter is going to be our R variable.\\nAnd then our second parameter will be the HTML parser\\nto tell Beautiful Soup to use the HTML parser\\nto read the data.\\n\\nAnd we'll call the type function\\nand pass in our soup object just to double check\\nthe type of our soup object,\\nwhich of course, should be a beautiful soup.\\nSo I'll run this and yeah, there we have it.\\nIt's a beautiful soup object.\\nSo just remember here that you can use any web link you want\\nto basically scrap data from any webpage on the internet.\\nNow, I'm going to show you how to script a webpage\\nand save your results.\\n\\nFirst, let's start by printing out our soup object.\\nSo we're going to call the print function,\\npass in soup.prettify.\\nAnd then let's just take a look\\nat the first 100 characters.\\nI'll run this and okay,\\nso we're seeing the very tip top of the webpage\\nthat's located at the analytics.us.gov url.\\nNext, what we should do is just use the find\\nall function to find all of the A tags\\nand then retrieve from within them\\nthe A tag values from within those.\\n\\nTo do that, we're going to create a loop.\\nSo it'll be a for loop\\nand for each link in soup.find all,\\nwe're looking for the A tags.\\nAnd then we want to print, oh,\\nlooks like I forgot a colon here at the end of this.\\nOkay, so then for each of those,\\nwe want to print the link and then we need to get the A.\\n\\nSo that's going to be link.get.\\nAnd within this get method, we're going to pass a perimeter\\nthat reads href and run this.\\nOkay, cool. So now we have a list of links.\\nAnd what this has done is it's actually gone\\nthrough the analytics.usa.gov webpage,\\nand it's looped through all of the text on that page\\nand basically printed out only the web links.\\n\\nBut if you wanted to see what that entire body\\nof text actually looks like,\\nthen you can use the get text method.\\nSo let's just try that out really quickly.\\nWe're going to say print, and we'll pass in our soup object.\\nAnd off of that, we will call the get Text method\\nand run this.\\nOkay, so there is a lot of content.\\nSo what I'm going to do is just click this option\\nas view it as a scrollable element.\\n\\nAnd here is the body of text that's sitting on that webpage.\\nWe have scraped it directly from the webpage in real-time.\\nThe benefit of using this for loop\\nthat we created here is it basically went through all\\nof the text and got us exactly what we needed,\\nwhich was the links instead of us having to kind of pick\\nand choose through the body of text.\\nAnd in this case, I'm not even seeing the hyperlinks.\\n\\nSo if you just want to scrape links from a webpage,\\nthen you might as well use this method up here.\\nLet's go ahead and just prettify this,\\nso we can kind of take a look at it more easily.\\nTo do that, we'll call the print function,\\nwe'll pass in our soup object, soup.prettify\\nand then let's just look at the first 1,000 characters.\\n\\nSo we'll, 0:1000 and run this.\\nOkay, so here, we're seeing the links printed out,\\nand it's a lot more manageable to read,\\nand it's a lot easier to read than the output from above.\\nNow, what I want to do is create a for loop\\nto pass through our soup object and find all of the A tags\\nthat have an attribute of href.\\nSo we'll say for link in soup.find all.\\n\\nAnd the first parameter will be a string\\nthat reads A with attribute equal to href.\\nSo we're going to say attris equal to,\\nand then we're going to create a dictionary,\\nand we're going to pass a string that reads href.\\nAnd then for all of these tags that are returned,\\nwe want the loop to match against them irregular expression\\nthat reads HTTPS and print out only those.\\n\\nSo to make that happen,\\nwe're going to call the compile function\\nfrom the regular expression library, so that's re.compile.\\nAnd within that,\\nwe're going to pass a string that reads HTTP.\\nAnd then for any results that match this expression,\\nwe just print them out.\\nSo we're going to call the print function and pass in our link.\\nAll right, and I'm going to check the syntax here\\n'cause it looks like something is off.\\n\\nYeah, I am missing the closing parentheses here\\nto close out the tuple.\\nOkay, so I'm going to run this,\\nand that's a long list of tags\\nwith links within them.\\nLet's just take a look at what the data type is here.\\nSo to do that, we'll say,\\ntype and pass in our link object.\\nSo as you can see, this is actually a tag object,\\nand what we basically have is we have all of our A tags\\nthat have an attribute of href\\nand also have an HTTP batch within them.\\n\\nIt isn't useful for you\\nto have this result stuck within a Jupyter Notebook though,\\nso you'll want to know how to actually save this\\nas an external file.\\nTo do that, we're going to create a new text file\\ncalled parsed data.\\nAnd so, we're going to create a file variable,\\nand say, file is equal to,\\nand then we'll call the open function.\\nWe'll pass in a string\\nthat reads parsed_data.text,\\nand the second parameter\\nwill be a string that reads W.\\n\\nWhat that's doing is it's telling Python\\nthat we want to write into that text file,\\nso W stands for write.\\nAnd then what we want to do is for each of the links\\nthat was just printed out,\\nwe want to print those now into the parsed data text file.\\nSo now, what we actually need to do is\\nthat we can just go ahead\\nand copy this code from above of our for loop.\\nWe'll copy this and then we'll just reuse it\\nfor efficiency's sake.\\n\\nI'm going to paste it here,\\nand it's just performing\\nthe same operation now as it did before.\\nBut instead of printing out here into Jupyter,\\nit's going to go ahead,\\nand it's going to generate a soup link,\\nand that's going to be a string.\\nSo instead of printing the link,\\nit's going to create a new variable called soup link.\\nAnd that link is going to be equal to a string.\\nAnd this string is going to be derived\\nfrom each link within the soup object.\\nSo what I'm going to do here is\\nthat I'm going to move this down,\\nand I'm going to say soup_link is equal to str\\nand then I'm going to pass in a link there.\\n\\nAnd then for each soup link, we want to print that out.\\nSo we need to update this print function,\\nso that it's printing soup_link.\\nAnd then we're going to write into the file.\\nSo we're going to say file.write,\\nand we're going to pass in our soup link, soup_link.\\nAnd what this loop is going to do is that it's going\\nto pass it the entire beautiful soup object,\\nand it's going to find all of the links\\nand print them out until it finds no more links,\\nand then it's going to flush the file and close the file.\\n\\nSo to make that happen, we say file.flush,\\nfile.flush,\\nand file.close.\\nAnd let me just check really quick\\nfor any issues with the syntax.\\nOkay, we'll run this and great.\\nOkay, so we have a list of tags\\nwith the links inside of them.\\n\\nSo essentially, this is\\nwhat should be written into our text file, correct?\\nSo let's check that.\\nI could show you the shortcut of where\\nto find the file, but I also want to show you how\\nto use the present working directory command\\nto show you where to go to retrieve that text file.\\nSo you need to pull up your present working directory,\\nand if you just call the command, %pwd and run this,\\nit's going to tell you exactly where you can go\\nto find the text file that was just printed.\\n\\nSo it's in our folder\\nthat we're actually working within in code spaces.\\nThe extension is printed out here for us.\\nAnd then of course, the shortcut is you\\ncould just go up to this explorer,\\nand since we're working in the notebooks folder,\\nyou can actually just find the file written here.\\nSo let's look at that and then you can see,\\nokay, here are our links.\\nIt looks like they're not quite as nicely formatted,\\nbut it's the same information\\nthat has been printed out in the Jupyter Notebook,\\nexcept for now it's a text file,\\nwhich can, in times, be more convenient.\\n\\nThe only thing I would mention here is\\nthat you can still see a bunch of stray tags.\\nAnd a lot of times when you're doing web scraping,\\nno matter how much data formatting you do,\\nthere's always these stray characters.\\nBasically, a lot of times,\\nthere are data processing requirements\\nafter you scrape the data.\\nSo expect to spend some time data munging\\nafter you do web scraping.\\nBut if you ever find yourself again in a position\\nwhere you can't get data from a website\\nbecause it's been placed on different pages\\nor in weird formatting, remember how to use beautiful soup\\nto scrape the data for you.\\n\\nAnd in the next lecture,\\nyou're actually going to learn how\\nto build a whole web scraping application\\nthat will print out links for you in such a way\\nthat they're perfectly formatted.\\nAnd so, stay tuned because that's next.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4586145\",\"duration\":1236,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Asynchronous scraping\",\"fileName\":\"3006708_en_US_07_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1985,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about writing parallelized scrapers with asyncio.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":43183549,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's look at asynchronous web scraping\\nand how we can use this method to\\nspeed up the process of extracting data.\\nA normal web scraper can take a lot of time\\nto extract large amounts of data,\\nor in some cases, it might just fail altogether.\\nThis usually happens when a web scraper makes a request\\nand then waits for the response\\nbefore jumping to the next request.\\nDuring this time,\\nthe CPU remains idle,\\nwhich wastes a lot of CPU time.\\nCircumventing this process,\\nasynchronous web scraping allows us\\nto make multiple requests simultaneously\\nwithout waiting for the response of each request.\\n\\nIt also reduces CPU idle time.\\nIn the demonstration of asynchronous scraping\\nthat's coming up,\\nwe will use the aiohttp and the asyncio libraries.\\nAiohttp is a client and server side library\\nthat allows us to make asynchronous requests.\\nOn the other hand,\\nasyncio is a Python library,\\nwhich is used to write concurrent code.\\nLet's take a look at it inside of Python.\\n\\nFor this demonstration,\\nwe're going to be working with new libraries.\\nOne is aiohttp, and the other is asyncio.\\nSo just to be careful,\\nlet's go ahead and do a pip install of each of those\\nto make sure you've got them installed in your environment.\\nSo we'll start with pip install aiohttp,\\nand run this.\\n\\nAnd then let's do pip,\\npip install asyncio,\\nand run this.\\nOkay, great.\\nNow, your notebook is already coming preloaded\\nwith the libraries that you'll need.\\nAnd we'll mainly be working with aiohttp and asyncio,\\nbut also with BeautifulSoup, CSV,\\nand the regular expression library.\\n\\nAll you need to do to import those\\nis just to run that code block there.\\nAnd since we're working within a Jupyter Notebook,\\nthere's already an event loop that's running on the backend.\\nSo we cannot start a new event loop.\\nFor this reason,\\nwe need to run nest asyncio.\\nWhat this module does is that it patches asyncio\\nto allow a nested use of asyncio.\\nSo first we'll do a pip install.\\n\\nAnd we want to do a pip install of nest-asyncio.\\nand then let's import nest_asyncio,\\nand we'll call it Supply Function.\\nSo to do that,\\nwe'll say nest_asyncio.apply,\\nand we'll run this.\\n\\nNext, let's write an asynchronous python function\\nthat scrapes all of the links\\nfrom a given webpage's HTML content\\nand saves them into a CSV file.\\nThis can be especially useful\\nfor gathering data for web analysis, SEO monitoring,\\nor even just cataloging content.\\nSo we'll begin by defining our asynchronous function,\\nwhich we will call scrape and save links.\\nWe'll use the async keyword\\nto define an asynchronous function.\\n\\nSo we'll say async def scrape_and_save_links(text):.\\nAsynchronous functions are part\\nof asynchronous programming\\nand they allow us to handle long waiting operations\\nlike network requests more efficiently.\\nThey let other parts of your program run\\nwhile waiting for these operations to complete.\\n\\nInside the function,\\nthe first thing we'll do is parse the HTML content\\nwith BeautifulSoup.\\nSo we'll say soup equal to,\\ncall the BeautifulSoup constructor.\\nWe'll pass in our text object\\nand we'll define the parser as HTML.parser.\\nNow, we want to save these links to a file.\\nTo do this, we'll open a file in append mode.\\nIf the file doesn't exist, it will be created.\\n\\nIf it does exist, we'll add to the end of it.\\nWe don't want to add any unintended new lines.\\nSo we'll set new line equal to a blank string,\\nfile equal to open,\\nand then we will pass a string that reads CSV_file.\\nThe next parameter will be a string that reads A.\\nAnd then lastly,\\nour new line needs to be equal to an empty string.\\n\\nWith the file opened,\\nwe create a CSV writer object.\\nThis object is responsible for converting our links\\ninto a format that's suitable for a CSV file,\\nwhich by convention uses commas to separate items.\\nSo here we'll say, let me move this up a little.\\nOkay, writer equal to CSV.writer.\\n\\nAnd we pass in our file\\nand we just set our delimiter equal to a string\\nthat's got a comma in it.\\nNext we'll loop through all the A tags in our Soup object.\\nThe final method looks for these tags\\nand with the attrs parameter,\\nwe specify a regular expression to match the href attributes\\nthat start with HTTP.\\nThis way, we're only getting actual web links.\\nSo here we'll say for link in Soup.findall\\nwe'll pass in our A tag\\nand then we'll say attrs equal to dictionary\\nthat contains href.\\n\\nAnd then we'll pass our\\nregular expressions compile function.\\nSo re.compile,\\nand we'll ask it to look for HTTP.\\nThe colon at the end of this.\\nEach link we find is extracted using link.get('href').\\nEach link we find is extracted using link.get('href').\\nAnd we use our CSV writer to write this link into our file.\\n\\nSo we'll say link is equal to link.get('href')\\nand then writer.writerow\\nand we'll pass in our link.\\nAnd lastly, we need to close the file.\\nFailing to close the file could lead\\nto data not being written correctly\\nor the file being left open unnecessarily,\\nwhich is a resource link.\\n\\nSo to close the file, we will write file.close,\\nand then let's just run this whole thing.\\nOkay, so here we have our function.\\nNext we're going to define another piece\\nof our web scraping toolkit,\\nwhich will be the fetch function.\\nThis asynchronous functions responsible\\nfor making web request\\nand then passing the content it retrieves\\nto our scrape and save links function.\\nLet's start by defining our function with the async keyword.\\n\\nThis indicates that it's an asynchronous function\\nand it allows us to use the await inside the function,\\nwhich is essential for performing\\nasynchronous IO operations.\\nSo we'll call the fetch function\\nand we want to fetch our session\\nand the URL.\\nNow, we'll enter a try block.\\nThis is where we'll perform our web request.\\nThe reason we use a try block is\\nbecause network operations are unpredictable.\\n\\nThere might be connectivity issues,\\nthe server might not respond,\\nor there could be a myriad of other issues that could arise.\\nSo we'll say try...\\nInside the try block,\\nto send a get request to the URL\\nwe passed into the function.\\nThe session.get method is an asynchronous method,\\nso we'll use the async with statement.\\nWe'll say async with session.get(url) as response.\\n\\nOne thing I want to point out here is\\nthat the async with statement ensures\\nthat the session is properly closed\\nafter we're done with it even if an error does occur.\\nOnce we have the response from our get request,\\nwe want to retrieve the text of that page.\\nWe do this with await response.text.\\nThe await keyword is used to wait for the operation\\nto complete without blocking the entire program.\\nSo we will say text is equal to await response.text.\\n\\nWith the text of the response in hand,\\nwe now want to scrape the links from it.\\nHere we use the asyncio create task\\nto kick up our scrape and save links function.\\nThis function call creates a new task that runs concurrently\\nwith other tasks including the main program.\\nSo we'll say task is equal to asyncio.create_task,\\nand then scrape and save links(text).\\n\\nWe don't want to move on until\\nwe've actually scraped and saved the links,\\nso we await the task to ensure\\nit completes before proceeding.\\nThis is a key point,\\neven though we're doing things concurrently,\\nsometimes we need to wait for one task to finish\\nbefore starting another.\\nSo we'll say await task here.\\nAnd lastly, we have an except block.\\nThis is our safety net.\\n\\nIf anything goes wrong with a network request\\nor the scraping, instead of crashing our program,\\nwe catch the exception and print out the error message.\\nThis is just a general best practice for debugging\\nand ensuring the robustness of your program.\\nSo we'll say except exception as e:\\nprint(str) passing e,\\nand that's our fetch function.\\n\\nOh!\\nOkay. Looks like there's a syntax error.\\nSo let me take this async in and move it over.\\nMove this line over this line.\\nTab it over, tab await over.\\nOkay.\\nJust clean up the indentations a bit.\\n\\nOkay. Yeah.\\nSo it was just a matter of cleaning up the indentations\\nand that's our fetch function.\\nIt's designed to handle web request asynchronously,\\nscrape the content for links\\nand manage errors gracefully.\\nWith asyncio, this function will work efficiently\\nas part of an asynchronous python application,\\nfetching data and processing it\\nwithout blocking other operations.\\nNow we're ready to write a function\\nthat orchestrates the whole web scraping operation.\\n\\nWe'll name this function scrape,\\nand its job will be to manage multiple URLs\\nand ensure that we fetch and process them concurrently.\\nThis is where the true power\\nof asynchronous programming lies\\nhandling multiple IO bound tasks at once\\nwithout waiting unnecessarily for each one to complete\\nbefore starting the next.\\nHere's how we do it.\\nSo we say async def scrape,\\nwhich will accept a list of URLs to process.\\n\\nWithin this function,\\nwe'll initiate an empty list named tasks.\\nThis list will store the future tasks that we will create\\nand then execute concurrently.\\nEach task will be a web scraping operation for a single URL.\\nSo here we'll say tasks equal to,\\nand we'll just put an empty list.\\nNext we set up an asynchronous context manager using\\naiohttp client session.\\n\\nAiohttp is an asynchronous HTTP client for Python,\\nwhich allows us to make multiple HTTP requests concurrently\\nby using async with, we ensure that the session is closed\\nautomatically once all operations\\nwithin the block are completed.\\nSo we'll say async with aiohttp.ClientSession\\nas session\\nand colon.\\n\\nNow, we loop over each URL in the URL's list.\\nFor each URL we call the previously defined fetch function,\\nwhich fetches the URL's content and processes it.\\nWe append the resulting task, a coroutine object\\nto our tasks list.\\nSo we'll say for URL in URLs,\\ntasks.append(fetch(session,url)).\\n\\nAfter we have iterated through all the URLs\\nand created a task for each, we use asyncio.gather\\nto run all of these tasks concurrently.\\nAsyncio.gather takes a list of coroutines\\nand schedules them to run concurrently\\nby prefixing tasks with an asterisk.\\nWe're unpacking the list so\\nthat gather receives individual tasks as arguments.\\nSo we will say await asyncio.gather,\\nand then we'll pass in an asterisk and then tasks.\\n\\nNow let's make sure our indentation is correct here.\\nSo this should actually be moved up one.\\nOther than that, we need to also add a colon here.\\nAnd then it looks pretty good, so I'll run it.\\nOkay, no problems.\\nOne thing I want to point out here is\\nthat the await keyword is crucial.\\nIt means that the scrape function will wait\\nuntil all the fetch tasks have been completed.\\n\\nEach fetch task involves sending a request to a URL,\\ngetting the response and then passing that response\\nto scrape and save links,\\nwhich saves the links into a CSV file.\\nOnce await asyncio.gather tasks completes,\\nwe know that all the URLs have been processed\\nand the links have been saved.\\nSo here we are at the concluding portion\\nof our web scraping session.\\nUp to this point,\\nwe've built all of the individual components we need\\nfor our web scraping application.\\n\\nWe have the scrape and save links function\\nto extract links from the HTML content\\nand save them to a CSV file.\\nThe fetch function to get the HTML content from our URL\\nand the scrape function to manage all\\nof our fetch calls concurrently.\\nNow it's time to use them.\\nLet's create a object called URLs\\nand we're going to set it equal to a list of URLs.\\nLet's make the first URL,\\nhttps://analytics.usa.gov.\\n\\nAnd then our second URL will be\\npython.org.\\nSo we'll say https://www.python.org.\\nAnd lastly about we use LinkedIn,\\nso https://www.linkedin.com.\\n\\nI'm going to do a forward slash just to make sure\\nall of our ducks in a row here, clean up the formatting.\\nAnd the next line is where we actually start\\nour scraping operation.\\nSo we'll say asyncio.run(scrape),\\nwhere URLs is equal to URLs.\\n\\nThose are the URLs we wrote into the list above.\\nAnd run this.\\nOkay, it looks like I missed the N here.\\nIt should be asyncio.\\nOkay, run this.\\nSo it looks like it's found some objects\\nthat are not callable,\\nbut we should still have\\nall of the links saved in the CSV file.\\nSo let's go ahead and go into our explorer here\\nand look for a CSV file.\\n\\nOkay, so it looks like we have a problem\\nwith one of our functions.\\nSo what I'm going to do is I'm going to go back up\\nand just check this syntax really quickly.\\nAnd what I can see here is that\\nthis indentation needs to be moved out.\\nAnd then also this A needs to be changed to a capital A.\\nAnd then we will run this code block again,\\nand okay, fix the problem.\\n\\nAnd now all of these links from these webpages\\nhave been scraped asynchronously.\\nThey will be saved in the CSV file.\\nSo we can look over here in the explorer section\\nand you'll see we have a CSV file.\\nAnd here is the CSV file with\\nall of the links from each of the three pages\\nthat we referenced in our list.\\n\"}],\"name\":\"7. Data Sourcing via Web Scraping\",\"size\":201619135,\"urn\":\"urn:li:learningContentChapter:4579304\"},{\"duration\":2756,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4584144\",\"duration\":314,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to Streamlit\",\"fileName\":\"3006708_en_US_08_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":401,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Get introduced to the spectrum of dashboarding utilities for Python, including Streamlit, Voila, Panel, and Dash. Learn why Streamlit is preferable.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7107321,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Streamlit, Dash, Voila, and Panel\\nare the leading frameworks\\nwithin the Python dashboarding ecosystem.\\nLet's discuss the features of these frameworks.\\nI'll state a design assumption.\\nThen we'll decide which framework\\nis best for that assumption.\\nStreamlit is a Python library\\nthat's specifically built for machine learning engineers\\nand data scientists.\\nStreamlit makes it easy to create and share\\nbeautiful custom web apps\\nfor machine learning and data science projects\\neven if you don't have any prior knowledge\\nof web development.\\n\\nin the matter of a few minutes,\\nyou can use it to build and deploy powerful\\ndata applications.\\nSome of the primary features of this framework\\nare it only supports Python language.\\nIt supports all of the Python plotting libraries\\nlike Matplotlib, Seaborn, Plotly, what have you.\\nAnother nice thing about Streamlit\\nis it's an open source framework.\\nWe do not need any web development knowledge\\nto build a web application using Streamlit,\\nand it's also incredibly easy to use and manage,\\nrequiring very, very little bit of code.\\n\\nIt runs on its own server\\nand it doesn't support Jupyter Notebook,\\nbut it can be deployed on most of the deployment servers.\\nNext, there's Panel.\\nPanel is another Python library\\nthat lets you create custom,\\ninteractive web apps and dashboards\\nby connecting user-defined widgets\\nto plots, images, tables, or text.\\nThe best thing about Panel\\nis that you can build a simple dashboard application\\nfor a complicated system in Jupyter Notebook,\\nand you don't need to switch tools along the way.\\n\\nHere are some of the main features of Panel.\\nJust like Streamlit, Panel also supports Python language.\\nIt supports all of the main Python Ploting libraries\\nlike Matplotlib, Seaborn, and others.\\nIt's an open source framework\\nand it can be used to create multi-page web applications.\\nA nice thing about Panel\\nis we don't need any web development knowledge\\nto create web applications using it.\\nPanel offers amazing design flexibility,\\nand you can use it to create your templates as well.\\n\\nIt supports Jupyter Notebooks,\\nand with it you can create an end-to-end application\\nwithin a notebook environment.\\nLastly, Panel can easily be deployed\\nto most deployment platforms.\\nNext, there's Voila.\\nVoila is a tool that lets you turn any Jupyter Notebook\\ninto a standalone web application.\\nIt allows you to create interactive web pages,\\nand it has great support for widgets, such as IPyWidgets.\\n\\nHere are some of the more prominent features\\nof Voila.\\nIt supports multiple languages, such as Python, C++,\\nand Julia.\\nIt also supports most of the Python plotting libraries.\\nIt's an open source framework\\nand you don't need any web development knowledge\\nto create web applications using Voila.\\nVoila makes it really easy to create basic dashboards,\\nbut unfortunately it has limited design flexibility.\\nIt only offers a few templates,\\nbut it does give you an option to create your own templates.\\n\\nThe nice thing about Voila\\nis it has exceptional support for Jupyter Notebooks.\\nLastly, Voila can be easily deployed\\nto most of the deployment servers.\\nAnd last but not least, there's Dash.\\nDash is a web application development framework from Plotly.\\nIt's built specifically for developing data applications.\\nDash offers users a very easy way\\nof developing dynamic dashboards.\\nSome of its main features are that it supports\\nPython, R, and Julia languages.\\n\\nDash is primarily built for using\\nwith Python's Plotly Library.\\nIt explicitly supports multi-page web applications,\\nand it's an open source framework.\\nThat said, basic knowledge of HTML is needed\\nfor developing a web application.\\nIt's really easy to create a basic dashboard application\\nusing Dash and Dash offers incredible design flexibility.\\nUnfortunately, though it doesn't provide any support\\nfor Jupyter Notebooks.\\n\\nOne more thing about Dash\\nis that it's got plenty of deployment options\\nand can be deployed on most of the deployment servers.\\nNow, let's assume we need to develop a dashboard\\nfor a non-technical audience.\\nIn this case, Streamlit would be an ideal\\ndata dashboarding solution simply because\\nit's more simple and structured than other options.\\nIf you're looking for a more mature\\ndata dashboarding solution\\nand your primary goal is to develop dashboards\\nfor non-technical users,\\nthen Streamlit should be your choice.\\n\\nMoving forward, we'll get started working with Streamlit\\nand exploring its features.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4588030\",\"duration\":168,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Environment setup\",\"fileName\":\"3006708_en_US_08_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":261,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Get a basic introduction to Streamlit and how to import the required packages.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5665410,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] You know, most Streamlit developers claim\\nthat it's really easy to set up Streamlit\\nand that you can use it\\nto build a web application in minutes using Python.\\nHow about let's test out that claim to see if it's true.\\nWe'll start by setting up Streamlit,\\nand once we have that up and running,\\nwe'll explore some of its more prominent features.\\nStarting first with the installation process,\\nStreamlit works fine with Anaconda,\\nso we'll install it in an Anaconda environment.\\n\\nTo install Streamlit, we'll need the following commands.\\nWe'll need pip install streamlit, streamlit hello,\\nand pip install upgrade protobuf.\\nLet's open the terminal.\\nIn this demonstration,\\nwe're going to be working inside the terminal here.\\nSo for building Streamlit applications,\\nyou always need to do a pip install\\nof any libraries you'll need when building them.\\nAnd so in this case, we're just working with Streamlit.\\nSo we would say, pip install streamlit.\\n\\nRun this.\\nAnd then once we have that installed,\\nwhat we need to do is we need to import it\\ninto our environment.\\nSo we're going to say import streamlit as st.\\nAnd what we're actually doing here is we're building\\na very, very simple Streamlit application\\nthat just writes the phrase Hello World onto a webpage.\\nSo in order to do that, we're going to call the right function\\nand pass in a string that reads Hello World.\\n\\nSo we'll say st dot write.\\nAnd then Hello World.\\nOkay, and so we should be good to go.\\nIt's a very simple routine.\\nAnd then to run this, what we need to do\\nis we need to say streamlit,\\nspace run, space,\\nand then we need to copy the file path\\nof the Python file we're working in.\\nSo in this case, it's 0802B.\\n\\nSo I open up Explorer, I right click on the file name,\\nI copy path, and then I can just paste it down here.\\nOkay, so Chrome is asking me\\nif that's okay with me to paste.\\nAnd so I say allow.\\nAnd then I hit Enter to run.\\nAnd then here you click this button\\nto open it up in a browser.\\nAnd here is your very first Streamlit application\\nthat reads, Hello World.\\n\\nCongratulations.\\nNext I'm going to show you how to use Streamlit\\nto start building charts and visualizations.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4590007\",\"duration\":546,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Create basic charts\",\"fileName\":\"3006708_en_US_08_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":966,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to create basic charts in Streamlit, including a simple line chart, bar chart, and pie chart.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":19553621,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay,\\nwe're going to be using pandas, NumPy, Streamlit,\\nand Matplotlib.\\nAnd as with any Streamlit coding demo inside of Codespaces,\\nwe need to do a pip install of the required libraries.\\nSo let me just do that real quick.\\nDo pip install pandas.\\nSo the next thing to do is pip install numpy.\\n\\nAnd then we need to pip install streamlit.\\nAnd lastly, pip install matplotlib.\\nOkay, so now, we should have all of our libraries\\navailable inside of our environment.\\nSo the next thing we need to do\\nis just to import them.\\nSo, we'll start off by saying\\n\\\"import pandas as pd,\\\"\\nand then, \\\"import numpy as np,\\\"\\n\\\"import streamlit as st,\\\"\\nand \\\"import matplotlib,\\\" and we need the pyplot module,\\nso we'll say \\\"matplotlib.pyplot as plt.\\\"\\nNext, we'll create a dataframe with random numbers,\\nand set the column names.\\n\\nOkay, so I'm going to take this terminal here,\\nand just minimize it,\\nso we have more working space.\\nOkay, so first we'll define the column names.\\nWe'll do call_names equal to a list.\\nThe first column name will be called \\\"column1,\\\"\\nand then the next will be column2,\\nand the third is, of course, column3.\\n\\nAnd now let's create a DataFrame called \\\"data.\\\"\\nAnd we'll set data equal to the DataFrame constructor.\\nAnd within it, let's pass the random number generator.\\nAnd we'll create a numpy array with size 30 rows\\nand three columns, containing random numbers.\\nAnd after that, we'll set the column names.\\nSo for the random number generator,\\nlet's use np.random.randint, pass in a 30,\\nwhich basically says that we want to have 30 datapoints.\\n\\nAnd then let's define the size of our DataFrame here,\\nso again, we want 30 rows and three columns,\\nso we'll just say \\\"size=,\\\"\\nand then pass in 30 and three.\\nAnd then lastly, we need to define the column names.\\nSo to do that, we'll just say \\\"columns=col_names.\\\"\\nOkay, so I have a typo there, where that needs to be names.\\n\\nThis is the part\\nwhere we're going to now create the line chart.\\nFor this, we'll use Streamlit's line chart function.\\nSo that's st.line_chart,\\nand we'll pass in our DataFrame data.\\nAnd above the chart, let's print chart name,\\nwhich will be \\\"line graph.\\\"\\nSo we just create a string here.\\nAbove the chart, we write a string called \\\"line graph:\\\"\\nwith a colon at the end.\\n\\nLet's run this code and visualize the line graph.\\nSo to do that, we need to go down to our terminal here.\\nI'm going to bring it back up so we can see a little better.\\nAnd we're going to say, \\\"streamlit run,\\\"\\nand again, we need to copy the file path for this file.\\nSo we're working in 080_3b.\\nGo to the explorer, and then copy the path here.\\n\\nPaste, and then hit enter.\\nAnd then open in browser, and click the button.\\nAnd we created this chart in Streamlit,\\nusing only one line of code,\\nwhich shows really how easy it is to use.\\nAnd in the next step I'd like to show you\\nhow to create a bar chart.\\nSo, for this,\\nwe're going to use Streamlit's bar chart function.\\nI think the easiest way to do this, actually,\\nwill be just to copy and paste the code up here.\\n\\nOkay, so let's change out the title.\\nIt's going to be \\\"bar graph.\\\"\\nAnd then the function is now, instead of st.line_chart,\\nit's st.bar_chart.\\nAnd then we can go back to the terminal,\\nand I'm just going to copy and paste the run code here,\\nenter it in, hit enter.\\nSo it's running.\\n\\nOkay, so after we update the code here,\\nand then what we can do is we can just go back over\\nto our application in the web browser.\\nAnd you see that our bar chart has been added,\\nwhich is a nice, dynamic bar chart.\\nAnd it's been added directly below the line graph.\\nNow, Streamlit doesn't have its own pie chart function,\\nbut we can use a pie chart in Matplotlib,\\nor other Python visualization libraries,\\nand pass it into Streamlit's chart function\\nfor Matplotlib to display in Streamlit.\\n\\nTo display the pie chart in Streamlit,\\nlet's first create a simple dataset\\ncontaining types of animals, and their heights.\\nSo, we'll create a variable named \\\"animals.\\\"\\nSet it equal to a list, with the name cat, cow, and dog.\\n\\nSo these are the types of animals\\nrepresented in our animal variable.\\nAnd then let's assign them some heights.\\nSo we'll create another variable named \\\"heights,\\\"\\nand we'll set that equal to another list,\\nwhich will be 30, 150, and 80.\\nOkay.\\nSo now, let's just take this dataset\\nand use it to create a pie chart.\\n\\nSo let's create a label for the pie chart first.\\nWe'll do that by creating a string,\\nand just writing \\\"pie chart:\\\" with a colon in it.\\nAnd then, now we'll need to use Matplotlib\\nto create the pie chart.\\nSo we'll say, \\\"fig and ax is equal to plt.subplots.\\\"\\nGoing to move this up a little just so you can see it better.\\n\\nAnd then we'll call the pie method.\\nSo we'll say \\\"ax.pie,\\\"\\nand the first variable we'll pass in here will be heights.\\nAnd then for our labels,\\nlets set that equal to the animals.\\nNow let's pass the pie chart\\nin Streamlit's pie plot function.\\nSo to do that, we'll say \\\"st.pyplot,\\\"\\nand we'll pass in the figure object.\\n\\nSo now we've written all of our code.\\nIn order to see this displayed in our web browser,\\nthat's also of course very simple.\\nWhat you need to do is just go over to the browser,\\nand then hit the refresh.\\nAnd then you can scroll down, and you can see the pie chart\\nnow shows up.\\nIn this demo,\\nwe displayed three different types of charts in Streamlit\\nusing less than 10 lines of code,\\nwhich is really impressive.\\nNext, we're going to do a deeper dive\\ninto line charts in Streamlit.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2714163\",\"duration\":522,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Line charts in Streamlit\",\"fileName\":\"3006708_en_US_08_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":811,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Explore various options for displaying line charts in Streamlit.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17244047,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's take a deeper dive\\ninto creating line charts in Streamlit.\\nSo as with the other Streamlit demonstrations\\nthat we're doing here inside of code spaces,\\nwe need to do a pip install of our libraries.\\nSo for this demonstration,\\nour libraries will be time NumPy Panda Streamlit,\\nand matplotlib.\\nWe don't need to do a pip install for time,\\nbut we do need to do a pip install NumPy\\npip install\\npandas.\\n\\nThis is just to be super sure that\\nyou have the libraries available inside your environment.\\nDo pip install Streamlit.\\nAnd then lastly, pip install matplotlib.\\nLet's see, there was a small typo here,\\nso I'm going to change that O to B and rerun this.\\n\\nOkay, good.\\nSo now we have our libraries available to us.\\nNow we need to import them into\\nour Python file here.\\nSo we'll start by importing time.\\nSo I say import time,\\nimport\\nnumpy as np,\\nimport pandas as pd,\\nimport streamlit as st,\\nand import matplotlib.pyplot\\nas plt.\\n\\nNext,\\nlet's create a line chart\\nwhich will grow over time\\nand we'll use the line chart add rows function\\nto grow the chart.\\nSo we'll start by first generating a numpy array\\nwith a single random value.\\nCall it rows.\\nSo we'll say rows is equal to np.random.randn.\\nIn here we will pass the shape of array one cross one.\\n\\nNow let's print the chart name.\\nWe'll call it Growing Line Chart.\\nSo to do that we just write a string\\nand pass in the title that we want for the chart.\\nTo create the line chart using Streamlit,\\nwe need to use the line chart function.\\nSo we'll say st.line_chart\\nand we'll parse in our rows.\\n\\nAnd then let's set this whole thing equal to chart.\\nAfter that, we'll create a loop\\nand we'll grow the line chart in the loop.\\nSo we'll say for i in range,\\n1 through 100, do this.\\nNext, let's generate a random number\\nand append it to the array.\\nThen we'll parse this array to add rows function.\\n\\nSo we'll say new_rows\\nequal to row zero\\nplus np.random.randn,\\nand parse the shape of the array, which is a one by one.\\nNow let's add this array to the chart.\\nTo do that, we'll call the add rows method off\\nof the chart object.\\nSo let's say chart.add_rows\\nand we'll parse in our new rows.\\n\\nAfter that, we'll set rows equal to new rows\\nand then we'll call time.sleep to stop the loop\\nfor five milliseconds.\\nSo here we'll say rows = new_rows.\\nAnd then we'll end this with time.sleep.\\nAnd we will parse in 0.05 for five milliseconds.\\n\\nOkay, so that needs to be a period instead of a comma.\\nSo I'm just going to kind of look through here\\nand make sure my syntax is okay.\\nThis is pretty simple code of course,\\nbut okay so now we go back to the terminal\\nand we need to say streamlit run\\nand we'll copy our file path here,\\npaste it in.\\n\\nOkay, so we have to allow pasting in the chrome browser\\nand then hit enter.\\nIt's going to run it\\nand then click the open in browser button.\\nAnd there is our growing line chart.\\nAnd streamlit really gives us a flexibility\\nof creating graphs\\nin any other Python visualization library\\nand then allowing you to show them inside of streamlit.\\nSo let's create a line chart using mapplotlib.\\n\\nFirst we'll create an array with random values.\\nSo let me go back over to our Python file.\\nAnd so we'll create an array called values\\nand we'll set it equal to np.random.rand,\\nwe'll pass in a 10.\\nThat's just saying that we want the length of the array\\nto be 10 data points.\\nNext, let's just create a little title for this chart.\\n\\nSo we'll call it matplotlib's line chart.\\nAnd\\nokay, it looks like\\nthis comma here isn't going to work\\n'cause it closes the string,\\nso I'll just take that out\\nand okay, that'll be our title.\\nNext, let's plot the values with mapplotlib.\\nSo we'll say fig and ax.\\n\\nWe'll set these objects equal to plt.subplot,\\nsubplots,\\nand then we'll call the plot method off of our ax object.\\nSo ax.plot\\nand we'll pass in our values here.\\nNow let's parse the line chart into streamlit\\nusing the pyplot function.\\nSo we'll say st.pyplot\\nand we'll parse in our figure object.\\n\\nSo then the next thing you need to do\\nis just let's go back over to our webpage\\nand we can hit refresh.\\nAnd now we have got our matplotlib line chart.\\nHere's our growing, here's our growing chart,\\nand then we've got our max matplotlib line chart.\\nAnd so it looks like\\nwe have far too many of these matplotlibs line charts\\nso let's go back to the code\\nand see if we can figure out what went wrong there.\\n\\nOkay, so after looking over the code here a little bit,\\nI couldn't find anything wrong with it\\nand so I went and just refreshed the screen\\nand the mapplotlib line chart\\nworking as expected.\\nSo you can see now here we only have one chart,\\nand after doing a little bit of research,\\nI discovered that this is actually\\na known issue with the context manager for mapplotlib.\\n\\nIn any case, it cleaned itself up here\\nwithin our streamlit, as you can see\\nand so we are good to go.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2715034\",\"duration\":691,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Bar charts and pie charts in Streamlit\",\"fileName\":\"3006708_en_US_08_05_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":1065,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Explore various options for displaying pie charts and bar charts in Streamlit.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":22156396,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] As usual, for using Streamlit in Codespaces,\\nwe need to do a pip install of our required libraries.\\nSo in this demonstration, we're going to be using Pandas,\\nNumPy, Streamlit, and Matplotlib.\\nSo I'll just do a quick pip install of each of these.\\nPip install numpy,\\npip install streamlit,\\nand pip install matplotlib.\\n\\nOkay, so great.\\nNow we have the libraries that we need.\\nI'm going to minimize the terminal here\\nand start working inside of the Python file.\\nWe'll need to first import our libraries, of course,\\nso we will import pandas as pd,\\nimport numpy as np,\\nimport streamlit as st,\\nand import matplotlib.pyplot\\nas plt.\\n\\nAnd in a previous Streamlit demonstration,\\nwe created a bar chart using Streamlit's bar chart function.\\nThis time, let's create a bar chart in Matplotlib\\nand display it in Streamlit.\\nThe bar chart we'll create\\nis a group bar chart comparing the heights\\nand weights of the animals.\\nFirst, let's define the data for the charts.\\nSo we're going to say animals.\\nSo our first variable,\\nand it's equal to\\na list of animal types.\\n\\nSo we'll have 'cat', 'cow',\\nI'm going to go through and just get the placeholders ready\\nfor 'dog' and 'goat'.\\nOkay, now let's create a second variable called heights.\\nWe'll set it equal to a list of numbers.\\nThat'll be 30, 150,\\n80, and 60.\\n\\nAnd then their weights in kilograms\\nwill be set equal to 5, 400,\\n40, and 50.\\nAnd we'll start first by defining the subplots.\\nSo fig, ax are equal to\\nplt.subplots.\\nNext we'll define the label locations\\nand the width of the bars.\\n\\nSo for that, we'll say x is equal to,\\nand then we'll call the arrange function.\\nSo that's np.arrange.\\nAnd we'll pass in length, so len.\\nAnd then we want the len to be heights.\\nSo we'll pass in our heights object,\\nand we'll set our width here equal to .40.\\n\\nWidth equal to 0.40.\\nNow let's draw the first set of the group of bars\\nfor the animal's heights.\\nTo do that, we will call the bar method\\noff of the ax object.\\nSo ax.bar.\\nAnd here, we'll first pass the position of the bars.\\nSo x minus 0.2.\\n\\nAnd then we'll pass the heights list next.\\nSo first let's just say x minus 0.2,\\nand the next parameter will say heights and width.\\nAnd then let's set the color of this bar\\nequal to red.\\nOkay, and then I'm going to copy and paste this code\\nto make a second group of bars\\nfor the animal's weights.\\n\\nSo let's change the position\\nto x plus 0.2 here.\\nAnd then we will replace the heights variable\\nwith the weights variable.\\nAnd then we can change the color here to orange.\\nNext, we'll set a legend for this chart.\\nSo to do that, we need to call the legend method\\noff of the ax object.\\nSo ax.legend.\\n\\nAnd then we'll pass in a list\\nwith the titles we want in the legend.\\nSo the first will be height,\\nand then the second will be weight.\\nThey should, of course, be in strings.\\nRephrase, and these labels should,\\nof course, be in strings.\\nNow we'll set the labels\\nand their positions on the x-axis.\\n\\nSo to do that we'll call the set_xticks method\\noff of the x object.\\nWe'll pass in our x variable.\\nAnd then let's also set some tick labels.\\nSo to do that we'll use the set\\nxtick labels method.\\nSo we'll say ax.set_xticklabels.\\n\\nAnd then we want our labels to be from our animals variable,\\nso we'll pass our animals variable.\\nAnd so here we'll pass the list of animal names.\\nAnd now let's call Streamlit's pyplot function\\nto display the Matplotlib chart in the Streamlit app.\\nSo to do that we'll say st.pyplot,\\nand we'll pass in our fig object.\\n\\nLet me just take a quick look at the syntax here.\\nDon't see any obvious problems,\\nso let's go ahead and then just run this.\\nSo what I'm going to do is in the terminal,\\nsay streamlit run.\\nWe need to copy the file path here,\\ncopy path, paste it in, hit Enter.\\nAnd then click the Open in browser button.\\n\\nOkay, amazing.\\nto plot out a Matplotlib function\\non an actual web browser.\\nNow let's look at how to do the same thing in a pie chart.\\nLet's go back over to our Python file.\\nAnd then what we're going to do\\nis we're going to create an advanced pie chart of some animals\\nand their heights and centimeters.\\nAnd we'll create the pie chart using Matplotlibs library,\\nand then display it using Streamlit again,\\nof course, like we did with the bar chart.\\n\\nIn the pie chart, the pie slices will be ordered\\nand plotted counterclockwise.\\nA wedge of a pie chart can be made to explode\\nfrom the rest of the wedges of the pie chart\\nusing the explode parameter of the pie function.\\nSo let's define that now.\\nWe'll say explode equal to, and then create a list,\\nand we'll say 0.2, 0.1,\\n0.1, 0.1.\\n\\nNow let's create the plot.\\nSo we'll say plot_pie, ax.\\nBoth of these objects are equal to\\nplt.subplots.\\nAnd then we'll call the pie method\\noff of the ax object,\\nso ax.pi.\\nAnd we'll pass in the height\\nof the animals, so heights.\\n\\nAnd then we'll set explode equal to\\nour explode object that we just created.\\nWe want our labels to be set equal to\\nthe list of names in our animals object.\\nSo we'll put labels equal to animals.\\nAnd next we'll pass in autopct as a parameter,\\nwhich enables you to display the percentage values\\nusing python string formatting.\\n\\nSo we'll say autopct\\nis equal to, and then we will write a string\\nthat says %1.1f%%.\\nSo %1.1f%%.\\nAnd that's it's string format.\\nThe next parameter is the shadow parameter.\\n\\nAnd so we're going to set shadow here equal to true.\\nIn the next line, we will call the axis method\\noff of the ax object.\\nSo we'll say ax.axis.\\nAnd then passing a string that reads equals.\\nIt should be equal, not equals.\\nSo I'll take that S out.\\nThe equal here acts as a keyword.\\nEqual aspect ratio ensures that pie is drawn as a circle.\\n\\nNext we'll call the pyplot function.\\nSo that's st.pyplot.\\nAnd we'll pass in our plot_pie object.\\nSo I'll go over to our browser and hit Refresh.\\nWow, that's amazing.\\nLook at that gorgeous pie chart.\\nIt looks so much better\\nthan what you can create in default,\\nin basic Matplotlib.\\n\\nAnd also another added advantage\\nis that this is showing now on a webpage\\nthat you can share with other users,\\ninstead of just having it stuck inside\\nof your Jupyter Notebook environment.\\nNow let's look at creating statistical charts in Streamlit.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2715035\",\"duration\":515,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Create statistical charts\",\"fileName\":\"3006708_en_US_08_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":740,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to create statistical charts in Streamlit. This video covers histograms, boxplots, and scatter plots.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":18753819,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] On this coding demo,\\nwe're going to create statistical charts using streamlit.\\nAnd so as usual, we need to do a pip install\\nof the libraries we need.\\nSo that'll be streamlit, seaborn, matplotlib,\\nand pandas, and sklearn in this case.\\nSo we'll start with a pip install of streamlit,\\nand then do a pip install seaborn,\\npip install matplotlib,\\npip install pandas,\\nand then we'll do pip install sklearn.\\n\\nOkay, so that should be pip install scikit-learn.\\nOkay there, now we have installed\\nall of the libraries we need for this demonstration.\\nSo let's import them in our Python file.\\nSo we'll say import streamlit as st,\\nimport seaborn as sns,\\nimport matplotlib.pyplot as plt,\\nimport pandas as pd.\\n\\nAnd then for the data for this demonstration,\\nwe're going to use the iris dataset.\\nSo we're going to import that from scikit-learn.\\nSo we'll say from sklearn.datasets\\nimport load_iris.\\nAfter that, let's load the iris data.\\nSo to do that, we'll create an object called iris_data\\nand we'll set it equal to load_iris function.\\n\\nNow we'll load iris dataset into a data frame.\\nWe'll call it data.\\nSo we'll say data =,\\nand then we will call the DataFrame constructor.\\nSo that's pd.DataFrame,\\nand we'll pass in our iris_data.data,\\nand then let's pass the column names.\\nSo we'll say columns = iris_data.feature_names.\\n\\nFirst, we'll create a histogram chart from the data frame\\nusing seaborn and streamlits pyplot function.\\nSo first let's just write fig = plt.figure,\\nand we'll also be using the histplot function from seaborn.\\nSo we'll say sns.histplot,\\nand we'll pass in our data frame.\\n\\nSo we'll say data=data,\\nand the number of bins\\nwe want to create in the histogram.\\nSo we'll say bins here=20.\\nNow let's show the histogram and the streamlit app.\\nTo do that, we would just call the pyplot function.\\nSo that's st.pyplot,\\nnow passing our fig object.\\nAnd just looking really quickly for any typos,\\nI see that I need to change this E to an O, load_iris.\\n\\nAnd aside from that, it looks pretty good.\\nSo the next thing I need to do\\nis just to say, streamlit run,\\ncopy the file path,\\npaste it in,\\nand hit Enter.\\nOkay, and then we'll open in browser.\\nSo our application is running.\\nLook how pretty that is.\\nSo with respect to colors,\\neach of the four features,\\nsepal length, sepal width, petal length, and petal width\\nare represented by a different color in the histogram.\\n\\nAnd this makes it easier to distinguish\\nbetween the distributions of each feature.\\nSo we have 150 data points\\nthat have been bend into 20 separate intervals.\\nAnd the value of this histogram\\nis that it provides a visual summary\\nof the numerical data in the dataset,\\nwhich allows us a quick understanding of the distribution\\nof measurements across the iris species.\\n\\nSo for example, these peaks here in the histogram\\nrepresent the most common measurement ranges\\nwhile the spread indicates variability within the data.\\nNext, let's create a boxplot on the iris dataset.\\nSo to do that,\\nwe'll say fig = plt.figure.\\nThen we'll call seaborn boxplot function.\\nSo that's sns.boxplot,\\nand we'll pass in our data.\\n\\nSo here we'll say data=data,\\nand then we will call streamlits pyplot function\\nto plot this out.\\nSo that's st.pyplot,\\nand we'll pass in our fig object.\\nAnd then go over to our browser and hit refresh.\\nOkay, so nice.\\nNow we have a nicely labeled boxplot.\\nAnd this is similar to the boxplot we explored\\nearlier in this course when we were discussing\\nhow to identify outliers in a dataset.\\n\\nSo you can see these points here past the whiskers\\nin the sepal width field.\\nThese points are highly suspect\\nfor being outliers in the dataset,\\nand you would want to examine and treat those data points\\nbefore moving into machine learning.\\nThe last plot we'll draw is a scatterplot.\\nSo we'll just create another fig object,\\nfig = plt.figure.\\n\\nAnd then we're going to use seaborn's scatterplot function.\\nSo that's sns.scatterplot,\\nwe'll say data=data,\\nand then plot this out using streamlits.\\nSo that's st.pyplot,\\nand passing our fig object.\\nAnd then I'm going to go back over to the browser\\nand hit refresh.\\n\\nAnd now we have a scatterplot\\nof all of the 150 data points\\nthat are contained within the iris dataset.\\nThey are color coded according to feature,\\nso this makes it easier for us\\nto get at glance view of the distribution of each feature\\nand basically what the values of the features are\\nwithout having to dig too much into the actual data itself.\\n\\nAnd that's it for Streamlit.\\n\"}],\"name\":\"8. Collaborative Analytics with Streamlit\",\"size\":90480614,\"urn\":\"urn:li:learningContentChapter:4583166\"},{\"duration\":70,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4588031\",\"duration\":70,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Next steps\",\"fileName\":\"3006708_en_US_09_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":93,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1694005,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In terms of next steps,\\nI definitely encourage you to go over to the Python\\nfor Data Science Essential Training,\\nIntroduction to Machine Learning Course.\\nThat course was built specifically\\nas the perfect follow up for this one.\\nAlso, I want to encourage you to go ahead\\nand start practicing your new data skills\\nby working to create efficiencies in your daily workflows\\nwith your employer or in your business.\\nLastly, I want to invite you to join our community,\\nThe Convergence.\\n\\nThe Convergence is an online community space\\nthat's dedicated to empowering operators\\nin the data industry by providing news and education\\nabout evergreen strategies, late-breaking data\\nand AI development, and free or low cost upscaling resources\\nthat you need to thrive as a data leader,\\nthat you need to thrive as a leader\\nin the data and AI space.\\nTo join us in The Convergence, simply visit this page,\\ndata-mania.com/newsletter, and drop your details\\nso that we can send you a kickoff email\\nthat's loaded to the brim with free goodies\\nto get you ahead in your data career.\\n\\n\"}],\"name\":\"Conclusion\",\"size\":1694005,\"urn\":\"urn:li:learningContentChapter:4589022\"}],\"size\":942069028,\"duration\":27863,\"zeroBased\":false},{\"course_title\":\"Introduction to Data Science\",\"course_admin_id\":4457428,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":4457428,\"Project ID\":null,\"Course Name\":\"Introduction to Data Science\",\"Course Name EN\":\"Introduction to Data Science\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"&lt;p&gt;The world of data science is reshaping every business, regardless of industry, location, or role. And there\u00e2\u20ac\u2122s never been a better time to get up to speed and learn the basics of this booming field. In this course, designed specifically for beginners, explore the world of data science, its opportunities and innovations, and the fundamental skills required for success.&lt;/p&gt;&lt;p&gt;Join Python trainer and data science expert Lavanya Vijayan as she shares what data science is and how it differs from other common data-related careers. Discover some of the most important tools used in the trade to develop your understanding of data libraries and data manipulation. Along the way, get an introduction to exploratory data analysis, data cleaning, data visualization, sampling, testing, estimating, and more. By the end of this course, you\u00e2\u20ac\u2122ll know how to use inference and statistical analysis to make more reliable predictions for your business.&lt;/p&gt;&lt;p&gt;This course was created by &lt;a href=https://www.onlymadecraft.com target=_blank&gt;Madecraft&lt;/a&gt;. We are pleased to host this training in our library.&lt;/p&gt;&lt;img src=https://media.licdn.com/media/AAYAAgCwAAoAAQAAAAAAAHppnBQxgeyWS2CsU3aDDPcMgw.jpg alt=Company logo for Madecraft; the letter M configured as part of a printing press width=20% height=20% /&gt;\",\"Course Short Description\":\"Get to know the exciting world of data science in this beginner-friendly course.\",\"Content Type\":\"SKILLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":\"21401000, 20516011\",\"Instructor Name\":\"Madecraft Licensor, Lavanya Vijayan\",\"Instructor Transliterated Name\":\",\",\"Instructor Short Bio\":\"Full-Service Learning Content Company|Technical Curriculum Architect and Data Science Instructor at Madecraft\",\"Author Payment Category\":\"LICENSED, NONE\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2023-07-31T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/introduction-to-data-science-22668235,https://www.linkedin.com/learning/introduction-to-data-science-revision-2023\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Beginner\",\"LI Level EN\":\"Beginner\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Data Science\",\"Primary Software\":null,\"Media Type\":\"Video\",\"Has CEU\":\"Yes\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":7255.0,\"Visible Video Count\":34.0,\"Contract Type\":\"LICENSED, NO_CONTRACT\"},\"sections\":[{\"duration\":55,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4487774\",\"duration\":55,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Beginning your data science exploration\",\"fileName\":\"4457428_en_US_00_01_WL24\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Data science is a vast field, with plenty of opportunities and exciting innovations happening. After this course, you'll be able to define data science, recognize the data science lifecycle, and utilize fundamental skills in data science. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1527871,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"In the world today, it's hard to find a hotter\\nfield than data science.\\nIt's a fast-growing field that is reshaping the world. From\\nself-driving cars to helping treat cancer,\\nthe applications of data science are vast and powerful,\\nand taking time now to learn data science helps you open doors and\\nunlock opportunities that can help advance your career.\\nMy name is Lavanya Vijayan.\\nI'm a programming and data science instructor and an advocate\\nfor Stem education.\\nIn this course, I'll share the foundations\\nof data science, what it is, and how it's applied.\\n\\nThen I'll help you understand data design and introduce you to the\\ncomputational tools that are fundamental to data science.\\nFinally, you'll go through the stages of the data science\\nlifecycle.\\nBy the end of this course, you'll have a better understanding\\nof data science and you can begin to craft your journey to becoming\\na data scientist yourself.\\nSo let's get started.\\n\"}],\"name\":\"Introduction\",\"size\":1527871,\"urn\":\"urn:li:learningContentChapter:4488795\"},{\"duration\":401,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4493806\",\"duration\":235,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Demystifying data science\",\"fileName\":\"4457428_en_US_01_01_LA24\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Data science is rapidly increasing in popularity and demand and is a valuable skill as both a career or a skill within an existing role. After watching this video, you'll be able to define data science and its core principles.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6942964,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"There are many definitions out there for data science.\\nI like the definition given by Joseph Gonzalez,\\nwho's a professor at UC Berkeley.\\nAccording to him, data science is the application of\\ndata-centric computational and inferential thinking to understand\\nthe world and solve problems.\\nData scientists have a unique role in industry,\\ngovernment, and other organizations,\\nand that role is different from the roles of others,\\nsuch as data engineers,\\nstatisticians, and business analysts.\\n\\nThe first difference I want to help you understand is a\\ndifference between data scientists and data engineers.\\nData engineers work to make sure data flows smoothly between the\\nsource and the destination.\\nNow, the source is where data is collected,\\nand the destination is where data is extracted and processed.\\nData scientists work to make sure that value is extracted\\nfrom data smoothly.\\nData engineers optimize data flow while data scientists optimize\\ndata processing and data scientists work with data\\nengineers as well as business people to define metrics,\\nestablish how data is collected,\\nand ensure that data science processes work well with\\nenterprise data systems.\\n\\nAnd when data scientists and data engineers work together,\\nit's important for the data scientists to write code that's\\nreusable by the data engineers.\\nThe next difference I want to help you understand is the difference\\nbetween data scientists and statisticians.\\nNow, the amount of data that data scientists work with\\nis often massive, so they spend a lot of time with\\ntasks like large scale data collection and data cleaning.\\nMeanwhile, statisticians rely on more traditional and smaller scale\\nmethods of data collection, such as surveys,\\npolls, and experiments.\\n\\nData scientists try out different methods to create machine\\nlearning models, and then they choose the method\\nthat results in the best model.\\nOn the other hand, statisticians work on improving\\none simple model to best fit the data,\\nand data scientists do more than just analyze data.\\nThey also implement algorithms that process data automatically.\\nAnd this enables data scientists to provide automated predictions\\nand actions. To help bring this to life,\\nI want to give you some examples of the types of things that data\\nscientists can automate through data analysis and algorithms,\\nsuch as analyzing NASA pictures to find new planets or asteroids or\\nautomated piloting planes, cars and more.\\n\\nYou can automate book recommendations on Amazon or\\nfriend recommendations on Facebook.\\nYou can use automation and computational chemistry to\\nsimulate new molecules for cancer treatment.\\nAnd automation can help with early detection of an epidemic or\\nestimating the value of houses in the US in real-time,\\nlike on Zillow, or matching a Google AD with a\\nuser and a web page to maximize the chances of a conversion.\\nAutomation can help return highly relevant results to Google\\nsearches or detect credit card fraud and tax fraud and automation\\neven helps with weather forecasts.\\n\\nThe next difference I want to help you understand is the difference\\nbetween data scientists and business analysts.\\nBusiness analysts focus on database design and\\nROI assessment.\\nAnd some business analysts work on finance\\nplanning and optimization and risk management and others manage\\nprojects at a high level.\\nNow, data scientists can help business analysts.\\nFor example, data scientists can help automate the production of\\nreports and speed up data extraction.\\nAccording to data scientist, Vincent Granville,\\nthe collaboration between data scientists and business analysts\\nhas helped business analysts extract data that's 100 times\\nlarger than what they're used to and ten times faster.\\n\\nSo there you have it.\\nThe power of data science is changing our world,\\nand that's a great reason for you to continue learning.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4492758\",\"duration\":112,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"The value of data science\",\"fileName\":\"4457428_en_US_01_02_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Data science can be used across numerous fields and offers important benefits to the world around you. After watching this video, you'll be able to explain the value of data science.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3456120,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"The data stored and used by the world has dramatically\\nincreased each year.\\nAnd the good news is data sets in the right hands can help predict\\nand shape the future.\\nThink about it. Companies use data to run and grow their\\neveryday business,\\nand data science can help companies make quicker and better\\ndecisions which can take them to the top of their market.\\nAlso, data science affects many areas of our everyday lives.\\nFor example, it's already used in areas such as quick and\\neasy customer service, intelligent navigation,\\nrecommendations, and voice to text.\\n\\nData science has other uses, such as helping to improve the\\nresolution of an image and detecting fraud by analyzing the\\nbehavior of financial institutions in real-time.\\nData Science also has applications in robotics and thus has the\\ncapacity to help do things like assisting elderly people and\\npeople with disabilities.\\nData science can have positive contributions to society.\\nOne of them is reshaping industries like health care.\\nThe amount of data produced by patients and illnesses\\nrises by the second,\\nopening new opportunities for better structured and more\\ninformed health care.\\n\\nThe challenge is to carefully analyze the data in order to be\\nable to recognize problems quickly and accurately,\\nsuch as diagnosing medical conditions and predicting\\ndangerous seismic events.\\nData science also has the power to save rare species.\\nComplex predictive models and algorithms can create insights and\\nhelp scientists analyze threats to wildlife and create a solution\\nthat can save animals.\\nData science is redefining how people and organizations solve\\nchallenging problems and understand their world.\\n\\nThe more you understand data science,\\nthe more you'll be equipped with the tools to improve your\\ncommunity and make the world a better place.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4487773\",\"duration\":54,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Defining the data science life cycle\",\"fileName\":\"4457428_en_US_01_03_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Data scientists follow a specific workflow. After watching this video, you'll be able to describe the data science lifecycle and the main goal of each stage.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2230072,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Data\\nand it's called the Data Science Life Cycle.\\nIn this course, you'll learn more about this\\nprocess and what each stage of it entails.\\nBut for now, I'll give you a quick preview.\\nThe first stage of the data lifecycle is to formulate a\\nquestion you have or a problem you want to solve.\\nNext, you acquire and clean data that is relevant to your\\nquestion or problem.\\nThird, you conduct exploratory data analysis.\\nFourth, you would use prediction and inference to draw conclusions\\nfrom the data.\\n\\nNow, it's common for you to discover more questions you have\\nor problems you need to solve after the fourth stage.\\nSo you would go through the process repeatedly,\\nand that's why there's a positive feedback loop.\\nThe data science lifecycle is critical to how data scientists\\napproach their work,\\nand now you know the major stages of this process.\\n\"}],\"name\":\"1. Defining Data Science\",\"size\":12629156,\"urn\":\"urn:li:learningContentChapter:4491769\"},{\"duration\":392,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4487772\",\"duration\":240,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Reducing bias with probability sampling\",\"fileName\":\"4457428_en_US_02_01_LA24\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Data design, the process of data collection, is important in data science. After watching this video, you'll be able to use data design to reduce sampling biases.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7695281,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Data design is the process of data collection.\\nWhen you collect a sample of data, you want to make sure that there's\\nas little bias as possible in the way you collect the sample.\\nProbability sampling is a family of sampling methods that assigns\\nprecise probabilities to the appearance of each sample to\\nreduce bias as much as possible in data design.\\nI want to help bring this to life and show you a few methods\\nof probability sampling.\\nSuppose you have a population of eight people.\\nEach person is given a distinct letter from A through H.\\n\\nOne method of probability sampling is called simple random sampling,\\nalso known as SRS.\\nA simple random sample is taken by sampling at random without\\nreplacement.\\nTo take a simple random sample of Size 2 from the population\\nof eight people, you can write each letter from A\\nthrough H on a separate index card,\\nplace all the cards into a hat, mix the cards as well,\\nand draw two cards from the hat without looking.\\nHere are all possible.\\nsimple random samples of Size 2 from the population\\nof 8 people.\\n\\nSo when selecting a sample, you could get any of these samples\\nwith equal chance.\\nNow another method of probability sampling is called\\ncluster sampling.\\nA cluster sample is taken by dividing the population into\\nclusters and then using SRS to select clusters at random. To take\\na cluster sample of Size 2 from the population of 8 people,\\nyou can pair up each person to form four clusters of 2 people\\nper cluster and then use SRS to select one cluster.\\n\\nThe selected cluster is a sample of Size 2.\\nSuppose you pair up the individuals like this,\\nthose are your clusters.\\nSo when selecting a cluster, you could get either AB,\\nCD, EF, or GH with equal chance.\\nNow, you might be wondering how useful is a sampling method?\\nSo I'll tell you some of the pros and cons of cluster sampling.\\nI'll start with the pros.\\nCluster sampling makes sample collection easier,\\nand many agencies use cluster sampling to conduct surveys.\\n\\nFor example, it's much easier to survey 100 people in each city\\nthan to survey thousands of people distributed across the entire US.\\nSo what are the cons?\\nCluster sampling tends to produce more variation in estimation,\\nso you have to take larger samples when using cluster sampling.\\nAnother method of probability sampling is called stratified\\nsampling.\\nstrata and then producing one simple random sample per stratum.\\n\\nTo take a stratified sample of Size 2 from the population\\nof 8 people, you would first divide the\\npopulation into 2 Strata. Strata 1 would contain A,\\nB, C, D, and E, and Strata 2 would contain F,\\nG, and H.\\nThen you would use SRS to select one individual from each strata.\\nThis results in a sample of Size 2.\\nHere are the possible samples you could get.\\nNow, you might have noticed that the strata in this example\\nare different sizes.\\n\\nIn stratified sampling, the strata do not have\\nto be the same size, and that can be an advantage.\\nFor example, you could stratify the US by occupation,\\nthen take the samples from each strata of size proportional to the\\ndistribution of occupations in the US.\\nThis would ensure that occupations that are not as common would still\\nshow up in a stratified sample.\\nA simple random sample might completely miss people\\nin such occupations.\\nSo stratified sampling helps you ensure that subgroups of the\\npopulation are well represented in the sample,\\nwhich can lessen variation in estimation.\\n\\nHowever, stratified sampling can sometimes be more difficult to\\nperform because sometimes you don't know how large\\neach strata is.\\nNow you know the importance of probability sampling and three\\nmajor sampling methods.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4485934\",\"duration\":152,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Using non-probability sampling\",\"fileName\":\"4457428_en_US_02_02_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"You can also collect data with non-probability sampling techniques. After this lesson, you'll be able to recognize what these techniques are and the scenarios in which using them can be beneficial. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5847259,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Sometimes, data collection can be very expensive and time-consuming,\\nand you may not have access to all the members of a population.\\nIn that case, you would not be able to take a random sample.\\nInstead, you could use a different technique called non-probability\\nsampling, which is a subjective,\\nin other words, non-random method.\\nWith this method, there is no guarantee that every\\nindividual in the population has a chance of being included\\nin the sample.\\nSo this method has higher risks of sampling bias,\\nbut it's also easier and less expensive to implement.\\n\\nIt's often used in qualitative or exploratory research,\\nwhere the goal is to gain an initial understanding of\\na specific phenomenon.\\nIn this lesson, I will introduce key types of non-\\nprobability sampling with examples of each.\\nThe first is volunteer sampling.\\nWith this approach, volunteers make up the sample.\\nFor example, if a researcher wants to learn more about the experience\\nof having a particular disease, they could circulate a survey\\nwhere the eligibility requirement is that the participant\\nhas the disease.\\n\\nThose who meet that criteria and want to participate voluntarily\\nshare their responses to the survey questions.\\nAnother method is purposive sampling.\\nWith this approach, a researcher uses their best\\njudgment to select a sample that they think is a best fit.\\nFor example, if a researcher wants to learn more about the\\nexperiences of employees with disabilities,\\nthe sample could be intentionally selected to suit that goal.\\nAnother method is quota sampling, which is one of the most common\\nforms of non-probability sampling.\\n\\nWith this approach, sampling is done until a specific\\nnumber of units,\\nin other words, quotas, for various subpopulations\\nhave been selected.\\nThis helps in achieving sample size objectives and ensuring that\\nmembers of different subpopulations are included.\\nFor example, if a researcher wants to learn more about consumer\\npreferences of individuals across different socioeconomic groups,\\nthey could use quota sampling.\\nOne more method is snowball sampling.\\nIn snowball sampling, participants recruit other\\nparticipants to join the sample.\\n\\nThis is used when it's hard to directly reach members of a\\npopulation with the required characteristics.\\nFor example, if a researcher wants to learn more about the musical\\ntechniques of artists in niche genre,\\nthey may consider using snowball sampling.\\nIf you need to collect data with limited resources for a time\\nsensitive project and probability sampling is not an option,\\nyou can consider using one of these non-probability\\nsampling methods.\\n\"}],\"name\":\"2. Starting with Data Design\",\"size\":13542540,\"urn\":\"urn:li:learningContentChapter:4485935\"},{\"duration\":348,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4488794\",\"duration\":136,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Comparing Python and R\",\"fileName\":\"4457428_en_US_03_01_LA24\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"You'll want to set up your data science projects to be successful, and using Jupyter Notebook is a great way to do so. After this lesson, you'll be able to set up Jupyter on your computer and familiarize yourself with the Jupyter environment.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4134450,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"R and Python are among the most popular languages for\\ndata science,\\nI want to help you understand the differences and provide a basic\\ncomparison between these two languages and point out\\nthe strengths of each.\\nPython is generally used when data analysis tasks need to be\\nintegrated with web applications, or statistical code needs to be\\nincorporated into a production database.\\nMeanwhile, R is mainly used when the data analysis tasks require\\nstandalone computing or analysis on individual servers.\\n\\nPython emphasizes productivity and code readability.\\nIn contrast, R focuses on better user-friendly data analysis\\nstatistics and graphical models.\\nPython is used by programmers that want to delve into data analysis\\nor apply statistical techniques and by developers that\\nturn to data science.\\nOn the other hand, R has been used primarily in\\nacademics and research.\\nHowever, it's rapidly expanding into the enterprise market.\\nCoding and debugging is easier to do in Python due to Python's\\nnice syntax.\\n\\nThe indentation of the code affects its meaning.\\nA piece of functionality is always written the same in Python.\\nMeanwhile, statistical models can be written with only\\na few lines in R.\\nThere are style sheets in R, but not everyone uses them and the\\nsame piece of functionality can be written in several ways in R.\\nPython's focus on readability and simplicity makes its learning\\ncurve relatively low, and Python is considered a good\\nlanguage for new programmers.\\nHowever, R has a steep learning curve at the beginning.\\n\\nOnce you know the basics though, you can easily learn advanced\\ntechniques.\\nR is not hard for experienced programmers.\\nPyPi stands for Python Package Index and it's a repository of\\nPython software consisting of libraries.\\nUsers can contribute to PyPi, but it's a little bit complicated\\nin practice. And CRAN stands for the Comprehensive R\\nArchive Network.\\nIt's a huge repository of R packages that users can\\neasily contribute to.\\nPython and R both have strengths and weaknesses,\\nand depending on your needs, you might find yourself using\\none, the other, or both.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4489776\",\"duration\":212,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Setting up your Jupyter environment\",\"fileName\":\"4457428_en_US_03_02_MM24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6665021,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"In data science,\\nthe Python 3 programming language is a useful computational\\ntool, and you'll see me using Python 3 throughout\\nthis course.\\nTo write programs, you need an environment\\nspecifically built for coding.\\nNow there are various platforms out there,\\neach providing a unique coding environment and the more you\\ntalk to those who code, the more you'll hear strong\\nfeelings towards one platform or another.\\nWhat's important to understand here is that all platforms will\\nprovide an ability to get started and apply the skills you're\\nlearning in this course.\\n\\nAnd the platform I'm using is called Jupyter Notebook.\\nI've included a link in the resource file.\\nLet's set up your environment.\\nSo I'm on the Anaconda distribution website, and I'm going\\nto scroll down to see the latest version that's available\\nfor me to download.\\nAt the time of recording, it's 3.7. For you might be a\\ndifferent version and that's totally fine.\\nNow, you'll want to click on \\\"Download\\\" and it should begin\\ndownloading for you.\\nSo I've already downloaded this, so I'm going to go ahead and open\\nup my downloads folder to find the installer there.\\n\\nSo now, I'm going to double click\\nto open up the installer.\\nNow, you're going to click through some of these dialogs,\\nso press continue.\\nYou're going to click \\\"Agree,\\\"\\nthen click \\\"Install\\\" and enter your password.\\nGreat.\\nWe're all installed here.\\nSo let's go ahead and click \\\"Continue\\\" and let's hit \\\"Close.\\\"\\nYou can either keep it or move to trash.\\n\\nI'm going to choose to move to trash.\\nNow you want to open terminal.\\nAnd if you're on a Mac like I am, you can do command space\\nand type terminal into spotlight and hit \\\"Enter.\\\"\\nAnd now you want to type in \\\"jupyter notebook,\\\"\\nmake sure everything is lowercase and that there's a space\\nbetween the two words,\\nlike this and you want to hit \\\"Enter.\\\"\\nAll right. There on the home page of your Jupiter and you're\\nready to get going.\\n\\nThe first thing you want to do is click on \\\"New.\\\"\\nThis is on the upper right area of your screen.\\nAnd you want to click on \\\"Python 3.\\\"\\nThis is how we create a new notebook with Python 3.\\nAnd Python 3 is the latest version of Python at this point.\\nSo one of the first things you want to do is to name\\nyour notebook.\\nAnd you do this by clicking on \\\"Untitled\\\" and typing in\\na new notebook name.\\nI'm going to type in \\\"Practice.\\\"\\nOnce you type that in, you can click \\\"Rename.\\\"\\nAnd there you go,\\nyou've renamed your notebook.\\n\\nNow, there are a lot of buttons and menu options here,\\nbut all you need to worry about right now is this box right here.\\nThis box is called a cell, and you can type code into\\na cell and then run it.\\nSo I'm going to go ahead and type in a line of code and I'm\\ngoing to type print,\\n\\\"Hello, world.\\\"\\nAnd there are two ways to run a cell.\\nYou can either click \\\"Run\\\" at the top here or if you're on a Mac,\\nyou can hit \\\"Shift-Enter.\\\"\\nI'm going to hit \\\"Shift-Enter.\\\"\\nAnd there we go,\\nHello\\nworld is printed out, and if you're following\\nalong here,\\nyou just wrote your first piece of code.\\n\\nNow your Python 3 environment is all set and you're ready to\\nlearn how to use this computational tool\\nin data science.\\n\"}],\"name\":\"3. Utilizing Computational Tools\",\"size\":10799471,\"urn\":\"urn:li:learningContentChapter:4488796\"},{\"duration\":1201,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4488793\",\"duration\":161,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Defining tabular data\",\"fileName\":\"4457428_en_US_04_01_MM24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Once you have access to a dataset, you will need to interact with it and read the data most quickly and efficiently. After watching this video, you'll be able to read tabular data with Python.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4950882,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Data sets can be structured in different ways.\\nA data set structure refers to the arrangement of the data\\nin the data set.\\nThere are several ways to structure data,\\nbut a lot of data scientists prefer working with tabular data,\\nand the main reason is tabular structure is just more convenient\\nto work with.\\nTabular data is arranged in rows and columns.\\nData files are stored in specific formats.\\nOne of the most common file formats for storing tabular data\\nis Comma-Separated Values or CSV, where each record is stored as a\\nline in the file and each file is separated by a comma.\\n\\nIf your data is stored in a CSV file,\\nyou can use the read_CSV method from a library called\\nPandas to quickly read the file into memory.\\nPandas is a powerful library in Python that provides easy to\\nuse data analysis tools.\\nFor example, say you're working with data regarding the names of\\nbabies born in the United States and this data is stored in a CSV\\nfile called us_baby_names.csv.\\nTo access the data, you can read in the\\nfile like this.\\n\\nThe first thing I'm going to do is import the Pandas Library.\\nAnd I do this by writing the keyword import followed by pandas.\\nAnd I'm also going to add in as pd.\\nThis means that going forward in this notebook,\\nI need to use pd to refer to pandas.\\nAnd I'll go ahead and run the cell.\\nNow, I want to read in the CSV file called us_baby_names_csv and a\\nreminder that that CSV file is stored in the same directory as\\nthis notebook. To read in the CSV file,\\nI'll use the read CSV method located in the Pandas library.\\n\\nSo I'll type pd.read_csv ()\\nand now I'll type in the name of the CSV\\nfile as a string.\\nSo I'll type in 'us_baby_names.csv', and I'll go ahead\\nand run the cell. There,\\nthat's what the data looks like.\\nAnd this object is called a Pandas data frame.\\nNow, if I create a variable and store this data frame in that\\nvariable, whenever I want to access the data,\\nall I have to do is refer to that variable.\\n\\nSo I'll go ahead and create a variable.\\nI'll call it us_babies,\\nand I'll assign it to the data frame that I get when I\\nread in the CSV file.\\nI'll go ahead and run the cell.\\nNow, when I type in us_babies and run the cell,\\nI have access to the same data set.\\nNow you know what tabular data is, how it looks, and how to use the\\nPandas Library to read in CSV files.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4487771\",\"duration\":373,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Reading tabular data\",\"fileName\":\"4457428_en_US_04_02_MM24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Being able to read data effectively is only half the battle, you also want to be able to analyze the data for insights. After this lesson, you'll be able to use what you're reading within the tabular data to formulate preliminary interpretations of your data. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11939342,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Pandas is a powerful library in Python that provides\\neasy-to-use data analysis tools.\\nWhen you use pandas to read in data,\\nyou get a data frame.\\nA data frame is a tabular data structure where each column is\\nlabeled and each row is labeled.\\nFor example, say you're working with data regarding the names of\\nbabies born in the United States, and this data is stored in a CSV\\nfile called us_baby_names.csv.\\nThe first thing I'm going to do is import the Pandas library.\\n\\nAnd I do this by writing the keyword import followed by pandas.\\nAnd I'll also add in as pd.\\nNow, this means that going forward in this notebook,\\nI need to use pd to refer to pandas.\\nI'll go ahead and run the cell.\\nAnd now, I want to read in a CSV file called us_baby_names.csv\\nthat I have stored in the same directory that this notebook\\nis stored.\\nSo I'm going to use the read CSV method from the Pandas\\nlibrary to do that.\\nSo I'll type in pd.read_csv ()\\nand now I'm going to type in the name of the CSV\\nfile as a string 'us_baby_names_csv'.\\n\\nAnd I'll go ahead and run the cell.\\nAnd now you can see the data.\\nAnd this object here is called a pandas DataFrame.\\nNow, I can create a variable called us_babies and assign\\nit to this data frame,\\nso whenever I want to access this data,\\nall I have to do is refer to that variable.\\nSo I'll go ahead and run the cell.\\nSo now, if I want to see the data again,\\nall I have to do is type in us_babies into this cell\\nand run. There,\\nthat's my data set.\\nAs you can see, the columns are labeled ID, name,\\nyear, gender, and count, and the rows are labeled 0, 1, 2\\nall the way till 1,825,433.\\n\\nNote that gender in this data set refers to the gender assigned\\nto these babies at birth.\\nNow, the labels of a data frame are called the indices of\\nthe data frame,\\nand they make data manipulation easier.\\nNotice that each row represents a distinct pair of baby\\nname and birth year.\\nLet's use pandas in this data set to answer the following question.\\n\\\"What were the five most popular baby names in 2014 in the US?\\\"\\nFirst, I'll show you how to access the year column from the\\nus_babies data frame.\\n\\nAnd I do this by writing us_babies [year].\\nNote that the name of a column is always expressed as a string.\\nI'll go ahead and run the cell. There,\\nand this object is called a Pandas series.\\nNow, I want to slice out the rows of the us_babies data frame\\nwith the year 2014.\\nIn other words, I only want the rows of the US\\nbabies data frame that correspond to names that were given to babies\\nborn in the year 2014.\\nSo I first want to create a series that contains true for each row I\\nwant to keep and false for each row I want to drop.\\n\\nI do this by comparing each year in US babies with 2014.\\nSo I'll write us_babies [year] == 2014.\\nAnd I'll run the cell. There,\\nas you can see, each year from the year column was\\ncompared to 2014 and I got a series of true and false.\\nJust a note, these true and false values are known as booleans\\nin programming.\\nNow I want to use loc.\\nLoc is a method that allows you to access a specific group of rows\\nand columns from a pandas data frame.\\n\\nLoc can take in a series of Booleans such as the one I\\njust created as an input.\\nSo I'll pass in the series of Booleans as a first input to loc,\\nand then the second input will be a colon.\\nI'll type in us_babies.loc[us_ babies[Year] == 2014, :]\\nNow, this part means that I want to access only the rows where\\nthe year is 2014.\\nAnd the colon here denotes that I want to access all the columns for\\nthe rows were years 2014.\\n\\nNow, I'll go ahead and run the cell.\\nThere we go.\\nNow, I'll create a variable named us _babies_2014 and assign\\nit to this data frame,\\nand I'll run the cell.\\nSo now, I can use us_babies_2014 to access the data frame\\nI just created.\\nThe next step is to sort the rows in descending order by count.\\nTo do this, I'll use the sort values method.\\nI'll pass in count as a first input and ascending equals\\nfalse as a second input.\\n\\nAnd that second input is what allows me to sort in\\ndescending order.\\nSo I'll type in us_babies_2014.sort values (Count, ascending = False)\\nand I'll run the cell.\\nAnd I got a new data frame.\\nThat's because the method sort values returns a new data frame.\\nNow, it'll be convenient to store this new data frame in a variable\\nso that I can refer to the variable whenever I want to\\naccess the data frame.\\n\\nSo I'll go ahead and create a variable.\\nI'll call it \\\"sorted_us_2014\\\"\\nand I'll assign it to this data frame, and I'll run the cell.\\nSo now, when I type in, \\\"sorted_us_2014\\\" and run the cell,\\nI have access to the sorted data frame.\\nFinally, I want to slice out the first five rows of sorted_us_2014\\nI loc is a method that works similarly to loc but takes in\\nnumerical indices instead of the names of columns as inputs.\\nSo I'll go ahead and type in sorted_us_2014.i loc [0:5]\\nand I'll run the cell. There.\\n\\nSo as you can see, the notation 0:5\\nallows me to access the first five rows of the sorted_us_2014\\ndata frame.\\nAnd according to the data set I'm working with,\\nEmma, Olivia, Noah, Sophie, and Liam are the five most\\npopular baby names in 2014 in the United States.\\nNow you know how to access specific columns,\\nselect subsets, and sort the rows of a data set.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4493805\",\"duration\":149,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Interpreting tabular data\",\"fileName\":\"4457428_en_US_04_03_MM24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Tabular data manipulation and drawing conclusions from data is a crucial component of data science. After watching this video, you'll be able to start gathering insights from your dataset. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4851332,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Imagine I have a data set that has information on baby names for\\neach state in the US.\\nNotice that each row represents a distinct set of baby name,\\nbirth year, and birth state.\\nNow, let's use pandas and the states underscore babies data set\\nto answer the following question.\\nWhat were the five most popular baby names in 2014 in California?\\nIn this lesson, I'll show you how to do this\\nusing the Pandas library.\\nAs a reminder, this is what the state's babies data\\nframe looks like.\\n\\nNow, the first thing I want to do is slice out the rows\\nfor the year 2014.\\nSo I'll use the loc method.\\nThe first input indicates that I want only the rows of the states\\nbaby's data frame that correspond to names of babies born in various\\nstates in the year 2014.\\nThe second input indicates that I want all columns for the\\naforementioned rows. And since the loc method returns\\na new data frame, I'll go ahead and store it in\\na variable which I'll name,\\n\\\"states_babies_2014.\\\"\\nAnd I'll run the cell now.\\n\\nNow, I'll type \\\"states_babies_2014\\\" and run the cell.\\nThere you go.\\nThe next thing I want to do is slice out the rows of the states\\nbabies 2014 data frame that correspond to the state,\\nCalifornia.\\nTo do this, I'll use the loc method again.\\nThe next step is to sort the rows of the CA baby's 2014 data frame\\nin descending order by count.\\n\\nTo do this, I'll use the sort values method.\\nFinally, I want to access the first five rows of the sorted\\nCA 2014 data frame.\\nTo do this, I'll use the i loc method.\\nSo according to this data set, Sophia, Noah, Isabella,\\nJacob and Emma are the five most popular baby names in\\n2014 in California.\\n\\nYou can use these techniques to filter and sort data\\nframes on your own.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4489775\",\"duration\":353,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Gathering insights\",\"fileName\":\"4457428_en_US_04_04_MM24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"The goal of data science is to identify and answer specific questions. After watching this lesson, you'll be able to evaluate what questions you want to answer and what types of questions are ideal for your scenario.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11635360,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Let's use pandas and the states_babies data set to\\nanswer the following question. \\\"What were the most popular female and\\nmale baby names in California in each year?\\\"\\nNow, I'll show you how to do this using the Pandas library.\\nAs a reminder, the loc method allows you to access a specific\\ngroup of rows and columns from a Pandas data frame,\\nand the method receives two inputs.\\nThe first input describes which rows to take from the data frame,\\nand the second input describes which columns to take from the\\ndata frame. And loc returns\\na new data frame containing the rows and columns you specified\\nSo I'll type in ca_babies = states_babies.loc [state_babies['state'] = = CA , :]\\nAnd I'll go ahead and run the cell.\\n\\nThis specifies that I only want the rows of the states babies data\\nset where state is California and the colon specifies that I want\\nall the columns for those rows.\\nSo now I'll type in ca_babies and hit \\\"Run.\\\"\\nThere we go.\\nNow, I'll show you how to group the CA babies data frame by both\\nyear and gender.\\nTo do this, I'll use the group by method.\\nNow, the group-by-method allows you to group the data in a data\\nframe based on the criteria you specify.\\n\\nIn this case, the criteria, I'll specify\\nare a list of the names of the columns by which I want\\nto group my data.\\nI want to group my data by year and gender.\\nSo I'll pass in a list containing year and gender as input\\nto the group by method.\\nSo I'll type in ca_babies.group by(['Year' , 'Gender'])\\nAnd note that in programming, the items of a list are enclosed\\nby square brackets and items are separated from each\\nother by commas.\\n\\nNow, when you call the group by method on a data frame,\\nit does not return a new data frame.\\nRather, it returns an object called a data frame\\ngroup by object.\\nAnd this object contains information about the groups\\nthat were created.\\nSo I'll use another method that operates on those groups to create\\na new data frame that suits my needs.\\nAnd this method is called the AG method.\\nNow the AG method can receive an aggregation function as input and\\nuse it to aggregate the groups of data.\\nAG applies the aggregation function to every column\\nin each group,\\nand each column of a Pandas data frame is a Pandas series.\\n\\nIn my case, I've grouped my data on year and gender,\\nand I need an aggregation function that takes in a Pandas series as\\ninput and finds the most popular baby name in that series.\\nI'll define this function myself, so I'll create a cell.\\nOver here,\\nI'll start with the keyword def, followed by the name of the\\nfunction and I'll go with popular.\\nNext is parentheses and in the parentheses I'll type in s,\\nwhich will represent the input that this function receives.\\n\\nS will be a Pandas series containing baby names.\\nNext is a colon and I'll hit \\\"Enter\\\" to get to the next line.\\nNow, it would be easier if this function receives baby names in\\norder of highest count to lowest count.\\nSo what I'll do is I'll sort CA babies before I call the group\\nby method in the AG method.\\nI'll use the sort values method to do that.\\nSo I'll type in sort_values ()\\nI'll type in \\\"count\\\" since I want to sort the data based on count.\\n\\n, ascending = false because I want to sort\\nin descending order.\\nAnd then I'll have a period there.\\nNow, I'll go back to the aggregation function\\nI was defining.\\nwhich is a Pandas series\\ncontaining baby names in order of highest count to lowest count.\\n\\nAnd it returns the most popular name in S.\\nWhat I just wrote is called a multiline comment.\\nIt's not read by the computer.\\nIt's just for me and you.\\nNow, I'll use the i loc method.\\nI'll write return s.i loc[0].\\nThis means that I am returning the item in S that has index zero.\\nIn other words, I am returning the first item in S.\\nThis item is the most popular name in S because S is ordered by count\\nsuch that the first item in S corresponds to the highest count\\nand the last item in S corresponds to the lowest count.\\n\\nI'll go ahead and run this cell.\\nNow, I'll call the AG method,\\nand I'll pass in the aggregation function that I just defined,\\nwhich I named popular.\\nAlso, I'll create a variable named CA grouped and assign it to the\\ndata frame that I'll get from doing this.\\nI'll go ahead and run the cell now.\\nNow, I can access the group data using CA grouped,\\nso I'll type in \\\"CA grouped,\\\"\\nand I'll hit run. There,\\nthis data frame contains the most popular female and male baby names\\nfor each year in California.\\n\\nNow, I want to point something out regarding the order of my code.\\nI defined the function popular in this cell,\\nand then I grouped the CA babies data set in the next\\ncell using popular.\\nThis is because I need to establish what popular means and\\nwhat popular does before I use popular.\\nNow you know how to group a data set and aggregate the group data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4490795\",\"duration\":165,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Answering specific questions\",\"fileName\":\"4457428_en_US_04_05_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5308486,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Now, let's use the Pandas Library and the us_babies\\ndataset to answer the following question, \\\"How frequently does your\\nfirst name occur across the years in the US among baby names?\\\"\\nFor this question, you'd want to go back to working\\nwith the us_babies dataset,\\nsince this question does not address specific states.\\nAs a reminder, this is what the US babies data frame looks like.\\nFirst, I want to slice out the rows of the US babies data frame\\nthat contain my name.\\nTo do this, I'll use the loc method.\\n\\nAs a reminder, loc allows you to access a specific set of rows and\\ncolumns from your data set.\\nSo go ahead and type in us_babies.loc[us_babies['Name'] = = Lavinya , :]\\nNow, this part indicates that I only want the rows of US babies\\nwhere name is Lavinya and the colon indicates that I want all\\nthe columns for the aforementioned rows.\\n\\nNow, I'll go ahead and store the data frame that loc returns here\\nin a variable named us_lav.\\nAnd I'll run the cell.\\nNow, when I type in us_lav and run,\\nI have access to that data frame.\\nNow, I want to create a horizontal bar plot that displays the counts\\nof my first name over the years.\\nTo do this, I'll use the plot.barh method. plot.barh allows\\nyou to create a horizontal bar plot based on the data in\\nyour pandas dataframe.\\nA horizontal bar plot presents quantitative data with rectangular\\nbars with lengths proportional to the values that they represent.\\n\\nSo I'll go ahead and type in us_lav.plot.barh(x = 'Year' , y = 'Count')\\nNow, this part indicates that year will be displayed on the x-axis,\\nand this indicates that count will be displayed on the y-axis.\\nNow, we'll go ahead and run the cell.\\nThere we go.\\nThis plot shows the counts of the baby name Lavinya being compared\\nacross 1994-2014 in the US.\\n\\nBy observing this plot, I can tell how frequently my name\\noccurred across the years among baby names in the United States.\\nYou can read more about the plot.barh method by following the\\nlink in the resource file.\\nAnd that's it.\\nNow you know how to set the index of a data set according to your\\nneeds and construct a horizontal bar plot of your data.\\nNow that you know some of the key methods in the Pandas Library for\\ntabular data manipulation, go ahead and try it for yourself.\\n\"}],\"name\":\"4. Structuring Your Tabular Data\",\"size\":38685402,\"urn\":\"urn:li:learningContentChapter:4485936\"},{\"duration\":561,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4489774\",\"duration\":51,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Defining exploratory data analysis\",\"fileName\":\"4457428_en_US_05_01_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Statistical data types, including numerical and categorical data, are at the core of most data science operations. After watching this video, you'll be able to recognize the differences between these data types and why each of them is significant. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1544505,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Exploratory data analysis, also known as EDA,\\nis a major stage of the data science lifecycle.\\nThe goal of EDA is to deeply understand the data you\\nhave in front of you.\\nIn EDA, you visualize and transform the data so that you\\ncan pick up on patterns, issues, and anything interesting\\nin the data.\\nIn this process, you'll find out what questions you\\nwant to answer using the data.\\nAnd during EDA, it's important to not make\\nassumptions about the data and to be open to the possibilities of\\nwhat you'll find. When conducting EDA.\\n\\nYou would examine the statistical data types present\\nin your data set,\\nas well as the key properties of your data.\\nThis process helps you understand the data and determine how\\nrelevant the data is to the question you're trying to answer\\nor the problem you're trying to solve.\\nAnd now you know what exploratory data analysis entails.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4492757\",\"duration\":202,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Recognizing statistical data types\",\"fileName\":\"4457428_en_US_05_02_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"EDA involves determining the key properties of the data you have. After watching this video, you'll be able to hone in on what the key properties of data are and how you can identify these properties within your dataset.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5180537,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"At the beginning of exploratory data analysis,\\nit's important to understand what kind of data lies in your data set.\\nIn the context of tabular data,\\nthere are three major statistical data types to keep in mind.\\nThe first is nominal data, which means data that has\\nno inherent order.\\nExamples of this data include political party affiliation\\nsuch as Democrat,\\nRepublican, independent, et cetera, or computer operating system\\nsuch as Windows, macOS, Linux, et cetera,\\nas well as things like gender and languages spoken.\\n\\nNext, is ordinal data, and this means data that falls\\ninto ordered categories.\\nExamples of this include clothing size such as small,\\nmedium, large, extra large or highest level of education,\\nsuch as high school,\\nundergrad, master's, Ph.D, or even a Yelp rating on a\\nscale from 0-5 stars.\\nThe third main data type is numerical data,\\nwhich means data that consists of amounts or quantities.\\nExamples of this include things like height,\\nweight, price, distance, blood pressure.\\n\\nNow, identifying the statistical data types that are present in\\nyour data set helps you make informed choices when you conduct\\nfurther analysis on the data.\\nThe data types in your data set correspond to specific operations\\nyou can perform on the data in a meaningful way.\\nIn other words, the type of data you have\\ndetermines what you can do with that data.\\nFor example, say you have a data set of job titles,\\nsalaries, and highest education levels of employees in a company.\\nEach row corresponds to an individual and each individual is\\nassigned a unique ID number and say the highest level of education\\nis represented by a number using a scale from 1-5,\\nwhere one means high school,\\ntwo means undergrad, three means masters and\\nfour means Ph.D.\\n\\nThe ID numbers and job titles are both nominal data.\\nThe salaries are numerical data, and the highest levels of\\neducation are ordinal data.\\nComputing the mean of the values in the salary column\\nis meaningful.\\nYou'll find out the average salary of an employee at the company.\\nHowever, computing the mean of the values in the highest level of\\neducation column is not meaningful.\\nFor example, you could get a mean of 2.7, but 2.7 doesn't\\ntell you much.\\nHowever, within the highest education level column,\\ncomparing one value to another, value can provide useful\\ninformation.\\n\\nFor example, say one person has a one, meaning a high school\\nbackground,\\nand another person has a two, meaning an undergrad background.\\nWell, you could say that the second person has a higher\\neducation level than the first person.\\nNow, within the ID number column, computing\\nthe mean of the values is not meaningful.\\nThe mean of the ID numbers doesn't tell you much.\\nNow, let's say you compare the ID number located in one row to the\\nID number located in another row.\\nSay these ID numbers are 103 and 105,\\n105 being larger than 103\\ndoesn't tell you much about the employees represented\\nby these rows.\\n\\nHowever, the fact that 103 and 105 are distinct numbers tells you\\nthat these rows represent two distinct employees in the company.\\nSo when you take time to understand the statistical data\\ntypes that are present in your data set,\\nyou'll have a better idea of what type of analysis you can\\ndo with your data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4485933\",\"duration\":308,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Distinguishing properties of data\",\"fileName\":\"4457428_en_US_05_03_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11900190,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Exploratory data analysis, also known as EDA,\\ninvolves determining the key properties of the data you\\nhave in front of you.\\nNow, I'll go over the main properties you investigate\\nwhen conducting EDA.\\nAsk yourself these questions about each property.\\nThe first main property is known as granularity,\\nwhich is what each record in the data represents.\\nConsider asking how fine or coarse is the data.\\nThe next property is scope.\\nThe scope of the data set refers to the coverage of the data set in\\nrelation to what you're interested in analyzing.\\n\\nWhat does the data describe?\\nDoes the data cover the topic you're interested in?\\nAfter that, there's temporality.\\nThis refers to how the data is situated in time and specifically\\nto the date and time fields in the data set.\\nWhen was the data collected?\\nFinally, there's faithfulness.\\nHow accurately does a data describe the real world?\\nShould you trust this data?\\nFor example, say you have data regarding shows and movies that\\nwere on Netflix from 2015-2021 globally.\\n\\nNow, let's identify the key properties of this data.\\nI'll start with granularity.\\nFirst, I'll import the necessary packages,\\nload the data into a data frame, get the shape of the data frame,\\nand display the first few rows.\\nLet's call this data set A.\\nIn this data set, each record represents a single\\nshow or movie that was available to stream on Netflix.\\nNow, let's say I group the Netflix data by director and aggregate the\\ngroup data using the count method.\\n\\nThis counts up the show or movie titles for each director so that\\nin the resulting data frame, each row represents a\\ndistinct director.\\nLet's call this data set B. In this data set,\\neach record represents a single director.\\nNote that within the data for a director,\\nthere can be multiple titles.\\nData Set B has a coarser granularity than data set A.\\nThe granularity of your data is important because it determines\\nwhat kind of analysis you can perform on your data.\\n\\nUsually, finer granularity is safer than coarser granularity.\\nIn most cases, you can use grouping and pivoting to transform\\nthe granularity of your data from fine to coarse.\\nHowever, there aren't as many tools to transform data\\nfrom coarse to fine.\\nNext, I'll discuss the scope of the Netflix data.\\nThis data set contains shows and movies that were added to Netflix\\nbetween 2015 and 2021.\\nIf you're interested in shows, in movies that were added to a\\ndifferent streaming platform, however, this data set would not\\nhave the appropriate scope for that goal.\\n\\nIn general, a larger scope is more useful than a smaller scope,\\nsince you can transform a larger scope into a smaller scope through\\nfiltering some things out.\\nBut a lot of times, you can't go from a smaller scope\\nto a larger scope.\\nFor example, if you had a data set containing shows and movies from\\nonly the United States, it would not be possible to infer\\ndetails about shows and movies outside of the US.\\nHowever, with this data set of shows and movies from various\\ncountries around the world, you could filter it down and get a\\nsubset to examine shows from one specific country if you wanted.\\n\\nAnother property I can look at is temporality. In this data set, the\\ndate_added column represents when a title\\nwas added to Netflix.\\nWhile the release_year column represents when the title\\noriginally came out. The date_added column\\ncontains dates and day-month-year format while the release year\\ncolumn contains year only.\\nThere are various formats in use around the world for\\nexpressing dates, and it's important to keep in mind\\nthese differences when analyzing data.\\n\\nAlso to consider how the data is situated in time,\\nyou could examine the release_year column.\\nHere, I've sorted the release years in ascending order and it seems\\nthat they span from 1925-2021.\\nSo there are two different dimensions to how this data\\nis situated in time.\\nOne is through date_added and the other is\\nthrough release_year.\\nThe release years go back farther in time than the dates of when the\\nitems were added to Netflix.\\n\\nAnother property to consider is faithfulness.\\nA data set is considered faithful if you think it accurately\\ncaptures reality.\\nUsually, an unfaithful data set may contain unrealistic or incorrect\\nvalues, dates in the future that represent events in the past,\\nnon existent locations,\\nnegative values that represent amounts or quantities,\\nor extreme outliers.\\nIt might contain inconsistencies such as an age column and a\\nbirthday column that contradict each other.\\nIt could have hand-entered data with misspellings or columns not\\nbeing in the right spot due to being shifted over or even data\\nfalsification with email addresses that seem fake, repeated\\ninformation,\\nespecially repeated use of uncommon values.\\n\\nAfter you understand the properties of your data,\\nyou will make more informed choices when conducting further\\nanalysis on that data.\\n\"}],\"name\":\"5. Using Exploratory Data Analysis\",\"size\":18625232,\"urn\":\"urn:li:learningContentChapter:4489777\"},{\"duration\":329,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4491768\",\"duration\":82,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Explaining data cleaning\",\"fileName\":\"4457428_en_US_06_01_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Before you dive in and start cleaning your data, you'll want to gain some baseline information to help you navigate this process. After this lesson, you'll be able to ask questions to help better understand your data and how it was generated so you can most effectively clean it. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2305532,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Data cleaning is a process of combing through raw data and\\ntransforming it so that it's conducive to analysis\\nthat follows.\\nData cleaning can be time-consuming,\\nbut it's a critical stage of the data science lifecycle.\\nData cleaning often addresses missing values,\\nthe formatting of values, the structure of the data overall,\\nextracting information from complex values,\\nunit conversion, interpretation of magnitudes,\\nand so on.\\nFor example, say you have a data set regarding the final exam grade\\nof a particular class from the spring 2019 semester.\\n\\nAnd let's say you're interested in finding out the median grade.\\nLet's say some of the students exams were lost after they were\\ncollected and thus were not graded,\\nand these missing grades were replaced by zeros in the data set.\\nNow, you would not be able to accurately determine the median\\ngrade without addressing the zeros that replace missing grades.\\nThis is an example of a situation where you'd need data cleaning to\\naddress the incorrect values.\\nOther examples of data that would need data cleaning are missing\\ndata, misspellings,\\nrows that are duplicated, dates and addresses expressed in\\ninconsistent formats, and potentially outliers\\nor extreme values.\\n\\nData cleaning is an important process that data scientists\\nperform in order to make the data suitable for further analysis,\\nas well as drawing conclusions from the data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4490794\",\"duration\":247,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Questions to guide data cleaning\",\"fileName\":\"4457428_en_US_06_02_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8849678,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"When you begin the process of data cleaning,\\nthere are a few important questions to ask about the data.\\nFirst, are there missing values in the data set?\\nSecond, are there duplicate entries in the data set?\\nThird, are data points represented by the appropriate data types?\\nFor example, say you have data regarding shows and movies that\\nwere on Netflix from 2015-2021 globally.\\nIn this demo,\\nI'll start with the data loaded into a Pandas data frame.\\nFirst, let's get the shape of the data frame.\\n\\nThis tells me that there are 7,787 rows and 11 columns.\\nNow, let's check if there are missing values in this data.\\nYou can use the isna method followed by the sum method to get the\\nnumber of missing values in each column of the data frame.\\nThe output shows that there are missing values in the director,\\ncast, country, date added, and release your columns.\\nYou can also get the total number of rows that contain\\nmissing values.\\n\\nSo there are 3,631 rows with missing values.\\nAnd this indicates that the missing values make up about\\n46% of the data.\\nSince the majority of the rows do not have missing values,\\nyou can proceed with dropping the rows that have missing values.\\nAfter dropping those rows, you can confirm the shape\\nof the data now.\\nThe number of columns is the same as before,\\nbut now there are 4,808 rows.\\n\\nThis is still a substantial amount of data to work with.\\nNext, you can investigate for duplicates.\\nTo do this, you can use the duplicated method from pandas.\\nThis method returns a boolean, either true or false for each row\\nindicating whether that row is a duplicate entry or not.\\nAdding the sum method to the end allows you to get the total number\\nof rows that are duplicates.\\nSo there are zero duplicates in this data set.\\n\\nWhen working with date and time fields in a data set,\\nit's important to check the data type of those fields and ensure\\nthat they are in a meaningful and useful format.\\nThe D type attribute can be used to get the data type.\\nAs the output shows here, the date added column contains\\nvalues of the object data type denoted by the O.\\nThe object data type in pandas is a generic data type for a series\\nand typically means the values in the series are strings.\\nTo confirm, you can use the type function to get the data type of\\nthe first entry in the date added column.\\n\\nThe output is str, which confirms that the data type\\nof the first entry in the date added column is string.\\nRepresenting the dates that shows or movies were added as strings\\nis not that helpful.\\nFor example, you may want to compare when one title was added\\nto when another title was added. Using the pd.2\\ndate time method can help you convert this data into date time\\nformat so that you can make such comparisons meaningfully.\\nFinally, identify the data type of the release year column.\\n\\nThis shows that the release years are represented as integers,\\nwhich is the appropriate data type.\\nSince they're integers, you can easily compare the release\\nyear of one title to the release year of another title.\\nNow, cleaning every aspect of a data set may take too long,\\nbut on the other hand, not cleaning a data set at all\\nwould lead to drawing inaccurate conclusions from the data.\\nSo it's important to try to find a balance when cleaning data.\\nAlso, keep in mind that the decisions you make during data\\ncleaning impact all the analysis you perform afterwards.\\n\\nSo if there's a column you choose not to clean in your data set,\\nhave caution when using that column in analysis.\\nAlso, it's helpful to explicitly keep track of what changes you\\nmade to your data set during data cleaning,\\nso you can refer back to your notes when doing analysis.\\nNow you know a few important approaches to data cleaning.\\n\"}],\"name\":\"6. Cleaning Your Data\",\"size\":11155210,\"urn\":\"urn:li:learningContentChapter:4485937\"},{\"duration\":867,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4490793\",\"duration\":62,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Demystifying data visualization\",\"fileName\":\"4457428_en_US_07_01_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Different types of visualization correspond to different types of data. After watching this video, you'll be able to recognize how data visualization is best completed when you're working with qualitative datasets. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2224894,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Data visualization is an essential tool for data science.\\nVisualization brings data to life.\\nOften, a good data visualization can convey trends and anomalies in\\nthe data more efficiently than a written description.\\nAlso, data visualization can be a great way to communicate your\\npredictions and conclusions to other people,\\nand you'll need to use some computational tools to create\\ndata visualizations.\\nTwo useful visualization tools that I'll be introducing in this\\ncourse are Python's Matplotlib and Seaborn libraries.\\n\\nMatplotlib is a library in Python that allows you to create two\\ndimensional plots of your data.\\nYou can find documentation for Matplotlib in the resource file.\\nAnd Seaborn is a library in Python that's based on Matplotlib and\\nallows you to create multidimensional plots and more\\nadvanced visualizations of your data.\\nYou can check out the documentation for Seaborn\\nin the resource file.\\nNow you know what data visualization is and a couple of\\ntools that help you visualize your data in Python.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4491767\",\"duration\":317,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualizing your qualitative data\",\"fileName\":\"4457428_en_US_07_02_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Different types of visualization correspond to different types of data. After watching this video, you'll be able to recognize how data visualization is best completed when you're working with quantitative datasets. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9698737,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Usually different types of charts are used to visualize different\\ntypes of data.\\nand its subtypes are nominal data and ordinal data.\\nA reminder that nominal data is data that has no inherent order,\\nwhile ordinal data is data that falls into ordered categories.\\nA bar chart is one of the most commonly used charts to visualize\\nqualitative data.\\nIn this lesson, I will show you how to create\\nbar charts in Python.\\nFor example, say that you have a data set on Airbnb listings\\nin New York City, New York for 2019,\\nlast updated on August 12th, 2019.\\n\\nFirst, I'll import the Python libraries that I'll need,\\nso I'll go ahead and start with import pandas as pd.\\nImport matplotlib.pyplot as plt.\\nImport seaborne as sn.\\nAnd I'll run the cell.\\nNote that going forward, when I code in this notebook,\\nI need to use pd to refer to pandas, plt to refer to\\nmatplotlib.pyplot and sn to refer to Seaborn.\\n\\nNext, using the read CSV method from the Pandas Library.\\nI'll read in the CSV file Airbnb NYC 2019.csv which is located\\nin the same directory as this notebook and I'll store the pandas\\ndata frame that gets returned in a variable named listings.\\nSo I'll go ahead and type out listings = pd.read_csv('Airbnb_NYC_2019.csv')\\nand I'll run the cell.\\n\\nNow I'll type out listings and I'll hit \\\"Run.\\\"\\nThere we go.\\nThis is my data set.\\nNow, let's say I want to create a bar chart that displays the count\\nof Airbnb listings in each neighborhood group of New York\\nCity from the listings data frame.\\nTo do this, I'll use the count plot method from the\\nSeaborn library\\nfollowed by the show method from Matplotlib.pyplot.\\nSo I'll type in sne.count plot (x = 'neighborhood_group', data = listings)\\nfollowed by plt.show().\\n\\nNow, the count plot method helps me create the bar chart.\\nSetting the data parameter to listings indicates that I want to\\ndisplay data that comes from the listings\\ndata frame. And setting X to neighborhood group indicates that\\nI want to display the data that comes from this particular column.\\nNow, I'll go ahead and run the cell.\\nThere we go.\\nThat's a bar chart.\\nAnd from this bar chart, it appears that Manhattan is a\\nneighborhood group with the highest count of Airbnb listings\\nin New York City in 2019.\\nNow, let's say I want to create a bar chart that displays the\\naverage price of Airbnb listings in each neighborhood group of New\\nYork City from the listings data frame.\\n\\nTo do this, I'll use the bar plot method from the Seaborn library,\\nfollowed by the show method from Matplotlib.pyplot.\\nI'll start with sn.barplot (x = 'neighborhood_group', y = price, data = listings) followed by plt.show()\\nThe bar plot method allows me to create a bar chart and the show\\nmethod allows me to show the bar chart that's created.\\n\\nI set the parameter data to listings to indicate that I want\\nto display data that comes from the listings data frame.\\nI set the x parameter to neighborhood group because\\nneighborhood group is going to be displayed on the x axis and I set\\nthe parameter y to price because price is going to be displayed on\\nthe y axis, and I'll go ahead and run the cell.\\nThere we go.\\nNow, the black lines that you see through the middle of each bar\\nmark confidence intervals that were generated by the\\nbar plot method.\\nNow, confidence intervals are an important concept,\\nbut it's not in the scope of this video,\\nso I will not go into detail about it at this particular moment.\\n\\nSo let's say I want to recreate the bar chart above without\\nthose black lines.\\nTo do this, I'll use the bar plot method again.\\nAnd this time, I'll also include a parameter named CI,\\nwhich I'll set to false.\\nSo I'll go ahead and type in sn.barplot (x = 'neighborhood_group', y = price, data = listings, ci = false) followed by plt.show()\\nand I'll run the cell.\\n\\nThere we go.\\nThe black lines are gone.\\nAnd from this bar chart, it appears that Manhattan is a\\nneighborhood group with the highest average price for Airbnb\\nlistings in 2019 in New York City.\\nNow you know how to create different types of bar charts to\\nvisualize qualitative data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4487770\",\"duration\":488,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualizing your quantitative data\",\"fileName\":\"4457428_en_US_07_03_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14657549,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Two types of charts that are commonly used to visualize\\nquantitative data are histograms and scatter plots.\\nNote that quantitative data is another term for numerical data,\\nwhich means data that consists of amounts or quantities.\\nIn this lesson, I'll show you how to create\\nthese charts in Python.\\nFor example, say that you have a data set on Airbnb listings\\nin New York City for 2019, last updated on August\\n12th of that year.\\nFirst, I'll import the Python libraries that I'll need,\\nso I'll start with import pandas as pd,\\nimport matplotlib.pyplot\\nas plt,\\nimport numpy as np and I'll run the cell.\\n\\nNote that going forward, when I code in this notebook,\\nI need to use pd to refer to pandas, plt to refer to\\nmatplotlib.pyplot and np to refer to Numpy.\\nNow, Matplotlib.pyplot is a collection of functions that allow\\nyou to create figures and plots and make changes to\\nthem in Python.\\nNumpy is a useful library for scientific computing in Python and\\ncontains functions that allow you to create data efficiently.\\nNext, using the read CSV method from the Pandas library,\\nI'll read in the CSV file Airbnb NYC 2019.csv which is located\\nin the same directory as this notebook and I'll store the pandas\\ndata frame that guests return in a variable named listings.\\n\\nSo I'll go ahead and type in listings = pd.read_csv('Airbnb_NYC_2019.csv').\\nAnd I'll go ahead and run this cell.\\nNow, I'll type in listings and run this cell.\\nThere we go.\\nThis is my data set.\\nNow, I'll use the hist method for Matplotlib.pyplot to create a\\nhistogram that displays a distribution of the quantitative\\ndata in the price column of the listings data frame.\\n\\nSo I'll type in plt.hist(listings ['Price'])\\nand on this line, I'll use the x label method to label the x-axis\\nwith price in US dollars.\\nSo I'll type in plt.x label ('Price(in US dollars)').\\nAnd on this line, I'll use the show method to show the histogram\\nthat gets created.\\n\\nSo I'll type in plt.show() In the histogram that\\nI'll get from this cell, the prices will be grouped into\\ncontiguous intervals called bins.\\nNow, I'll go ahead and run this cell. There,\\nthat's a histogram.\\nHowever, in this histogram, it's difficult to really see where\\nthe data lies with respect to the bins.\\nSo I'll recreate the histogram using the hist method and this\\ntime, I'll include the optional bins parameter,\\nwhich I'll set to a specific range of numbers and I'll create the\\nrange of numbers using the arange method from the Numpy library.\\n\\nSo I'll type in plt.hist(listings ['price'], bins = np.arange (0, 1100, 40))\\nThe first number I passed into the arange method represents the\\nleft end of the first bin.\\nThe second number represents the right end of the last bin.\\nAnd the third number represents the step size or the\\nwidth of each bin.\\n\\nOn the next line, I'll type plt.x label ('price in US dollars').\\nAnd on the next line, I'll type in plt.show().\\nAnd I'll run the cell.\\nThere we go.\\nNow, it's easier to see where the data lies with respect\\nto the bins.\\nAnd it appears that in New York City in 2019,\\na lot of the prices of Airbnb listings lie between around\\n$50 to around $500.\\n\\nNow, let's talk about scatter plots.\\nScatter plots are usually used to compare two sets of\\nquantitative data.\\nLet's say I want to create a scatter plot to compare the prices\\nof Airbnb listings and the number of reviews for those listings.\\nTo do this, I'll use the scatter method from matplotlib.pyplot.\\nSo I'll type in plt.scatter(x = listings ['price'], y = listings ['number of reviews'])\\nOn the next line,\\nplt.xlabel ('price').\\n\\nplt.ylabel('number of reviews')\\nfollowed by plt.show.\\nWhen I called the scatter method, I set the x parameter to listing\\nsquare brackets price.\\nNow, this indicates that I want price to be displayed\\non the x-axis,\\nand this indicates that I want number of reviews to be displayed\\non the y-axis.\\nI use the x-label method to label the x-axis and I use the y-label\\nmethod to label the y-axis.\\n\\nNow I'll go ahead and run the cell.\\nThere,\\nthat's a scatter plot.\\nNow, let's say I want to restrict the x-axis so that the scatterplot\\nonly goes up to a price of 1,100.\\nTo do this, I'll recreate the scatter plot using the\\nscatter method.\\nAnd this time I'll also use the x lib method for matplotlib.pyplot.\\nSo I'll type in plt.scatter(x = listings ['price'], y = listings ['number of reviews'])\\nplt.xlabel('price')\\nplt.ylabel('number of reviews')\\nplt.xlim(0, 1100)\\nfollowed by plt.show()\\nand I'll go ahead and run this cell.\\n\\nThere we go.\\nNow, let's say I want to decrease the size of the points\\non the scatter plot.\\nTo do this, I can recreate the scatter plot using the\\nscatter method.\\nAnd this time I'll also include the size parameter\\nwhich is named s,\\nand I'll set it to 5.\\nSo I'll type in plt.scatter(x = listings ['price'], y = listings ['number of reviews'], s = 5)\\nAnd on the next line plt.xlabel('price') plt.ylabel('number of reviews')\\nplt.xlim(0, 1100) followed by\\nplt.show().\\n\\nAnd I'll run the cell. There,\\nthe points on this scatter plot are a bit smaller and this plot\\nis a little easier to look at.\\nAt first glance of this plot, I notice a trend.\\nIt appears that listings with lower prices have more reviews.\\nThis leads me to ask the following question, \\\"Is there an association\\nbetween the price and the number of reviews for listings?\\\"\\nThis is just one example of a type of question that can come up when\\nyou're visualizing your data, and you'll want to conduct further\\nanalysis to investigate this question.\\n\\nSo visualizing your data helps you observe trends,\\nwhich leads you to ask questions which then informs the kind of\\nfurther analysis you perform.\\nNow you know how to use histograms and scatter plots to visualize\\nyour quantitative data.\\n\"}],\"name\":\"7. Using Data Visualization\",\"size\":26581180,\"urn\":\"urn:li:learningContentChapter:4489778\"},{\"duration\":1480,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4491766\",\"duration\":43,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Defining inference\",\"fileName\":\"4457428_en_US_08_01_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Hypothesis testing is a helpful method you can use to identify if the results you're seeing in the data are meaningful. After watching this video, you'll be able to define a hypothesis test, recognize how they're used, and the core aspects of setting up a hypothesis test.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1582438,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Data scientists use inference to quantify how certain they are\\nthe trends they see in their data will be found in new data,\\nand they use inference to draw conclusions about a population\\nusing their data set.\\nFor example, inference is a useful process in election forecasting,\\npredicting the test scores of students in future exams based on\\ntheir test scores in past exams and more.\\nA couple of methods that are essential to inference are\\nhypothesis tests and confidence intervals,\\nand these methods involve resampling which enables you to\\ngeneralize the conclusions you draw so they are applicable\\nnot just to your data, but also to data that you\\nhave not seen before.\\n\\nNow you know what inference is and why it's important\\nin data science.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4493804\",\"duration\":313,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Designing a hypothesis test\",\"fileName\":\"4457428_en_US_08_02_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Hypothesis testing allows data scientists to make informed conclusions based on the data that they observe. After watching this video, you'll be able to create a permutation test for a hypothesis.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10076535,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"To help answer questions about the world,\\ndata scientists use hypothesis test to make informed conclusions\\nbased on the data that they observe.\\nNow, data collection is not always precise,\\nso when you notice some trends in your data,\\nyou might be wondering whether those trends occur due to random\\nfluctuations in data collection or due to some real phenomena.\\nAnd this is where hypothesis testing comes in.\\nIn this lesson, I'll show you how to design\\na hypothesis test.\\nLet's say I have a fictional data set that represents a sample\\nof avocado trees.\\n\\nI have a new fertilizer, and I want to test if the new\\nfertilizer has an effect on how long the avocado takes to grow.\\nI have data on a sample of avocado trees,\\nand for each tree, the data set contains the amount\\nof time it took for the avocado to grow and whether the new\\nfertilizer was used.\\nNow, I've opened up the exercise file for this lesson.\\nFeel free to open the exercise file on your end as well\\nto follow along.\\nIn the first code cell, I imported all the python\\nlibraries that I'll need.\\nNext, I loaded the avocado data set into a Pandas data frame.\\n\\nSave that in a variable called avocado info and display\\nthe first few rows.\\nThe names of the columns in avocado info are growth\\nduration and fertilizer.\\nNow, I want to determine whether there is an association between\\nthe duration of growth and whether the fertilizer was used.\\nSo I'll design a hypothesis test.\\nI will create two hypotheses.\\nThe first hypothesis is called a null hypothesis.\\nThe null hypothesis usually does not state that there are\\nassociations between variables and usually attributes trends observed\\nin the sample to random chance.\\n\\nSo my null hypothesis will be the following. In the population, the\\ndistribution of growth duration of avocado trees that receive\\nfertilizer is the same as that of avocado trees that did not\\nreceive fertilizer.\\nIf the two distributions are different in the sample,\\nit's due to random chance.\\nAnother way I can state this null hypothesis is the following\\nIn the population, on average, avocado trees that\\nreceive fertilizer took the same period of time to grow as those\\nthat did not receive fertilizer.\\nThe second hypothesis is called an alternative hypothesis.\\n\\nThe alternative hypothesis usually attributes trends observed in the\\ndata to associations between variables.\\nSo my alternative hypothesis will be the following.\\nIn the population, the distribution of growth\\nduration of avocado trees that receive fertilizer is different\\nfrom that of avocado trees that did not receive fertilizer due to\\nan association between growth duration and whether fertilizer\\nwas used.\\nAnd on average, avocado trees that received\\nfertilizer took a shorter period of time to grow than those that\\ndid not receive fertilizer.\\n\\nNow, the goal of a hypothesis test is to decide between the null\\nhypothesis and the alternative hypothesis.\\nIt would help to visualize the data.\\nYou can plot two histograms and overlay them.\\nTo do this, I use the hist plot method from the Seaborn library.\\nI called sns.histplot, passed in avocado info as the data, growth\\nduration as the variable that should be plotted on the x-axis,\\nand fertilizer as the variable that determines the hue.\\nThe hue determines the color of the bars for each histogram.\\n\\nThen I created a legend using the legend method from plt and passed\\nin the appropriate labels.\\nFinally, I used plt.show to display the overlaid histogram\\non the screen.\\nThe orange histogram corresponds to avocados that were fertilized.\\nAnd the blue histogram corresponds to avocados that were\\nnot fertilized.\\nFrom this plot, it appears that on average,\\nthe growth duration of avocados that receive fertilizer may be\\nshorter than the growth duration of those that did not.\\n\\nThis leads me to ask, \\\"Is this trend due to random chance?\\\"\\nIn other words, when the sample was collected\\nfrom the population, did it just so happen that the\\nsample contains data with such a trend?\\nOr is there an underlying association between growth\\nduration and whether fertilizer was used?\\nThe hypothesis test I am designing can be used to answer\\nthis question.\\nThe next step is to come up with a test statistic.\\nThe test statistic helps you decide between the two hypotheses.\\n\\nThe null hypothesis says that on average,\\nthe growing time is the same for avocado trees that receive\\nfertilizer and for those that did not.\\nAnd the alternative hypothesis says that on average,\\navocado trees that receive fertilizer took a shorter period\\nof time to grow than those that did not receive fertilizer.\\nSo my test statistic can be the following.\\nThe average growing time among avocado trees that receive\\nfertilizer minus the average growing time among avocado trees\\nthat did not receive fertilizer.\\n\\nSo how exactly does this test statistic indicate which\\nhypothesis is better supported by the data?\\nWell, smaller values of this test statistic would indicate that the\\nalternative hypothesis is better supported,\\nwhile larger values of this test statistic would indicate that the\\nnull hypothesis is better supported.\\nNow you know how to design a hypothesis test,\\nidentify a question you want to answer,\\ncreate null and alternative hypotheses, and come up with a\\ntest statistic.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4488792\",\"duration\":253,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating a permutation\",\"fileName\":\"4457428_en_US_08_03_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"If you have two or more samples of data, you'll find that using a permutation test will be helpful for you to prove your hypothesis. After this lesson, you'll be able to identify the core elements of setting up and executing a permutation test. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8611148,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Hypothesis testing allows data scientists to make informed\\nconclusions based on the data that they observe.\\nI have a Pandas data frame that contains data on the time taken by\\na sample of avocado trees to grow avocado fruits and whether the\\ntrees receive fertilizer.\\nIf a tree received fertilizer, the corresponding value in the\\nfertilizer column would be true.\\nOtherwise, it would be false.\\nI want to determine whether there's an association\\nbetween growth\\nduration and fertilizer usage, and I've already designed\\na hypothesis test.\\n\\nMy null hypothesis states that on average,\\navocado trees that received fertilizer took the same period of\\ntime to grow than those that did not.\\nAnd my alternative hypothesis states that on average,\\navocado trees that receive fertilizer took a shorter period\\nof time to grow than those that did not.\\nSo I came up with the following test statistic The average growing\\ntime among avocado trees that receive fertilizer minus the\\naverage growing time among trees that did not receive fertilizer.\\nNow, before I conduct the test, I have to assume one of the\\nhypothesis is true.\\n\\nUsually, the null hypothesis is what you assume to be true before\\nyou conduct the test.\\nSo I'll assume that the trend I observed between growing duration\\nand fertilizer usage is due to random chance.\\nIt's time to conduct the hypothesis test.\\nI'll perform a specific type of hypothesis test that's known\\nas a permutation test.\\nIn a permutation test, you randomly permute the data.\\nIn other words, you randomly rearrange the data.\\nIn this lesson, I'll show you how to create\\na permutation.\\nI've opened up the corresponding exercise file.\\n\\nFeel free to open it on your side as well to follow along.\\nIn the first code cell, I imported all the Python libraries\\nthat I'll need.\\nAnd then I loaded the avocado data set into a Pandas data frame.\\nSave that in a variable called avocado_info\\nand display the first few rows.\\nThe next step is to compute the observed value of the\\ntest statistic.\\nThis can be directly computed from your data.\\nFirst, I created two subsets of the data.\\nThe first subset corresponds to avocado trees that receive\\nfertilizer, and I saved it in a variable\\nnamed fertilized.\\n\\nThe second subset corresponds to avocado trees that did not\\nreceive fertilizer, and I saved it in a variable named\\nnot_fertilized.\\nUsing these subsets of the data, I computed the observed value\\nof the test statistic, which I stored in a variable\\nnamed observed_test_stat.\\nUsing these subsets of the data, I computed the observed value of\\nthe test statistic which I stored in a variable named\\nobserved_test_stat.\\n\\nThe mean method allows me to calculate the mean or the average\\nof the numbers in a sequence.\\nThere, that's the observed value of the test statistic.\\nNext, I want to show you how to randomly permute the data in the\\ngrowth duration column of avocado_info.\\nThere is a helpful method in the Pandas library I can use\\nand it's called sample.\\nWhen used on the growth duration column,\\nthe sample method returns a random sample of items from that column.\\nI set the frac parameter to one to indicate that I want all the items\\nin the column to be in the sample that gets returned.\\n\\nThe frac parameter represents the fraction of items that will be in\\nthe sample that gets returned.\\nThere.\\nNow, the sequence of numbers on the right side are the randomly\\npermuted items from the growth duration column,\\nand the sequence of numbers on the left side are the indices.\\nThese indices are the indices from the growth duration column,\\nexcept they're not in order here due to permutation. To generate new\\nindices that are in order,\\nI use the reset index method from the Pandas library.\\n\\nThere.\\nNow, a new set of indices have been added on the left.\\nHowever, the old set of indices is still there next to the new set.\\nTo get rid of the old set of indices,\\nI can include the drop parameter and set it to true when calling\\nthe reset index method.\\nThere you go.\\nThis is a permutation of the data in the growth duration column.\\nNow you know how to create a permutation,\\nand this is an important building block to words conducting\\na permutation test.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4491765\",\"duration\":395,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Conducting a permutation test\",\"fileName\":\"4457428_en_US_08_04_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"You can use a confidence interval to test your hypothesis or estimate. After this lesson, you'll be able to solve complex equations by bootstrapping your confidence interval.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13823353,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"The process behind creating a permutation is an important\\nbuilding block for conducting a permutation test.\\nIn this lesson, I'll show you how to conduct\\na permutation test.\\nFeel free to open the corresponding exercise\\nfile to follow along.\\nNow, I'll need to permute the data more than once,\\nso it'd be helpful to have a function that I can call whenever\\nI want to permute the data.\\nSo I've defined a function that allows me to do this.\\nAnd I named the function \\\"Perm.\\\"\\nI started with the keyword def, followed by the name\\nof the function, which is perm parentheses,\\nand then data. And data will be a generic representation of the data\\nI can pass in as input to this function.\\n\\nNext was colon.\\nIn the body of the function,\\nI used the keyword return followed by a call to the sample method\\nsetting frac to one,\\nfollowed by a call to the reset index method setting drop to true.\\nI want to determine if there's an association between growth\\nduration and whether fertilizer was used.\\nIn order to do this using a permutation test,\\nI need to permute my data many times. During each permutation,\\nI need to simulate the test statistic under the null\\nhypothesis and I need to keep track of all the simulated values\\nof the test statistic.\\n\\nSo I started by creating a variable named\\nsim_test_stat and setting it to an empty\\narray using the array method from the Numpy library.\\nNote that an array is like a storage box that allows you to\\nstore an ordered sequence of items and sim_test_stat\\nwill be used to collect all the simulated values of the test\\nstatistic that will be generated during the permutation test.\\nNext, I created a variable named reps to represent the number of\\nrepetitions that will occur in the permutation test.\\nIn other words, rep stores the number of times\\nI will permute the data.\\n\\nI want to permute the data many times.\\nSo I gave reps the value of 10,000.\\nThen I created a for loop, a reminder that a for loop allows\\nyou to repeat a particular process a certain number of times.\\nThe process used to conduct a permutation can be repeated 10,000\\ntimes in order to conduct 10,000 permutations.\\nSo I called the range function and passed in reps.\\nThis makes sure that the for loop repeats that many times.\\nThe next step is to call the perm function that I defined earlier in\\norder to permute the data in the growth duration column of avocado\\ninfo and store the result in a variable named perm info.\\n\\nThen I created a new data frame with two columns.\\nThe first column will contain the data from perm info,\\nand the second column will contain the data from the fertilizer\\ncolumn of avocado info.\\nAnd I stored this new data frame in a variable named df. When\\ncalling pd.data frame, I had to specify the names of the\\ncolumns for this new data frame and the data that needs\\nto go in each column.\\nSo I passed in a dictionary that maps column name to column data.\\nPermuted duration is mapped to perm info and fertilizer is mapped\\nto the fertilizer column from avocado info.\\n\\nSo df has two columns.\\nThe first column is named Permuted Duration and the second column\\nis named Fertilizer.\\nThen I created two subsets of the data in df.\\nThe first subset corresponds to the permuted growth durations of\\navocado trees that receive fertilizer,\\nand the second subset corresponds to the permuted growth durations\\nof avocado trees that did not receive fertilizer.\\nI saved the first subset in a variable named fertilized,\\nand I saved the second subset in a variable named not_fertilized.\\nThe next step is to compute the value of the test statistic\\nfor this permutation, which can be stored in a\\nvariable named stat.\\n\\nTo do this, I again used the mean method from the Numpy library.\\nSo I called np.mean on fertilized and I called np.mean\\non not fertilized\\nand then I took the difference.\\nIn other words, I subtracted the two.\\nI need to keep track of all the simulated values of the\\ntest statistic.\\nSo I need to store stat in sim test stat. To do this,\\nI called the append method from the Numpy library and reassigned\\nsim_test_stat to update its value.\\nNow I'll run this cell.\\nIf you're following along, note that 10,000 permutations will\\nbe performed through the cell.\\n\\nSo it may take a while for the cell to finish execution.\\nSo be patient.\\nGreat.\\nIt just finished.\\nIn the next cell,\\nI want to check out what sim_test_stat contains.\\nLooks good.\\nMy goal is to draw a conclusion.\\nSo my next step is to use sim_test_stat to compute the P value.\\nIn general, the P value represents the approximate probability that\\nthe observed value of the test statistic or a more extreme value\\nshows up among the simulated values of the test statistic under\\nthe assumption that the null hypothesis is true.\\n\\nIn this test, the P value will be the proportion of simulated values\\nof the test statistic that were less than or equal to the observed\\nvalue of the test statistic.\\nTo compute this proportion, I'll divide the number of\\nsimulated values that satisfy this condition over the number\\nof simulated values.\\nTo count the number of simulated values that satisfy the condition,\\nI used the count non-zero method from the Numpy library and I\\npassed in sim_test.less than or equal to observed test stat.\\nA reminder that the observed test stat variable contains the\\nobserved test statistic.\\n\\nThe number of simulated values is the same as the number of times I\\npermuted which is stored in the variable reps.\\nSo to compute the P value, I divided the call to np.count\\nnon zero by reps and stored it in a variable named p_value.\\nI can check out what P value is in this next cell.\\nThere.\\nThe P value is 0.0.\\nIn other words, 0%.\\nFinally, I can draw a conclusion and I will use a P value threshold\\nof significance, also known as a P value\\ncutoff to do so.\\n\\nIf my P value is below my P value cutoff,\\nthen I'll reject the null hypothesis.\\nThe most commonly used P value cutoffs are 5% and 1%.\\nI'll use 5% as my P value cutoff.\\nSince my P value of 0% is lower than my P value cutoff of 5%,\\nI reject the null hypothesis.\\nI conclude that there is a statistically significant\\ndifference between the average growing time among avocado trees\\nthat received fertilizer and the average growing time among avocado\\ntrees that did not receive fertilizer.\\n\\nNow you know how to conduct a hypothesis test.\\nOnce you've designed your hypothesis test,\\nyou compute the observed value of the test statistic,\\nsimulate the test statistic under the null hypothesis\\nmany times, compute the P value,\\ncompute the P value to the P value cutoff of your choice,\\nand draw your conclusion on which hypothesis is true.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4490792\",\"duration\":476,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Bootstrapping a confidence interval\",\"fileName\":\"4457428_en_US_08_05_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":19654317,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Oftentimes, data scientists need to estimate an unknown parameter\\nof a population, have only one random sample from\\nthe population and cannot take more samples due to time and cost.\\nIn such situations, data scientists use a process\\ncalled bootstrapping.\\nBootstrapping allows you to simulate new random samples from\\nthe population by resampling from your original sample.\\nResampling from your original sample consists of sampling at\\nrandom with replacement from your original sample many times.\\nEach time you simulate a new random sample,\\nyou compute an estimate of the unknown parameter of the\\npopulation based on the sample, and you collect all the estimates\\nyou compute along the way.\\n\\nThen, using your estimates, you can create something called\\na confidence interval, and you can say that the value of\\nthe unknown parameter lies in that interval with a certain\\nlevel of confidence.\\nIn this lesson, I'll show you how to bootstrap\\na confidence interval.\\nI have a Pandas data frame named avocado_info that\\ncontains data on a sample of avocado trees. For each\\navocado tree,\\nthe data set indicates the time the tree took to grow avocado\\nfruit and whether the tree was given fertilizer.\\nNow, let's say that I want to estimate the following population\\nparameter, the difference between the average growth duration of\\navocados that receive fertilizer and the average growth duration of\\navocados that did not receive fertilizer.\\n\\nI can use bootstrapping to create a 95% confidence interval and I'll\\nbe 95% confident that this interval will contain the value\\nof the population parameter.\\nFeel free to open the exercise file for this lesson\\nto follow along.\\nI've imported all the Python libraries that I'll need,\\nloaded the data into a data frame and displayed the first few\\nrows of the data frame.\\nNow, I'll show you how to take a random sample with replacement\\nfrom an existing sample.\\nI'll first create two subsets of the data in avocado info.\\n\\nThe first subset corresponds to the growth durations of avocado\\ntrees that receive fertilizer, and I save that in a variable\\nnamed fertilized.\\nThe second subset corresponds to the growth durations of avocado\\ntrees that did not receive fertilizer.\\nAnd I saved that in a variable named not_fertilized.\\nNow, I'll take a random sample with replacement from the sample of\\ngrowth durations of fertilized avocado trees.\\nThis can be accomplished by using the random.choice method\\nfrom the Numpy library.\\nI called np.random.choice and passed in fertilized,\\nwhich is the existing sample and I set the size parameter to the\\nlength of fertilized using the LEN function.\\n\\nThis indicates that I want the new sample to have the same length\\nas the existing sample.\\nThere,\\nthis is a new sample of growth durations of fertilized\\navocado trees.\\nNext, I'll take a random sample with replacement from the sample\\nof avocado trees that did not receive fertilizer.\\nTo achieve this, I again use the random.choice\\nmethod from the Numpy library.\\nSo I called np.random.choice, passed in not fertilized, and set\\nsize to the length of not fertilized using the\\nLEN function again.\\n\\nThere,\\nso this is a new sample of growth durations of avocado trees that\\ndid not receive fertilizer.\\nNow, I need to take a new sample many times so it would be helpful\\nto have a function that I can call whenever I want to take\\na new sample.\\nI can define the function myself and I'll name the function\\n\\\"resample.\\\"\\nI started with the keyword \\\"def,\\\" followed by the name\\nof the function, which is resample,\\nthen parentheses and I passed in a ridge_sample which will\\nbe a generic representation of the original sample that will be\\npassed in as input to this function and then a colon. In\\nthe body of the function,\\nI used the return keyword and called np.random.choice.\\n\\nI passed in a ridge sample and set size to the length of\\na ridge sample.\\nNow, I'll go ahead and run this cell.\\nNext, I defined a function that helps me perform bootstrapping and\\ncompute the mean for every sample I simulate along the way.\\nThis function will return a collection of all the means and I\\nnamed this function \\\"bootstrap.\\\"\\nI started with the keyword \\\"def,\\\" followed by the name of the\\nfunction which is bootstrap and then parentheses and passed in a\\nridge sample which will be a generic representation of the\\noriginal sample that will be passed in as input to this\\nfunction and then comma followed by reps.\\n\\nReps will be a generic representation of the number\\nof times to resample,\\nwhich will also be passed in as input to this function and then a\\ncolon. In the body of this function, I created a variable named \\\"means\\\"\\nand set it to an empty array using the array method from\\nthe Numpy library.\\nNote that an array is like a storage box that could store an\\nordered collection of items, so I use the variable means to\\nstore and keep track of the means that are computed from the new\\nsamples that are drawn during bootstrapping.\\nNext is a for loop, a reminder that a for loop allows\\nyou to repeat a particular process a certain number of times.\\n\\nThe process used to take a new sample can be repeated many times\\nin order to take many new samples.\\nSo I called the range function and passed in reps,\\nwhich makes sure that the for loop repeats that many times.\\nThen I called the resample method that I defined earlier to take a\\nnew sample from the original sample,\\nand I stored the result in a variable named \\\"new_sample.\\\"\\nNext, I used the mean method from the Numpy library to compute\\nthe mean of the new sample, and I stored the result in a\\nvariable named new_mean.\\n\\nI want to collect the means\\nI compute across all new samples so I need to store new mean in the\\nvariable means. I used the append method from the numpy library to\\nupdate the variable means.\\nSo I called np.append and passed in means, comma, new mean and\\nreassigned means to the result.\\nThis adds new mean to the end of the array that stored in\\nthe variable means, and that's the end of the for loop.\\nAt the end of the function, after the for loop, I use the return\\nkeyword followed by means.\\n\\nNow I'll go ahead and run the cell.\\nThe next step is to call the bootstrap function I just defined\\nin order to simulate new samples of growth\\ndurations of avocado trees that were fertilized,\\nsimulate new samples of those that did not, compute the mean of each\\nnew sample, and compute estimates of the population parameter.\\nSo I created a variable named fertilized means and I set it to a\\ncall to the bootstrap function where it passed in fertilized, 10,000\\nAnd then I created a variable named not_fertilized means,\\nand I set it to a call to the bootstrap function where I passed\\nin not_fertilized, 10,000.\\n\\nNext, I created a variable named estimates and I assigned it\\nto fertilized means minus not fertilized means.\\nNow, I'll go ahead and run this cell.\\nIn the next cell,\\nI'll check out what the variable estimates contains.\\nThere you go.\\nNow, to get an idea of the distribution of estimates,\\nI could create a histogram using the hist plot method from\\nthe Seaborn library.\\nSo I called sns.histplot and passed in estimates. And on the\\nnext line, I called plt.show so that the plot displays\\non the screen.\\n\\nThere,\\nthis is my histogram.\\nThe final step is to construct a 95% confidence interval.\\nTo do this, I'll take the two point fifth and 97 point fifth\\npercentiles of the estimates I computed during bootstrapping.\\nI'll use the percentile method from the Numpy library.\\nSo I called np.percentile passed in estimates, 2.5,\\nand then I called np.percentile again and this time I\\npassed in estimates, 97.5.\\n\\nThere you go.\\nThis confidence interval allows you to say with 95% confidence\\nthat the population's mean difference in growth duration\\nbetween avocado trees that were fertilized and those that were not\\nis between approximately -11.34 and -7.19.\\nNow you know how to bootstrap a confidence interval.\\n\"}],\"name\":\"8. Using Inference and Statistical Analysis\",\"size\":53747791,\"urn\":\"urn:li:learningContentChapter:4493807\"},{\"duration\":1556,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4485932\",\"duration\":106,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Defining prediction for data science\",\"fileName\":\"4457428_en_US_09_01_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Classification is an important machine learning technique you can use when working with data. After watching this video, you'll be able to describe classification and explain how it is used.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3582338,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"An important aspect of data science is discovering what data\\ncan tell us about the future.\\nFor example, based on a person's social media profile,\\nwhich upcoming movies will likely interest them?\\nWhat does climate and environmental data say about\\ntemperatures a few decades from now?\\nHow can a database of past clinical trial reports be used to\\nestimate the risks of a new clinical trial?\\nAnswering questions like these involves making predictions.\\nIn this lesson, I'll identify two major types of\\nprediction tasks the data scientists work on.\\n\\nThe first is classification.\\nClassification is about predicting the value of a categorical\\nvariable.\\nThe variable you want to predict is categorical,\\nwith dog and cat being the possible categories,\\nin other words, classes.\\nNow, there are many techniques the data scientists have developed for\\napproaching classification.\\nOne method is K-nearest neighbors, which uses the distance between\\nthe data point that you want to classify and the data points that\\nare already classified.\\n\\nThe classes of the data points that are nearest to the new data\\npoint inform the predicted class of the new data point.\\nThe second is regression.\\nRegression is about predicting the value of a continuous variable.\\nFor example, predicting the retail price of a car based on the\\ncar's acceleration rate.\\nThe variable you want to predict is continuous.\\nPrice can be any number and isn't restricted to discrete intervals.\\nThere are many methods for regression as well.\\nOne method is linear regression, which models a linear relationship\\nbetween variables and the linear model can be used to predict\\nthe dependent variable.\\n\\nNow you have an overview of what prediction means in data science,\\nas well as some key methods that you'll encounter in this area.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4493803\",\"duration\":127,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Navigating classification\",\"fileName\":\"4457428_en_US_09_02_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"k-NN or k-Nearest Neighbor is a common data science algorithm. After watching this video, you'll be able to define the k-Nearest Neighbor algorithm and how to use it to classify data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4658565,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Classification is an important machine learning technique and\\nit's a type of prediction task.\\nClassification is a vast topic that consists of numerous\\nconcepts.\\nSo for the purposes of this course,\\nI'll provide a quick introduction to classification.\\nClassification is a process of making categorical predictions\\nusing data. In classification, you have some data that has\\nalready been placed in the correct categories,\\nand the goal is to learn from this data in order to make good\\npredictions for new data that you encounter in the future.\\nHere are a few examples where classification can be used\\nin the real world.\\n\\nClassification can be used to make weather forecasts.\\nWeather stations predict tomorrow's weather based on\\ntoday's weather and the weather from the previous days,\\nit can be used to classify emails as spam and not spam,\\nas well as to classify phone calls as spam and not spam.\\nClassification can also be used to predict if a patient has\\na particular disease.\\nIt can also be used to predict which candidate a person will\\nvote for in an election.\\nYou can use classification to predict the genre of a song,\\nthe genre of a movie or the genre of a TV show.\\n\\nAlso, classification can be used on dating apps and dating sites to\\npredict whether two users are compatible in order to provide\\nusers with recommendations on who to connect with.\\nNext, I'll introduce some key terms used in classification.\\nA situation where you want to make a prediction is called\\nan observation.\\nEach observation has certain aspects that describe\\nthe observation,\\nand these are called attributes.\\nThe attributes are known, and each observation belongs\\nto a specific category, which is called a class.\\n\\nThe class is not known. in classification,\\nthe goal is to correctly predict the classes of observations using\\nthe attributes of the observations.\\nIn order to make predictions, you need something called\\ntraining data.\\nbeen correctly classified, so you would analyze the training\\ndata and then build something called a classifier.\\nA classifier is an algorithm that helps you classify future\\nobservations whose classes you do not already know.\\nNow you know what classification is and why it's useful.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4487769\",\"duration\":193,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Recognizing the k-NN algorithm\",\"fileName\":\"4457428_en_US_09_03_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"k-Nearest Neighbors is a great algorithm to use, but navigating it with an example can be helpful to fully grasp the concept. After this lesson, you'll be able to apply the k-Nearest Neighbors algorithm and use it to perform classification within a dataset. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6496295,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"There are a lot of different methods to approach\\nclassification,\\nbut for the purposes of this course,\\nI'll give you a quick introduction to one of the methods.\\nAnd there are a lot of resources out there for you to learn\\nabout other methods.\\nIn this lesson, I'll introduce the K-nearest\\nneighbor algorithm, which is a powerful method to\\napproach classification.\\nBut before I do, I want to go over some key terms\\nused in classification.\\nAn observation is a situation where you want to make\\na prediction.\\nEach observation has certain aspects that describe\\nthe observation,\\nand these are called attributes.\\n\\nThe attributes are known and each observation belongs to\\na specific category, which is called a class,\\nand the class is not known. In classification,\\nthe goal is to correctly predict the classes of observations using\\nthe observations attributes.\\nIn order to make predictions, you need training data.\\nTraining data consists of observations that have already\\nbeen correctly classified, so you would analyze the training\\ndata and then build something called a classifier.\\nA classifier is an algorithm that helps you classify future\\nobservations whose classes you do not already know.\\n\\nBinary classification is performed when the classification task at\\nhand involves observations that each belong to one of two classes.\\nMulticlass classification is performed when the classification\\ntask at hand involves observations that each belong to one of\\nmultiple classes and by multiple I mean more than two.\\nNow the K-nearest neighbor algorithm,\\nalso known as a K-NN algorithm, works as follows.\\nFirst, you pick a number k.\\nNote that it may be convenient to pick an odd number so that you\\ndon't have to deal with ties.\\n\\nNext, you pick a set of attributes from your training data that are\\nmost relevant to the classification task at hand.\\nWhen you encounter a new data point,\\nyou determine the new data points k-nearest neighbors.\\nIn other words, you determine the K training data\\npoints with the shortest distances to the new data point.\\nThen you find out the class that is most frequent among the K\\nnearest neighbors and use that class to classify the\\nnew data point.\\nFor example, let's say I want to build a binary classifier that\\nhelps me classify emails as spam or not spam and say that I have a\\ndata set containing training data.\\n\\nThe training data set specifies the attributes and the correct\\nclasses for a bunch of emails.\\nSome of the attributes given in the training data set are\\nthe body of the email, the subject of the email,\\nthe date time stamp, whether the message was a reply\\nand whether the message was a foreword.\\nFirst, I would observe the given attributes.\\nNext, I would try to see if I can come up with more attributes that\\nare relevant to the classification task at hand,\\nusing the given attributes.\\nFor example, the length of the subject as well as the length of\\nthe body may be relevant to determining whether an email\\nis spam or not spam.\\n\\nSo I can add these attributes to my training data set\\nafter computing the length of the bodies and the lengths of the\\nsubjects for the emails in my training data.\\nNext, I would analyze the training data and look for patterns or\\ntrends between the attributes of the emails and the classes\\nof the emails.\\nThen from the attributes I have, I would choose the attributes that\\nI think are most relevant to spam filtering.\\nNow you have insight on the K-nearest neighbor algorithm and how\\nit's used in classification.\\nSo if you want to classify some data,\\ntry using K-NN.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4487768\",\"duration\":449,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Implementing k-Nearest Neighbors\",\"fileName\":\"4457428_en_US_09_04_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Regression is all about exploring relationships, which is often what evaluating data involves. After this lesson, you'll be able to define regression as a task of prediction in data science. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16723185,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"In this lesson, I'll go over an example of\\nimplementing the K-Nearest Neighbors or K-NN algorithm.\\nLet's say you have a data set that was collected to help doctors\\ndiagnose chronic kidney disease or CKD.\\nEach row in the data set represents a patient who was\\ntreated in the past and whose diagnosis is known.\\nFor each patient, you have a set of measurements\\nfrom a blood test.\\nYour goal is to develop a way to classify future patients,\\nas has CKD or doesn't have CKD based on their blood test results.\\n\\nI've opened up the exercise file for this lesson.\\nFeel free to open the exercise file on your end as well\\nto follow along.\\nIn the first code cell, I started by importing all the\\npython libraries that I'll need.\\nThis includes imports from Pandas, Matplotlib, Seaborn and Sklearn.\\nSklearn is another powerful library for data science.\\nIt contains many useful methods for implementing machine learning\\nalgorithms in Python.\\nYou can learn more about it using the link in the resource file.\\nNext, I loaded the CKD data set into a Pandas data frame.\\n\\nThen I used the head function to get the first few rows\\nof the data frame.\\nAs a data-cleaning step,\\nI renamed the column named Blood Glucose Colon random to just\\nglucose for simplicity.\\nIt's easier to work with column names that are shorter,\\nand I used the head function again to make sure that the data\\nframe was updated.\\nTo get a better understanding of the data,\\nI accessed the shape attribute of the data frame.\\n\\nThis tells me that there are 158 rows and 25 columns.\\nI also used the info function to get some basic information\\nabout the columns.\\nThis shows that there are no missing values in the columns,\\nand there are a variety of data types in this data set.\\nWhen performing classification, it's important to check for class\\nbalance in the data set.\\nClass balance refers to how evenly distributed the values in the\\noutcome variable are across the different classes.\\n\\nClass balance is important because you don't want to build a\\nclassifier that is biased towards one class over another.\\nA perfect split is very rare. When there are two classes,\\na common standard is that anything more extreme than a 9010 split\\nis considered too imbalanced.\\nAnd in that case, there are techniques like\\nresampling that can be used to balance the data.\\nI can use the value counts method from the Pandas library to\\ncheck for class balance.\\nIn this data set, there's a 7327 split\\napproximately,\\nwhich is not too imbalanced.\\n\\nSo I can proceed with this data set as is.\\nNow, let's use visualizations to get more insights about the\\nrelationships between variables in the data.\\nLet's start with the scatterplot of glucose over hemoglobin and\\ncolor in the data points to distinguish between\\nthe two classes.\\nClass 1 represents has CKD and Class 0 represents\\ndoes not have CKD.\\nThis plot shows a clear pattern.\\nData points in the lower right tend to represent people who do\\nnot have CKD and the rest tend to be folks with CKD.\\n\\nYou could also use this plot to visually simulate how a nearest\\nneighbor algorithm would work here.\\nLet's say a new data point has been identified.\\nYou have the glucose and hemoglobin attributes of this new\\ndata point and you want to predict its class.\\nYou could find the point in the training set that is nearest\\nto the new point.\\nIf that nearest point is a CKD point, you would classify the\\nnew point as CKD.\\nIf the nearest point is a not CKD point,\\nyou would classify the new point as not CKD instead.\\nBut the separation between the two classes won't always be as clean\\nif you choose a different pair of attributes.\\n\\nFor instance, let's say you choose glucose and white blood\\ncell count.\\nLet's create a scatterplot for this.\\nAs the scatterplot shows, sometimes, folks with CKD have glucose\\nand white blood cell levels that look identical to those of\\nsomeone without CKD.\\nSo a classifier is inevitably going to make the wrong prediction\\nfor them if the prediction is based on the nearest point.\\nThe solution is generalizing the nearest neighbor algorithm,\\nmaking it a K-nearest neighbor algorithm.\\n\\nFor example, K could be three.\\nSo instead of using the nearest point,\\nyou could use the three nearest points to make a prediction.\\nAnd really k can be any number like 4 or 5 and so on,\\nbut it's usually convenient to pick an odd number so that you\\ndon't have to deal with ties.\\nLet's go with three for now.\\nNext, I isolated the y variable.\\nThis is also known as the outcome variable,\\nthe target variable, or the dependent variable.\\nEssentially this is the variable that you want to predict.\\n\\nThen I isolated the X variables.\\nThese are also known as the features or the independent\\nvariables.\\nto use to predict Y.\\nAnd from this, I selected the X variables that are numeric.\\nIn other words, having integer or float\\ndata types.\\nNext, I split X and Y into training and testing sets using the train\\ntest split function from sklearn.\\n\\nThis function randomly splits the data.\\nI set test size to 0.25, which means 75% of the data goes\\ninto training and 25% goes into testing.\\nI also specified random state so that this process can\\nbe reproduced.\\nIf I don't specify the random state parameter,\\nthen each time I run this train test split, I would get\\na different result.\\nAnd then I checked that the training and testing sets were populated.\\n\\nI also checked the shapes of the training and testing sets.\\nThe number of columns in x_train and x_test match, the number of\\nrows in x_train and y_train match, and the number of rows in\\nx_test and y_test match.\\nNext, I created a K-neighbor's classifier object and saved\\nit in a variable named K-NN.\\nAnd then I fit the K-neighbor's classifier to the training data.\\nNext, I used the fitted neighbors classifier to predict on the\\ntraining set and save those predictions in a variable named\\npreds_train.\\n\\nI then computed the accuracy of the predictions for the training\\nset using the score function from Sklearn.\\nThe output shows that the accuracy of the predictions on the training\\nset is about 86%.\\nFinally, I got the predictions on the testing set using the\\npredict function again, and I saved those predictions\\nin a variable named preds_test\\nAnd here are those predictions.\\nAnd to compare, here are the true values from the testing set.\\n\\nLastly, I computed the accuracy of the predictions for the testing\\nset using the score function again.\\nThe output here shows that the accuracy of the predictions\\non the testing set is 85%.\\nNow that you've gone through this example,\\nyou can go ahead and try implementing the K-NN algorithm to\\nperform classification on a data set of your choice.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4492756\",\"duration\":172,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Navigating regression\",\"fileName\":\"4457428_en_US_09_05_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"You can use linear regression to help you predict the value of one variable using the value of another. After this lesson, you'll be able to recognize the value of linear regression and articulate how it functions. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6002454,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"In data science, regression can be used to predict the value of\\na continuous variable.\\nThere are different types of regression techniques,\\neach suited for a different scenario.\\nA couple of the major techniques include linear regression and\\npolynomial regression.\\nLinear regression can be used to model a linear relationship\\nbetween variables and use the linear model to predict the\\ndependent variable.\\nMeanwhile, polynomial regression can be used to model a non-linear\\nrelationship between variables and use the non linear model to\\npredict the dependent variable.\\n\\nWithin linear regression, there are two types.\\nThe first is simple linear regression and the second is\\nmultiple linear regression.\\nSimple linear regression is about using one independent variable,\\nalso known as feature to predict the value of the dependent\\nvariable\\nindependent variables, also known as features to predict\\nthe value of the dependent variable.\\nIn the rest of this lesson, I'll show you how to navigate\\nquestions that can be answered through regression with a couple\\nof example context to make it clear.\\n\\nImagine that a data professional is interested in making\\npredictions about user engagement for a mobile app.\\nHere's the question they might ask, \\\"How much does each in-app feature\\ninfluence user engagement?\\\"\\nThe in-app features might include a live chat with customer support\\nand FAQ section that updates weekly and a community space to\\nconnect with other users.\\nThe next step is to determine which variable in the data should\\nbe the outcome variable.\\nIn other words, the variable that will be\\npredicted. If they have access to data about user session lengths,\\nin other words, how long users spend in the app\\neach time they open it,\\nthe outcome variable can be session length.\\n\\nThen they should identify how the outcome variable is measured.\\nSession length can be measured by number of minutes,\\nwhich is continuous. Because the outcome variable is continuous and\\nthey're interested in how much each feature influences\\nthe outcome variable,\\nthey could proceed with linear regression.\\nIf there is only one feature of interest,\\nthey would build a simple linear regression model.\\nOr if there are multiple features of interest,\\nthey would build a multiple linear regression model.\\nNow, imagine a different context, making predictions about patient\\nresponses to medical treatments.\\n\\nA possible question could be, \\\"How much does each factor influence a\\npatient's response to a medical treatment?\\\"\\nFor example, if the goal of the treatment is to improve white\\nblood cell or WBC count and you have access to that data,\\nWBC count can be the outcome variable.\\nThis outcome variable is a continuous measure,\\nso you could use linear regression to address this task.\\nAs these examples demonstrate, regression can be used to answer\\na variety of questions.\\nAs a fun exercise, think of a question that involves\\npredicting a continuous variable that you're interested in,\\nand think about how regression can be used to achieve that goal.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4489773\",\"duration\":140,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Checking assumptions of regression\",\"fileName\":\"4457428_en_US_09_06_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Linear regression uses a dependent and independent variable to help you test and form the relationships within your data. After this lesson, you'll be able to use linear regression to make predictions for your dataset. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3674974,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"When using linear regression to answer question or\\nmake predictions,\\nit's important to be aware of the assumptions that must be met.\\nIn this lesson, I'll introduce four key\\nassumptions of simple linear regression.\\nThe first is linearity.\\nTo detect if this assumption is met,\\nplot the independent and dependent variables on an x, y coordinate\\nplane and make sure that the points on the plot appear to\\nfall along a straight line.\\nIf the visualization looks like a random cloud or resembles a\\ncurve rather than a line, then the assumption is not met,\\nwhich means a linear model would not fit the data well and you\\nmight need a different or a more complicated model for\\nyour data set.\\n\\nIn contrast, a plot that shows the data points clustering around a\\nline indicates that linear regression would be an appropriate\\nmodel to represent the relationship between X and Y.\\nThe next is normality.\\nThis assumes that the residual values or errors are normally\\ndistributed.\\nResiduals are calculated after the model is built and they indicate\\nhow far off the predictions are from the true values.\\nSo you can't check this assumption until after you build the model.\\nBut once the model is built, you can create a specific\\nplot called a quantile-quantile or q-q plot\\nof the residuals.\\n\\nIf the points on the plot appear to form a straight diagonal line,\\nthen you can say that the normality assumption is met.\\nThen there's independent observation.\\nThis means each observation in the data set is independent.\\nIt's helpful to use the context around how the data was collected\\nand the variables used in order to determine if this assumption\\nis met.\\na scatterplot of the fitted values versus residuals should\\napproximately resemble a random cloud of data points.\\n\\nIf there are any patterns in that plot,\\nthen you might need to re-examine the data.\\nFinally, there's homoscedasticity.\\nThis means that the variance of the residuals is constant.\\nTo check whether this assumption is met,\\ncreate a scatterplot of fitted values,\\nalso known as predictions versus residuals,\\nThere should be constant variance along the values of the\\ndependent variable.\\nIn other words, there will be no clear pattern or\\nyou will notice a random cloud of data points in this plot.\\nHowever, if you observe a cone-shaped pattern,\\nfor example, then the assumption is not met.\\n\\nNow you know the main assumptions of linear regression.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4485931\",\"duration\":369,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Implementing linear regression\",\"fileName\":\"4457428_en_US_09_07_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13547207,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Imagine that you're part of a team that provides insights about\\nmarketing and sales.\\nYou've been assigned to a project that focuses on the use of\\ninfluencer marketing, and you'd like to explore the\\nrelationship between radio promotion budget and sales.\\nYou're provided a data set that includes information about\\nmarketing campaigns across TV, radio, and social media,\\nas well as how much revenue and sales was generated from\\nthese campaigns.\\nThe goal is to build a linear regression model that can predict\\nsales based on radio promotion budget.\\n\\nIn this lesson, I'll demonstrate how to use linear\\nregression to solve this problem.\\nI'll be going through the exercise file for this lesson.\\nFeel free to open it up on your end as well to follow along.\\nPython libraries that are most commonly used for implementing\\nlinear regression include the Stats-models Library and\\nthe Escalar Library.\\nIn this demo, I'll be using the Stats-models library.\\nI've started by importing the Python libraries that I'll need.\\nThe next step is to load the marketing and sales data from the\\nprovided CSV file into a Pandas data frame and display the first\\nfew rows of the data frame.\\n\\nThe data includes TV promotion budget,\\nradio promotion\\nbudget, social media promotion budget, type of influencer that the\\npromotion is in collaboration with, and sales.\\nAnd note that the quantities in this data are expressed in\\nmillions of dollars.\\nAnd then I got the shape of the data frame.\\nThis indicates that there are 572 rows and five columns.\\nNext, it's important to check for missing values.\\nThis can be done using the isna method followed by the sum method.\\n\\nAs the zeros here indicate, there are no missing values\\nin any of the columns.\\nTo visualize the data, I'll create a plot of pairwise\\nrelationships between variables using the pair plot method\\nfrom the Seaborn library.\\nIn the scatterplot of sales over radio,\\nthe points appear to cluster around a line.\\nThis indicates that a linear model may be suitable for modeling\\nthe relationship between sales and radio,\\nand the linearity assumption of linear regression is likely met.\\n\\nNext, I'll split the data set into train and test. To get train,\\nI called the sample method from pandas.\\nI passed in frac a 0.75 and random state of zero.\\nThe 0.75 indicates that 75% of the data will go into train.\\nSpecifying the random state parameter ensures that this\\nprocess can be reproduced.\\nAnd then I assign test to be the remaining rows from the\\noriginal data frame, and then I'll confirm that the\\ntrain and test sets were populated.\\n\\nAfterwards, I'll select a subset of each with just the variables\\nneeded for the linear regression model.\\nSince the goal is to use radio to predict sales,\\neach subset needs to include only those two columns.\\nI can then confirm that these subsets were populated as well.\\nThe next step is to use the OLS function to create an OLS object.\\n\\nOLS stands for Ordinary Least Squares, which is a common approach\\nfor linear regression.\\nWhen calling the OLS function, I passed in the formula and\\ndata parameters,\\nthe formula is sales tilde radio, which specifies that sales will be\\nthe Y variable and radio will be the X variable and I specified\\ntrain subset as the data for OLS.\\nI saved the OLS object in a variable named ols_market.\\nThen I can create the linear regression model by fitting\\nthe OLS object.\\n\\nThis automatically fits the model to the training data.\\nTo get a summary of the model, I can call the summary method.\\nFrom the summary output,\\nI can identify the coefficients.\\nThe y intercept is 42.8594 and the slope is 8.4170.\\nOne way to interpret this is companies with $1 million more in\\ntheir radio promotion budget accrue $8.417 million more\\nin sales on average, according to this data.\\n\\nNow that the model is built, it's important to continue\\nchecking whether the assumptions of linear regression are met.\\nI can call the regplot method from Seaborn to visualize the data\\nalong with the line of best fit from the linear regression.\\nThis shows an approximately linear relationship between the X and Y\\nvariables which confirms the linearity assumption.\\nNext, I'll compute the residuals.\\nThe residuals, quantify how far off the model's predictions\\nare from the true values.\\nI can then create a histogram to visualize the distribution\\nof the residuals.\\n\\nThe histogram shows that the distribution of the residuals\\nis approximately normal, so the normality assumption of\\nlinear regression is likely met.\\nThen I created a quantile-quantile plot,\\nalso known as a q-q plot.\\nIn this plot,\\nthe points closely follow a straight diagonal line.\\nSo this confirms that the normality assumption is meant. To\\nget the predictions also known as fitted values on the\\ntraining data,\\nI called the predict method.\\n\\nThen I created a scatterplot of the training predictions over the\\nresiduals along with the horizontal line at Y = 0.\\nIn this plot, the points resemble a cloud like structure and don't\\nseem to follow an explicit pattern.\\nSo it looks like the independent observation assumption is not\\nviolated and the residuals appear to be randomly spaced.\\nSo the homoscedasticity assumption seems to be met as well.\\nFinally, I called the predict method again,\\nthis time to get the predictions on the test data.\\n\\nAnd that's it.\\nNow you know how to implement linear regression using methods\\nfrom the Stats-models library to make predictions.\\nThe approach shown in this lesson can be used for other\\ndata sets as well.\\nSo if you're interested in predicting a continuous variable\\nfrom another data set,\\ngo ahead and try implementing linear regression.\\n\"}],\"name\":\"9. Using Prediction in Data Science\",\"size\":54685018,\"urn\":\"urn:li:learningContentChapter:4487775\"},{\"duration\":65,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4493802\",\"duration\":65,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"4457428_en_US_10_01_LA24\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2486321,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"Congratulations on completing this course.\\nThere's a lot to learn about data science,\\nbut the learning definitely does not stop here.\\nWhile watching someone walk through examples is important\\nto learning how things work, it's equally important to\\npractice on your own\\nafter watching. I also recommend that you check\\nout the following resources.\\nThe first is Kaggle, an online community of data\\nscientists and other developers who participate in contests,\\nshare their datasets, discuss their findings, and more.\\nThe data sets posted on Kaggle are free for the public to use.\\n\\nThe next resource is the book, Developing Analytic Talent:\\nBecoming a Data Scientist by Vincent Granville.\\nAnd another great resource is the website towards data science.\\nThis site is a platform for people to stay updated on cool\\nadvancements in the field of data science,\\nexchange ideas, and access step by step tutorials that address\\nvarious topics in data science.\\nSo that's it.\\nI hope you can see how data science is changing our world.\\nAnd when you take time to learn data science,\\nyou can be a part of making the world a better place.\\n\\nThank you for watching and good luck.\\n\"}],\"name\":\"Conclusion\",\"size\":2486321,\"urn\":\"urn:li:learningContentChapter:4487776\"}],\"size\":244465192,\"duration\":7255,\"zeroBased\":false},{\"course_title\":\"Python Code Challenges for Data Analysis\",\"course_admin_id\":3981841,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":3981841,\"Project ID\":null,\"Course Name\":\"Python Code Challenges for Data Analysis\",\"Course Name EN\":\"Python Code Challenges for Data Analysis\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"In this course, Jonathan Fernandes\u00e2\u20ac\u201dan expert in Generative AI and Large Language Models\u00e2\u20ac\u201dexplores the essentials of data analysis using Python. Learn how to load data, filter it by specific criteria, and manipulate data structures to achieve the desired results. Discover the use of CoderPad to effectively compare and evaluate different Python data structures. Dive deep into hands-on challenges like sorting data by various metrics and finding elements with the most nominations. By the end of this course, you will be proficient in using Python for data analysis tasks and capable of tackling complex data problems. This course is ideal for aspiring data analysts, Python programmers, and anyone interested in data science.\",\"Course Short Description\":\"Learn about performing data analysis using Python and solving related code challenges. Discover how to use CoderPad for evaluating and comparing different data structures.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":10223359,\"Instructor Name\":\"Jonathan Anand Fernandes\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Expert in Generative AI and Large Language Models\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2025-02-25T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"No\",\"Has Challenge/Solution\":\"Yes\",\"LIL URL\":\"https://www.linkedin.com/learning/python-code-challenges-for-data-analysis\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Advanced\",\"LI Level EN\":\"Advanced\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Data Science\",\"Primary Software\":\"Python\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":2063.0,\"Visible Video Count\":7.0,\"Learning Objectives\":\"Analyze the data and select a specific format.,Choose the correct python container to capture the data.,Apply and work with correct Python data structures for a task.,Subdivide the problem into chunks where each performs a single function.\",\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":261,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5234211\",\"duration\":33,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Data analysis using Python\",\"fileName\":\"3981841_en_US_00_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":34,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to perform data analysis using Python.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3723740,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Jonathan] Jobs in data are still in high demand\\nand that's because data is everywhere,\\nbut without analysis,\\nit's just a jumbled mass of numbers and facts.\\nIf you are looking to improve your skills in data analysis\\nusing Python, this course is for you.\\nWe're going to use Python to extract key insights\\nfrom a real world dataset.\\nAnd if you are a movie buff,\\nyou'll love the dataset we'll work on.\\nHi, I'm Jonathan Fernandes with Years\\nof Python expertise under my belt.\\n\\nLet's get to it.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5239047\",\"duration\":228,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"How to use CoderPad\",\"fileName\":\"3981841_en_US_00_02_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":279,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to use CoderPad.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6898403,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] For the challenges in this course,\\nwe'll be using CoderPad, which is integrated\\ninto the LinkedIn Learning Course website.\\nNow all you have to do is to click on the challenge,\\nand this makes it really easy to watch videos\\nand write some code and go back and forth\\nand test your solutions all on one site.\\nNow, I recommend that you use CoderPad on a desktop browser,\\nbut you can use\\nthe LinkedIn Learning mobile app if you prefer.\\nNow, as you can see, there are four screens in CoderPad,\\nand you can enlarge them and shrink them as you need\\nwith these little handles over here.\\n\\nNow the first screen you're going to want to look at\\nis the instructions, and that's over on the top left.\\nNow I wrote them for each challenge,\\nso they're really important, and please read them carefully.\\nThis will tell you all about the coding challenge\\nand what the expected output is, and some of the test cases.\\nThe answer screen over on the top right\\nis where you're going to actually enter your code.\\nNow, I've tried to make this as easy as possible\\nfor each challenge.\\nSo in general you'll see an empty function\\nor some starter code.\\n\\nAnd in this example, for example, it has the function name,\\nwhich is find_largest,\\nand you just have to write your code inside of this.\\nNow, just below the answer panel is the test code,\\nand here you can see an actual value\\nthat'll be used to test your function\\nand how that code will be used.\\nAnd after you've written some code,\\nclick the test my code button on the bottom right\\nand then take a look at the fourth window,\\nwhich is the console output,\\nwhich is over on the bottom left.\\n\\nSo I select Test my code.\\nNow you can see that I have a couple of test cases here,\\nand these numbers over here\\ncorrespond to these test cases.\\nAnd so that's four test cases over here\\nand four outputs that you can see in the console output.\\nNow, keep an eye on the console output,\\nespecially if your code is failing.\\nYou're going to see all of your program output\\nin the print statements and errors\\nthat'll help you debug your code.\\n\\nNow when you click the test my code button,\\nyou'll see a message indicating\\nwhether your code returned a correct result.\\nSo let me go ahead and try and look at the first test case.\\nAnd so you can see that the test case\\nthat I have here is find the largest\\nvalue for a list,\\nand I only have one entry in my list, which is five.\\nSo if I just try and return five\\nand select Test my code,\\nyou can see that it passes the first two tests,\\nbut it fails the other two.\\n\\nIf I take a look at the third test case\\nwhere I have a positive number\\nand the rest of the numbers in the list are negative,\\nand I try and return just the value one,\\nwhich is the highest value in that list,\\nyou can see that I've passed the third test case,\\nbut I failed the other two.\\nAnd so your solution needs to pass all of the test cases.\\nSo let's go ahead and put in the right result.\\nSo to find the largest number,\\nI'm going to have to return the max of that list.\\nSo I use the built in max function\\nand the max of the list numbers.\\n\\nAnd let me go ahead and test my code.\\nAnd you can see down here in the console\\nthat I have a successful result\\nand it shows all of the values that my code returned.\\nNow when you are finished each code challenge,\\nreturn to the courses table of contents,\\nand click the next video to see my solution.\\n\"}],\"name\":\"Introduction\",\"size\":10622143,\"urn\":\"urn:li:learningContentChapter:5234212\"},{\"duration\":1802,\"entries\":[{\"urn\":\"urn:li:learningContentAssessment:67980df73450e61387991e33\",\"duration\":900,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Load JSONL data\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1229208\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:5232268\",\"duration\":410,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Load JSONL data\",\"fileName\":\"3981841_en_US_02_02_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":566,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Get the solution to the code challenge.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13055024,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's just head over\\nto the test section first,\\nand I'm just going to select test my code.\\nAnd you can see that\\nthe two tests we have here is the output.\\nSo I'm expecting the output from the get data to be a list,\\nand there are going to be 234 entries in that list.\\nSo that's basically means there are going to be 234 films\\nor movies that we have in this file.\\nAnd also, I want to just confirm that the type\\nthat I'm returning is a list.\\nLet's go ahead and work our way\\nthrough reading the data from the JSON line file.\\n\\nSo what I want to do is open my file.\\nSo with open, the file name is file name,\\nand I want to read in the file and let's say as file.\\nAnd now, I need to store all of the data in a list.\\nSo let me call my list, movies,\\nand I want to work my way through the file.\\nSo for line in file\\nand because it's a JSON object, I want to load it in.\\n\\nSo let's me call that line, movie.\\nSo json dot and let me load my string\\nand let me just test out the output.\\nSo I'm going to go ahead and just say print movie\\nand then I want to add that to my list.\\nSo movies, append and movie.\\nAnd because I don't have a whole lot\\nof real estate over at my console output,\\nI'm just going to print it for the first one.\\nAnd so, I'm going to go ahead\\nand just break out of the for loop here.\\nAnd then what I want to do is then return my list\\nthat's called movies.\\n\\nSo let's see how this goes.\\nAnd you can see we've got the first line which is showing,\\nwhich is for the film, \\\"Shawshank Redemption.\\\"\\nExcellent. So this is looking really good.\\nNow, the next thing we need to do\\nis to pass the release date.\\nSo that's the field Released over here\\nand we need to pass this as a date,\\ntime.date object, right?\\nSo let me just go ahead and copy this release date\\nfor \\\"Shawshank Redemption,\\\"\\nso that we can just go ahead and try this out.\\n\\nSo I'm just going to go ahead and comment this section out,\\nand let's try out the date-time piece.\\nSo I want to work with date, time,\\nand the string passing module,\\nso string, strptime.\\nAnd what I need to provide\\nas input over here is the release date.\\nAnd so, I'm just going to go ahead\\nand paste what I copied earlier.\\nNow, because I've worked with daytime objects a fair bit,\\nI know what the formats are.\\n\\nSo for example, I know that this date time here\\nis a percentage D.\\nNow, I also remember that if I wanted\\nto use the month object, which is a three-character string,\\nit's going to be a percentage, lower-case B.\\nAnd if I'm using the year here,\\nand because it's four numbers, it's a percentage capital Y.\\nNow if you don't know that,\\nyou can always check out the Python documentation.\\nAnd let's go ahead and check out what output we get here.\\n\\nAnd you can see that we get the output that we expect.\\nSo this is a date-time object,\\nbut I need it to be of date time with a date,\\nrather than showing also the time.\\nSo all I need to do is to add a date at the end of that,\\nand that should give us the output we want.\\nSo we're now in a position\\nwhere every time we read in a line,\\nwe can read it in as a date time.date object.\\nSo I'm going to go ahead and copy this line,\\nand I'm going to move it over here.\\n\\nAnd so, I'm just going to call this date released.\\nAnd so, this is going to allow me to take\\nin the line and because it's in the field, movie,\\nso I want the movie released,\\nand I'll be able to store that in the date released.\\nNow, I need to then go ahead\\nand update the released field for movie.\\n\\nSo movie released\\nand assign it the date-time object, which is date release.\\nSo let me go ahead and uncomment this section\\nand get rid of this print statement.\\nAnd it looks like I need\\nto get rid of that colon over there.\\n\\nNow, it looks like I've got\\na stray, right-hand bracket over there.\\nSo let me just get rid of that and let me go ahead\\nand select test my code.\\nAnd the reason that I've got a key error here is\\nbecause it's not released with a small R,\\nbut it's actually a capital R.\\nSo let me go ahead and replace that for both of these lines.\\nAnd this looks great.\\nSo it looks like we've been able\\nto read in that object as a date time,\\nand you can see that that's the case over here.\\nNow, let's just try this for more than one movie.\\n\\nSo let me get rid of the break,\\nand you can see the problem that I have here is\\nthat I've got a value error.\\nAnd the reason I have a value error is because,\\nand this is because for one of the movies,\\nthe released date isn't in the format,\\nday, month, and year, rather it's just any.\\nSo what I'll want to do is to have a try and accept section.\\nI want to read in the line and then have my try clause.\\n\\nAnd then because I'm expecting a value error,\\nI have an accept value error,\\nand I want to ignore that line,\\nso I just want to go ahead and do a continue.\\nLet me just spell value error correctly.\\nOkay, let me just get rid of this print movie,\\nand let's test my code out again.\\n\\nAnd you can see that this has passed both the tests.\\nNow, this is just one way to solve this problem.\\nYou can go ahead and create your own code\\nand as long as your code pass both these tests,\\nthat's absolutely fine.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:67980e3f3450e61387991e35\",\"duration\":300,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Filter by year\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1229209\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:5233241\",\"duration\":155,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Filter by year\",\"fileName\":\"3981841_en_US_02_04_C_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":195,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Compare different Python data structures to evaluate the required result.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5530983,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's head over\\nto the test code section first.\\nSo I'm going to go ahead and select \\\"Test my code.\\\"\\nAnd you can see that these four test cases\\nthat we have correspond to these four blocks\\nthat we have over here.\\nOkay, let's go ahead and take a look at filter by year.\\nNow I just want to get a feel for the data,\\nso I'm going to work my way through the movies list.\\nSo for movie in movies,\\nI'm going to just go ahead and print out the movie.\\nAnd because I don't have a whole lot of space\\nin my console output, I just want to print the results\\nfor the first movie.\\n\\nSo I'm just going to go ahead and select a break.\\nAnd you can see that I've got the output\\nfor the first movie, which is \\\"The Shawshank Redemption.\\\"\\nAnd now we want to be able to filter by the release date.\\nSo let me just go ahead and grab that field.\\nNow let's dig a little bit deeper\\nand let's add our condition.\\nSo if the movie,\\nand that's the released.\\nNow because this is a datetime.date object,\\nI can use the year attribute.\\n\\nAnd I want to check that this is equal\\nto the year parameter over here.\\nAnd if that's the case,\\nthen I need to be able to print the title.\\nSo let me just go ahead and grab the field title here.\\nNow this should give me the title for a film\\nthat matches this.\\nAnd so this is by 1983.\\nAnd so let's go ahead\\nand we should only see a movie title if all is well.\\n\\nAnd you can see\\nwe've got \\\"Star Wars VI: Return of the Jedi.\\\"\\nOkay, now we need to return a list.\\nSo let's go ahead\\nand make sure that we are returning a list\\nand so let me convert this to a list comprehension.\\nSo return, square brackets,\\nand let me go ahead and grab the bit\\nthat I want to return which is the movie title.\\nLet me get rid of the print.\\nThen I want to work my way through each of the movies\\nand this is my condition\\nand I just want to get rid of the break and the colons here\\nand we should be able to return our list in this way.\\n\\nSo let's go ahead and test our code.\\nAnd you can see that we've passed all of the four tests.\\nNow this is just one way to go about solving this problem.\\nNow if you found another way to do this\\nand it's passed all the four tests,\\nthen that's absolutely fine.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:67980e7c34501e94b55f9259\",\"duration\":300,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code Challenge: Released by year\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1229212\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:5239048\",\"duration\":123,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Released by year\",\"fileName\":\"3981841_en_US_02_06_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":157,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Calculate the correct result by selecting the appropriate element from the data structure.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4273384,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's take a look\\nat the test code section first.\\nSo these two test cases correspond\\nto these two sections that we have over here.\\nNow, let me make sure that I understand\\nwhat my data looks like.\\nI'm just going to comment this out.\\nAnd so let me work my way through\\nthe movies list for movie and movies.\\nI just want to go ahead and print the movie.\\nI just want to go ahead and print the first movie\\nbecause I don't have a whole lot of space\\non my console output.\\n\\nAnd you can see that I've got the first movie,\\nwhich is \\\"The Shawshank Redemption.\\\"\\nNow, anytime I want to count objects in Python,\\nI almost always will use counter\\nfrom the collections module.\\nSo let's go ahead and do that.\\nSo from collections import counter,\\nand let me just go ahead and return the counter.\\nAnd what's nice about this is,\\nI can actually work my way through the movie and movies\\nand count the object that I want.\\n\\nAnd in this case, what I want to count\\nis the release date by year.\\nAnd so it's this field over here.\\nSo for movie and movies, I want to go ahead\\nand count the number of released years,\\nbut because this is a datetime.date object,\\nI can use the year attribute\\nand I want to go ahead and count that.\\nAnd so this will allow me to count the number\\nof movies released each year.\\n\\nNow, you'll notice that I need to return this\\nas a dictionary.\\nSo let's convert this to a dictionary.\\nLet me just format this a little bit,\\nclean it up a bit, and let's test our code.\\nYou can see we've passed both tests,\\nand this is just one way to go about this.\\nNow, if you have another way of doing this,\\nthat's absolutely fine as long as it passes these two tests.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:67980eb23450436b581012d1\",\"duration\":900,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Advanced Code Challenge: Sort by runtime\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1229220\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:5233242\",\"duration\":517,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Sort by runtime\",\"fileName\":\"3981841_en_US_02_08_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":675,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Create new Python containers to capture intermediate results.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":18086056,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's go ahead and check out\\nour test code section.\\nAnd you can see, we've got an error about,\\nwith the sort by runtime,\\ngot an unexpected keyword argument shortest.\\nAnd that's because this corresponds to the first test\\nwhere we're passing shortest and longest.\\nAnd if you look at the function definition,\\nwe've just got movies\\nand we don't have shortest and longest.\\nSo let's go ahead and change that.\\nNow, I want to ensure that everything after movies\\nis going to be a keyword argument.\\n\\nSo I can do that by forcing that\\nby doing a asterisk and then shortest.\\nAnd let's give it a default value,\\nso the shortest sensible value for a movie\\nis going to be zero.\\nAnd I don't know what the longest length\\nof any of the movies here are,\\nso I'm just going to go ahead and specify 10,000 minutes.\\nLet's see if this fixes this error.\\nAnd you can see that we've now got these three sections,\\nwhich correspond to these three test cases.\\n\\nNow, let's take a look at the format of our data.\\nSo for movie and movies,\\nprint movie.\\nAnd I'm just going to go ahead\\nand print the details for the first movie.\\nAnd you can see that we've got the title.\\nAnd we've got the runtime over here.\\nSo let me just go ahead and grab that.\\nAnd I can go ahead and print out the title,\\nwhich is the field that I want over here.\\n\\nAnd you can see I've got the runtime for the movie\\n\\\"The Shawshank Redemption.\\\"\\nNow, let's think a little bit about\\nthe overall solution, right?\\nAnd the structure of the solution is,\\nwe'll want to return a runtime and the title\\nfor the movies, right?\\nAnd so let's go ahead and return.\\nAnd we want to sort them,\\nso return sorted.\\nAnd we want to return a tuple.\\n\\nA tuple of movie runtime and title,\\nworking our way through all of the movies.\\nAnd the condition is,\\nif the movie runtime\\nis going to be greater than or equal to shortest\\nand less than or equal to longest.\\n\\nAnd so that's the overall structure of our solution.\\nBut we haven't really talked about the format of runtime.\\nSo let me just go ahead and comment the section out.\\n\\nAnd let's take a look at the format of runtime.\\nSo for movie and movies,\\nI want to just print out the runtime field.\\nAnd you can see that we've got a whole load of results here.\\nAnd it looks like in every case\\nwe've got a number followed by a space and a min.\\n\\nNow, instead of eyeballing the entire file,\\nwhat I want to do is to just confirm\\nthat all of them have the same pattern.\\nSo I'm going to look for a space and a min,\\nand ensure that that's in every line.\\nSo what I'm going to say is, if space min,\\nand I want to confirm that if it's not the case,\\nthat the space min is in movie runtime,\\nthen I want to just print out unexpected format.\\n\\nAnd you can see that there are no entries\\nthat have an unexpected format.\\nSo that's great.\\nSo what we can do is,\\nlet's head back to movie runtime.\\n\\nAnd let's try and figure out\\nhow we can only capture this numeric piece\\nfrom the string.\\nNow, given this is a string,\\nI can use the split method,\\nthat will separate this out into a list\\nwhere we have these two components,\\nthe number and the text min.\\nSo let me show you what I mean.\\nSo this is the string,\\nand then I can use the split method here\\nto split by this space\\nthat we have in the middle of all of the entries.\\n\\nAnd you can see that I've got a list in every case.\\nAnd what I want to do is,\\nI want to just capture the first bit over here, right?\\nSo I'm going to go ahead\\nand create a new function\\nto allow me to do this.\\nSo I'm going to call this pass runtime.\\nAnd it's going to take in the movie runtime,\\nwhich is going to be a string.\\nAnd this is going to return an integer value.\\nSo what I want this to do is to pass the movie runtime\\nand return the integer value of the runtime.\\n\\nRight? And so we will take any movie runtime,\\nwhich is going to be a string,\\ngoing to split it,\\nso we're going to end up with a list.\\nWe want the first bit, which is going to be the runtime,\\nwhich is this bit.\\nWe don't care about the next bit,\\nwhich is the min, right?\\nSo we ignore that.\\nAnd then we want to return the integer value of this.\\n\\nAnd just to be non-ambiguous about this,\\nI'm going to include a space here,\\n'cause this would, by default,\\nsplit by white spaces.\\nRephrase, and this would, by default,\\nsplit by white spaces.\\nRephrase, and this would.\\nRephrase, and this would split by.\\nRephrase, and this would split white spaces by default.\\nOkay, so we're now in a position\\nto incorporate the past runtime\\ninto our original solution.\\nSo let me just go ahead and get rid of all of this.\\n\\nSo you want to return sorted\\nwhere we have the movie runtime.\\nAnd we want to be able to pass this movie runtime.\\nSo we can do that by using the pass runtime.\\nSo we can pass the movie runtime,\\nsend it across to the pass function.\\nAnd so we have the movie runtime\\nand movie title tuple for movie and movies\\nif shortest is less than.\\n\\nAnd then we want to go ahead and pass this movie runtime too.\\nAnd that should be it.\\nAnd you can see that we've passed all of the tests.\\nSo these three sections correspond to these three tests.\\nNow, this is just one way\\nto go about solving this problem.\\nYou can create your own solution\\nas long as it passes these three tests.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:67980edd3450ec6b4eee6e24\",\"duration\":1200,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Advanced Code Challenge: Find the most nominations\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1229223\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:5237106\",\"duration\":597,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Find the most nominations\",\"fileName\":\"3981841_en_US_02_10_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":778,\"solutionVideo\":false,\"editingNotes\":\"Please add audio PU to end of main movie: 3981841_en_US_02_10_nominations_C_PU_VT.mov\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Combine the results from previous solutions to solve additional requirements.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":22113806,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's head over to our test code.\\nAnd so, this single test corresponds\\nto the section over here.\\nNow, if we take a look at\\nthe function movie most nominations.\\nAnd let's take a look\\nand get a better understanding of our data.\\nSo, for movie in the list movies, and I want to go ahead\\nand just print the awards.\\n\\nAnd you can see that\\nthis is what the awards field looks like.\\nNow, let's think a little bit about\\nwhat our solution will look like.\\nSo, we wanted to get the maximum value for the nominations.\\nAnd so, it's probably going to look something like this.\\nWe wanted to use the max inbuilt function\\nwhere we're going to be passing in our movies,\\nand we want to use the Lambda function, so Lambda\\nmovie\\nin the (movie['Awards']) .\\n\\nAnd if I was to go ahead and just print this,\\nand let me just comment out the top two lines.\\nNow, you'll remember that the actual output we need\\nis the movie title.\\nAnd so, our solution will look something like this,\\nwhere we return the max, and then we want to\\npull out just the\\ntitle.\\n\\nAnd so, we'll want to return\\nsomething that looks like this.\\nNow, the only problem with having used the max function\\nin this way is that it's extracted the awards\\nfrom over here.\\nBut what it's done is it's extracted it\\nbased on the highest alphabetical value\\nor the lexicographical value.\\nAnd we want to ensure\\nthat it's not looking at the entire string,\\nbut rather, just looking at the final section,\\nwhich it has the number of nominations.\\n\\nSo, let's go ahead and just comment this section out.\\nAnd let's take a look at the Awards field.\\nSo, you can see that we've got a string,\\nand it looks like at the first part of the string,\\nwe've got nominated for the number of Oscars\\nor the number of wins, and so on.\\nAnd it's the final section\\nthat has the total number of nominations.\\nNow, it's the second bit that we care about,\\nbut let's make sure that each of the lines\\nfollow the same format.\\n\\nNow, it looks like we've got the nominations\\nand the wins in the first section,\\nand this is separated by an ampersand symbol.\\nAnd then, we finally got the last section,\\nwhich has the number of total nominations,\\nwhich is what we care about.\\nSo, let's confirm that every single line\\ndoes in fact follow this format.\\nSo, for movie in movies,\\nif.\\nAnd what I want to do is I want to take (movie['Awards']).\\n\\nWell firstly, let me just go ahead\\nand demonstrate that I can\\nseparate it out using a list.\\nSo, I'm going to take that string (movie['Awards']).\\nI'm going to split it based on this pattern\\nthat I see over here, which is space, ampersand, space.\\nAnd I'm going to go ahead and print the list that we see here.\\nNow, what I do want to do, rephrase.\\n\\nNow, I need to confirm that I have the format,\\nwhere I will always only have two items in a list.\\nAnd then, that will ensure that every single line\\nfollows the expected format with the first section,\\nthe ampersand symbol,\\nand then the final section which we care about.\\nAnd so, I'm going to confirm that that's the case.\\nAnd let me just go ahead and write some code to do that.\\nSo if the length of the\\nlength of this list\\nis not equal to 2,\\nthen I'm going to say it's an unexpected format.\\n\\nAnd so, let's go ahead and run that bit of code.\\nAnd you can see that we've got no instances\\nwhere the length of the list is not equal to 2.\\nSo that means every single line follows the expected format.\\nSo, let's go ahead and try and extract\\nthe numeric value for the nominations.\\nSo, I'm going to grab that.\\nGoing to get rid of the entire line here, and\\nlet's go ahead and print out the lists.\\n\\nAnd let me go ahead and close the final bracket footprint.\\nAnd you can see that we've got the lists\\nfor each of the lines.\\nNow, let's go ahead and extract the last item of the list.\\nSo, the first item is this section here,\\nwhich is given by 0.\\nAnd the bit that we want to extract\\nis the second section here, which is given by 1.\\n\\nNow, I'm going to make this a little bit easier to see.\\nAnd I'm just going to add a left bracket here,\\nwhich will allow me to format this.\\nAnd so if I just extract the\\nlast item, which is 1,\\nyou can see that I've got the total number of nominations.\\nNow, what I want to do\\nis I want to get rid of all of the text over here.\\nAnd so, you can see I've got nominations,\\nnominations total, nomination, and so on.\\n\\nSo, I want to replace each of these strings\\nwith an empty string.\\nAnd this will allow me to get rid of all of the text strings\\nso that I'm finally left with the numerical value.\\nSo, I want to replace\\nnominations\\nwith an empty string.\\nAnd let's see what the output is here.\\nThat's great.\\nNow, you can see that we are still ended up\\nwith one nomination and a whole load of totals,\\nso let's do exactly the same thing.\\n\\nAnd you can see the importance of first doing\\nthe nominations versus the nomination.\\nSo, the order is really important.\\nBecause this way, we can ensure\\nthat we remove all of the nominations with an s first,\\nand then followed by the ones with a single nomination.\\nAnd then, I want to go ahead and replace.\\nWell, let's take a look at the output from here.\\n\\nAnd then, now what I want to do\\nis get rid of all of the totals\\nand any\\nother white spaces.\\nGreat, and it looks like\\nI'm now left with only numeric values.\\nNow, let me go ahead and convert all of the numeric values\\nto integer because these are still\\nstring values,\\nand\\nI now have only numeric values,\\nand these are the number of total nominations.\\n\\nAnd so, we can go ahead and just\\ngrab this entire section\\nand move it\\nto this section here,\\nwhich has our (movie['Awards']).\\nAnd let me get rid of the\\nsection at the top.\\nAnd let's just make sure that we have\\nthe right number of brackets, so\\nthat bracket closes the int,\\nthis one will close the movie.\\n\\nThis one will close the maximum.\\nAnd we should then be able to just extract the title.\\nAnd you can see that this has passed the test.\\nNow, this is just one way to go about solving this problem.\\nIf you found another way where you've been able to return\\nthe title Oppenheimer, then that's perfectly okay.\\nI hope you've enjoyed these code challenges,\\nand if you're looking for more, I've got code challenges\\non working with data and object oriented programming.\\n\\nNow, if you're looking to dig\\na little bit deeper into Python,\\nI also have a course called\\n\\\"Eight Things You Must Know in Python,\\\"\\nthat is in the LinkedIn Learning Library.\\n\"}],\"name\":\"1. Python Code Challenges for Data Analysis\",\"size\":63059253,\"urn\":\"urn:li:learningContentChapter:5238041\"}],\"size\":73681396,\"duration\":2063,\"zeroBased\":false},{\"course_title\":\"Python Data Analysis\",\"course_admin_id\":4571000,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":4571000,\"Project ID\":null,\"Course Name\":\"Python Data Analysis\",\"Course Name EN\":\"Python Data Analysis\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"&lt;p&gt;Data science has transformed the way that government and industry leaders look at both specific problems and the world at large. Curious about how data analysis works in practice? In this course, instructor Michele Vallisneri explains what it takes to get started with data science using Python.&lt;/p&gt;&lt;p&gt;Michele demonstrates how to set up your analysis environment and provides a refresher on the basics of working with data structures in Python. Then, he jumps into the big stuff: the power of arrays, indexing, and tables in NumPy and pandas. He also guides you through two sample big-data projects: using NumPy to identify and visualize weather patterns and using pandas to analyze the popularity of baby names over the last century. Challenges issued along the way help you practice what you've learned. Plus, learn about the skills in the basic tasks of data analysis: importing and wrangling, summarizing and visualizing, modeling, and reasoning. &lt;/p&gt;&lt;p&gt;This course is integrated with GitHub Codespaces, an instant cloud developer environment that offers all the functionality of your favorite IDE without the need for any local machine setup. With GitHub Codespaces, you can get hands-on practice from any machine, at any time\u00e2\u20ac\u201dall while using a tool that you\u00e2\u20ac\u2122ll likely encounter in the workplace. Check out the \u00e2\u20ac\u0153Set up: Using Codespaces\u00e2\u20ac\u009d video to learn how to get started.&lt;/p&gt;\",\"Course Short Description\":\"Interested in using Python for data analysis? Learn how to use Python, NumPy, and pandas together to analyze data sets large and small.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":4666188,\"Instructor Name\":\"Michele Vallisneri\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Professor of Gravitational Physics at the Swiss Federal Institute of Technology\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2025-01-09T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"Yes\",\"LIL URL\":\"https://www.linkedin.com/learning/python-data-analysis-24296803,https://www.linkedin.com/learning/python-data-analysis-revision-fy25-q2-coderpad-codespaces,https://www.linkedin.com/learning/complete-guide-to-python-data-analysis-coderpad-codespaces,https://www.linkedin.com/learning/python-data-analysis-1-tools\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"General\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Data Science\",\"Primary Software\":\"Python\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":13399.0,\"Visible Video Count\":57.0,\"Learning Objectives\":\"Demonstrate proficiency in using Python data structures, including tuples, lists, dictionaries, sets, and comprehensions, as well as advanced data structures like defaultdicts and data classes.,Manipulate and analyze data effectively using NumPy, including creating arrays, indexing, performing mathematical operations, working with special arrays like records and dates, and leveraging the NumPy ecosystem.,Utilize pandas to work with structured data, including creating and manipulating DataFrames and series, indexing, performing mathematical and plotting operations, and conducting database operations.,Import, wrangle, and preprocess data using pandas, including cleaning, filtering, reshaping, tidying, and simulating data, as well as linking and merging databases.,Summarize and visualize data using various techniques, such as exploring data, summarizing quantitative and categorical data, visualizing distributions and categorical data, and comparing variables, leveraging tools like Plotly and Dash for interactive dashboards.\",\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":548,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:6010025\",\"duration\":55,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"From data to insight with Python\",\"fileName\":\"4571000_en_US_00_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":110,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Using Python, you can analyze data rapidly using powerful tools adopted by a large and helpful community. This video shows you how to load a weather dataset and create an impressive visualization using just a few lines of code, which the instructor works through in detail in chapter four of this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2820303,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Python is an excellent tool for data analysis\\nbecause it's friendly, pragmatic, and powerful.\\nBut what I like the most about Python\\nis that it can be a lot of fun.\\nWhether you're interested in data analysis for work, school,\\nor for your own personal interest,\\nPython can make it enjoyable.\\nWe will start this course by looking at tools\\nfor data analysis with three rapid tutorials, each followed\\nby a practical project.\\nWe will review Python data structures\\nand their manipulation in the core language.\\n\\nThen we will switch to the three typical phases\\nof a data analysis project, importing and wrangling data\\nto prepare for your analysis, summarizing and visualizing\\nto explore and understand the dataset,\\nand building statistical models to explain the data.\\nHi, I'm Michele Vallisneri.\\nLet's get started with a complete guide\\nto Python data analysis.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6007138\",\"duration\":82,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you need to know\",\"fileName\":\"4571000_en_US_00_02_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":90,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"To learn the most from this course, you need an elementary knowledge of the Python language, which you can obtain from other LinkedIn Learning courses. However Python's data structures\u2014which are crucial to data analysis\u2014are reviewed in this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2115131,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Before you take this course,\\nyou'll want to have a basic working knowledge\\nof programming in Python.\\nAlthough we will review the aspects of Python\\nthat are essential to any data analysis task,\\nI will not teach you every feature that we will need.\\nIt will also be helpful to have an understanding\\nof basic mathematical and statistical concepts,\\nsuch as logic operations, functions,\\naverages, minima and maxima, and so on.\\nIn the future, as you move from basic data analysis\\ninto data science,\\nyou will need a deeper knowledge\\nof mathematics and statistics\\nand a familiarity with the particular field\\nthat you are studying.\\n\\nHowever, a strong programming foundation\\nand being able to rely on a robust tool such as Python\\nwill make it easier for you\\nto learn mathematics and statistics,\\nnot just by studying and reading,\\nbut also by practical experimentation.\\nNowadays, in addition to many resources\\non the web and in the literature,\\nyou can rely on assistance from large language models.\\nThey seem to know a lot about programming\\nand even about data analysis libraries.\\nI encourage you to take full advantage of them.\\n\\nHowever, the basic knowledge\\nand context that you will learn here\\nwill help you ask the right questions\\nand identify cases\\nwhere the neural network is just making things up.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5994435\",\"duration\":43,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What is new in this update\",\"fileName\":\"4571000_en_US_00_03_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":47,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Like the original version, this course covers the foundations of data analysis with Python\u2014data structures, NumPy, pandas, and Matplotlib\u2014using practical real-world examples. The course is updated to reflect recent changes and new features in the interfaces of those modules and in the Python language. GitHub Codespaces and CoderPad challenges have been added to the course so you can learn interactively and explore on your own.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1095226,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The first part of this course,\\nwhich covers the core language NumPy and pandas,\\nis an update to my Python data analysis course,\\nand it reflects the most recent changes\\nand improvements in the core language and in the libraries.\\nThe second part of this course is entirely you.\\nIt is based on my real-world experience in analyzing data,\\nand it covers the typical workflow\\nof data analysis in Python.\\nIn addition, this updated course was created using\\nGitHub Codespaces where you have all that you need\\nto experiment without complex installation\\nor tools on your computer.\\n\\nI have also created five CoderPad interactive challenges\\nin which you will be able to demonstrate your new skills\\non the same data sets that we analyzed.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6007139\",\"duration\":241,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Set up: Using Codespaces\",\"fileName\":\"4571000_en_US_00_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":373,\"solutionVideo\":false,\"editingNotes\":\"Overlay: 1:50 https://docs.github.com/en/codespaces/overview, 4:40 https://code.visualstudio.com/docs/datascience/jupyter-notebooks. \\n\\nUSE TK2. In both videos, there's a long pause while the codespace is running that must be removed \\n\\nhttps://github.com/LinkedInLearning/complete-guide-to-python-data-analysis-4571000\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch00-introduction > 00_04_codespace.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8611764,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The exercise files for this course\\nare collected in a GitHub repository organized by chapter.\\nYou can clone the Git repository\\nor download the files to your computer\\nusing the code button.\\nFrom that same button,\\nyou can also start an online codespace,\\na full development environment hosted in the cloud.\\nThat's what I recommend you do\\nto follow along with this course.\\nHere, I have one going already named Glorious Winner,\\nbut I'll create a new one.\\n\\nIf you're not logged into GitHub yet,\\nyou will be prompted to log in\\nor create a free GitHub account.\\nAnd the first time you open a codespace,\\nit will take a few minutes to set up.\\nYou now have control over a virtual Linux machine\\nin the cloud, which connects to an instance\\nof Microsoft Visual Studio Code in your browser.\\nThis is a powerful editor and programming environment.\\nThis virtual machine is pre-installed\\nwith the Python distributions\\nwith all the packages that we need.\\n\\nThe editor is preloaded with extensions\\nthat will help our work in Python.\\nLet me close the terminal at the bottom\\nand focus on the files in Explorer on the left.\\nThese are all our exercise files organized by chapter,\\nalong with all the data files that we will analyze.\\nTo follow along with each video,\\nyou will click on the corresponding Jupyter Notebook file.\\nThe notebook opens in the editor on the right,\\nand it is prefilled with the code\\nthat we will be running organized in cells.\\n\\nBefore I can run code,\\nI need to select a Python executable kernel\\non the top right.\\nIn this course, we will always use the default Python 3.12.\\nTo select the cell, I click into it,\\nor I use the arrows to go up and down within the notebook.\\nTo run the code in a cell,\\nI hit Shift + Return on the keyboard.\\nThe cell output will appear just below it.\\n\\nJupyter Notebook will happily display figures\\nas well as text.\\nSome cells may take a few seconds to run,\\nbut they're usually pretty fast.\\nTo edit the content of a cell,\\nI select it, then hit Return.\\nWe'll mouse click inside the cell.\\nThen I can edit its content and run it again.\\nThe notebook is automatically saved with your code\\nand the resulting output.\\n\\nYou can learn more about using notebooks\\nin many tutorials on the web.\\nThe completed folder within each directory\\nholds the completing notebooks in the state that we have\\nafter we've gone through them in each video.\\nIf you should happen to close this browser window,\\ndon't worry.\\nThe codespace is automatically saved in the cloud\\nand you can get it back from the repository.\\nYou can also restart the codespace from here\\nif you return to the course after a pause.\\n\\nGitHub codespaces work on mobile devices,\\nbut if you can, I recommend that you practice\\nalong with this course on a laptop or desktop computer.\\nIf you wish to create your own copy of the repository,\\nyou can do so by forking the repository\\non the top right of the GitHub screen.\\nThen if you start a codespace from the fork,\\nyou'll be able to commit the changes that you make\\nin the codespace.\\nFor that, you'd use the Source Control tab.\\n\\nYou're, of course, free to download the code\\nand use your own Python installation instead.\\nIn that case, you will need to install all the packages\\nlisted in the requirements.txt file\\nwithin the repository.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6009039\",\"duration\":127,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"CoderPad challenges\",\"fileName\":\"4571000_en_US_00_05_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":165,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4263895,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] This course includes\\nautomated code challenges\\nthat appear when you click on the challenge links\\nin the course's table of contents.\\nEach challenge includes instructions\\nand a couple of code editors you can use to create\\nand test your own solution to the challenge.\\nThe challenges are hosted by CoderPad,\\nand they appear in the same area of the course page\\nwhere you watch the course's videos.\\nWe recommend using a desktop browser\\nfor the best experience with code challenges,\\nbut you can use the LinkedIn Learning mobile app\\nif you prefer.\\n\\nThe code challenge has four areas:\\nInstructions in the top left.\\nA code editor for your answer in the top right.\\nAnother code editor where you see\\nhow your code is used in the bottom right.\\nAnd a console for output in the bottom left.\\nYou can use these drag handles\\nto allocate space as you like.\\nTo get even more horizontal space,\\nyou can collapse the course's table of contents on the left.\\nA challenge has instructions\\nthat include the description of what you need to do\\nwith the parameters and desired results.\\n\\nHere we are told that we need to find\\nthe largest number in the list\\nand we need to provide the function find_largest,\\nthat takes a list, as discussed in the parameters,\\nand returns the maximum, as discussed in the result.\\nYou create your answer in the top right code editor.\\nThere are comments in the starting code\\nshowing where to put your solution.\\nWe can just use the built-in function max.\\nWhen you click Test my code,\\nyou see a message indicating\\nwhether your code return the correct result.\\n\\nHere it did.\\nBut if your code is not successful,\\nyou can ask for help.\\nThe show_expected_result and show_hints variables\\ndetermine whether you see the expected result.\\nAnd possibly some hints.\\nChanges variables to control the output.\\nGood luck with the challenges in this course.\\n\"}],\"name\":\"Introduction\",\"size\":18906319,\"urn\":\"urn:li:learningContentChapter:5994436\"},{\"duration\":1504,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2732061\",\"duration\":328,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Warmup with Python loops\",\"fileName\":\"4571000_en_US_01_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":411,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch01-datastructures > 01_01_loops.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"There are many use cases in data analysis where you need to operate on data in a sequential fashion. In Python, you do so with looping constructs. This video provides a refresher on the Python loop syntax by solving a simple problem\u2014finding all the ways in which you can break a dollar into a set of coins.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10212509,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We begin every video\\nby importing the standard set of Python modules\\nthat we will need throughout this course.\\nThis cell does that,\\nso we execute it by selecting it with a cursor\\nand pressing Shift + Return.\\nIf you've written computer code before,\\nyou must be familiar with loops.\\nThey occur in many different computing problems,\\nand they allow us to automate repetitive operations.\\nThey're also a good topic\\nto start a quick review of the Python code language.\\nWe see loops in a concrete example.\\n\\nConsider a combinatorial problem\\nof breaking $1 U.S.\\ninto all possible combinations of coins.\\nFor instance, four quarters,\\ntwo quarters and five 10 cent dimes, and so on.\\nNow, if we have to add coins up to $1,\\nwe can use at most $1 coin, two 50 cent coins,\\nfour quarters, and so on.\\nWe can also mix different values.\\nYou can already see\\nhow we're going to need several nested loops\\nto figure out all the combinations.\\n\\nWe start with a very simple-minded strategy.\\nFor each face value, we enumerate all possible quantities,\\nzero to one $1 coins, zero to two half dollar coins,\\nand so on.\\nSo we'll need six nested loops.\\nLuckily, Python will keep track of them.\\nThe basic structure of a loop in Python\\nis for variable in iterable.\\nWe follow that with a block of code that we are repeating\\nwith a variable taking on the values\\nprovided by the iterable.\\n\\nBut what is an iterable?\\nWe can think of it as a black box\\nthat keeps providing new values until it runs out.\\nA simple example is a Python list.\\nPerhaps the most commonly used iterable is range,\\nwhich provides integer values\\nfrom a start value to an end value,\\nexclusive of the end value.\\nThis means that range 0 to 10 counts from zero to nine.\\nThere are many reasons for this convention.\\n\\nFor instance, by looking at the end value,\\nwe see immediately the total number of elements,\\nin this case, 10.\\nIn the end, we just have to accept this\\nas one of the building assumptions of the language.\\nRange has a couple more interesting features.\\nIf we omit the start value, it is assumed to be zero.\\nAnd also we can provide a step argument\\nto move through the range in increments larger than one.\\nWhen we do that, we also need to specify the initial value\\nto avoid confusion.\\n\\nLet's go back to our dollar and build out our nested loops.\\nWe need from zero to one $1 coins,\\nwhich means that the first range object will be range two.\\nWe write it here as 1 + 1.\\nWe then proceed with other coins, one loop for each.\\nEach nested loop is indented with respect to its parent.\\nWe do this with a tab.\\nInside the innermost loop,\\nwe will check whether the total amount is $1.\\n\\nIf so, we had the combination to a list,\\nwhich we initialize as the empty list.\\nSo let's try this out.\\nVery good.\\nThe first few combinations seem to check out fine.\\nIt turns out that there are 293 ways\\nto get $1 out of change.\\nThis is the solution to the problem that we post.\\nHowever, in this analysis,\\nit often happens that the solution raises a new question.\\n\\nFor instance,\\nhow many ways to make $2 out of change, or three?\\nDoes the number of combination increase linearly\\nor quadratically?\\nWhat we need to do is to take the code we wrote\\nand generalize it to answer those questions.\\nBefore we do so, we make a couple of changes to our code\\nto make it faster and easier to generalize.\\nThis is an example of refactoring.\\nThe first change is that we will not loop by count,\\nbut by value using the step argument.\\n\\nFor instance, instead of the loop over quarters\\nfrom zero through four, we will loop over the values.\\nThe second change is to recognize\\nthat we don't actually need the innermost loop.\\nAs long as the total up to that point is less\\nor equal $1,\\nthen we can always make up the difference with pennies.\\nSo we write one fewer loop.\\nThis looks all right.\\n\\nWe're almost done.\\nNow we can take our code\\nand make it into a function find_combinations\\nby replacing the value 100 everywhere\\nwith a variable argument.\\nWe also wrap our computation with def\\nand return statements as appropriate for a function.\\nSo let's see how we can make 2 and $3.\\nYou may be curious to know how fast this number grows.\\nA Matplotlib plot should give us an idea.\\n\\nIn fact, this number grows approximately\\nas the fifth power of the total value.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6006171\",\"duration\":406,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Tuples, lists, and the slicing syntax\",\"fileName\":\"4571000_en_US_01_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":572,\"solutionVideo\":false,\"editingNotes\":\"pickup at 4:43\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":true,\"exerciseFileDisplayText\":\"ch01-datastructures > 01_02_list_tuples.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"The built-in sequence types\u2014tuples, lists, ranges, strings, and buffers\u2014in which data elements are associated with indices, set the basic framework for accessing sequential data in Python and transfer directly to NumPy. This video provides a refresher on the creation and indexing of sequences, reviews the important distinction between mutable and immutable types, and highlights the slicing interface.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12821580,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] That was a good warmup about loops.\\nWe now move on to review lists and tuples,\\nwhich are perhaps the quintessential Python data structures.\\nThey're very useful on their own,\\nbut they're also foundational for data science\\nbecause they set the standard interface\\nfor accessing elements and ranges of elements,\\nbut they're all in an index.\\nPython calls that slicing.\\nThe same interface is used in NumPy,\\nthe most important Python library\\nto manipulate large amounts of numerical data.\\n\\nSo about lists, as you know, they provide a way\\nto store an arbitrary number of Python objects,\\nsuch as strings, floating point numbers,\\nother lists, or any other object,\\nand to access them using a numerical index.\\nIn Python, lists are denoted by brackets,\\nand their elements are separated by commas\\nFor instance, the tree of nephews from the Disney universe.\\nThe length of a list is obtained with len.\\n\\nThe empty list is written with an empty set of brackets,\\nand obviously it has length zero.\\nIndividual list elements can be accessed by index,\\nstarting with zero for the first element\\nand ending at the length of the list minus one.\\nFor instance, the first nephew is Huey,\\nso the last nephew would be at index two.\\nIf we look beyond the end of the list, we get an error.\\n\\nWe can also index from the end,\\nstarting at minus one and going back.\\nThe last nephew is Louie,\\nand the penultimate nephew is Dewey.\\nThis bracket indexing notation can also be used\\nto reassign elements.\\nLet's do it for all these ducks with a simple loop.\\nAn important point is that lists\\ndo not need to have homogeneous content,\\nsuch as all strings or all numbers.\\n\\nWe can mix it up.\\nWe can have a list consisting of a number,\\nanother list, and a string.\\nWe can verify that an element exists in a list\\nusing the in operator.\\nHuey alone does not belong in nephews,\\nbut Huey Duck does.\\nTo add a single element at the end of a list, we use append.\\nYou see that here, we're using Python\\nin an object-oriented way by accessing a method,\\nspecifically append, of the list object.\\n\\nIt sounds sophisticated, but it's actually very natural.\\nTo add multiple elements in one go, we use extend.\\nTo concatenate two lists, we use a plus.\\nLast, we can insert elements at any position in a list\\nusing the insert method.\\nWe have seen how to build up lists,\\nnow let's break them down.\\nWe can delete elements either by their index, with del.\\n\\nSo we took out Scrooge, or by their value, with remove,\\nand so we took out Uncle Donald.\\nWe may want our list sorted,\\nand we can do this in place\\nmodifying an existing list with sort,\\nor we can make a new sorted list\\nout of an existing one with sorted.\\nThis demonstrates also how to sort backwards.\\n\\nIt's very easy to loop over a list.\\nWe don't even need indices for that.\\nMoving on to slices.\\nBeyond working with individual list elements,\\nwe can manipulate them in contiguous groups.\\nWe'll take a numerical example,\\nthe first few squares of the natural numbers.\\nThe convention to select the slice in Python\\nis the same as for loops.\\nThe first index is included, the last is not.\\nIt pays to imagine that the indices sit\\non the edges of the elements.\\n\\nSo if we want the first two squares,\\nwe'd write a slice that goes from zero to two.\\nThere are a few more tricks and shortcuts\\nthat we can use in slicing.\\nWe can omit the starting index to start at the beginning.\\nWe can omit the ending index\\nto include elements to the end.\\nWe can omit both to get a copy of the entire list.\\nWe can move through the indices in steps.\\nAnd finally, we can use negative indices\\nto count from the end.\\n\\nOne last trick is reversing a list using a negative step.\\nWe also use slices to reassign a subset of items\\nor to delete them.\\nAt this point, I think I've ruined my list of squares.\\nNow for tuples, which look like lists,\\nbut with parenthesis instead of brackets.\\nTuples are sometimes described\\nas immutable versions of lists.\\n\\nOnce a tuple is defined,\\nwe cannot modify its element, so add new ones.\\nThis is a feature, not a bug.\\nIt ensures the integrity\\nand makes it possible\\nto use tuples as keys in dictionaries or indices.\\nNevertheless, we can perform the same indexing\\nand slicing tricks as for lists,\\njust not assignment.\\nOne context in which one sees tuples often in Python\\nis triple unpacking,\\nwhere Python statements or expressions\\nare automatically evaluated in parallel over a tuple.\\n\\nFor instance, we assign multiple variables at once.\\nAnd the parenthesis can even be omitted\\nwhen there's no room for ambiguity.\\nTuples appear also\\nwhen we iterate over multiple variables at once.\\nFor example, using the enumerate iterator,\\nwhich lets us loop over list index\\nand list elements together.\\nOne final useful trick is unpacking a tuple\\nto pass it to a function that requires multiple arguments.\\n\\nThat's done with an asterisk.\\nConversely, the asterisk or star operator\\ncan also be used to define functions\\nwith a variable number of arguments\\nwhich then are collected into a single tuple.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6006170\",\"duration\":272,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Dictionaries and sets\",\"fileName\":\"4571000_en_US_01_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":400,\"solutionVideo\":false,\"editingNotes\":\"pickup at 3:06\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":true,\"exerciseFileDisplayText\":\"ch01-datastructures > 01_03_dicts_sets.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Python dicts associate unique keys with values, while sets express collections of data items without duplication. These data types complement sequences in many common data-analysis tasks, and provide a basis for pandas DataFrames. This video provides a review of the creation, updating, and indexing of Python dicts and sets.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8334196,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The other super important data structure\\nin Python is the dictionary or dict.\\nWhile lists and tuples give us a way to retrieve values\\nby their numerical index,\\ndictionaries associate values to unique keys.\\nDicts are written with curly braces\\nseparating items with commas.\\nEach item is given as key colon value.\\nFor instance, here are the capitals\\nof some of my favorite countries.\\nThe length of a dictionary is obtained with len,\\nand the empty dictionary is denoted by empty braces.\\n\\nJust as we do with lists,\\nvalues are accessed with a bracket notation,\\nbut instead of a number, we're going to use a key,\\nwhich is usually a string.\\nThe same notation can be used to add items to a dictionary.\\nTrying to access a non-existent item results in a key error.\\nTo avoid that, we can check beforehand\\nwhether an item exists in a dictionary\\nusing the in operator.\\nCombined two dictionaries,\\nwe can unpack them within the same braces\\nusing the double star unpacking operator,\\nwhich works in a similar way to the star operator for lists.\\n\\nIf keys are repeated, the last one is used.\\nWe can also update a dict in place using another dict.\\nSimilarly to lists, we can delete items by key.\\nIn fact, keys do not need to be strings.\\nAny Python object that is hashable may be used as a name.\\nHashable means that Python can convert\\nthe object to a number.\\nThat's true for strings, numbers, and tuples,\\nbut not for lists.\\n\\nWe can see the internal representation\\nof the keys with hash.\\nLooping over a dictionary is very similar\\nto looping over a list.\\nHowever, there are three different kinds\\nof loops you may want to write.\\nOver keys, over values, and over both together.\\nThe pairs are known as items.\\nThe most straightforward syntax loops over the keys,\\nbut we can also write this more explicitly.\\nHere are the keys of this dictionary.\\n\\nThis is a special iterable object, much like range,\\nwhich can however be turned into a list.\\nThe other two dict looping constructs are over values\\nor over keys and values together using tuple unpacking.\\nValues and items are also special iterable objects.\\nBeginning in Python 3.6 for the C Python interpreter\\nand Python 3.7 for the very language,\\nthe order of insertion is preserved for dicts.\\n\\nThis means that when we loop over keys or items,\\nwe get them in the order\\nin which we originally inserted them.\\nThat's not the case in previous version of Python.\\nSo the standard library defined a special object,\\nordereddict, to preserve the order.\\nThat's not necessary now.\\nThere is, however, another specialized dictionary\\ndata structure that is very useful.\\nThat is the defaultdict,\\nwhich you set up to return a default value\\ninstead of an error when an item has not been set.\\n\\nOur default will be, I don't know.\\nNow let me add the original capitals into this object\\nand demonstrate that, what's the capital of Canada?\\nI don't know.\\nDicts are very important in Python\\nsince they underlie many aspects of the language itself,\\nsuch as the methods and attributes of classes\\nwhich are stored internally in dicts.\\nAnd the interface by which we access dict values using keys\\nis also adopted in the Python data analysis library Pandas,\\nso it's good to be familiar with it.\\n\\nLast, I want to mention sets.\\nYou can think of them as bags of items.\\nThese can be of any immutable types\\nand they're never duplicated.\\nSo adding Africa twice does nothing.\\nYou can check if an item exists in a set.\\nYou can add an item, you can remove it,\\nand you can loop over a set, but there's no indexing.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6007137\",\"duration\":228,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Comprehensions\",\"fileName\":\"4571000_en_US_01_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":304,\"solutionVideo\":false,\"editingNotes\":\"at 0:55, use highlights on screen to emphasize parts of list comprehension as I discuss them; same thing at 2:04\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch01-datastructures > 01_04_comprehensions.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Python comprehensions provide a readable and expedient way to create, transform, and filter sequences, dicts, and sets; they are used extensively in the next chapter. This video provides a review of the comprehension syntax\u2014including multilevel comprehensions\u2014and draws analogies to Python loops.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7961757,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] When working with data in Python,\\nthere are many cases when we want to iterate\\nover a list or dict, perform an operation on every element,\\nand then collect all the results into a new list or dict.\\nWe can certainly do that with a for loop.\\nFor instance, we can compute the first 10 squares,\\nstarting with an empty list,\\nand adding elements to the list in the body of the loop.\\nBut we can do better. We can be more Pythonic.\\nPython offers a great feature comprehensions\\nthat lets us write shorter, more easily readable code.\\n\\nIn essence, the comprehension will be a compressed version\\nof the for loop.\\nLet's go through the steps.\\nWe want a list, so we have brackets.\\nIn front, we have the body of the loop, the square.\\nIn the back, we write the looping construct\\nfor variable in iterable.\\nHere, it's a range.\\nThe result is the same as writing out the loop,\\nbut we managed to express the same meaning\\nin a very readable and very efficient way.\\n\\nWe can also filter the list of elements\\nthat we are creating, adding an if close.\\nFor instance, we may want to collect only the squares\\nthat are divisible by four.\\nAgain, quick and readable.\\nIn Python 3, comprehensions largely replace\\nthe map and filter built-in functions,\\nwhich are important in so-called functional languages,\\nbut did not really belong in Python.\\nThe syntax for a dictionary comprehension\\nis also rather intuitive.\\n\\nFor instance, let's create a dictionary\\nthat will get us the square of an integer\\nfrom the integer itself.\\nIt's a dictionary, so we need braces.\\nIn front, we have the key colon value pair.\\nIn the back, the looping construct.\\nWe can also add an if close if we want,\\nbut I don't need one here.\\nDict comprehensions are sometimes used\\nto transpose an existing dict.\\nLet's go back to our capitals,\\nwhich we wrote as a dictionary index by country.\\n\\nHere's the transpose.\\nCountries indexed by capital.\\nSometimes you see a naked comprehension\\nwith the body and loop without the brackets.\\nThat's called a generator expression,\\nand it's useful to generate a sequence\\nand immediately consume the elements one by one\\nwithout ever storing them in a list or dict.\\nFor instance, to take the sum of the first 10 squares,\\nwe may write the interior part\\nof our earlier list comprehension\\nand feed it directly to the Python built-in sum.\\n\\nDoing this saves memory and time,\\nwhich is important if you deal with large amounts of data.\\nComprehensions are incredibly useful\\nwhen we manipulate lists, dicts, and sets of data.\\nHere's a summary of their syntax.\\nIn addition to lists and dicts,\\nyou can also use them for sets and tuples.\\nOnce you start taking advantage of comprehensions,\\nyou're now going to stop.\\nYou start doing all sorts of acrobatics,\\nsuch as nested comprehensions to make a list of lists,\\nwhich you can think of as a matrix,\\nor a single comprehension with nested loops\\nto flatten the matrix into a list.\\n\\nThe only difference is that we have only\\none set of brackets.\\nNowadays, comprehensions are a key part\\nof the core Python language.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6010024\",\"duration\":270,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data classes\",\"fileName\":\"4571000_en_US_01_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":407,\"solutionVideo\":false,\"editingNotes\":\"at 5:31, show overlay link to https://docs.pydantic.dev/\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch01-datastructures > 01_05_dataclasses.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Data classes are the best way to encode and process structured, heterogeneous data in Python. This video provides a review of their creation and use as well as their validation using Pydantic.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9602871,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let us look at Python data structures\\nfrom the perspective of a data scientist or a data analyst.\\nWhat are the options to store tabular data,\\nsuch as a table of famous people\\nwith their names and birthdays?\\nA list of Python dicts is certainly a possibility.\\nIt's easy to access the columns by the key\\nand to query the rows using comprehensions.\\n\\nFor instance, famous people with a birthday on July 15.\\nAnother possibility are tuples,\\nor even better, the namedtuples from the collections module\\nin the Python starter library.\\nWith these, we create a specialized tuple\\nthat associates labels with columns.\\nThe syntax to create a person is intuitive.\\nWe can also meet the labels.\\n\\nThe columns can be accessed with a dot notation\\nof Python object attributes.\\nAlthough regular tuple indices would also work.\\nWe can convert these tuples\\nfrom and to a dictionary using ** unpacking,\\nand the namedtuple method asdict.\\nIf you're wondering about the underscore\\nis there to avoid confusion in case you really want to use\\nasdict as a label.\\n\\nPython 3.7 introduced an alternative to tuples and dicts\\nfor storing records, dataclasses,\\nin the module dataclasses.\\nIn a dataclass, we list the fields in order\\nand specify their type, such as int or string.\\nWe can also provide a default value.\\nThis is how we would set up a person record\\nwith name, last name, and birthday.\\nThe syntax to initialize a dataclass is intuitive\\nand we can either use or omit the labels.\\n\\nAs with tuples, we access fields by name.\\nSo far, this is very similar to namedtuple,\\nhowever, dataclass is a full Python class,\\nso we can define methods that operate on the fields,\\njust as we do in any Python class.\\nThe first compulsory variable in a method\\nrefers to the particular instance of the class.\\nFor instance, a person.\\nLet's do methods that return a person's full name\\nby combining name and last name,\\nand that provide a prettier printout.\\n\\nLet's see them in operation.\\nDataclasses have a number of other useful features,\\nsuch as freezing, so that fields cannot be changed,\\nsorting, which works by comparing fields in their order\\nor with a custom comparison function,\\nand special computed fields.\\nHere, I encourage you to stop the video for a moment\\nand experiment in your notebook with this variance.\\n\\nOne thing that we have not seen is how the type of a field,\\nsuch as string, is used with dataclasses.\\nIn fact, by default, it's not.\\nBut it is made available to third-party packages,\\nfor instance, to validate data entry.\\nAn excellent package for that purpose is Pydantic.\\nTo use it, we replace the standard dataclass decorator\\nwith its equivalent in Pydantic.\\n\\nWe also write a custom validator for the birthday,\\nwhich we'll try to convert it to a Python datetime object,\\nand raise an exception if that's not possible.\\nHere are two examples of failed validation.\\nPydantic is a very sophisticated\\nand powerful package with many features.\\nIt's also compatible with many data analysis\\nand data science packages.\\nIf your project requires substantial data validation,\\nit'll pay to dig into Pydantic.\\n\\nThis completes our overview\\nof basic data structures in Python.\\n\"}],\"name\":\"1. Python Data Structures\",\"size\":48932913,\"urn\":\"urn:li:learningContentChapter:6010026\"},{\"duration\":569,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5994434\",\"duration\":63,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview: Finding anagrams\",\"fileName\":\"4571000_en_US_02_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":81,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this chapter, you can practice Python data structures with a simple example on textual data\u2014listing and analyzing anagrams in English and French dictionaries. This video discusses the basic technique (signature) that is used to find anagrams and outlines the general strategy to map the technique to data structures.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1517164,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In chapter one,\\nwe have reviewed Python loops,\\ndata containers, and comprehensions.\\nWe will get some practice\\nwith these elements of the core language\\nin a simple, practical project,\\nfinding anagrams in the English dictionary.\\nAs you know, two words are anagrams of each other\\nwhen their letters can be rearranged\\nto turn one word into the other.\\nFor instance, stop can be anagrammed into post, spot,\\ntops, pots, and opts.\\nWe'll use this simple strategy.\\n\\nWe define the signature of a word\\nas the sorted list of its letters including duplicates.\\nSo the signature of Python would be hnopty.\\nTwo words are anagrams of each other\\nif they have the same signature.\\nThus, we are going to make a Python dict\\nof all the words in a dictionary indexed by the signature.\\nLooking up if a word has an anagram\\nwould then be as simple as computing its signature\\nand looking it up in the dict.\\nLet's begin.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2732060\",\"duration\":183,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Loading dictionaries from text files\",\"fileName\":\"4571000_en_US_02_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":226,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch02-anagrams > 02_02_loading.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In many cases, parsing a text file manually is the fastest way to complete a simple data analysis task. This video shows how to load a list of words from a file, and briefly discusses how Unicode allow you to handle international character sets transparently.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5808217,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We begin by loading a dictionary from a file.\\nThe repository contains the 1934 English dictionary\\nthat is distributed with many Unix systems.\\nWe will load it into a list.\\nIn Python, we talk of idioms\\nwhen we think of code constructs\\nthat have become the preferred way\\nto achieve a certain goal.\\nOne example is looping through all the lines\\nof the text files.\\nFor that, we open the file for reading\\nand use the file as a notable in a for loop,\\nwhich has the result of giving us the lines one by one.\\n\\nFor the moment,\\nwe will just collect all the lines into a list.\\nWe get more than 200,000 words.\\nLet's look at the first 10.\\nVery good.\\nI do see two problems, though,\\nevery word ends in the new line character,\\nand also some words are capitalized,\\nwhich will interfere with our signature scheme.\\nWe can fix both issues using Python string methods.\\nTo strip leading and trailing white space,\\nincluding new lines, we can apply strip.\\n\\nAnd to switch the entire string to lowercase,\\nwe use the method lower.\\nWe now have something more interesting to do\\nin the body of the loop.\\nAh, I do see a duplicate, which comes from the A\\nappearing both in uppercase and lowercase.\\nOne way to get rid of that\\nis to build not a list, but a set.\\nSo once again, we iterate through the file,\\nbut we replace the initial empty list with an empty set,\\nand we replace the method append with add.\\n\\nGiven that the body of the loop is just one line,\\nwe can do it more idiomatically with a comprehension.\\nFinally, to get the list in alphabetical order,\\nwe can just wrap the set in the Python built-in sorted.\\nWe are now ready to make anagrams.\\nBy the way, if you want to try a different language,\\nsuch as French, you just need the right dictionary.\\nNow, Python strings are natively Unicode,\\nmeaning that they can handle\\ninternational character sets transparently.\\n\\nWe need some accents for the French language.\\nThe characters are encoded internally using multiple bytes,\\neither one, two, or four, as needed.\\nThe only care that we need to take\\nis to tell Python which encoding\\nto use for the files we read and write.\\nThe Unicode standard includes multiple encodings\\nthat map character sets to their representations in bytes.\\nWe just need to know the right one.\\nThe most common are UTF-8 and UTF-16,\\nbut a few legacy encodings are also useful.\\n\\nAnd indeed, the French dictionary\\nincluded in your exercise files\\nwas written using ISO 8859, known also as Latin-1.\\nSo all we need to do is to tell Python to open the file\\nwith that Latin-1 encoding.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6008087\",\"duration\":257,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Finding anagrams\",\"fileName\":\"4571000_en_US_02_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":372,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch02-anagrams > 02_03_finding.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"See how to use Python comprehensions to create Python dicts that let you list anagrams and sort them in various way. These examples show the flexibility and legibility of Python comprehensions.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9198964,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] We pick up our project where we left it\\nIn the last video.\\nWe have made a sorted list of lowercase words.\\nNow remember our strategy of comparing signatures.\\nA signature is the sorted list of the component letters.\\nWe need a function to make signatures.\\nTaking again, the string aaron as an example,\\nlet's see what happens if we sort it,\\nwhich we can do with a built-in sorted.\\nWe get a sorted list of the letters.\\nWe can use this as a signature to verify,\\nsay that Elvis lives.\\n\\nOr at least it lives, as anagram of Elvis.\\nUnfortunately, David Bowie does not.\\nFor convenience, we will collate the list of characters\\nback into a single signature string.\\nThe way this is achieved in Python looks a little strange.\\nSince we need to call a method join\\non the string that specifies the connector.\\nThe connector that we want is really the empty string.\\n(keyboard clatters)\\nFinally, we make a function\\nthat performs this operation in general,\\nand not just on aaron.\\n\\nOur strategy is based on building a python dictionary\\nof words indexed by signatures.\\nThat is the keys will be signatures,\\nand the values will be sets of words.\\nWe'll call it words_by_signature.\\nWe can make it with loop.\\nThis works fine, but we should improve on this awkward\\nand repetitious code.\\nThe problem is that if a signatory\\nis not already in the dictionary,\\nthen we need to create a set with a single word.\\n\\nBut if it is there, we need to add a new word\\nto the existing set.\\nWe can avoid this complication by using a default dict\\nthat will automatically initialize a signature\\nwith an empty set.\\nThe argument to default dict must be a function\\nthat makes the default value.\\nWe call it a factory sometimes.\\nSo in this case, set will do.\\nIf you call it without parameters,\\nyou just get the empty set.\\nPerfect. This code is much nicer.\\n\\nWe can save some memory\\nand CPU by removing all signatures\\nthat correspond to a single word.\\nEvery word is an anagram of itself,\\nbut that's not very interesting.\\nA dict comprehension will do the job.\\nRemember to iterate on both key and value,\\nwe use dict items.\\nAnd we use an if clause\\nto select the non-trivial anagram set.\\nSo finding the anagrams of python\\nis as simple as looking for its signature in our dictionary.\\n\\nWe can do this with a simple function. Works fine.\\nWhat about my name? Nothing.\\nI didn't really expect one.\\nBut perhaps we shouldn't get an error\\nwhen no anagram is found.\\nHow about we just return the empty set?\\nWe can do that by catching the exception\\nand doing otherwise.\\n(keyboard clicks)\\nThat works fine.\\nNow that we have set up this machinery,\\nthere are many interesting investigations we could do.\\n\\nFor instance, we could find the longest anagrams,\\nwhich we get by sorting the signatures by length.\\nWe can use sorted,\\nbut we need to tell it to sort by length,\\nnot alphabetical order, and to reverse the result.\\nWe can wrap this list into comprehension\\nto see the actual anagrams.\\nThe longest have 22 letters.\\nLooking at this list,\\nI must say that these are all compound medical words\\nthat are not very creative.\\n\\nThey really mean the same thing.\\nHow about the set of anagrams with the most words?\\nFor this, we sort the dict values instead of the keys.\\nThe two longest groups have 10 elements,\\nthough some of these words are not easily recognizable.\\nWell done.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:675a33f234509d909dad8cd9\",\"duration\":600,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code challenge: Find palindromes\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1170165\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:6006169\",\"duration\":66,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Find palindromes\",\"fileName\":\"4571000_en_US_02_05_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":124,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2527305,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We need to find all palindromes\\nof length seven or above in the dictionary.\\nWe know that two palindromes will also be anagrams,\\nso we loop over all sets of anagrams,\\nwhich we find in the anagrams_by_signature input.\\nOne way to consider all pairs of words\\nis to use itertools.combinations.\\nTo test if the words are palindromes\\nwe can just reverse the order of one of them\\nand check for equality.\\n\\nThen we make a list of the palindromes we found.\\nRemember that we need to return sets, hence the braces.\\nLet's try this out.\\nHmm, what's wrong?\\nI see, I forgot to check the length.\\nPerfect.\\n\"}],\"name\":\"2. Project: Finding Anagrams\",\"size\":19051650,\"urn\":\"urn:li:learningContentChapter:6010027\"},{\"duration\":1578,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:6003342\",\"duration\":246,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"NumPy overview\",\"fileName\":\"4571000_en_US_03_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":268,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Whenever you need to manage large one-dimensional or multi-dimensional collections of homogeneous data\u2014such as numerical arrays\u2014you can turn to the NumPy library, which provides speed and memory savings. NumPy is also a basic building block to interface with C or Fortran code. This video introduces the basic structure of NumPy arrays and the notion of dtype.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6461124,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this chapter, we introduce NumPy,\\na powerful library\\nthat supports large multidimensional arrays\\nwith a vast collection of mathematical functions\\nto operate on them efficiently.\\nNumPy is a fundamental part of the Python ecosystem,\\nand it provides the foundation for many data analysis\\nand numerical libraries in applications,\\nincluding SciPy for mathematics, Matplotlib for plotting,\\npandas and statsmodels for statistics,\\nscikit-learn for machine learning,\\nand scikit-image for image processing.\\n\\nNumPy is also crucial in interfacing with compiled code\\nin C, C++, or Fortran.\\nIn addition, if you learn NumPy,\\nyou will be able to use deep learning frameworks\\nsuch as PyTorch and JAX,\\nwhich share the same array interface,\\nas well as specialized array libraries\\nthat are interoperable with NumPy.\\nFor instance, CuPy to work with arrays on fast GPUs,\\nDask to spread arrays across computers,\\nXarray for arrays with labels,\\nand by PyData/Sparse for sparse arrays with many zeros\\nand efficient memory layout.\\n\\nSo let's talk about how NumPy arrays\\nare different from Python containers.\\nPython variables are often described as labels.\\nThey are not little copies in computer memory\\nready to receive a value.\\nRather, the values are independent objects\\nwith their own space and memory,\\nand Python variables are just names\\nassociated with the values.\\nSo you can have more than one variable\\nreferring to the same object.\\nThis mechanism is very flexible,\\nand it makes it possible to have lists and dictionaries\\nwith heterogeneous elements.\\n\\nIn fact, you can think of a list\\nas a numbered sequence of labels.\\nHowever, this scheme is not very efficient\\nwhen we need to deal with many values of the same type.\\nIn that case, you want to reserve space in memory\\nand store all the values side by side,\\nand that's exactly what a NumPy array is.\\nOrganizing data in this way is both faster\\nand more memory efficient,\\nand it's also necessary\\nto interface Python with other languages\\nsuch as C or Fortran,\\nwhich count on data being laid out in memory\\nin this fashion.\\n\\nHere I'm showing you a schematic\\nrepresenting one dimensional\\nand two dimensional NumPy arrays.\\nThe actual data items sit side by side in memory,\\nand they all have the same size.\\nWe identify them by one index\\nin the case of a one dimensional array,\\ntwo indices for a two dimensional array, and so on.\\nThe index ranges from zero to N minus one,\\nwhere N is the dimension of the array.\\nIn the case of a two dimensional array,\\nthe dimension, of course, can be different.\\nSince, as we said, all the data items in an array\\nneed to have the same size,\\nNumPy needs to be precise about identifying data types,\\nin fact, more precise than Python.\\n\\nWhile Python has just one type of integer\\nand one type of floating point number,\\nNumPy has several.\\nNumPy identifies different types of integers\\ndepending on the number of bits\\nthat each of them takes up in memory,\\nint8, int16, int32, int64.\\nThe most common of these is int64.\\nThere are also unsigned versions of these integers.\\nAs for floating point numbers,\\nwe have float16, 32, 64, and on some platforms, float128.\\n\\nThe most common floating point type is float64,\\na double precision floating point number,\\nwhich is also the same as a standard Python float.\\nThere are other more specialized types\\nsuch as booleans, true or false, bytes, unicode strings,\\nvoid, which is used for record arrays,\\nand object, which is an effective pointer\\nto arbitrary Python objects.\\nThe underscores in their names\\nare there to differentiate the types\\nfrom the python built-in types.\\n\\nSo let's see NumPy arrays in action.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5994433\",\"duration\":315,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating NumPy arrays\",\"fileName\":\"4571000_en_US_03_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":444,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch03-numpy > 03_02_creating.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"There are various ways to create NumPy arrays, depending on your needs. Learn how to make empty arrays, transform Python data structures into arrays, and load arrays from files in various formats.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9661794,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's get started with NumPy arrays.\\nThe easiest way to get one is to load it from a file.\\nNumPy recognizes several file formats.\\nThese include, of course, simple text files.\\nI have prepared one for you\\nthat describes a very well known lady.\\nThe file is called monalisa.txt.\\nLet's have a look at its contents.\\nSo we see that each line is a sequence of integers\\nand we have 200 lines all together.\\nNumPy will load this file without any trouble.\\n\\nThe result is a two-dimensional array.\\nWhen we display it, NumPy omits some rows\\nand columns so it fits on the screen.\\nWe can query this object for the number of dimensions.\\nTwo, since it's an array.\\nFor the shape, 200 by 134.\\nFor the total number of elements\\nand the number of bytes in memory.\\nAnd for the type of the array elements.\\nAn eight byte floating point number.\\n\\nOkay, so clearly this is an image\\nand we can use the matplotlib function imshow\\nto display a two dimensional and NumPy array as an image.\\nAlthough we should probably use\\na better color map, gray scale.\\nI have also prepared a color version of the painting\\nand I have saved it in NumPy's native binary format,\\nwhich works across platforms.\\nLet's load it up.\\nThe NumPy comment is np.load as opposed to load text.\\n\\nThis is now a 3D array, where the last dimension is used\\nto store red, green, and blue components.\\nImshow understands this without any problem.\\nNow that we know how to load a NumPy array,\\nhow about making one ourselves?\\nThis sway is from a Python list or a nested list of lists.\\nNumPy automatically sets a data type\\nthat can fit all the elements that we provide.\\n\\nThe shape is three by three and the D type is int64.\\nAnother common operation is creating an empty array.\\nLet's do a one-dimensional array, a vector.\\nWe use NumPy zeros with the shape and the data type.\\nD is a shorthand for an eight byte float.\\nWe can also do a two-dimensional array, a matrix,\\nby giving the shape eight by eight is a tuple.\\nHere we give the data type more explicitly as flow64,\\nbut it's really the same thing.\\n\\nAll the array metadata are as expected.\\nIt is sometimes useful to make an array\\nof zeros in the shape of another existing array,\\nusing zeros like.\\nOtherwise, we can make an empty array.\\nHere, the memory is allocated but not initialized,\\nso we may get some random looking values\\nor sometimes just zeros.\\nWe can also create an array of regularly spaced numbers.\\n\\nFor that, we use np.linspace\\nand specify the extreme, for instance, zero and one,\\nas well as the total number of elements.\\nWe can't show a 1D array as an image,\\nbut we can certainly plot it with matplotlib.\\nI will use a thick marker specified by lowercase o.\\nAnother way to make a regularly spaced array\\nis with arange, an extension of Python range\\nthat is not limited to integers.\\nKnow that the end of the range is still excluded\\njust as it would be in range.\\n\\nNumPy can also give us arrays of random numbers.\\nWe just need to specify the shape.\\nBy default, we get numbers uniformly distributed\\nbetween zero and one.\\nMatplotlib can show us that this looks suitably random.\\nThe np.random submodule offers a lot more,\\nfor instance, randomrandint,\\nto get integers in a specified range.\\nOr random.standard_normal\\nto get normally distributed numbers.\\n\\nIt's always good to show those\\nwith a simple matplotlib histogram.\\nTo close this video, let me show you how to save\\nan array to file.\\nNp.save will create a cross-platform binary file.\\nThe file ending is conventionally NPY,\\nbut it doesn't need to be.\\nOn the other hand,\\nsavetxt will create a readable ASCII table.\\nIt's all there.\\nI'll leave you with a one page cheat sheet\\ncovering creating arrays from files out of zeros\\nand from random numbers.\\n\\nThe cheat sheets from this chapter are collected\\nin the document numpy.pdf in your exercise files.\\nBut of course there are many good tutorials about NumPy\\non the web and the official documentation is also excellent.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2732059\",\"duration\":386,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Indexing NumPy arrays\",\"fileName\":\"4571000_en_US_03_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":550,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch03-numpy > 03_03_indexing.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In the analysis of large data collections, you often need to focus on subsets of data or restructure its storage. Learn how to select subarrays by specifying boundaries and strides (slicing), or by applying conditions to the data (fancy indexing). This video also discusses restructuring data\u2014for example, adding or removing axes\u2014and the distinction between views and copies.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11794175,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's see\\nhow we can access individual elements\\nand ranges of elements in NumPy.\\nWe will demonstrate on our good old friend Mona Lisa.\\nSo I will start by loading that array.\\nThis is a three dimensional NumPy array with dimensions\\nthat correspond to height 1,198 pixels\\nwith 804 pixels and color, the red, green,\\nand blue components.\\nThe syntax to get an individual pixel is just an extension\\nof Python list indexing,\\nexcept that we include multiple indices among brackets.\\n\\nFor instance, a point roughly in the middle\\nwould be on row 600 and column 400,\\nand we'll grab the red component.\\nSince version two of NumPy, the D type\\nof the element is displayed explicitly.\\nHere, it's an eight bit integer.\\nIt's usually possible to use these values interchangeably\\nwith Python objects.\\nShould you want to convert the NumPy number\\nto a standard python object, you can do it\\nwith the constructor's int and float.\\n\\nBack to the picture.\\nIf we wish to look at pixels at the bottom right corner,\\nwe may count back from the boundary of the array\\njust as we would do for a list.\\nThis will be the same as computing the actual indices.\\nIf we try to index elements beyond the boundaries,\\nwe get an index error as we should,\\nand of course, we can use indexing\\nto assign values to the elements.\\nOnce you get used to multi-indexing,\\nyou have the temptation of trying it on nested Python lists,\\nbut it doesn't work there.\\n\\nLet me demonstrate with a very uninspired list.\\nYes, that doesn't work.\\nTo work with a list of lists, we first need\\nto extract an entire row,\\nand then the element from that.\\nSlicing also works in a very similar way to Python lists.\\nFor instance, we could grab a section\\nin the middle of the painting.\\nWe can omit either the starting\\nor the ending index that will default to the beginning\\nor the end respectively.\\n\\nTo grab the entire range across the dimension,\\nwe use a lonely colon.\\nThere is an even shorter hand for multiple food slices.\\nThe lips is sign.\\nWe can also specify a step such as every 20th pixel.\\nThis of course, reduces the resolution of the picture.\\nNow, see the black dot in the middle?\\nIt's there because earlier we assign zero\\nto all three color channels for pixel 600, 400.\\nThat single pixel was invisible at high resolution,\\nbut it's one of those selected by the slice\\nwith steps of 20.\\n\\nHow about slicing backwards?\\nThat works too and creates a reflection.\\nIf we mix slicing and indexing,\\nwe reduce the emotionality of the array.\\nThis is a single row.\\nSince this is now a vector, a one dimensional array,\\nwe can't show it as an image, but we can still plot it.\\nIt's quite random looking.\\nNote that fixing one of the indices is not the same thing\\nas asking for a slice of one,\\nwhich will preserve that dimension.\\n\\nSlicing can also be used on the left side\\nof an assignment statement.\\nWe can do that to modify elements in bulk\\nand kind of deface our painting.\\nHere, I assign the same value\\nto all array elements in this slice, but I can also set them\\nwith another array of the right shape.\\nInstead of boring gray, we'll make some noise.\\nNumPy arrays support also an especially useful form\\nof indexing that is not available with lists.\\n\\nThis is known as fancy indexing.\\nThat is, using arrays to index other arrays.\\nTo demonstrate,\\nlet me grab my low resolution grayscale Mona Lisa.\\nI'm going to threshold this image\\nby zeroing out all the pixels\\nwith gray values darker than approximately 50%.\\nSo first I need to figure out which ones they are.\\nThe result of this condition\\nis a two dimensional Boolean array\\nwith the same size as monalisa_bw.\\n\\nI use this Boolean array to select the dark pixels alone\\nand modify only those.\\nSo here's the thresholded image.\\nFinally, I want to point out\\nanother very important difference\\nbetween lists and NumPy arrays.\\nWhenever you slice a list, you are making a copy of it.\\nSay, I have a simple list of six elements.\\nI take a slice of the first four.\\nNow, assigning to the slice\\ndoes not modify the original list.\\n\\nBy contrast, a slice of a NumPy array\\nis a new array object that points to the same area of memory\\nwith modified bookkeeping\\nto represent the different boundaries.\\nSo if I modify the slice, say, to make it darker,\\nhere's my slice.\\nA vertical segment from 300 to 500.\\nHere's making it darker\\nby dividing all the color values by three.\\n\\nAnd here's the original image\\nshowing that modifying the slice\\nalso affected the original memory area.\\nIf you want a true copy instead, pointing to new memory,\\nwhich can be modified without effect in the original,\\nyou have to make that copy explicitly.\\nAlso, fancy indexing will also create a new array\\nwith independent memory.\\nAgain, here's a quick cheat sheet on slicing and number.\\n\\nAs you see, it starts the same as vital lists,\\nbut adds the complication of multiple dimensions\\nand the powerful idea of fancy indexing using other arrays.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2732058\",\"duration\":345,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Doing math with NumPy arrays\",\"fileName\":\"4571000_en_US_03_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":468,\"solutionVideo\":false,\"editingNotes\":\"Slide 2 looks different in the video but is correct in the file\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch03-numpy > 03_04_math.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"With NumPy, speed and agility arise from the ability to operate on entire arrays at once. Learn how to perform mathematical operations that transform arrays or combine them together, while preserving or modifying their structure.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11806225,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] NumPy is very good at math.\\nWhile interpreted languages such as Python have a reputation\\nof being slow, we can do math very fast in Python\\nas long as we write code that operates on entire arrays.\\nThat's because the loops that are needed\\nto perform math over the array are then implemented\\nin compiled C within the NumPy library.\\nSo how do we do math with arrays?\\nLet's start by creating a one-dimensional vector\\nof equally spaced real values between say,\\nzero and five times pi.\\n\\nWe wish to compute the sine of all those values.\\nThe regular function in the math library, one, two.\\nInstead, we use the NumPy function of the same name.\\nThe result is another NumPy array with the same shape as X.\\nHere's a plot.\\nNumPy has a function for pretty much everything\\nyou can find in the standard math library.\\n\\nThe NumPy functions are known as universal functions,\\nand they operate element-wise across entire arrays.\\nThey include arithmetics, logical operations, trigonometry,\\nhyperbolic functions, exponentials and logarithms,\\nchecking for infinity and for not a number,\\nrounding, and much more.\\nWe created a plot of a function by giving two vectors\\nto matplotlib so that it would use one for the x-axis\\nand one for the y-axis.\\n\\nThe Y range was set automatically.\\nBy repeating the plot statement,\\nwe can show multiple functions together.\\nMatplotlib will automatically cycle through colors\\nso that we can distinguish the curves.\\nSo let's try our sine together with a cosine\\nin a logarithm.\\nTo see which is which we add labels and request a legend.\\nThere are many more options in matplotlib\\nregarding the style of the lines\\nand the formatting of the plot.\\n\\nWe'll see more in the rest of this course,\\nand you can look up many examples and docs online.\\nWe can also perform operations\\nthat involve more than one array.\\nEverything goes smoothly if we match array shapes.\\nBy contrast, operations between arrays\\nof different shapes will generally fail.\\nNumPy doesn't know what to do.\\nThere is one important exception,\\nwhich is known as broadcasting.\\n\\nWith broadcasting,\\nNumPy tries to make sense of operations between arrays\\nof different dimensions.\\nThe simplest case, which is rather intuitive,\\nis adding a single number to an entire array,\\nwhich adds it to every element.\\nSo we would say that the addition\\nof the number is broadcast through the array.\\nTo make an example in two-dimensions,\\nwe'll use our friend Mona Lisa again.\\nThe image is 200 rows by 134 columns.\\n\\nI can multiply every column by a different number\\nby making a vector of length 134.\\nIn this case, NumPy matches the second dimension\\nof the array with a single dimension of the vector,\\nbroadcasting the values along the rows.\\nMy vector will be a gradient that modifies the luminosity\\nof the columns across the image.\\nLet's see the images side by side.\\n\\nHere I've used matplotlib's subplot\\nto make a one by two arrangement.\\nWhat about the other way around?\\nIf I make a vector of length 200,\\nyou'd think we could apply it on the left\\nto multiply every row by a single value,\\nbut that does not work.\\nWhat happens with broadcasting is that NumPy matches\\narray shapes starting from the last dimension on the right,\\nexpanding missing dimensions into size of one,\\nand broadcasting along all dimensions of size one.\\n\\nSo to match the rows of the image\\nwith those of the gradient, we need the y grad\\nto have shape 201, which we can do\\nwith the slicing syntax and with NumPy new axis.\\nSo we turn the vector into a two-dimensional array.\\nNow, NumPy can broadcast along the second dimension\\nand apply the same multiplication along each row.\\nSince Python version 3.5,\\nPython includes a special matrix multiplication operator,\\nwhich is the @ symbol.\\n\\nIt is put to good use by NumPy.\\nIn fact, it means different things\\nfor different array shapes.\\nFor two vectors of the same size,\\nit returns the top product.\\nWhereas the standard multiplication sign would perform\\nan element by element product.\\nThe @ symbol can also denote a matrix by vector product.\\nHere, a two by three array multiplying a three vector.\\nThe @ symbol can also denote\\nmatrix by matrix multiplication.\\n\\nNumPy supports many other useful operations,\\nincluding manipulations to reshape and combine arrays,\\nstatistical operations, linear algebra,\\nlogical operations, sorting, searching, counting,\\nand fast Fourier transforms.\\nWe see some of them in action in the weather data project\\nand in the rest of this course.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6007136\",\"duration\":286,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Special arrays: Records and dates\",\"fileName\":\"4571000_en_US_03_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":377,\"solutionVideo\":false,\"editingNotes\":\"At time 1:04, highlight the text row that starts \\\"dtype=[(...\\\"\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch03-numpy > 03_05_special.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"NumPy offers very convenient facilities for managing data of different types together using record arrays, and handling dates with the datetime64 dtype. Learn how to create and use record arrays as well as datetime64 data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9613489,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In the last video for this chapter,\\nI want to show you two NumPy features\\nthat are not always covered in tutorials\\nbut are still very useful.\\nThe first is record arrays,\\nwhich allow us to mix different data types\\nand give descriptive names to fields.\\nWe see a much more powerful version\\nof this in the Pandas library,\\nbut sometimes it's good to use it within NumPy.\\nThe other feature is datetime objects,\\nwhich as the name says, can encode a date and a time.\\n\\nI will load a simple example of a record array,\\nwhich I have saved in the NumPy format.\\nThis is a partial David Bowie discography.\\nEach entry shows a record's name, date of release,\\nand top rank in the UK charts.\\nThe data type is a list, which shows the name\\nand D type of each field.\\nFor title, it's U32,\\nwhich denotes a Unicode string of length 32.\\nFor release date, it's M8D,\\nwhich denotes a NumPy datetime object\\nwith the precision of a day.\\n\\nThere's an eight because\\nthe datetime objects are eight bytes.\\nFinally, the top rank is an eight byte integer.\\nOh, and if you're wondering about the less than symbols,\\nthose refer to the endianness of the data types,\\nthe order in which the bytes are stored in memory.\\nA less than sign denotes little endian numbers.\\nThis table, which you can also find\\nin the NumPy cheat sheet,\\nshows the most common NumPy data types,\\ntheir memory usage, and the data type string.\\n\\nDo remember that in NumPy, all strings have fixed length\\nand that assigning a longer string\\nthan the defined length will truncate it.\\nWe'll cover the datetime and timedelta D types in a moment.\\nBut back to David Bowie and record arrays.\\nWe could also load his discography from a text file.\\nThat takes a little more work\\nbecause we have to specify the D type of every field\\nas well as the field delimiter.\\nHere, it's a comma.\\n\\nBut the result is the same.\\nSo once we have a record array, what can we do?\\nEach record looks like a Python tuple,\\nand we can extract or modify the elements\\nas we would for a tuple using two indices.\\nBut we can also use the dictionary interface\\nusing field names.\\nUsing the field names on the entire array\\nwill give us a view of a column.\\nTo create a record array in Python code,\\nwe have to provide the data records as tuples,\\nand we need to be careful about describing the data types.\\n\\nFor a discography, it would look like this.\\nNow, for dates and times in NumPy.\\nThe D type that we need is called datetime64,\\nto avoid confusion\\nwith the datetime object in the Python standard library\\nand to remind us that each element takes 64 bits.\\nWe initialize datetime objects from strings,\\nand we can give as much detail as we want.\\nThe string format is ISO A601,\\nwhich goes from larger to smaller units.\\n\\nThat is from year to month to day, and so on.\\nSo here are three dates of increasing precision.\\nWe can also create a NumPy datetime from a standard\\nlibrary datetime object.\\nWe specify a granularity of D\\nto avoid setting the time at exactly midnight.\\nThe Python datetime module has a lot of functionality\\nthat can be useful before we bring data into NumPy.\\nFor instance, if we need to pass a generic string format,\\nwe can do so by specifying the format itself.\\n\\nNumPy datetime objects can be compared,\\nand they can be subtracted,\\nresulting in a time delta object.\\nHere it's specified in minutes.\\nThe nice thing about these datetime64 objects\\nis that they work across NumPy.\\nFor instance, we can use the NumPy function diff,\\nwhich computes the difference\\nbetween successive array elements to see\\nhow long it took David to come up with each new record.\\nIt seems that 'Ziggy Stardust' was especially quick.\\n\\nAnother example of using standard NumPy functions\\nwith datetime64 is making a range of date.\\nConsistently with the usual convention in Python,\\nthe last day in the range is excluded.\\nThis completes a quick overview of NumPy.\\n\"}],\"name\":\"3. NumPy\",\"size\":49336807,\"urn\":\"urn:li:learningContentChapter:6010028\"},{\"duration\":1257,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:6008085\",\"duration\":54,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview: Analyzing weather\",\"fileName\":\"4571000_en_US_04_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":62,\"solutionVideo\":false,\"editingNotes\":\"At 00:10, show overlay 'search for \\\"GHCN daily\\\" on data.noaa.gov/onestop'. At 00:42, show overlay 'www.ncei.noaa.gov/pub/data/ghcn/daily'\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this chapter, you can practice using NumPy by loading, modifying, and plotting weather data from the National Oceanic and Atmospheric Administration. This video examines the files that are going to be analyzed and outlines the analysis goals.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2385425,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this chapter,\\nwe are going to experiment with NumPy\\non a real world use case,\\nanalyzing weather data from NOAA,\\nthe National Oceanic and Atmosphere Administration.\\nThe GHCN Daily is an integrated database\\nof daily climate summaries\\nfrom land surface stations across the globe.\\nSummaries in this case means variables,\\nsuch as the minimum and maximum temperatures,\\nthe total precipitation, and so on.\\n\\nDigging through the portal,\\nwe find the location of the data files.\\nFrom here, we will download the station list\\nand use it to locate data for a station.\\nWe will load data, fill missing values,\\nand smooth time series.\\nFinally, we will create a visualization\\nof daily temperatures inspired\\nby the \\\"New York Times\\\" weather plots.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6008084\",\"duration\":290,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Loading station and temperature data\",\"fileName\":\"4571000_en_US_04_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":376,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch04-weather > 04_02_loading.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"NumPy allows you to load data in many different formats without writing custom code. Learn how to use Python to download files from the web and use NumPy to load a fixed-width table.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13295975,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Before loading the data,\\nit's always a good idea to start\\nby looking at the documentation.\\nBrowsing through the file listing at the noaa repository\\nwe see a readme.txt.\\nWe'll start there.\\nThe readme file is also included\\nin this chapters directory.\\nIt describes the content of the repository,\\nthe format of the weather data files,\\nwhich have an ending DLY,\\nand which contain data for a single station\\nformatted with fixed width columns.\\n\\nLots of detail here.\\nNext, the readme file\\ndescribes the format of a file ghcnd stations.txt.\\nThat gives us the location, elevation,\\nand ID for each station in the network.\\nWe'll start with this station's database.\\nFor your reference,\\nI have copied the description of the files format\\ninto this notebook.\\n\\nTo load a fixed width text file such as this,\\nwe can use numpy genfromtext.\\nIt needs rather precise information.\\nWe specify the width\\nof each field in the parameter delimiter.\\nWe can direct the widths from the table above,\\nbut we need to increase them\\nto include the spaces between columns.\\nNext, we provide names for each column\\nand we specify the D type of each column.\\nWe'll need a string of 11 characters,\\nthree double precision floats, and a few more strings.\\n\\nAnd last, with auto strip,\\nwe instruct numpy to remove leading and trailing spaces\\nfrom all the strings it passes.\\nThe result of running his is a numpy record array\\nwith more than a hundred thousand entries.\\nThankfully, numpy shows us only a few lines\\nat the top and bottom.\\nBy plotting longitude against latitude,\\nwe get an idea of the impressive global coverage\\nof the database.\\nWe need to make the dots small\\nso that they are not too crowded.\\n\\nEven so, the US and Europe are just masses of ink\\nor colored pixels.\\nHow about stations in California?\\nWe can use fancy indexing with a boolean condition\\nto downselect our dataset.\\nCoverage is still impressive.\\nWhat if we need a specific station?\\nFancy indexing, again, comes to the rescue.\\nWe select all stations for which it is true\\nthat the name field equals Pasadena.\\n\\nThere's only one.\\nThere must be more stations whose name begins with Pasadena.\\nWe can find them using the string function starts with\\nwhich in numpy sits without string functions\\nin the submodule numpy.char.\\nSo there are several.\\nWe will stay with the first station\\nthat we found for Pasadena,\\nwhich belongs to the HCN network\\nand is likely to have more data.\\nI've copied the Pasadena file into this repository.\\nSo let's have a look at the first few lines.\\n\\nIt's rather messy,\\nbut we recognize the station ID\\nat the beginning of each line,\\nfollowed by year and month,\\nthe name of an element,\\nsuch as a minimum or maximum temperature,\\nand 31 data points, one for each day of the month.\\nEach data point consists of a value and a flag\\nwhich we will mostly ignore.\\nWe could use genfromtext again,\\nbut I don't want to waste your time setting it up.\\nSo I prepared a small Python module, get weather,\\nwhich takes care of parts in the file,\\nand returning consecutive daily values for a year.\\n\\nFor simplicity, we will forget about February 29th\\nin leap years.\\nThe module uses the library Pandas to clean and reform data,\\nbut it returns pure numpy arrays.\\nAfter we introduce Pandas later in this course,\\nI encourage you to go back and look at getweather.pi\\nand see what I did there to pass the file.\\nSo we import that module and look at the help text.\\nSeems straightforward enough.\\n\\nLet's try it on Pasadena.\\nRequesting TMIN and TMAX,\\nthe minimum and maximum temperature.\\nExcellent.\\nWe see that the result is a numpy record array\\nwith two double precision fields, TMIN and TMAX.\\nThe values are in degrees Celsius.\\nSome measurements are missing,\\nand they're represented as nan, not a number.\\nThis function will be a great foundation\\nfor our work in the rest of this chapter.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2732057\",\"duration\":260,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Cleaning weather data\",\"fileName\":\"4571000_en_US_04_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":353,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch04-weather > 04_03_cleaning.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Data often needs to be cleaned or otherwise edited before analyzing it. Learn how to integrate missing data in the weather files you have loaded using a simple interpolation technique.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9978661,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We pick up where we left\\nand load temperature data for Pasadena, California.\\nThis is a time series,\\na sequence of values organized chronologically,\\nusually with equal cadence, that is the same time interval\\nbetween every two consecutive samples.\\nTo get a sense of the data,\\none of them begins by computing its average value\\nand its extreme, the minimum and the maximum.\\n\\nWith NumPy, we use mean, min, and max.\\nBut wait, we get nans. What's going on here?\\nSome values are missing from the data file,\\nand indeed they're represented as nans.\\nI can see some here.\\nNow, the mean of a sequence that includes a nan\\nis going to be, well, a nan.\\nIn fact, how many values are missing?\\nThe NumPy function is nan, creates a Boolean array of nans.\\n\\nWe can then count the instances of true values here\\nby using a neat trick.\\nIf it were arithmetic with Booleans in Python,\\nthey are converted to integers\\nwith false counting as zero and true as one.\\nIt follows that we can count the trues in a Boolean array\\nby obtaining its sum with numpy_sum.\\n15 days.\\nWhat can we do?\\nMissing values are so common\\nthat NumPy offers versions of its functions\\nthat simply ignore them,\\nnanmin and nanmax, for instance.\\n\\nThis works, but for some applications,\\nwe do need an uninterrupted series of numbers.\\nWe could just set the nans to the average of the column.\\nThis is yet another application of fancy indexing\\nbecause we want to modify only the nan elements.\\nThis works, and we can tell which values we change\\nbecause they have more digits than all the others,\\nwhich were encoded with limited precision in the GHCN file.\\n\\nThis integrated dataset can now be plotted\\nwithout discontinuities.\\nA more powerful approach\\nto restoring missing values is interpolation.\\nWe use neighbor values to compute a plausible number\\nfor the values that are missing.\\nLet me demonstrate in a toy problem.\\nLet's say we measure a function Y\\ndefined as an integer X between zero and eight,\\nexcept for some of the values.\\n\\nBy interpolating, we can compute values for the series\\nat smaller X intervals.\\nWe first define the desired X values using numpy.linspace.\\nAnd then to interpolate with NumPy,\\nwe use numpy.interp, which takes three arguments:\\nthe desired X values, the existing X values,\\nand the existing Y data.\\nThis yields linear interpolation,\\nwhich is in effect fitting straight segments\\nbetween the existing data points.\\n\\nI will plot the new interpolated data\\nand superimpose the original points in orange\\nto show that interpolation at an existing location\\nyields the same value.\\nFor more sophisticated interpolation schemes,\\nyou can look at what's available\\nin the scientific computation library SciPy,\\nand specifically in SciPy interpolate.\\nLet's use interpolation\\nto fill missing values in the Pasadena temperature data.\\nI need to load the data again\\nsince we fixed it using mean values.\\n\\nHere, now it's broken again.\\nTo interpolate, we select the good data points,\\nthose that are not nan.\\nThen we make an array of the X's\\nfor which we want values,\\nall days from 1 to 365,\\nand then we apply np.interp.\\nIt seems to work well.\\nWe celebrate by generalizing our Pasadena center code\\nso that it can fill up holes in any array\\nby interpolating it.\\n\\nAnd finally, we can plug the interpolated temperature series\\nin all their glory.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5994432\",\"duration\":299,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Smoothing and plotting time series\",\"fileName\":\"4571000_en_US_04_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":407,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch04-weather > 04_04_smoothing.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Many interesting datasets are organized as a time series\u2014numerical sequences sorted by date and time. Learn how to use NumPy to perform basic time series analysis tasks\u2014computing means and standard deviations, and smoothing time series.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11311305,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] So now, we know how\\nto load temperature data from any station,\\nhow to compute basic summaries such as mean, min, and max,\\nand how to integrate missing data points\\nusing interpolation.\\nHere's the code we need.\\nIn the last video, we looked at data for Pasadena.\\nLet's move to even sunnier skies now\\nby looking at weather in the town\\nof Hilo in big island, Hawaii.\\nWe'll use a custom loader,\\nand we fill the missing data for both T-min and T-max.\\n\\nOnce more, tupple unpacking is very useful.\\nTo summarize the data, we obtain the yearly average,\\nwhich gives us a sense of a typical value for T-min\\nand the min and max which span the range\\nof variation of these measurements.\\nWe can plot these values on top of the time series.\\nThe matplotlib function, XH line for axis, horizontal line,\\nplots a horizontal lines that spans the entire graph,\\njust what we need.\\n\\nAnother common way to measure range\\nof variation of a time series is\\nto compute the standard deviation defined\\nas the square root of the variance.\\nIf you need a refresher, Wikipedia will help.\\nWe then plot the time series,\\nthe mean and values one standard deviation\\nabove and below the mean.\\nMost of the time,\\nthe temperatures are included in this range.\\nGiven that this is Hawaii, it's also interesting\\nto look at the precipitation.\\nThe rainy season, which starts in November, is evident.\\n\\nLooking at the data this way is very informative,\\nbut we also see a lot of noise\\nand rapid variations between one day and the next,\\nwhich can obscure underlying trends.\\nTo remove the noise, we can smooth the data,\\nso that we see the slower long-term behavior\\nbelow the oscillations.\\nThe simplest approach to smoothing is replacing each value\\nwith the average set of its neighbors, including itself.\\nWith numpy, we can do so with numpy convolve\\nwhich slides a mass across a vector of data,\\nmultiplying the elements of the mask\\nwith the elements of the vector and summing the map.\\n\\nWe're going to use a very simple constant mask, normalized,\\nso that it sums to one.\\nIf you think about it, the resulting convolution vector\\nis going to be shorter than the original\\nbecause at the boundaries, we cannot perform\\nthe full convolution.\\nSo here's my mask. Let's try it out.\\nWe'll plot the original temperature series as dots\\nand the smoothed series as a continuous line.\\nThis works fine.\\nWe are reducing the quick oscillations\\nwhile emphasizing the underlying, slower trend.\\n\\nBy specifying that the convolution mode is valid,\\nwe are accepting that the result is indeed a shorter vector\\nthan the original time series.\\nBy contrast, if the mode is same,\\nwe get the full output vector,\\nbut we observe anomalies at the beginning and the end.\\nAs the mask slides over the boundary of the data,\\neffectively, it is multiplying zeros.\\nWith a little more work,\\nwe could pat the data, the boundaries,\\nusing measurements from the previous and following year.\\n\\nBut for simplicity, we'll just use shorter time series.\\nHere's a function that performs the smoothing\\nwith a constant mask of arbitrary length.\\nSo we plot TMIN and TMAX together for Hilo.\\nThis is an interesting plot.\\nI'd like to see it for other cities.\\nSo we take our code and generalize it to a function.\\nWe get the data, and for both TMIN and TMAX,\\nwe fill in missing values, smooth, and plot.\\n\\nWe'll also add a title and reasonable limits for the access.\\nLet's try this out.\\nWe can try to plot multiple years\\nto see if the Hawaii climate is stable.\\nIn fact, for such nice weather,\\nI can reduce the plotting range.\\nNow, let's compare cities in different climates.\\nLet's say, Pasadena, New York,\\nSan Diego, and Minneapolis.\\nTo make a 2x2 array of plots,\\nI use matplotlibs subplot which takes the shape\\nof the array in the number between one\\nand the maximum index over the figures.\\n\\nSo I loop over cities using the enumerate iterator,\\nwhich returns pairs consisting\\nof a numerical index and the list item.\\nThis is a very Pythonic construction.\\nThis is a very informative, comparative plot.\\nIf you could choose based on the weather alone,\\nwhere would you live?\\n\"},{\"urn\":\"urn:li:learningContentVideo:6007134\",\"duration\":280,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Weather charts\",\"fileName\":\"4571000_en_US_04_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":338,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch04-weather > 04_05_charts.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Using NumPy and Matplotlib together, you can create insightful visualizations with little effort. Learn how to compute daily temperature records and plot weather charts in the style of the New York Times.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11281684,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We're going to conclude our experimentation\\nby making a quick but impressive weather visualization\\nthat showcases the power\\nand flexibility of NumPY and Matplotlib.\\nA visualization is inspired\\nby the New York Times weather chart shown here,\\nand it'll present daily minimum and maximum,\\nthe purple band.\\nIn the context of their normal range, dark gray,\\nand of the historical records, light gray.\\nBy the way, let me show you how to add images\\nto a Jupyter Notebook.\\nIf I double click on the cell\\ncontaining the New York Times visualization,\\nwe see that it's just a markdown cell\\nwith a small chunk of HTML,\\nwhich in this case refers to a file\\nin the current directory,\\nbut we could also use a full URL.\\n\\nA shift+enter in the cell\\nand it goes back to showing the image.\\nSo let's make a plot.\\nWe will again use Pasadena as an example,\\nbut you will be able to do your own city\\nif it's represented in the noaa data sets.\\nAnd since we want records across many years,\\nwe need all the data that we can get.\\nThe Get Weather module\\nlets us query for one year of data at a time,\\nso we'll call it repeatedly\\nand collect the results with the list comprehension.\\nOnce we have that,\\nso they become the rows of a two dimensional array\\nusing NumPY vstack.\\n\\nSo we have 114 years.\\nThe results can be visualized with mat show.\\nWe use a color bar\\nto provide a reference to the mapping of values to color,\\nand we can specify the extent\\nto get more informative labels on the axis.\\nWe see some missing data, the white patches,\\nand we see that the middle of the year is generally warmer.\\nWe can also observe that winter\\nand summer minimum are getting warmer\\ntoward the end of the century.\\n\\nFor simplicity, we will forego filling nans\\nand use nan robust functions.\\nWe want the record temperatures for every day of the year.\\nThis means that we can use NumPY nan min on the teaming data\\nand specify axis zero\\nso that the minimum will be taken\\nacross each row of 114 years.\\nThen we do the same for TMAX using NumPY nan max.\\nSo let's see the records.\\n\\nNow for the normals.\\nIn the New York Times visualization,\\nthe normal temperature range for each day of the year\\nis defined as the average of the low\\nand high from 1981 to 2010.\\nSo we build another stacked array with this reduced range.\\nAnd again, we take means across rows.\\nSo axis is zero.\\nLet's have a look.\\nWe are ready to get this plot together.\\n\\nWe'll do Pasadena in 2020.\\nTo plot a field band, we use matplotlibs fill between,\\nwhich needs X axis coordinates\\nand the lower and upper lines that delimit the band.\\nFor the X axis coordinate,\\nI will use the day of the year from one through 365.\\nThe colors that I picked are my best approximation\\nto the New York Times graphic.\\nIn the title,\\nI'd like to show the average temperature for the year.\\n\\nI will compute it as the average of the mean minimum\\nand mean maximum temperatures.\\nAnd to build a string for the title,\\nI can use the very convenient formatted string literal\\nintroduced in Python three six.\\nIf we start a string with an F before the quotes,\\nwe can include variable names in braces,\\nwhich will be replaced by the values.\\nWe can also specify format\\nand instructions that we would use\\nin the string format interface.\\n\\nFor instance, two decimal digits\\nfor the average temperature.\\nHere it is.\\nThis is the best way to build up strings\\nthat include values.\\nOkay, let me put everything together\\nin a function and white plot.\\nIf you wish, you can stop the video here\\nand review the details.\\nAnd here's New York in 2022.\\n\\nNumPY is extremely powerful and flexible.\\nYou should learn about it in depth.\\nCoupled with matplotlib,\\nit offers a direct route to beautiful\\nand informative visualizations.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:675a342b3450c8d202a11574\",\"duration\":600,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code challenge: Temperature anomaly\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1170168\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:5994431\",\"duration\":74,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Temperature anomaly\",\"fileName\":\"4571000_en_US_04_07_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":130,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2936654,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We need to compute the temperature anomaly\\nin New York for all years in the dataset.\\nThe anomaly is defined from averages along a year,\\nso I'll be using NumPy mean along the second axis,\\nwhich has length 365.\\nIn fact, I need another mean\\nbetween the minimum and the maximum.\\n\\nNext, I need my reference,\\nwhich is the mean between 1945 and 1955.\\nWe are told that these are rows 65 to 75,\\nand I don't need the access keyword\\nsince we're averaging over everything.\\nThen I round and return.\\n\\nOh, I actually need to subtract this reference mean\\nfrom my result first.\\nLet's test.\\nPerfect.\\n\"}],\"name\":\"4. Project: Weather Data\",\"size\":51189704,\"urn\":\"urn:li:learningContentChapter:6003343\"},{\"duration\":1600,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:6008082\",\"duration\":98,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"pandas overview\",\"fileName\":\"4571000_en_US_05_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":103,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"pandas' powerful table objects (DataFrames) are extremely useful in the analysis of structured data that associates textual, date, and numerical information. This video introduces the basic structure of pandas Series and DataFrames, and compares them with NumPy arrays. It also highlights recent performance improvements made possible by Apache Arrow.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2266370,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Pandas has gained broad acceptance\\nin the Python community\\nas the leading data analysis library\\nin the data science stack.\\nIt's especially useful for statistics,\\nbut also to prepare data\\nfor visualization and machine learning.\\nWhile NumPy works with arrays,\\npandas deals with data tables,\\nwhich are known in the library as data frames.\\nThe main difference is that array rows, columns and elements\\nare identified by integers,\\nwhile data frame rows and columns\\ncan be indexed by strings, dates,\\nfloating point numbers, and other python objects.\\n\\nIn fact, any data frame column\\ncan be promoted to act as the index.\\nThat is just one of many table manipulations\\nthat are possible.\\nIn addition, pandas can read and write\\nmany common data formats.\\nIt provides a consistent way to handle missing data.\\nIt implements powerful database operations such as joins,\\nand it can even make plots.\\nHistorically, pandas used NumPy\\nto store data behind the scenes.\\n\\nBut since version 2.0,\\npandas has introduced Apache Arrow\\nas an alternative backhand.\\nArrow handles missing data, strings, and categorical data\\nmore efficiently than NumPy.\\nIt's optimized for memory efficiency and performance,\\nand it supports data formats that are interoperable\\nwith many other data science libraries and applications.\\nIf you want to do data analysis or data science with Python,\\nI really recommend pandas.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6006168\",\"duration\":443,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"pandas DataFrames and Series\",\"fileName\":\"4571000_en_US_05_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":594,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch05-pandas > 05_02_dataframes.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"To be efficient in data analysis, you often need to reorganize tables and to select subsets of their rows or columns. Learn how to create DataFrames from Python data structures or from files, inspect DataFrames, extract and modify their columns, and select data based on conditions\u2014pandas' version of fancy indexing and the query method.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16030014,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The central object in pandas\\nis the DataFrame,\\na data table with named and typed columns\\nand with an index that identifies the rows.\\nThe default index is just a range of numbers,\\nbut it can be any type of data.\\nIndex sample in this slide\\nwhere the DataFrame columns are name, date of birth,\\nand city, the index could be the Social Security number\\nor an alphanumerical employee ID.\\nA series is effectively a single column from a DataFrame\\nwith its own index.\\n\\nIt is the index that makes DataFrames and Series\\nmore powerful than NumPy arrays.\\nFor instance, we can combine two Series\\nindexed by timestamps\\neven if the indices are only partially overlapping.\\nPandas a will figure out which entries\\nwe can actually compute.\\nThe easiest way to make a pandas DataFrame\\nis to load it from a file.\\nWe start with a simple text file, Nobels.csv,\\nwhich contains a list of Nobel laureates.\\n\\nWith a year and discipline\\nin which they were awarded their prize\\nas well as their date of birth.\\nPandas reads CSV, reads this without breaking a sweat.\\nWe do need to provide names for the columns\\nsince the file itself doesn't have them.\\nRead CSV has many other options\\nincluding specifying separators other than commas,\\nskipping columns or rows,\\nconverting dates and more.\\nNow that we have a DataFrame, let's examine it.\\n\\nThe method info gives us basic information,\\nincluding the number of non-null values in each column,\\nthe Dtype of each column, and the total memory usage.\\nThe method head prints the first few rows.\\nWhile tail print the last few rows,\\nwe have a total of a thousand records\\nand we see that the columns are indeed year, discipline,\\nnobelist, and date of birth.\\n\\nThe columns have pyarrow Dtypes,\\nwhich are more powerful than the NumPy\\nor Python counterparts.\\nUnlike NumPy and Python, integers support a null value\\nwhich can be used for missing data.\\nUnlike NumPy, pyarrow strings have variable length.\\nUnlike Python, those strings are stored continuously\\nand efficiently in memory.\\nLast, pyarrow types for dates and times\\nare more extensive than those in NumPy.\\n\\nIn this case, while reading the CSV file, the arrow library\\nelected to use the memory efficient date32\\nwith the precision of a day.\\nAs I mentioned, the index plays a very important role\\nfor DataFrames.\\nThis DataFrame has the default integer index\\nranging from zero to a thousand.\\nWe look at indexing in detail in the next video.\\nTo grab individual DataFrame columns,\\nwe can use the Python dictionary syntax.\\n\\nWe can also use class attribute syntax with a dot.\\nEither way, the result is a pandas series.\\nThe series inherits the name and Dtype of the column\\nand the index of the whole DataFrame.\\nIf we need a naked array of the values,\\nwe get that with the property values.\\nThis now is an arrow object,\\nbut we can also turn it into a NumPy array.\\nThe method is to NumPy.\\n\\nSometimes it's useful to get a list\\nor actually an array of all the unique values in a column.\\nOther times, it's useful to have counts\\nof the times each value appears.\\nThis accounting confirms\\nthat the three scientists were awarded two prizes.\\nLinus Pauling for chemistry and peace,\\nMarie Curie for physics and chemistry.\\nJohn Bardeen for physics twice,\\nand Frederick Sanger and Barry Sharpless\\nfor chemistry twice.\\n\\nTo select records we can use also fancy indexing,\\nbuilding a Boolean expression that involves the columns,\\nlet's say prices in physics after 2019.\\nWe can also use the convenient,\\nif not very Pythonic query interface,\\nwhich takes the Boolean condition as a string.\\nWe have to mind our quotes here as it were,\\nmaking sure that we use single quotes for the query\\nand double quotes for the values inside it or vice versa.\\n\\nDon't worry about the warning,\\nwhich has to do with the internal workings of pyarrow.\\nSometimes it's not evident\\nhow we should write a filter in operation,\\nwith either a fancy indexing or a query.\\nFor instance, if we seek all the nobelists\\nfrom the Curie family,\\nwe will be tempted to write something like this,\\nchecking whether Curie is in the string nobelist.\\nThis, however fails rather spectacularly.\\n\\nInstead, we need to find the pandas string method contains,\\nwhich produces the Boolean array that we need.\\nThe selection confirms the incredible winning streak\\nof the Curie family.\\nMarie, her husband Pierre, and daughter Irene.\\nLet me show you how to create a DataFrame yourself.\\nFirst of all, you could make one from a NumPy record array,\\nfor instance, the David Bowie discography\\nthat we use that as an example in chapter four.\\n\\nThis is almost too easy\\nsince we already have all the information\\nthat we need in the right place,\\nand we just need to pass the record array\\nto pandas DataFrame.\\nThe resulting DataFrame uses NumPy for storage.\\nIf we want a more efficient arrow backend,\\nwe can use convert Dtypes.\\nStarting from scratch, instead,\\nwe may build the DataFrame from a list of dictionaries,\\nwhich means that we need to repeat the column names\\nover and over.\\n\\nOr from a list of tools by providing names for the columns.\\nOtherwise, we can build a DataFrame column by column\\nrather than record by record\\nfrom a dictionary of NumPy vectors or lists.\\nIt's also possible to use name tools or data classes.\\nI leave you with a one page cheat sheet\\ncovering the basics of DataFrame metadata,\\nas well as selecting columns and rows.\\nThe cheat sheets from this chapter are collected\\nin the document pandas-cheatsheet.PDF in the exercise files.\\n\\nBut of course, there are many good tutorials\\nabout pandas on the web,\\nand the official documentation is also excellent.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2732056\",\"duration\":252,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Indexing in pandas\",\"fileName\":\"4571000_en_US_05_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":349,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch05-pandas > 05_03_indexing.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Once you have your data organized, you may need to find the specific records you want. Learn how to index DataFrames with NumPy-like indexing, or by creating indexes. This video also discusses pandas' powerful but confusing MultiIndexes.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9969489,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We have seen how to load\\nand create data frames\\nand how to select records based on Boolean conditions.\\nHowever, for anything more complex than a simple selection,\\nyou want to use pandas indices.\\nYou will also need to use indices\\nwherever you need to modify values within a data frame.\\nSo let's load up our Nobels list database again.\\nHere, the index is the default integer range.\\nWe can instead use the nobel here.\\n\\nTo elevate the column to serve as index,\\nwe use the set index method, which creates a new data frame.\\nThe data frame is now shown with the year\\nat the front of each record.\\nAnd here's the index itself.\\nPandas indices do not need to have unique values,\\nand that's a feature, not a bug,\\nbecause we may very well want to select\\nall Nobels awarded in a given year.\\nThe pandas indexing notation is a bit surprising.\\n\\nTo select all the records with a given index,\\nwe use .lock followed\\nby the index value in brackets, not by parenthesis.\\nWe can also add a column name\\njust as if we were selecting a cell in a NumPy array.\\nThe result is a series.\\nIn addition to selecting individual index values,\\n.lock allows for slices.\\nIn a break from Python usage,\\nthe range is inclusive of its end value.\\n\\nIf we choose the years of the Great War, 1914 to 1918,\\nthen 1918 appears in the selection.\\nThis makes sense though,\\nbecause if the indices are not integers,\\nit will be very unclear\\nwhat is included in a Python style index.\\nWe are not limited to numerical indices.\\nWe could index by discipline.\\nWe can only request a slice of indices\\nif the index itself is sorted.\\nThat's achieved with the method sort index.\\n\\nFancy indexing, that is indexing with an array, also works.\\nAnd if you want NumPy style position based indexing,\\nyou can get it with .ilock.\\nAnd then slices are exclusive as in NumPy.\\nHere are the first 10 records,\\nwhatever they are in the data frame in its current order.\\nThis is still far from the end of the story.\\nPandas supports multiple indices known as multi-indexes.\\n\\nFor instance, we could have a data frame\\nthat's indexed by year and by discipline.\\nHere's the multi-index.\\nIf you need individual levels,\\nyou can use get level values,\\nyears and disciplines respectively.\\nNow, armed with this multi-index,\\nwe can select records by both year and discipline\\nby passing a tuple to .lock.\\nUsing only the first level of the index\\nresults in a new data frame with a simple index.\\n\\nSlicing with multi-indexes can get confusing\\nbecause it's hard to tell the lock indexer,\\nwhich rows and which columns we are really interested in.\\nYour best bet is to use pandas index slice\\nand then also specify the columns that you need\\nor use a column for all of them.\\nSo here, we requested years from 1901 to 1910,\\nspecifically for chemistry.\\nIf we want all years,\\nbut a single discipline, we just use a column.\\n\\nAnd again, request all columns.\\nMulti-indexing is powerful, but it can be confusing\\nand sometimes you'll have to resort to trial and error\\nor ask Stack Overflow or even ChatGPT.\\nHere's a one page summary of basic indexing in pandas.\\nIt's included in the PDF cheat sheet.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6008081\",\"duration\":389,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Math and plotting in pandas\",\"fileName\":\"4571000_en_US_05_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":568,\"solutionVideo\":false,\"editingNotes\":\"Seems long, but there's a pause in the middle to wait out noise. Overlay at 0:23 \\\"www.gapminder.org\\\"\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch05-pandas > 05_04_plotting.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In data analysis, you often need to transform tables by applying operations to one or more columns. Visualizing the transformed datasets is also crucial to understanding them. Learn about the basics of performing mathematical operations and plotting with pandas.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15706747,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We have seen how to load\\nand create data frames and how to select roles,\\nbut we have not done much with the values in the table.\\nMany data analysis tasks require\\nthat we perform computations on the\\ndata and visualize the data.\\nLet's try that with Pandas.\\nWe will use a data set consisting\\nof global population health and wealth statistics\\nfrom the amazing Gapminder website.\\nGapminder is a Swedish foundation created\\nby the late statistician, Hans Rosling,\\nto promote a fact-based worldview\\nand to fight misconceptions about global development.\\n\\nI have prepared a comma separated file with a selection\\nof Gapminder data.\\nThe data set includes a number of global statistics.\\nFor a country and a year, we get population,\\nlife expectancy,\\nthe percentage of children born alive who survived\\nto age five, the average number of babies\\npreferred by a woman,\\nand the early domestic product per person in 2017 dollars.\\nWe also get the geographical region.\\n\\nWe can ask Pandas to compute simple statistics\\non all the fields.\\nThe method describe does that.\\nWe see that there are almost 42,000 records in the datasets\\nfor years ranging from 1800 to 2023.\\nThis summary gives us the minimum, maximum, mean,\\nand standard deviation for all fields, as well as the 25th,\\n50th, and 75th percentiles,\\nwhich represent the typical range\\nof variation for a quantity.\\n\\nHans Rosling stresses that life expectancy improves\\nwith wealth and that the correlation is even clearer\\nif we look at the logarithm of GDP per person per day.\\nWe don't have a column in the database with\\nthat information, but it's very easy to compute it.\\nI divide the GDP per capita by the average number\\nof days in a year, and I use a Nampa Universal function,\\nwhich works fine with Pandas to get the logarithim.\\n\\nBy assigning values to a new column name,\\na new column is created in the data frame.\\nTo see global trends as a function of time\\nor to examine individual countries,\\nit makes sense to index by year and by country.\\nPandas has its own plotting interface,\\nwhich was designed to display multiple variables together.\\nWe want to show life expectancy as a function\\nof logarithmic cost domestic product,\\nso we select data for 1960 with lock\\nand then generate a scatterplot of those two variables.\\n\\nThe situation in 1960 is pretty clear\\nwith a clear correlation\\nbetween the two variables across all countries.\\nTo compare with the more recent year in the same plot,\\nwe need to grab a mat plot lib axis object,\\nfrom the first plotting function\\nand pass it to the second.\\nWe also change colors\\nand label the two clouds of points.\\nIn 2015, everyone is richer\\nand can look forward to a longer life.\\n\\nBeing rich is still helpful, but we observe a flattening\\nor life expectancy for the richest countries.\\nThe trend is the same for other statistical indicators such\\nas survival to age five.\\nThe data frame index by country lets us make easy plots\\nof the chronological evolution of an indicator such\\nas life expectancy for a country, such as Italy.\\nBefore we plot, we sort the index,\\nthe result may be jumbled otherwise.\\n\\nThe style of programming where I concatenate methods one\\nafter the other is in fact quite a dramatic for Pandas,\\nif not for Python.\\nOne may even say that Pandas speaks its own Python dialect.\\nHere's a comparative plot for three countries.\\nItaly caught up with the United States in 1960\\nand China is now coming closer\\ndespite the disasters 1960 famine.\\n\\nAnother interesting and important correlation is\\nbetween fertility rate and survival to age five.\\nTo look at the question globally,\\nwe can compute the average fertility rate overall records,\\nbut it doesn't mean much since it mixes data from\\nmany different years.\\nWe can use the Pandas group by functionality\\nto segment the data frame by year\\nbefore computing the average.\\nThe result is a series index by year,\\nwhich shows the average fertility rate\\nas a function of time.\\n\\nI will plot the series against the time series\\nof survival to age five.\\nThe plot methods allows us to add a second axis on the right\\nto show the range of a second variable.\\nThis plot shows forcefully\\nthat high natality is a consequence of infant mortality\\nand that women have many fewer children when they believe\\nthey will survive.\\nOn a smaller scale, we see the post 1950 baby boom.\\nTo gain even more insight, we can create a pivot table\\nthat segments the fertility means by both year\\nand geographical region.\\n\\nAnd Pandas offers a quick way to plot such a pivot table.\\nWe see that the drop in fertility came\\nafter the baby boom for Africa, America, and Asia.\\nEurope was already low\\nand decreasing since the beginning of the century.\\nHere's the corresponding plot for age five survival.\\nAfrica is now roughly where Europe was in 1950.\\nUsing Pandas built-in plotting functions is the quickest way\\nto make insightful statistical illustrations,\\nbut for maximum flexibility,\\nyou can always use the standard mapplotlib functions,\\nwhich are fully customizable on Pandas series.\\n\\nMapplotlib will recognized column names\\nand will try to use the index as the X coordinate.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6010023\",\"duration\":418,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Database operations in pandas\",\"fileName\":\"4571000_en_US_05_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":541,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch05-pandas > 05_05_database.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"While pandas is not a database, you can use it to combine, merge, and compare tables just as you would with a relational database. Learn about the basics of concatenating, joining, and comparing pandas DataFrames.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16717578,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Pandas is not a database.\\nIt's a library to manipulate structured data\\nand, specifically, tables in memory.\\nUnlike a database, it does not provide persistent storage,\\nand it does not support concurrency and access control.\\nHowever, it does implement some of the functions\\nof a database.\\nAs we have seen, we can select records\\nby imposing conditions on the index or on the fields.\\nFor a database, this will be called a query.\\n\\nIn this video, we'll show you\\nhow Pandas performs another operation typical of databases,\\nmerging tables by matching indices or fields.\\nFor a database, that's called a join.\\nWe are going to experiment with the dataset\\nfrom kaggle.com containing all the Oscars awarded\\nsince the inception of the Academy Awards in 1928.\\nSince we want to explore merging tables,\\nI have split the data set into a few pieces.\\n\\nWe'll put them back together.\\nThe first two pieces are tables of actors and actresses\\nthat have been nominated for a movie.\\nWe see that we have fields for the year of the award,\\nfor the award category, the name of the film,\\nand the name of the performer.\\nIn the initial years of the Oscars,\\nthere was a single actor category,\\nbut then it got split into leading and supporting roles.\\n\\nSame story for the actresses.\\nThe simplest way to combine these tables\\nis to concatenate them, which we do with pandas concat.\\nThen we can re-sort the table by year and category\\nand reset the index,\\nwhich otherwise would have duplicated entries.\\nAnother file hosts a table of the years\\nin which each movie was filmed.\\nWe can use the data in this table\\nto add the year of production into the table of nominations.\\n\\nThat's not super interesting because the production year\\nis usually the year before the award,\\nbut it will show you how to merge tables horizontally,\\naligning the rows based on the film name.\\nWe use the pandas function, merge,\\nand we specify that we want to match records by film name.\\nThis works great, but there is a subtlety\\nthat we should consider.\\nThe standard form of merge returns only records\\nfor which a key is found in both tables.\\n\\nIf we artificially remove some records from the movie table,\\nhere I'm using iloc, the corresponding nominations disappear\\nfrom the merge.\\nIt will be better for us if the year was left empty,\\nbut the records survived, and that's what we get\\nif we ask for a left merge with the keyword how.\\nMore generally, there are four possibilities for merge,\\nwhich are illustrated in this simple Venn diagram.\\n\\nIf we want only the records for which we found the key\\nin both tables, that's called an inner merge,\\nand that's the default in pandas.\\nIf we want all the records in the left array\\nmatch with records from the right\\nwhen one with the right key is present, that's a left merge.\\nVice versa, that would be a right merge.\\nLast, if we want all records for the left\\nand all those on the right match when possible,\\nthat's an outer merge.\\nSo here, if we try a right merge, we would have records\\nfor all the movies that had no nominations for best actor.\\n\\nWe will use an inner merge, the default.\\nBy the way, this is a many-to-one merger\\nbecause multiple nominations for the same movie\\nare matched with the same record from the movie's table.\\nThis happens automatically.\\nThere are many fun things we can do with this data set.\\nFor instance, we could see which actor\\nhad the most nominations\\nby counting the frequency of their names.\\nClearly, Meryl Streep,\\nbut for how many did she actually get an Oscar?\\nFor that, we need a table of all awards, yet another file.\\n\\nIt turns out that Meryl Streep had three Oscars.\\nDid anybody do better?\\nHmm, counting the frequency of names\\nin the award table doesn't help\\nbecause some prizes don't go to actors and actresses.\\nWe can achieve what we need instead\\nby merging the two series of counts\\nfor nominations and awards,\\nusing the indexes of names as the key.\\nAll the non-acting awards will disappear,\\ngiven that this is an inner merge.\\n\\nSo Katharine Hepburn actually did one better\\nthan Meryl Streep, even if she was nominated less.\\nThis type of merge by index, inner, and one-to-one\\nis so common, that in pandas, it can also be achieved\\nby concatenating tables horizontally.\\nThat's axis equals one.\\nLet's look at the movies with the most awards.\\nThree are tied with 11,\\nand let's see what awards they got.\\n\\nFor that, we merge the awards table with the counts\\nusing the film name as the key.\\nSince the film is a column in the award table,\\nbut it's the index for the series of counts,\\nwe need to tell pandas merge.\\nAfter the merge, we limit the output to the largest counts.\\nThese big films have only one acting award\\namong all of them.\\nI wonder why.\\n\\nOne last merge.\\nBy first selecting the category of acting awards only,\\nwe can see which actors won awards together.\\nIndeed, a couple of movies in 2023 and 2024\\nhad quite some luck in that respect.\\nPandas merges are best learned by experimentation.\\nI'm sure that once you do a few,\\nyou'll find them popping up everywhere\\nin your data analysis work.\\nThis concludes our overview of pandas.\\n\\nThere's a lot more that you can do with it,\\nand there's a lot more to learn\\nthan I can cover in this course.\\nI tried instead to give you a feel\\nfor what is possible and practical.\\nI encourage you to start working on a data set\\nthat interests you and to pick up more techniques\\nand knowledge using the many resources available on the web.\\n\"}],\"name\":\"5. pandas\",\"size\":60690198,\"urn\":\"urn:li:learningContentChapter:6009040\"},{\"duration\":819,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2732055\",\"duration\":38,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview: Analyzing baby names\",\"fileName\":\"4571000_en_US_06_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":44,\"solutionVideo\":false,\"editingNotes\":\"overlay www.ssa.gov/oact/babynames\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this chapter, you can practice using pandas by analyzing the popularity of baby names in the US as recorded by the Social Security Administration. This video examines the files that are going to be analyzed and outlines the analysis goals.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1317127,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this chapter,\\nwe are going to practice using Pandas\\nwith an intriguing real-world use case.\\nWe will analyze the US Social Security baby name catalog,\\nwhich lists the names given to male and female newborns\\nfor every year since 1880.\\nThis is a very simple data set,\\nbut it's great fun to play with,\\nand it has been mined, analyzed, and visualized\\nin many publications and websites.\\nWe will use the social security data to track the popularity\\nof names across all years,\\nand to extract the 10 most popular names in a year.\\n\\nAll of this with Pandas, of course.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6006167\",\"duration\":169,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Loading name datasets\",\"fileName\":\"4571000_en_US_06_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":215,\"solutionVideo\":false,\"editingNotes\":\"overlay https://www.ssa.gov/oact/babynames/limits.html at beginning. \\n\\nIf possible, mask the left side of screen to remove files 06_05_challenge.ipynb and 06_06_solution.ipynb, which shouldn't be there.\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch06-babynames > 06_02_loading.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Using Python and its libraries, you can gather and organize data very efficiently. See how to download compressed archives from the web, use Python to open them, load their contents, concatenate them into DataFrames, and save the resulting cleaned data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6270244,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] You can download\\nthe social security name dataset from their website.\\nWe'll use the national data file,\\nwhich gives you a Zip archive containing files\\nfor each year since 1880.\\nI have already unpacked them in your exercise files\\nin this chapter's directory under names.\\nHowever, you could unzip a file in Python using the zip file\\nstandard library module.\\n\\nJupyter lets us browse the contents of the names directory.\\nLet's see, what are these files like?\\nWe open one of them in read mode\\nand print out the very first few lines.\\nIt's a very simple comma-separated format, name, sex,\\nand the number of babies born that year with that name.\\nPandas read_csv shouldn't have any problem,\\nexcept that the CSV reader used the first record, Olivia,\\nto name the columns.\\n\\nWe will instead set the column names explicitly, better.\\nWe will load all the tables\\nand concatenate them into a single data frame.\\nTo avoid confusing data from different years,\\nwe can prepare the individual data frames\\nby adding a new column that specifies the year.\\nTo do that on the fly, directly on the output of read_csv,\\nwe can use data frame assign.\\nHere's an example.\\n\\nExcellent.\\nMaybe you have guessed that I arranged to load each file\\nwith a one-liner so that we can use comprehension\\nto concatenate the data frames.\\nThere are several things happening here,\\nso let's look at this carefully.\\nWe loop over all years between 1880\\nand 2024, excluded.\\nWe build up the file name using an fstring\\nand feed that to read_csv.\\nWe had the column specifying the correct year\\nfrom the loop variable and we pass\\nall the resulting data frames to Pandas concat.\\n\\nIn fact, there are no brackets here,\\nso we're using a generator expression,\\nwhich concat accepts quite happily.\\nIt's a very efficient way to build the data frame.\\nIt's all there, more than 2 million entries.\\nIf we want to squeeze out the last drop of performance,\\nwe can make sure that all the types\\nare backed by pyarrow.\\nAnd instead of CSV, we will save this data frame\\nto the very efficient arrow feather format.\\n\\nIt's compact and loads incredibly fast.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6003341\",\"duration\":312,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Comparing name popularity\",\"fileName\":\"4571000_en_US_06_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":397,\"solutionVideo\":false,\"editingNotes\":\"If possible, mask the left side of screen to remove files 06_05_challenge.ipynb and 06_06_solution.ipynb, which shouldn't be there.\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch06-babynames > 06_03_popularity.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Organizing pandas DataFrames with the appropriate index is important to gain easy access to the records you seek. Learn how to create a MultiIndex for your dataset, sort it by index value, and compare the popularity of common and similar names by plotting the corresponding time series together. Parallel and stacked plots of timelines are often insightful visualizations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11824473,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We are ready to start analyzing the data.\\nLet's load it up.\\nWe wish to examine the changing popularity of a name.\\nSo we can index the data to make that easier.\\nWe'll use a multi-index, indexing on sex first,\\nthen name, then year.\\nWe will also sort the index.\\nGetting the data set for any given name is, then,\\na simple exercise of indexing with the loc.\\nThis series is ready to plot.\\n\\nNotice how matplotlib automatically uses the index\\nto set the x-axis.\\nMary and John had peaks in the 1920s\\nand then again in the 1950s or '60s.\\nIt makes sense to consider the frequency of a name\\nas a fraction of the number of babies born in a year.\\nTo get that, we use groupby to group the unindexed frame\\nby sex and year and sum the number column.\\n\\nThis gives us the total number of births indexed\\nby sex and year,\\nwhich is itself an interesting thing to plot.\\nWe see the baby boomers coming on stage\\nin the '50s and '60s.\\nWe can now create a new column in all years indexed set\\nto the relative frequency of a name in a year.\\nLet's look at Mary and John again.\\nAs a percentage of all babies,\\nMary and John have steadily lost popularity.\\n\\nIn 1880, more than 8% of all baby boys was a John.\\nLet's make a generic function plot name\\nto make this kind of plot\\nand another function to plot a few names together.\\nWe'll be interested in absolute numbers\\nand percentages of births.\\nFor instance, let's compare Michael, John, David and Martin,\\nor for girls, Emily, Anna, Claire and Elizabeth.\\n\\nAnother interesting investigation is\\nto compare variants of the same name.\\nLet's take the two spellings of Claire,\\nthe older version, Clara,\\nand Italian and Irish spellings,\\nChiara and Ciara.\\nNotice how matplotlib does its best\\nto put the legend out of the way.\\nClaire is dominant, but Clara is having a resurgence.\\nNotice the purple spike just after 2004\\nwhen the singer Ciara release her first album.\\n\\nWe can make a slightly different cumulative\\nor stack plot that adds up the counts on top of each other.\\nFor that, we need a different table\\nwhere the rows are the Claire variants\\nand the columns are years.\\nFirst, indexing gives us the data we need.\\nNotice we're using a fancy index to get all the Claires.\\nNow, pandas' data frames have methods stack and unstack,\\nwhich can switch labels from columns\\nto the index and vice versa.\\n\\nUsing unstack will give us a table from this series.\\nWe have unstacked stacked level two of the index,\\nso we got new columns from the years.\\nUsing unstack on level one of the index, the name,\\nwould've given us a transposed table.\\nStack does the inverse operation, bringing us back\\nto a series with a three level multi-index,\\nexcept that now we are swapped around the index levels.\\n\\nBut let's go back to the unstacked table.\\nThis is just what we need.\\nWe may as well lose the index label for sex,\\nwhich doesn't do anything.\\nWe do that with droplevel.\\nAnd while we're massaging this table,\\nwe may as well replace the missing values with zeros,\\nwhich we do with the fillna.\\nWe are chaining operations, leaving one on each line.\\nSome pandas pros really like this style.\\nThe backslashes at the end of each line are needed\\nso that Python sees the whole expression as a single line.\\n\\nWe should be good now.\\nLet's go to matplotlib's stack plot.\\nAh, matplotlib, which still lives in the NumPy universe,\\ncannot quite handle the PyArrow integers.\\nThe error we get is about applying a specific NumPy function\\nto a type that is not understood.\\nNo big deal.\\nWe just convert everything\\nto NumPy integers using astype.\\nThe column names give us labels for the x-axis,\\nwhile the index makes a suitable legend,\\nquite nice and informative, except perhaps\\nfor the garish default colors chosen by matplotlib.\\n\\nSurely we can do better by selecting a different colormap.\\nAnd here it is, a history of Claires.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5994430\",\"duration\":186,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Compiling top tens\",\"fileName\":\"4571000_en_US_06_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":232,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch06-babynames > 06_04_topten.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In many cases, you need to sort and subset DataFrames based on values rather than indices. Learn how to find the top ten names over a range of years, which involves sorting the data, dropping columns and rows, and joining DataFrames together. These are examples of pandas operations that are broadly useful.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7201825,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this video, we will create yearly top tens\\nfor male and female names.\\nWe once again load the data for all years,\\nbut this time we index it slightly differently,\\nby sex and year only.\\nWe'll keep the names as a data column.\\nWe now build up our query by chaining Pandas methods.\\nGetting boys in 2000 is a simple matter\\nof a multi-index lock.\\nThen we get the most popular names\\nby sorting the DataFrame on number in descending order.\\n\\nThen Head gives us the top 10.\\nIn 2023, Liam was king, followed closely by Noah.\\nHow about girls?\\nIt's Olivia and Emma.\\nLet's build up a table to compare the top 10 names\\nover multiple years.\\nWe only need the names, so we get rid of the index\\nand of the number column.\\nWorking from this example, let us create a generic function\\nto return the top 10 for each year.\\n\\nNow, to form a table, we collect the series\\nas the columns as the DataFrame, labeled by year.\\nWe use a dictionary comprehension to structure the data.\\nWe see that Liam took over from Noah,\\nbut Noah kept a decent second place.\\nOliver rose rapidly, while William is now losing favor.\\nFor females, Olivia has overtaken Emma.\\n\\nAnd maybe Charlotte is on its way up.\\nLet's take the 2023 top 10 and plot their progress\\nacross the history of the database.\\nFor that, I first need to reindex the DataFrame\\nto grab individual names more easily.\\nSo we loop over the top 10,\\nrequest the corresponding name from our DataFrame\\nand plot the resulting history.\\n\\nThis is interesting.\\nAll the top female names have surged quite recently,\\nexcept for Evelyn, and to some extent Emma and Charlotte.\\nLooking at males, William and James are classics,\\nbut the rest are recent discoveries.\\nHow about all-time favorites?\\nWe get there quickly by selecting females, say,\\ngrouping by name, summing the numbers, and sorting by value.\\n\\nI will use a chaining style to code this up.\\nSo Mary and Elizabeth.\\nCompare with the modus totals of the 2023 top 10.\\nWe see if they have staying power.\\nPlotting histories for the overall top 10 names\\nshows that they gained their spots\\nin the first half of the 20th century\\nand lasted through decades.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:675a3490498e12fecece9441\",\"duration\":600,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code challenge: Unisex names\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1170172\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:6008080\",\"duration\":114,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Unisex names\",\"fileName\":\"4571000_en_US_06_06_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":251,\"solutionVideo\":false,\"editingNotes\":\"pickup at 3:06 exactly\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":true,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4135193,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We need to find the top unisex names in 2023.\\nDoing so will require a decent amount\\nof pandas manipulations,\\nwhich I'm sure you could achieve in different ways.\\nWhat I'm going to show you is just one possibility.\\nFirst, setting an index of sex and name will help us.\\nNext, we separate the boys and the girls.\\n\\nYou may have guessed\\nthat a merge operation will give us records\\nfor unisex names.\\nWe need to rename the two columns with a number of names\\nthat come from the left and the right data frames.\\nNext, we compute the total frequency of the names,\\nas well as the boys to girls ratio.\\n\\nThis will be our factor to condition.\\nYou see that we use a logical and\\nto combine the two pieces.\\nWe use the resulting Boolean array\\nto down select the data frame.\\nLast, we sort.\\nAnd we skim off the top.\\nLet's try this out.\\n\\nOops. What did I do wrong here?\\nI see.\\nI need to apply my condition on the ratio of the numbers\\nand not on the entire data frame.\\nHere we go.\\n\"}],\"name\":\"6. Project: Baby Names\",\"size\":30748862,\"urn\":\"urn:li:learningContentChapter:6010029\"},{\"duration\":1521,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:6006166\",\"duration\":112,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview: The structure of data\",\"fileName\":\"4571000_en_US_07_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":139,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"The best way to organize data is the case/variable structure, which is implemented directly with rows and columns in pandas DataFrames. It is also important to identify quantitative and categorical variables to encode them correctly. This video discusses how to include important metadata and data descriptors in a data dictionary.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3327066,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] So far,\\nwe've been casual in the way we speak about data.\\nAs we move into the second part of this course,\\nwe should get more precise with our terminology.\\nThe word data is plural.\\nThat's appropriate because we're usually interested\\nin the variability of our observations.\\nWe need to establish\\nhow things are different from each other.\\nSo a data set is like a catalog or a collection.\\nFor instance, for a planetary scientist,\\nthe data set of interest would be the planets\\nof the solar system.\\n\\nThe data set consists of cases, the planets,\\nand each case has attributes called variables.\\nFor instance, the mass of the planet,\\nor the period of its orbit around the sun.\\nIt is standard practice to organize data in a data frame,\\nin effect a table where each row\\nor record refers to one case\\nand each column or field to one variable.\\nIt is not by chance\\nthat the Pandas table object is also called a data frame.\\n\\nVariables can be quantitative, represented by a number,\\nor categorical,\\na description that can be put in words selected\\nfrom a fixed set of levels.\\nUsually, quantitative variables are given as pure numbers\\nwith standard units described in the codebook\\nor data dictionary for the data frame.\\nLikewise, the codebook would describe the levels\\nthat are available for a categorical variable.\\nThis arrangement, known as case variable organization,\\nis very simple,\\nbut it can accommodate many different sorts of data.\\n\\nIt is also affected directly in the data structures used\\nby statistical software.\\nIn our case, Pandas.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6009038\",\"duration\":234,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Importing data with pandas\",\"fileName\":\"4571000_en_US_07_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":309,\"solutionVideo\":false,\"editingNotes\":\"Pickup video is just tacked on at the end.\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":true,\"exerciseFileDisplayText\":\"ch07-wrangling > 07_02_loadingtext.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In the real world, data comes from many sources in many formats. pandas excels at importing many different file types\u2014including comma-separated values (CSV), Excel, HDF, Apache Arrow, and more. This video demonstrates data importing in pandas.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10579827,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We have already used pandas read_csv\\nto load simple text tables into pandas.\\nIn this video, we look in detail\\nat the task of importing textual data.\\nWe'll continue with example of planets.\\nLet's have a look at the file planets.csv\\nincluded in the exercise files.\\nWe see that all values are indeed separated by commas\\nand that each line is a case, a record.\\nThe names of the columns\\nor variables is given in the first line.\\n\\nSome values include commas as thousands separators\\nand they are enclosed in quotes.\\nA simple pandas code does a good job with this.\\nHowever, looking at the structure of the data frame,\\nwe see that some columns that should be numbers\\nhave become strings instead,\\nprobably because of the commons,\\nand that the date FirstVisited is also a string\\nrather than a date.\\nLuckily, pandas has options for that.\\n\\nWe can specify the thousands separator\\nand we can tell pandas explicitly\\nthat the FirstVisited column holds dates.\\nFor consistency, we can also require that the Diameter\\nand MeanTemperature fields should be floats\\nand that Rings and MagneticField should be categories\\nsince they have only two possible values.\\nPerfect. This is now a very clean, accurately encoded table.\\n\\nIt would've been harder to load this file\\nusing the PyArrow backend since in that case,\\nread_csv does not support thousands or par states.\\nWe can, however, convert the data frame\\nto PyArrow types after loading.\\nLet me show you a couple of common variants of read_csv\\nthat you may need in everyday code.\\nIf the columns are separated by spaces rather than commas,\\nas in this file,\\nwe use sep to specify the separator.\\n\\nHere, /s+ means any number of space like characters,\\nand we need a row string starting with an R\\nso that Python does not try to interpret the backslash.\\nIf the column names were missing in the first row,\\nas in the file Planets-noheader.csv,\\nwe need to tell that to read_csv,\\nso it won't try to use the first row\\nas names for the columns.\\n\\nHere, we did not tell it, and now we do.\\nEven better, we provide the column names ourselves.\\nPandas read_csv has many other useful options,\\nsuch as skipping rows at the top and bottom,\\nreading a fixed number of rows, selecting columns.\\nThere are options about parsing dates and floating numbers.\\nWe may also provide a custom Python function\\nto parse specific columns, and a lot more.\\nThese options are mirrored in the writing function to CSV.\\n\\nFor instance, we can specify the separator.\\nWe can instruct pandas\\nto write out null values instead of skipping them.\\nThat's generally a good idea.\\nWe can omit the index\\nor we can omit the header\\nor we can change the date format and so on.\\nThere's no substitute for the documentation.\\nAlthough these days, you can also ask for help\\nfrom your favorite large language model.\\n\\nThis table summarizes the most useful read_csv options.\\nIt's included in the pandas cheat sheet PDF document.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5994428\",\"duration\":348,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Cleaning data\",\"fileName\":\"4571000_en_US_07_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":455,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch07-wrangling > 07_03_moreformats.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Newly acquired data is seldom uniform and entirely well-formed. This video exemplifies how data may be cleaned with string operations, date transformations, duplicate removal, and proper encoding of categorical variables.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14384359,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In addition to simple ASCII tables,\\nPandas supports many other formats:\\nJSON, using web applications,\\nHTML and XML, which we may scrape directly from my website,\\nMicrosoft Excel spreadsheets,\\nHDF, the hierarchical format for scientific data,\\nthe very efficient binary formats\\nfrom the Apache Software Foundation,\\nsuch as Feather and Parquet,\\nproprietary statistic software formats\\nsuch as SaaS, Stata, and SPSS,\\nSQL databases, and finally,\\nthe internal binary Python format, Pickle.\\n\\nIn some cases, you need to install other packages\\nto support that functionality.\\nI've indicated those packages in this table,\\nbut Pandas itself will tell you if you need them.\\nThis list is not exhaustive.\\nOther formats are supported by third party packages,\\nso it's always worth Googling\\nor asking your favorite large language model.\\nIn this video, we'll concentrate on a few useful formats,\\nbut our considerations will apply more generally.\\nLet's first talk about saving.\\n\\nSay we did the work of loading a very clean table.\\nHow can we save it and preserve all this detail?\\nThe simplest way to do so is\\nwith Python's serializing capability known as pickling.\\nto_pickle will save, read_pickle will load back.\\nOccasionally, you may find that Pickles\\nare not compatible across different versions\\nof Python or Pandas,\\nso this format is best used internally within an application\\nrather than to exchange data with others.\\n\\nPersonally, I like the very efficient\\nand portable formats developed\\nby the Apache Arrow project, Feather and Parquet.\\nFor large data files,\\nthis can be several times faster than Pickle\\nand many times faster than text.\\nSo we can save with to_feather\\nand read with read_feather, similarly for Parquet.\\nLet's move on to JSON,\\nthe native JavaScript data format.\\n\\nThis is, strictly speaking, a text format,\\nbut it is meant more for machine reading\\nand writing than for people.\\nIn addition, the JSON syntax is very close\\nto Python lists and texts.\\nWe'll use a simplified version of the planet's dataframe\\nto exemplify JSON reading.\\nThe important point is that there are different ways\\nto organize a dataframe using dicts and lists,\\nand therefore JSON.\\nPerhaps the most straightforward way is\\nhaving one dict for each record.\\n\\nThis is also what we would need, for instance,\\nin the JavaScript plotting library, D3.js.\\nSo have a look at this file, Planets-records.json.\\nPandas has no problems loading this.\\nTo write out this variant of JSON,\\nwe'd use to_json with the option orient='records'.\\nIf you have an explicit index,\\nyou'd probably use a dictionary of dictionaries\\nas in the file, Planets-index.json.\\n\\nTo parse this, you need to tell Pandas\\nwith orient='index'.\\nOtherwise, you end up getting the transpose of the table.\\nTo reduce file size, a JSON file may be organized\\nto separate labels, both the index and the column,\\nand condense the values in a list of lists\\nas in file Planets_split.json.\\nThis is known as a split orientation.\\n\\nAnd again, we can load it\\nby telling Pandas that's what we have.\\nOther JSON options are similar to Read CSV,\\nfor instance, to set how dates and floats are parsed.\\nMissing values are always rendered as null in JSON,\\nwhich is mapped back to the correct\\nmissing data value in Pandas.\\nLast, I want to show you how Pandas can parse HTML.\\nWe look at a page from Wikipedia\\nabout athletics at the 2024 Summer Olympics.\\n\\nThere are several tables here.\\nYou don't even need the file.\\nYou can just provide a URL to Pandas.\\nBut for convenience, I have dumped the page\\nand included the file in the repository.\\nIf we just use Read HTML,\\nPandas tries to parse all the tables in the webpage,\\nand we get back a list of dataframes.\\n\\nBut we can use match to look for a specific word\\nand thus select a subset of tables.\\nWe could also use an HTML attribute such as ID,\\nbut that doesn't help us with these Wikipedia tables.\\nIn this page, by looking for rank,\\nwe get a table of medals listed by country.\\nParsing options are similar to Read CSV.\\nFor instance, we can indicate\\nwhich columns should provide the index.\\n\\nAnd we can remove the last row\\nsince we know how to make our own totals.\\nYou can imagine how useful it can be\\nto grab data on the internet\\nand get it into Pandas for analysis.\\nHowever, as we'll see in the next video,\\nHTML data often needs significant\\ncleaning and reorganization.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6009037\",\"duration\":403,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filtering, reshaping, and tidying data\",\"fileName\":\"4571000_en_US_07_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":589,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch07-wrangling > 07_04_cleaning.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After cleaning data, you need to identify the records that are important to analysis and cast them in structures that emphasize relevant information. This video also introduces the tidy data arrangement and demonstrates melting and unmelting operations to obtain it.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16760741,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] To demonstrate cleaning\\nand reorganizing data,\\nwe look at another table from the Wikipedia Paralympic page,\\nspecifically the table of medals in the 23 events for men.\\nWe locate it with rate HTML using the name of one\\nof the medalists as a match.\\nNow this Wikipedia HTML tables are made for display,\\nnot for data transfer.\\nSo there are several things here that we need to fix.\\n\\nFirst, we know that some records have no data\\nthat comes from HTML table dividers.\\nWe drop those records with dropna.\\nNext, I'm bothered by the word details attached\\nto every event name.\\nWe get rid of that with appenda string function\\nthat selects a slice within each value.\\nSo we cut out the last eight characters.\\nBefore we get to work on separating athletes and countries\\nand normalizing the performance values,\\nwe recast this data frame following the principles\\nof tidy tables.\\n\\nTidy tables were popularized by Hadley Wickham\\nand the basic idea is that each variable forms and column\\nand each observation forms a row.\\nTidy data is easier to manipulate, filter,\\nand summarize using tools like Pandas in Python\\nand it's easier to plot and visualize.\\nThe problem with that data frame is that if we wish\\nto find an athlete, we need to look\\nin three separate columns, gold, silver, and bronze.\\n\\nFurthermore, the type of medal won\\nby an athlete is not a variable,\\nbut it is encoded by the column\\nin which we find the athlete.\\nWe want to recast the table so that each medal\\nand athlete will get a separate record.\\nThe pandas data frame method melt is just what we need.\\nLet's focus on the athletes first.\\nWe want the values from three columns, gold, silver,\\nand bronze to be condensed into a single column athlete.\\n\\nAnd we want the names of those columns, gold, silver,\\nand bronze to become another column, medal.\\nWith pandas melt,\\nthe columns listed under ID VARs are preserved\\nand play the role of an index.\\nThe value VARs are the columns that provide the values\\nand their column names are saved\\nin the new column, VAR name.\\nSo you see that the first record in the original table,\\nhere, becomes three records in the melted version.\\n\\nLet's try that out.\\nExcellent. Now every athlete gets his own row.\\nWe'll do the same for the performance results,\\nthe times, heights, and lengths.\\nAnd you see where this is going.\\nNow we need to merge these two tables,\\nbut first we need to remove those annoying dot one\\nfrom the medal column.\\nThe panda string function replace works fine here.\\n\\nSo here's the merge. We merge on both event and medal.\\nWhat's left to do?\\nWe need to separate athlete and country\\nand to do that, we notice that the athlete field,\\nactually, includes a unicode non-breaking space\\nbetween the two.\\nWe can then split the strings on that.\\nTo do so, we use the string function split, followed by get.\\nGet perhaps is not the most obvious naming\\nfor taking an element in a two pole,\\nso you'll have to remember it.\\n\\nNow we have athlete and country. Are we quite done?\\nWell, the relay events have different formatting\\nwithout the non breaking space and it's not easy\\nto split those because some countries are one word\\nand some are two.\\nSo we will define a custom function to extract the country,\\ntaking different paths for individual and relay events.\\nWe could get a proper table of countries,\\nbut since this is Python\\nand we prize pragmatism, we write our code only\\nfor the countries we actually have.\\n\\nNow for the relay events records only,\\nwe apply this function to the athlete column\\nand assign the result back to the column itself.\\nOkay, now for the result field,\\nlet's see what values it takes.\\nThe parenthesis just give higher precision for the results,\\nso we may as well get rid of them.\\nFor that, we'll use a regular expression\\nthat matches anything within parenthesis.\\nIt's outside the scope of discourse for me\\nto explain regular expressions,\\nbut you can learn more about them\\nin the LinkedIn learning course by Kevin Skoglund,\\nor from many resources on the web.\\n\\nWe also get rid of PTS or M for meters.\\nWe can figure out the units based on the event.\\nAll that's left to do is\\nto split off the two letter annotations\\nthat denote records such as PB, AR.\\nFor that, we need to chain three string methods,\\nsplitting at the space, taking all the resulting items\\nexcept for the first and joining them back into a string.\\nHere we go, OR, WR, AR and NR are Olympic, world,\\ncontinental and national record.\\n\\nPB is personal best and SB is season's best.\\nBeautiful.\\nOur table is fully cleaned up and tidied up.\\nWe can celebrate by asking a few questions from the data.\\nWe ask, \\\"Which gold medals were also Olympic records?\\\"\\nJust six of them.\\nWe ask, \\\"How many bronzes were personal bests\\nor season bests?\\\"\\nSee the logical and, and the logical or,\\nthat we used to combine these conditions.\\n\\nWell, we're down to those athletes.\\nAnd which male athletes won the most medals\\nin individual events?\\nIf you follow the 2024 Olympics, you'd know already.\\nIt's Grant Fisher and Noah Lyles.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2732054\",\"duration\":330,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Simulating data\",\"fileName\":\"4571000_en_US_07_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":556,\"solutionVideo\":false,\"editingNotes\":\"Use take 2. Overlay at 0:23 https://faker.readthedocs.io/. Long pause in middle to cut out, waiting for airplane to subside\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch07-wrangling > 07_05_simulating.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Generating synthetic data is an important tool to test an analysis or a newly built tool. This video demonstrates data simulation using the Python package faker.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13645337,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Sometimes you need to develop\\na data analysis application\\nbefore you have access to the data.\\nIn that case, a good approach is creating fake data\\nwith the right structure and realistic content.\\nThen you can use that to populate your tables\\nand test your algorithms.\\nHowever, making these records by hand\\ncan be very time-consuming.\\nOf course, the Python ecosystem can help with that.\\nThe Faker package is a popular library to generate random,\\nrealistic-looking fake data,\\nsuch as names, addresses, dates, and more.\\n\\nLet's have a look at what it can do.\\nNote, there's a lot of detail in this code,\\nso it may help you to occasionally stop the video\\nand parse the code\\nto make sure we understand what's happening.\\nWe start by creating an instance of the Faker class.\\nThen we can call its methods to make up a person,\\na name, address, date of birth, city, and state.\\nTo make this a bit more interesting,\\nwe're going to create a database\\nof imaginary musical performers\\nof their songs and of the songs rankings\\nin a weekly billboard.\\n\\nWe start with an imaginary performer.\\nIn addition to name and date of birth,\\nwe put together a city and state of birth,\\nand we pick a random genre from a list using random element.\\nHere's an artist that's a Python dictionary.\\nWe collect 100 random artists in a DataFrame.\\nThe index is just a simple ordinal range.\\nThat's just fine as a unique ID,\\nwhich we will use to link to other tables.\\nSo we'll name the index explicitly artist_id.\\n\\nNow for imaginary songs, we'll include a random artist,\\na random title of up to 20 characters,\\nthe date of release in the last two years,\\nand the appropriate genre.\\nWell, yes, I agree that the title could be more incisive.\\nLet's try another one.\\nMaybe better.\\nI'll make a table with 500 songs.\\nAnd again, I'll adopt the a automatically generated index.\\nNow we're get into the weeds a bit.\\nI'd like to create a history of plays,\\nsay on the radio for each song.\\n\\nThe history will start from the release date,\\ncontinue for a random number of weeks,\\nand so that's how I compute the end date.\\nI will create a range of dates that covers the weeks.\\nWe use the Pandas function date_range.\\nA frequency parameter of W-MON gets us a Monday every week.\\nWe simulate a very simple history,\\ndeclining linearly from a random initial value,\\nand multiplying by random coefficients\\nbetween 0.8 and 1.2 to add some noise.\\n\\nIf you want to play with this,\\nyou can get as fancy as you wish.\\nGreat. Let's pull all of this together.\\nLooks good.\\nI'll make such a history for each song.\\nThe index from each history will be useful\\nbecause it gives us the number of weeks\\nsince the release of the song,\\nso we'll turn it into a regular column\\nwith reset_index and change its name to week.\\n\\nWe will concentrate on the last year only,\\ndownselecting our table using a Boolean condition.\\nWe also need to sort this table by date, earliest first,\\nand by number of plays, largest first,\\nbecause we want to get the top 10 each week.\\nTo collect all the top 10s, we group by date,\\ntake the first 10 records, and then set the data as index.\\nWe can then use loc to get the top 10 for a given week.\\n\\nWith another groupby operation.\\nWe can compute the rank of each top 10 song every week.\\nFinally, we merge with a song and artists tables\\nusing a song_id and artist_id to get titles and performers.\\nRemember that those IDs\\nare actually in the index of the tables so merge,\\nwe'll need options to reflect that.\\nWe are done.\\nWe have all the top 10s in a DataFrame\\nwith titles and artists.\\n\\nSo we may ask, is there any performer\\nwho has been as the top of the billboard more than once?\\nOh yes.\\nTammy Morris took four first spots with two songs,\\n\\\"Nor Control Attack\\\" and \\\"Whose of Small Send.\\\"\\nLet's save our work.\\nNot all the Pandas output formats\\ncan save multiple tables together.\\nPickles are always an option,\\nand so is one format\\nthat we did not discuss in detail, HDF5.\\n\\nWith HDF5, the DataFrame method to HDF\\nkeeps app appending tables to the same file.\\nYou can give a key,\\nwhich is how the individual tables are found in the file.\\nIn fact, HDF5 files can have a hierarchical structure\\nsimilar to a file system.\\nTo read the tables back into the Pandas,\\nwe'd use HDFStore with a simple dictionary syntax.\\nAnd that is all for a foray in faking data.\\n\\n\"},{\"urn\":\"urn:li:learningContentAssessment:675a34bb498ee7e9b17b3e97\",\"duration\":600,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code challenge: Country rankings\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1170173\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:2732053\",\"duration\":94,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Country rankings\",\"fileName\":\"4571000_en_US_07_07_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":140,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3142453,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] So we need to return a table\\nof the top four countries in athletics,\\nranked by medal count.\\nThis is complicated enough that I will do it in steps,\\nreturning a partial result every time.\\nCoderPad will continue to scold me, but too bad.\\nFirst, I will group by medals.\\n\\nThe array is called medals,\\nand counts the occurrence of each country's name.\\nLet's try this.\\nGood. This gets me a series with a multi-index.\\nRemember, we can move a level of the index\\nto columns using unstack.\\nRemoving the first level, here it goes.\\nBetter. Good progress.\\n\\nI also want to get rid of those NaNs.\\nThey should be zeros.\\nNext, we compute totals for each country\\nby summing over the columns.\\nGood.\\nLast, we sort.\\nAnd we take the first few.\\n\\nPerfect. Well done.\\n\"}],\"name\":\"7. Importing and Wrangling Data with pandas\",\"size\":61839783,\"urn\":\"urn:li:learningContentChapter:6006172\"},{\"duration\":1314,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:6009036\",\"duration\":80,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview: Exploring data\",\"fileName\":\"4571000_en_US_08_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":111,\"solutionVideo\":false,\"editingNotes\":\"overlay https://www.gapminder.org/tools at 1:37 and https://www.gapminder.org/fw/income-mountains/ at 1:43\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch08-summarizing > 08_01_intro.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Data summaries and visualizations are the fundamental tools of exploratory data analysis\u2014where you seek to understand a dataset\u2014and storytelling with data\u2014where you share the insights effectively and memorably. This chapter uses datasets from the Gapminder project, which reveals surprising truths about world-wide trends with carefully curated data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3306809,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Once we have imported our dataset\\nand we are satisfied that it is clean and refactored,\\nit's time to start exploring it,\\nsummarizing it, and visualizing it.\\nWe'll be trying to understand the typical values\\nand the range of variation of our variables,\\nand to identify which variables are correlated.\\nIn this chapter, we will again work with datasets\\nfrom the Gapminder project.\\nLet's have a look at the dataset.\\n\\nFor all the countries in the world\\nand for years starting in 1900,\\nthe dataset encodes a few basic trends:\\nthe approximate population, the expected lifetime,\\nthe percentage of children surviving to age five,\\nthe average number of babies per woman,\\nand the gross national product divided by population.\\nThe unit is 2017 equivalent dollars.\\nWe also look at the second dataset\\nthat describes the distribution of incomes in China\\nand in the USA in 1965 and 2015.\\n\\nThe Gapminder website includes a number\\nof beautiful and powerful visualizations.\\nWe will use Python to reproduce some of them.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6008078\",\"duration\":257,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Summarizing quantitative data\",\"fileName\":\"4571000_en_US_08_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":355,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch08-summarizing > 08_02_summarizing.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Descriptive statistics are indispensable to understand quantitative data by distilling typical values and their variation to observables such as mean, median, variance, and quantiles. This video demonstrates the computation of descriptive statistics in pandas.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8658798,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] To understand the dataset,\\nwe begin by summarizing it.\\nThat is, we quantify both the typical trends\\nof the variables and the variations around them.\\nIn short, the distribution of the data.\\nIn this video, we'll look at a few simple ways\\nto summarize the distribution of a numerical variable.\\nWe'll examine Gapminder data that describes the distribution\\nof incomes in China and in the US in 1965 and 2015.\\n\\nThese distributions are not very accurate,\\nbut they will be sufficient for our example.\\nFor each country and year, we have 1,000 entries\\ncorresponding to a sample of 1,000 representative people.\\nOne way to describe the variation of a variable\\nis by quantifying its range.\\nHowever, focusing on these extremes,\\nthe minimum and the maximum, is usually not very insightful.\\nIt's also imprecise, given that our set is a limited sample\\nof a population rather than a complete census.\\n\\nBoth minimum and maximum are statistics,\\ndescriptive numbers that we compute from the data\\nand that summarize the data.\\nOf course, another very important statistic is the mean,\\ncomputed by summing up all the data points\\nand dividing them by the number of points.\\nIn symbols, we write a sum of the data points, xi,\\ndivided by how many we have.\\nIn Pandas, we just type mean.\\n\\nWe can use groupby with the index variables\\nto compute the statistics separately\\nfor each country and year.\\nThe variance is a measure of variation tied closely\\nto the mathematical concept of normal distribution\\nTo compute it, we obtain divisions from the mean,\\nsquare them, and take the average.\\nAnd for a technical reason,\\nwe actually divide by the number of points minus 1.\\nIn Pandas, it's just var.\\n\\nThe standard deviation is the square root of the variance.\\nTaken together with a mean,\\nit gives a pretty good idea of the center\\nand spread of a distribution.\\nIn Pandas, it's std.\\nNext, the quantile.\\nThe quantile is a statistics that describes the value\\nfor which we know that a certain percentage\\nof the data points lie below.\\n\\nWe computed with quantile.\\nTaken together, the 25th and 75th quantiles\\nspecify a coverage interval\\nthat includes 50% of the data points.\\nThe 50th quantile is known as the median.\\nFrom this table, we say that in 2015,\\napproximately half of the Chinese population\\nmade less than $2,800,\\nand approximately 25% made less than 1,800.\\n\\nLast, the inverse of the quantile\\nconsists in finding the percentage of the population\\nat which a given value or score lies.\\nTo find it, we need to go outside Pandas to SciPy stats.\\nAnd to compute a non-Pandas statistic we group by,\\nwe use apply.\\nIn China in 1965,\\nno one in our sample made $5,000 a year.\\n\\nBy contrast, it was a very low income in the States in 2015.\\n4.5%.\\nThe Pandas method, describe,\\nreturns several summary statistics at once.\\nBetween 1965 and 2015, China made great strides,\\nbut it still had some way to go.\\nWe see this clearly if we take USA 2015 values\\nas a reference and use them to normalize,\\nto divide the other entries.\\n\\nVery good.\\nIn the next video, we move on\\nto plotting these distributions.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6009035\",\"duration\":354,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualizing distributions\",\"fileName\":\"4571000_en_US_08_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":454,\"solutionVideo\":false,\"editingNotes\":\"overlay at 6:34 seaborn.pydata.org\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch08-summarizing > 08_03_visualizing.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Visualizations such as box plots, histograms, and density plots provide immediate insight into the variation of variables in populations and their subgroups. Learn how to visualize distributions with pandas, Matplotlib, and Seaborn.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12844282,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We continue to analyze\\nthe income distributions in the US\\nand in China be plotting them.\\nHans Rosling argues convincingly\\nthat the logarithm of daily income\\nis the number that's really descriptive\\nof the lifestyle available\\nto a person anywhere in the world.\\nSo we compute that and plotted alongside yearly income.\\nThe summary statistics that we described in the last video\\nare brought together visually in a box plot,\\na PANDAS plot of kind box.\\n\\nThe box itself extends from the 25th\\nto the 75th quantiles with the line at the median.\\nThe so-called whiskers have a complicated definition.\\nThey're the minimum and maximum values in the dataset,\\nbut only if those do not stray too far\\nfrom the 25th and 75th quantiles.\\nPrecisely, not more than one and a half times\\nthe inter-quantile range between them.\\nIf they do stray out, they're considered outliers\\nand they're plotted individually.\\n\\nThat's what we see in the US income data\\nfor some wealthy individuals.\\nRemember, these are just representative points,\\nnot actual people.\\nTo compare China and the US in 1965,\\nwe reset the index of the dataset\\nand we tell PANDAS to stratify the plot by year and country,\\nthe scales here are so different that we don't see much.\\nIt's better to make box plots of the logarithm\\nof the income.\\n\\nFrom this plot, we understand that both countries improved\\nbetween 1965 and 2015.\\nIn 2015, China is not too far from the US of 1965.\\nRemember that the log 10 of zero means $1\\nand a log 10 of two, $100.\\nA much richer visualization\\nof a distribution is a histogram,\\nwhich divides the data into a set of continuous bins\\nof equal width, and then for each bin\\nshows a rectangle with high proportional\\nto the number of data points that belong there.\\n\\nThis shows that in 1965,\\nmost people in China had daily incomes\\nslightly below a dollar.\\nWe can enhance the histogram by marking the locations\\nof a few descriptive statistics using vertical lines,\\nXV line in map plot label.\\nTo distinguish between one line and the other,\\nwe use different line styles.\\nSo here we showing the mean, the median,\\nand the 25th and 75th quantiles.\\n\\nA density plot is effectively a smooth histogram,\\nwhich associates an estimated frequency\\nwith every value on the X axis.\\nHere I'm comparing the histogram\\nand the density plot,\\nso I have normalized the histogram\\nby setting density equal true.\\nThen the area under the curves is the same for both.\\nRemember that the smooth line is just an approximation.\\nIt's also dependent on the scale of the smoothing,\\nwhich is chosen automatically by which we can set directly.\\n\\nLet's compare histograms for China\\nand the US using log income.\\nIn 1965, there is basically no overlap.\\nThe poorest Americans are richer than the richest Chinese.\\nIn 2015, the situation is very different.\\nBoth the Chinese and the Americans are richer,\\nbut there's also significant overlap,\\nand of course there are many more Chinese than Americans.\\nTo see how that may affect our understanding of the data,\\nwe rescale the histograms using the respective populations.\\n\\nTo do that, we need an array of weights of the same length\\nof the data that we are plotting.\\nThis is complicated enough to do\\nthat I'm going to show you to you step by step.\\nSo here's the data that I need to histogram.\\nI also need the main gapminder dataset to get populations,\\nand here's the population of China in 2015.\\nThis query produces a series of length one,\\nI actually need a single value,\\nso I take the first and only record.\\n\\nI will multiply that value by an array of once\\nand divide by the length of the series.\\nThis weight means that every sample will represent\\napproximately 1.4 million people.\\nFinally, my weighted histogram.\\nI can repeat this for the two countries together.\\nRosling points out correctly\\nthat there's a lot of purchasing power\\nat the richer end of the Chinese population,\\nso corporations will do well to tap that market.\\n\\nYou may have noticed that PANDAS plots are rather drab.\\nOf course, you're free to fine tune them using Matplotlib.\\nAn alternative is using the plotting functions\\nof the Seaborn library,\\nwhich pays much more attention to presentation.\\nLater in this course,\\nwhich has the added advantage\\nof allowing interactivity.\\nIn Seaborn, the names and options\\nof the plotting functions are slightly different\\nthan in PANDAS or Matplotlib.\\n\\nHowever, it's easy to Google them\\nor to ask your favorite AI assistant.\\nLet's look at 2015.\\nHere are box plots and density plots, respectively.\\nPicking two of Seaborn's figure styles.\\nSeaborne plots are rather elegant.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6006165\",\"duration\":325,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Comparing quantitative variables\",\"fileName\":\"4571000_en_US_08_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":441,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch08-summarizing > 08_04_comparing.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Correlation statistics and multivariable plots help you understand how data variables change together. Learn how to create multivariate plots with pandas, Matplotlib, and Seaborn, and see how to use all encoding channels\u2014position, color, size, and texture\u2014to plot more than two variables together.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11938199,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Plotting two variables together\\nhelps us identify possible relations between them.\\nDo they rise and fall together?\\nIs it possible that changes in one may cause\\nor be caused by changes in the other?\\nWe often pick one column in a dataset\\nas the explanatory variable,\\nand see if indeed it explains the variation\\nin a response variable.\\nLet's once again load the gapminder dataset.\\nFor data with timestamps, the simplest thing to do\\nis to plot a variable using the date\\nas the explanatory variable.\\n\\nThat is, to plot a time series.\\nI will focus on my country of birth, Italy,\\nand show you a time series of population.\\nSo here's the restricted dataset,\\nand here's a scatterplot of population against year.\\nThe rate of change of the population\\ndoesn't change very much across two centuries.\\nThe same plot will be different for China or India,\\nespecially after 1950.\\n\\nHere I am superimposing two plots\\nby reusing the matplotlib axis returned by the first one.\\nGoing back to Italy, let's look at income per person\\nin 2017 equivalent dollars, again, as a time series.\\nWe will actually plot the logarithm of the income\\nby specifying logy=True.\\nHere one can see a big dip during World War II,\\nand in the top corner,\\na dip after the 2008 financial crisis,\\nand a smaller one for COVID-19.\\n\\nAll in all, the last 20 years in Italy\\nhave been rather disappointing.\\nWhat if we plot log income\\nagainst the variable related to quality of life,\\nsuch as life expectancy?\\nWe see that they are generally correlated\\nwith some turnarounds.\\nWe can get a better sense of those by adding color,\\nwhich is mapped to the year.\\nAn important point,\\nwhenever you use color to communicate information,\\nyou should be mindful of colorblindness.\\n\\nThe matplotlib color map varied this,\\nwas designed to be safe for various types\\nof color deficiencies.\\nPlot in a time series of life expectancy\\nconfirms that dips happened with the two wars,\\nand a much smaller dip in 2020 with COVID-19.\\nPlots on a monitor or on paper\\nare intrinsically two dimensional,\\nso it's hard to go beyond showing two variables.\\n\\nIn addition to exposition, white position, and color,\\nwhich we just use to encode the year,\\nwe can also encode variables using the size of markers,\\nthe shape, the contour, and perhaps their texture.\\nAll these dimensions are sometimes called scales.\\nWe will reproduce a classic Gapminder plot.\\nWe take the entire world in 2015\\nand plot log income, life expectancy,\\nand population using the size of the markers.\\n\\nThis size argument S specifies the areas in point squared,\\nso we need to scale the population column appropriately.\\nHere it is.\\nTo make it a bit clearer,\\nwe set a white contour for the markers.\\nWe can then use color to distinguish between continents.\\nFor that, we need to map the region column to a color name\\nand pass the resulting Pandas series to scatter.\\nHere's the map and here's the colorful plot.\\n\\nLast, we'll emphasize a single country\\nby changing the edge color.\\nThe USA has a black circle around the green.\\nThis is very pretty and very promising,\\nbut to understand\\nhow things are changing across the century,\\nwe can animate this plot by creating a function\\nthat takes a year as an argument\\nand passing that function to a Jupyter widget.\\nIn this case, we will use a simple slider\\nranging from 1950 to 2023 to select a year.\\n\\nAnd a dropdown selector\\nto decide which country should be emphasized.\\nHere's the generic plotting function,\\nhere's the Jupyter Library pywidgets,\\nand here's the interactive plot\\nwhere we set the argument's year and country\\nequal to a widget.\\nNow the slider gives us interactivity.\\nThe progress of all countries is evident\\nin income and life expectancy,\\nbut especially so for Asia.\\n\\nOrange, which now hosts more than\\nhalf of the world's population.\\nIndeed, interactivity is now a further element\\nthat we can exploit to visualize that effectively.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6007135\",\"duration\":298,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Summarizing and visualizing categorical data\",\"fileName\":\"4571000_en_US_08_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":400,\"solutionVideo\":false,\"editingNotes\":\"Overlay at 00:25 \\\"Daniel T. Kaplan, Statistical Modeling: A Fresh Approach\\\"\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch08-summarizing > 08_05_categorical.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Descriptive statistics for categorical data include counts and proportions, which can be organized in pivot and stratified tables and visualized in a variety of bar and pie plots. Learn how to use pandas to create summaries and visualizations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10905694,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We move on to categorical variables,\\nthose that take a value from a finite, discrete set.\\nHow do we describe variation in categorical variables?\\nWell, of course, with tables.\\nWe switch from Gapminder to the Whickham dataset\\ndiscussed by Kaplan\\nin his excellent textbook \\\"Statistical Modeling.\\\"\\nThe table records interviews with women\\nin Whickham, England in 1973\\nwho were asked if they were smokers.\\nThe interviews were followed up 20 years later.\\n\\nWhen it was recorded, if the women were still alive.\\nThe categorical variables in this case,\\nsmoker and outcome, are both binary, yes or no.\\nHere are the first five rows of the dataset.\\nUsing value counts, we can tally the explanatory\\nand response variable separately.\\nSmoker is the explanatory variable.\\nOutcome is the response.\\n\\nThis doesn't tell us much.\\nOther than that all the groups are represented fairly well,\\nsmokers and non-smokers,\\nwomen who survived for 20 years and those who didn't.\\nIf we want to see the values as fractions\\nof the total number of records,\\nwe add normalize equals true.\\nThese fractions are known in statistics as proportions.\\nWe are looking for an association.\\nThat is, we wish to evaluate a claim\\nthat the groups of smokers and non-smokers are different\\nwith regards to their longevity.\\n\\nSo we break down the proportion of outcomes by smoker group,\\nusing group by.\\nThe resulting series has a two level multi-index.\\nTo organize the proportions in a table,\\nwe move the inner index to a column using unstack.\\nThis stable in fact is a bit surprising.\\nIt seems that smoking\\nimproves the chance of being alive 20 years later.\\n\\nThe problem is that we're not controlling\\nfor other variables such as age.\\nIf smokers are younger on average\\nat the beginning of the study,\\nthen it stands to reason that more of them\\nwill be alive after 20 years no matter what.\\nTo investigate this puzzle,\\nwe use the method that statisticians call stratification.\\nWe divide the people in the table,\\nour cases, into age groups.\\nThe pandas function to do this is cut,\\nand we get to pick the boundaries of the bins.\\n\\nWe see that the result\\nis another categorical variable, age group.\\nLet's see the counts in each group.\\nWe see the young and middle aged women in 1973\\nare equally represented.\\nOlder women, less so.\\nThis time, we will first group the data by age group\\nand then by smoker status.\\nIn each age group,\\nnon-smokers have a better life expectancy.\\nSo this data represent an example of Simpson's paradox,\\na phenomenon in probability in statistics\\nin which a trend appears\\nin several different groups of data,\\nbut disappears or reverses when these groups are combined.\\n\\nLet's plot.\\nWe start by plotting counts separately for our response\\nand explanatory variables.\\nThis is a bar plot.\\nThe bars could also be horizontal.\\nFor that, we switch the kind from bar to bar H.\\nWe could even use a pie chart.\\nNow let's try to break up the visualization\\nso we show the outcome by smoker status.\\nWe have already created a table group by outcome.\\n\\nHere it is.\\nWe can plot that as bars.\\nWe can also do a stacked histogram\\nby specifying stacked equals two.\\nThis plot illustrates the original suspicious finding\\nthat smoking improves the outcome.\\nSo let's do stratification.\\nWe first create a table, group by age group,\\nand then plot with bars.\\nThis is a decent first try,\\nbut the multi-labels on the x-axis make this confusing\\nand difficult to parse.\\n\\nWe can do better by plotting\\nonly the fraction of people who are alive.\\nWe select them using lock.\\nWe plot with bars again.\\nAnd here, we do see that in every age group,\\nnon-smokers have a slight edge in outcomes.\\nThat is Simpson's paradox at work.\\nThis completes our tour of summarizing\\nand visualizing distributions with pandas and Matplotlib.\\n\"}],\"name\":\"8. Summarizing and Visualizing Data\",\"size\":47653782,\"urn\":\"urn:li:learningContentChapter:6003344\"},{\"duration\":1552,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2732051\",\"duration\":152,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview: Understanding data\",\"fileName\":\"4571000_en_US_09_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":177,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Statistics is the science of learning from data, and statistical modeling helps you to identify relationships between variables and make predictions about the future. This chapter returns to the Gapminder data and develops simple models to quantify trends and make hypotheses.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3583278,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this chapter, we will focus on the task\\nof understanding the meaning of data by modeling.\\nGiven a data set with multiple variables,\\nwe seek to capture the way in which the variation in one\\nor more response variables is caused by the variation\\nof one or more explanatory variables.\\nA model can be seen as a function\\nthat takes explanatory variables as input\\nand outputs response variables.\\nThe model will depend on a number of parameters\\nwhich are usually not known in advance.\\n\\nSo to fit a model to the data,\\nwe apply to the explanatory variables\\nfor each case in our data frame,\\nand we compare the response variables\\nas predicted by the model\\nwith the corresponding observed variables.\\nWe then modify the parameters of the model\\nuntil the difference,\\nthe residual between the predicted and observed values,\\nis minimized in a precise mathematical sense.\\n\\nOnce we have fit a model,\\nit becomes useful for two different goals.\\nFirst, its parameters may reveal important qualities\\nof the population and the study.\\nSecond, we can use the model to predict the response value\\nfor sets of explanatory variables\\nthat we have not yet observed.\\nPerhaps they're in the future.\\nTo choose between alternative models,\\nwe can compare the goodness of fit,\\nwhich is usually a single number derived from the residuals.\\n\\nThis is known as in sample goodness of fit,\\nbecause it tells us how well the model does on the data\\nthat was used to fit it.\\nDoing so will tend to overstate goodness of fit,\\nespecially with very complex models,\\nthe fit will tailor the model to the specific data set\\nthat we got\\nrather than the general characteristics of the data.\\nThere are mathematical techniques\\nthat can adjust in sample goodness of fit\\nby accounting for the complexity of the model.\\n\\nAlternatively, we can set apart some of the data,\\nexcluding it from the fit,\\nand then evaluators for those testing data.\\nDoing so is known as out of sample goodness of fit,\\nand it provides a more accurate measure of performance.\\nBut enough talk, let's get to our data and to Python.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6006164\",\"duration\":355,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Fitting models to data\",\"fileName\":\"4571000_en_US_09_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":485,\"solutionVideo\":false,\"editingNotes\":\"overlay at 1:24 www.statsmodels.org\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch09-modeling > 09_02_fittingmodels.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Linear regression is the simplest form of statistical modeling, which quantifies the effect of predictor variables on output variables. This video demonstrates simple and multiple linear regression with the user-friendly but sophisticated Python package statsmodels, using the standard tilde formula notation.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12467823,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] To demonstrate model fitting,\\nwe return to our gapminder data.\\nWe will take the year 1985\\nand study the relation of child survival rates\\nand number of babies per woman.\\nWe tell Pandas explicitly\\nthat the region is a categorical variable.\\nLet's make a quick scatter plot\\nas we learn to do in chapter eight.\\nWe map the continents to colors\\nand populations to areas of markers,\\nand since we'll use this plot repeatedly,\\nlet's make it into a function.\\n\\nWe immediately observe that the numbers\\nof babies per woman decreases\\nwith the probability of their survival.\\nThis makes sense in a sad, pragmatic way.\\nWe will fit models using\\nthe extensive Python package stats models\\nwhere we'll only sketch the surface\\nof what stat models can do,\\nand we will default to its OLS method,\\nwhich is found in the formula.api module.\\nOLS stands for Ordinary Least Squares.\\n\\nLeast squares means that models\\nare fit by minimizing the sum of square differences\\nbetween model predictions and observations.\\nOrdinary means that the model coefficients appear linearly\\nin the model formulas,\\nso they multiply the explanatory variables\\nor functions of the explanatory variables.\\nFor ordinary least squares,\\nthe best fit parameters can be found\\nusing the formulas of linear algebra.\\nYou can learn about that in any course\\nor textbooks of statistics.\\n\\nHowever, the nice thing about python\\nand stat models is that you don't even need\\nto remember them.\\nIn stats models, we specify models using formulas\\nin the teal denotation\\nintroduced by Chambers and Hasty in 1992\\nfor the language S.\\nThe formulas go like response variable, tilde, model terms.\\nAnd the simplest possible linear model\\nis just a numerical constant\\nwhich is denoted one in a tilde formula.\\n\\nOnce the model is defined,\\nwe fit it and see the result in parameters.\\nThere's only one.\\nThe conventional number\\nfor this concept parameter is intercept.\\nThe intercept is in fact just the mean of the data points.\\nApplying the model to our data, which we do with predict,\\nreturns the intercept for every case, kind of boring.\\nStill, we can plot this prediction against the original data\\nand evaluate the goodness of fit, clearly not the greatest,\\nsince we do not capture any variation.\\n\\nA slightly more complicated model\\ninvolves separate constants for every continent.\\nIn tilde notation, we include the categorical variable\\non the right hand side of the equation.\\nWe get one common constant, they intercept,\\nplus offsets for all the groups, minus one of them.\\nTo get all the groups, we write the constant as minus one.\\nThese parameters are in fact the same\\nas the means of the data grouped by region.\\n\\nPlotting this shows that each constant set\\nthe average for a continent.\\nHowever, Asia for one, has a huge variability here\\nthat begs to be explained.\\nMoving to more sophisticated models, we add child survival.\\nWe use its value directly,\\nso we call it a main term of the model.\\nHere, the fifth parameter, H5 surviving,\\ntells us that for every additional percent\\nof surviving children, the number of babies per woman,\\ndecreases by .15.\\n\\nThe constants are now larger,\\nand they have a different meaning.\\nThey give us the number of babies per woman\\nfor a hypothetical five-year survival rate of 0%.\\nAt least visually, our fit is improving.\\nIf we wish to have a different slope for every region,\\nwe can replace the main term with an interaction term\\nbetween H5 surviving and region.\\nThat's denoted by a colon until the notation.\\nWhen we do that, we see that the fact\\nof survivor rates is weaker in Africa\\nthan in the other continents.\\n\\nAnd here's the plot of the predictions,\\nwhere now the slope is different for every continent.\\nIndia and China, the two larger orange circles\\nseem to be outliers with respect to the fit.\\nPerhaps we can account for them\\nby including another main term for population.\\nThe best fit parameter here looks very small\\nbecause population is in millions,\\nbut basically one additional billion people\\nreduces the number of babies per woman by two.\\n\\nThe prediction is spot on for India,\\nbut a bit too low for China.\\nHere's a quick cheat sheet\\nfor setting up models in the tilde notation,\\nwhich is used in Python stats models,\\nbut also statistical languages such as RNS.\\nThe one thing we haven't demonstrated explicitly\\nis a so-called interaction\\nwhere two explanatory variables enter\\nthe model multiply together.\\nWe also haven't seen that we can include a function\\nof an explanatory variable.\\n\\nWell done. We've learned quite a bit.\\nIn the next video, we'll discuss\\nhow we can evaluate the goodness of fit of a model.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5994429\",\"duration\":308,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Model evaluation and selection\",\"fileName\":\"4571000_en_US_09_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":428,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch09-modeling > 09_03_evaluatingmodels.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"After fitting a model, it's important to understand if it does a good job of representing your data. This video demonstrates model evaluation using plotting and simple statistics such as mean square error and R squared. It also discusses strategies to select among competing models and refine partially successful models.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11783636,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] When we analyze data,\\nwe often have many different choices in setting up a model,\\nhow do we pick the best?\\nLet's resume things where we left them in the last video,\\nwith the Gapminder 1985 data\\nand our dataset plotting function,\\nwe also add another function\\nto plot the predictions of the models.\\nWe consider three different fits,\\none constant for every region,\\nconstants by region plus H5 surviving,\\nand the most complex fit we set up so far,\\nwhich includes constants by region, H5 survival,\\ngroup by region, and population.\\n\\nLet's see these three in action.\\nVisually, it looks like the most complex model does best,\\nbut we should dig into details.\\nThe first thing to do is to compare the residuals.\\nThis is how we compute them manually,\\nbut we can also get them from the fit object.\\nHistogramming all these residuals together\\nshows that the distribution of errors\\nfor the group constant fit is a bit broader,\\nbut not by much,\\nand that the other two models are very similar.\\n\\nTo distill the comparison into a single number,\\nwe can look at the mean squared error of the residuals,\\nwhich is conveniently available in the fit object.\\nThis is in a sense the typical error,\\nand it includes a very simple correction\\nfor the complexity of the model.\\nThese numbers though, are similar\\nand even the best model has a typical error\\nof one child per woman, not great.\\nBeyond the mean square there,\\nthere are various more sophisticated ways the statisticians\\nhave devised to evaluate goodness of fit.\\n\\nExplaining the mathematics is outside the scope\\nof this course, but we can still try them out.\\nOne is the R square statistic,\\nwhich qualitatively describes the fraction of data variation\\nthat is explained by the fit.\\nFor a perfect model, R squared would reach one.\\nAgain, the R squared for three models are similar.\\nWith the most complex model perhaps winning.\\nThe R square statistic,\\nhowever, does not take into account the number\\nof fit parameters, so it'll not penalize over fitting.\\n\\nA model that is very good on the data\\nused to train it but not predictive.\\nThe F statistic includes such a correction.\\nThe F statistic measures\\nhow much each fit parameter contributes to R squared\\ncompared to an explanatory variable that has nothing to do\\nwith the dataset, in fact, which takes random values.\\nIf F is zero, the model explains nothing\\nfor a fit of one, the model explains\\nabout half of the variation\\nand the larger DF,\\nthe greater the explanatory power of the model.\\n\\nWith this statistic,\\nwe see that the intermediate model, surviving fit,\\nseems to hit the sweet spot.\\nStep models can compute many measures of goodness of fit.\\nAsking for a summary of a fit,\\nwe collect them in a nice tabular format.\\nThe table includes also the best fit parameters\\nand there estimated errors.\\nThe table is where you'd go after\\nyou've taken a nice course in statistics,\\nbut before you go off and study a lot of math,\\nlet me show you a pragmatic approach to goodness of fit\\nthat is typical of machine learning.\\n\\nThe idea is to evaluate out of sample goodness of fit.\\nThat is, we exclude some of the data from the fit\\nto test the true predictive power of the model.\\nWe start by shuffling our data.\\nThe Pandas method sample is really meant to draw\\na number of random rows,\\nbut if we set the number to the length of the dataset,\\nthe effect will be the same as shuffling.\\nThen we take 90% of the data for training\\nand the rest for testing.\\n\\nLet's train our three models on the training data.\\nThe best fit parameters are a bit different\\nthan in the original fit,\\nand so are the mean square errors of the residuals.\\nWe need to compute them by hand for the test data.\\nHere we see that the testing means square error\\nis actually lowest for surviving fit.\\nThe same result we found with the F statistic.\\n\\nTo get accurate results, it's best\\nto repeat from many different random partitions\\nof the dataset and take the average.\\nThis is a very pragmatic, straightforward approach.\\nI recommend it.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6009034\",\"duration\":308,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Testing hypotheses with Monte Carlo\",\"fileName\":\"4571000_en_US_09_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":438,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch09-modeling > 09_04_montecarlo.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"The possibility of quickly performing a large number of simulations on a computer has revolutionized statistics, replacing complicated mathematics with simple, pragmatic approaches. This video demonstrates the idea of testing a hypothesis about data by computing statistics on a population of simulated data replications.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10884172,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this chapter,\\nwe have seen how modeling can help us understand data\\nby giving us estimates of model parameters.\\nThis understanding is necessarily imperfect, because models\\nare never faithful representations of reality.\\nNevertheless, they can be very useful.\\nBut we can also ask a different type of question,\\na question about the real property\\nof an underlying population\\nto which we have access only through a sample,\\nfor instance, the number of voters in a district\\nthat support a certain politician.\\n\\nThis number really exists, and we could compute it\\nif we could ask any single voter,\\nand if they replied, of course.\\nBut in practice, we will have access\\nonly to a limited sample, say 1,000 randomly chosen voters.\\nSo our estimate is going to be uncertain, but how uncertain?\\nStatistics offers sophisticated mathematical methods\\nto tackle these kinds of questions.\\nSince, however, this is not a course about statistics,\\nwhat I want to show you is that we can get answers\\nwith a little mathematics\\nby resorting to simulations based on random numbers.\\n\\nPhysicists call this Monte-Carlo,\\nbecause it's very much like gambling.\\nSo imagine a very important election\\nis taking place in your city,\\npitting incumbent mayor, Mr. Brown,\\nagainst a local celebrity chef, Mrs. Green.\\nYou work for the local newspaper,\\nand you're asked to poll your co citizens for their vote.\\nTo make things easy for you,\\nwe assume you can reach every voter by phone\\nand that every poll voter replies truthfully.\\nLaboriously, you call 1,000 voters\\nand ask for their voting intention.\\n\\nHere's a file of their answers.\\nAnd here are the measured proportions of vote\\nfor the two candidates.\\nIt seems that Brown will remain mayor.\\nHowever, the proportion is going to depend\\non the specific sample that we drew, how much.\\nLet's simulate 1,000 voter sample\\nwhen the true proportion is 51 to 49.\\nNumPy has a very handy function for this, random choice.\\n\\nWe tell you the votes, and we get the result\\nthat's a little different as you expect.\\nIf we do it again, we'll get again a different number.\\nSo let's repeat the simulation many times, 2,000,\\nand collect Brown's share.\\nPlotting a histogram of this sampling distribution,\\nwe see that the two share of 51% may result in infractions\\nbetween roughly 48 and 54 in a sample of 1,000.\\n\\nThis means also that if you observe a majority\\nof 51% for Brown in the sample,\\nthe winner may very well be Green.\\nComputing summary statistics\\nshows that the error is about 1.5%,\\nand the interquartile range is 50 to 52.\\nIndeed, that means that a true Brown majority of 51%\\ncould lead to a poll where Green is a winner\\nabout 25% of the times.\\nJournalists must be careful.\\n\\nThe fun thing is that once we simulate,\\nit's easy to verify our intuition.\\nFor instance, we could check\\nthat the slightly stronger true majority for Brown, 55,\\nwould almost never result in a poll that is uncertain.\\nAnd we can show that the larger poll of 10,000 voters\\nwould be more precise than a poll of 1,000.\\nStatisticians make precise statements\\nabout the uncertainty of polling\\nby giving confidence intervals.\\n\\nThey will say, \\\"Given the sampling, 95% of the times,\\n\\\"the true value would fall within this range.\\\"\\nHow do we get that?\\nA 95% confidence interval\\nmeans that we can be wrong 5% of the time.\\nWe split that equally between too high and too low.\\nSo we look for the true vote fraction\\nfor which the lowest 2.5% of the sampling distribution\\nlies below .51.\\n\\n55 is a bit too high.\\n54 is right.\\nAnd we look for the true vote fraction\\nfor which the highest 2.5% of the sampling distribution\\nlies above .51.\\nThat's not 47, that's about 48.\\nSo the 95% confidence interval\\nfor a finding of 51% in a 1,000 voter poll\\nis between 48 and 54%.\\n\\nThis is a simple analysis, but it gives you a good sense\\nof how professional pollsters\\nactually estimate their errors,\\nexcept they have to deal with the complications of reality,\\nsuch as the fact that any polling may over represent\\nsome segments of the population and under represent others.\\nThis would be a selection bias, as well as the fact\\nthat voters may not respond thoughtfully.\\nThis would be a response bias.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6009033\",\"duration\":362,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"A taste of machine learning\",\"fileName\":\"4571000_en_US_09_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":471,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch09-modeling > 09_05_machinelearning.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Machine learning goes beyond data analysis by teaching machines how to learn from data, usually in large quantities. This video demonstrates a simple classification analysis using the Python package scikit-learn.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12187012,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] If you're taking this course,\\nthere's a good chance you're thinking about moving on\\nto big data and machine learning.\\nThat's a technology of huge importance to the world today.\\nIt's also a huge subject,\\ncovered by many excellent courses in the LinkedIn library,\\nso all I want to do here is to give you a small taste\\nof machine learning\\nusing the tools that we have discussed\\nand using the very comprehensive scikit-learn library.\\nThe name machine learning is in fact very descriptive.\\n\\nIt's the science of getting computers to learn from data.\\nMachine learning covers many different tasks and use cases.\\nPerhaps the most straightforward application\\nof supervised learning is classification.\\nWe teach an algorithm to analyze an input\\nand assign it to one of a few predefined classes\\nby providing a number of examples,\\ninputs that are already labeled.\\nThe algorithm will then try to guess the correct label\\nfor a set of unlabeled examples.\\n\\nThat would be inference.\\nOnce more, we will use our Gapminder data\\nand focus on 2023.\\nWe'll try to answer a simple enough question.\\nCan we tell the continent of a country\\njust by looking at the macro variables of population,\\nlife expectancy, child survival,\\nbabies per woman, and GDP per capita?\\nWe collect these explanatory variables\\ninto a Pandas DataFrame,\\nand we make a series from the response variable,\\nwhich will be our class label, the continent.\\n\\nWe want the series to be categorical.\\nA categorical variable can be turned into numerical codes\\nas needed by the scikit-learn algorithm.\\nThese are the codes.\\nWe have already seen the standard modus operandi\\nof machine learning.\\nDivide your dataset into training data and testing data,\\nuse the training data to train a model or algorithm,\\nand then use the testing data to verify performance.\\nscikit-learn conveniently provides a function\\nto the divided dataset into training and testing.\\n\\nWe will use a classifier known as decision tree.\\nThis is an algorithm\\nthat splits data into branches,\\nbased on the explanatory variables,\\ncreating a tree-like model of decisions.\\nHere's an example where we need to distinguish\\nbetween breeds of dogs.\\nAt each node, we are making a decision based on a variable,\\nand we follow different branches based on that,\\nuntil we find a leaf that matches the input to a class.\\n\\nThe point of machine learning\\nis that we don't know\\nwhat decisions we need to make, which branches.\\nWe have to learn them from examples.\\nThe machine learning algorithm\\nadjusts the branching point of the decision tree\\nuntil the labels predicted for the examples\\nmatch the actual labels,\\nor at least they match them as well as possible.\\nSo we create our decision tree classifier object\\nand we train it.\\nIt only takes a moment.\\n\\nWe use the trained tree to assign labels\\nto our test examples.\\nThese are integer numbers,\\nbut we can match them back to categories.\\nLet's see how we did.\\nSeveral predictions are indeed correct,\\nbut some are just wrong.\\nIn fact, scikit-learn tells us\\nthat 69% of the predictions are correct.\\nWe could have computed this ourselves.\\nscikit-learn can give us a much more detailed report,\\nwhich answers questions that may be important\\nin practical use cases.\\n\\nA look at Africa, specifically, to explain what these are.\\nThe precision number says\\nthat out of all the times the classifier chose Africa,\\n83% were correct.\\nThe recall number says that\\nout of all the examples from Africa,\\n87% were correctly identified.\\nThe f1-score is an average of these two,\\nand the support\\nis a number of test dataset examples from Africa.\\nFinally, the averages at the bottom\\nare taken across classes, without weights,\\nand by waiting for the number of examples in each class.\\n\\nWe can also plot the confusion matrix,\\nwhich shows which two levels, the rows,\\nended up in which estimated class.\\nWe want the diagonal here to light up, clearly.\\nWe see that the classifier did okay with Africa and Europe,\\nbut it found it hard to distinguish\\nbetween America and Asia.\\nAlso, some countries in Asia and Oceania\\nended up classified into Africa.\\nCan we do better?\\nThere are many different classification algorithms\\nof various degrees of sophistication.\\n\\nOne step up from a tree is a random forest,\\nwhich combines multiple decision trees\\nto improve classification accuracy\\nand to control overfitting.\\nThe algorithm builds multiple trees during training\\nand using voting, or averages,\\nto combine their outputs during inference.\\nIn scikit-learn, we only have to change, then,\\nthe definition of our model,\\nbut we train it in exactly the same way.\\nHere's the prediction.\\n\\nThe accuracy went up a few percent,\\nbut the confusion matrix is similar.\\nThis is probably limitation of the data themselves.\\nPeople can thrive or fail to,\\nin similar ways, in different places on earth.\\nI was glad to serve you this machine learning appetizer.\\nIt's a fascinating discipline,\\nand I know that if you're interested,\\nyou will find many excellent resources on LinkedIn Learning\\nin the literature and all websites.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:675a34e9498ee7e9b17b3e99\",\"duration\":600,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code challenge: Gapminder model fit\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1170177\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:6006163\",\"duration\":67,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Gapminder model fit\",\"fileName\":\"4571000_en_US_09_07_C_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":107,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2447625,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, so we need to find a model\\nfor the babies_per_woman variable\\nthat has a sufficiently high explanatory power\\nas measured by the F statistic.\\nLet's set up a formula.\\nWe'll use the region and age5_surviving.\\nI define a model, fit it,\\nand return the resulting object.\\n\\nTest.\\nOkay, this is almost right.\\nThe F value is not quite there,\\nso this may take some experimentation.\\nWe could add a second explanatory variable,\\nbut the result is actually worse.\\nIn fact, we find that using a different function\\nof the age5_surviving variable is just what we need.\\nHow about the second power, the square?\\nPerfect.\\n\\n\"}],\"name\":\"9. Introduction to Data Modeling\",\"size\":53353546,\"urn\":\"urn:li:learningContentChapter:5994437\"},{\"duration\":1036,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:6009032\",\"duration\":132,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview: COVID-19 data\",\"fileName\":\"4571000_en_US_10_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":150,\"solutionVideo\":false,\"editingNotes\":\"overlay at 00:14, https://ourworldindata.org/coronavirus, and at 01:19, https://plotly.com/python/plotly-express/\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch10-covid19 > 10_01_overview.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5394692,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this chapter, we are going to practice\\nwhat we learned so far in terms of wrangling,\\nsummarizing, visualizing, and modeling data\\nusing a very rich dataset\\nfrom the Our World in Data Project:\\nthe COVID-19 dataset.\\nCOVID-19 was one of the most significant global events\\nof recent history,\\naffecting almost every person on the planet.\\nThe dataset offers the opportunity\\nto study the pandemic's progress\\nand its societal impact in real time.\\n\\nThis dataset gathers metrics\\nfrom authoritative organizations like the WHO, CDC,\\nand National Health Ministries,\\nand it includes data from nearly every country in the world.\\nTo reduce the size of the file,\\nI kept only some of the variables of the original dataset.\\nLooking at the dataframe\\nshows several familiar macroeconomic indicators\\nas well as a few COVID-specific measurements.\\nIn the next three videos, we'll work to summarize,\\nvisualize, and model this dataset,\\nand there will be wrangling throughout.\\n\\nFor visualizations, we'll use a modern plotting package,\\nPlotly, which can make interactive JavaScript-based plots\\nthat can be exported very easily to webpages.\\nTo give you a taste of what Plotly can do,\\nlet's look at the impact of the first year\\nof the pandemic in Europe.\\nI down select the dataset by choosing countries in Europe\\nand the date of December 31st, 2020.\\nHere it is.\\n\\nA choropleth plot is a map\\nwhere the colors of geographic features,\\nhere countries, are mapped to a variable.\\nThis is very impressive with so little code.\\nThe interactivity allows us to zoom and pan\\nand to get dataset specific information as we hover.\\nIn the spectrum of visualization packages,\\nPlotly is less about exploring data\\nthan about presenting them to others\\nin a clear and coherent fashion.\\n\\nLet's get started with our dataset.\\n\"},{\"urn\":\"urn:li:learningContentVideo:6003340\",\"duration\":260,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Summarizing COVID-19 data\",\"fileName\":\"4571000_en_US_10_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":347,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch10-covid19 > 10_02_summarizing.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9955129,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's start our analysis\\nof the COVID-19 dataset.\\nWe will look at cumulative quantities such as total cases\\nand total deaths at the end of the pandemic.\\nGrouping by country, and asking for the last value\\ngets us variables in 2024 indexed by country.\\nMaybe the most obvious question we can ask of the data\\nis which country had the most cases?\\nPANDAS offers a convenient built-in method and largest\\nto see the largest entries in a series.\\n\\nHere it is, the United States on the top.\\nAs printed, this series looks a bit vanilla\\nand it's not so easy to parse.\\nWe get a cleaner look by converting it to a dataframe.\\nEven better, we can apply some styling to the numbers\\nusing style format.\\nI like this very readable look.\\nI will make generic functions\\nto apply to series and dataframes,\\nand I will stick those functions into PANDAS,\\nadding methods to the series and data frame classes.\\n\\nThese methods are written as anonymous Lambda functions.\\nThis trick is known as monkey patching,\\nwhich is not entirely nice,\\nbut sometimes it's just too easy not to do.\\nHere it is at work.\\nSo it's clear that the US had the most cases.\\nWhat about the most deaths though?\\nLet's look at those and compare.\\nThe method enlarges generalizes nicely to a dataframe\\nif we pick a column to sort by,\\nagain the US,\\nyou may be thinking that the US had such a large number\\nof cases and deaths just because it has a larger population\\nthan other countries.\\n\\nThat's a good point.\\nWe should then look at cases and deaths\\nper million people of population.\\nWe add columns for those new variables to our data frame\\nand make the table again.\\nAh, this changes the picture significantly.\\nWith a few smaller countries looking definitely worse.\\nDividing deaths by cases gives us the death rate.\\nLet's create that column on the fly\\nusing the method assign on the dataframe,\\nthe death rate needs to be shown with at least one decimal,\\nso I changed the style.\\n\\nGiven to the nice method.\\nPeru has a very high death rate compared to other countries.\\nPerhaps Peru had special challenges in healthcare access.\\nThe other reason may be that the cases were under-reported,\\nbut the deaths were not.\\nIn fact, for some countries,\\nboth the number of cases and deaths due to Covid\\nmay be suspect, cases may go unreported\\ndepending on the testing on healthcare logistics.\\n\\nWhile the criteria for Covid-attributed deaths\\nmay be different in different countries.\\nTo sidestep these limitations and get a sense\\nof the true impact of the pandemic across the world,\\nthe data set includes numbers for excess deaths,\\nwhich are calculated by comparing cumulative deaths\\nfrom all causes during the pandemic,\\nwith a projected baseline of expected deaths\\nbased on previous years.\\nThese statistic captures not only deaths directly attributed\\nto COVID-19, but also those that may have occurred\\ndue to healthcare disruptions, changes in behavior\\nand other pandemic-related factors.\\n\\nWe look at the normalized total excess deaths,\\ncomputed per million people.\\nNote that this number, however, is not available\\nfor all the countries.\\nSo the top 10 again,\\nand this confirms Peru as a country\\nthat was heavily and in fact it's the only country\\nwhere the excess deaths are less than their reported\\nCovid deaths.\\nThe statistic highlights a few other countries\\nthat had severe damage, including Russia,\\nBolivia, and other Central American countries.\\n\\nThere are many more summaries and explorations you can do.\\nDo try a few.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5994427\",\"duration\":359,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualizing COVID-19 data\",\"fileName\":\"4571000_en_US_10_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":494,\"solutionVideo\":false,\"editingNotes\":\"pickup at 7:15\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":true,\"exerciseFileDisplayText\":\"ch10-covid19 > 10_03_visualizing.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14033901,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Ready to visualize?\\nWe load our dataset\\nand compute the normalized cumulative quantities.\\nWe left our exploration with the countries\\nthat showed the most estimated excess deaths.\\nWhat do these countries have in common?\\nIt's a question for modeling,\\nbut one that we can begin to explore by plotting.\\nSo is population predictive?\\nIs a continent especially bad?\\nWe plot those variables against excess deaths,\\nencoding their values with horizontal position, X,\\nand color, respectively.\\n\\nWe don't seem to see much correlation here.\\nTo improve this plot,\\nwe exclude points without a defined excess death rate.\\nWe drop now so that the axes are scaled right,\\nand we ask Plotly to use country names\\nwhen we hover around the point.\\nThen it will be clear which point is which country.\\nWhat about income?\\nThat's not very predictive, either,\\nand surely vaccinations may be indicative.\\n\\nThey're not.\\nMost of the deaths occurred\\nbefore vaccinations were available.\\nIt seems we're not getting anywhere.\\nLet's take a different tack\\nand plot total cases as a functional date\\ngrouped and colored by country.\\nThis plot certainly has lots of color,\\nbut it's hard to see much here,\\nother than the large number of cases\\nin the US and in China from 2023.\\n\\nSo let's focus on the first two years\\nby setting the range of the plot.\\nPlotly is smart enough to accept dates given as strings.\\nBetter, but I would say that the trends are obscured\\nby the different times\\nat which the pandemic began in each state.\\nLet's fix that.\\nWe find a date at which each country\\nreached at least five total cases.\\nAh, this is a bit tricky to do in Pandas.\\nIt didn't work to do the easy thing,\\ngroup the data by country and then apply our condition.\\n\\nA group series doesn't support that.\\nThis is the kind of problem in Pandas\\nwhere you need to search the internet extensively\\nto find a solution.\\nSometimes it's easier to just go around it,\\nso you can then filter first, then group by country,\\nand take the first role.\\nThis works.\\nFrom this dataset, we need only date and country,\\nand we use the country's index.\\nWe will call this the T0 of the pandemic for each country.\\n\\nNext, for each role,\\nwe compute the time elapsed from the appropriate T0.\\nSince the COVID-19 data frame and the T0 series\\nare both indexed by country,\\nPandas will now match each row to the correct T0.\\nVery good.\\nWe'd better convert this time deltas to integers, though,\\notherwise we may have trouble plotting.\\nWe use a method from the Pandas daytime accessor DT\\nto extract the day from the time delta,\\nand we assign the resulting value to new column.\\n\\nOkay, we are ready to plot using this new time variable.\\nAh, pandemics develop exponentially, at least at the start,\\nso it's appropriate to plot these numbers logarithmically\\nby setting log_y equals True.\\nWe're getting there,\\nand interactivity helps us make sense of this plot,\\nbut I still think there are too many traces\\nand too much is happening here.\\nEffective visualization often requires\\na careful selection of what we wish to show.\\n\\nWe will concentrate on larger countries,\\nmore than 50 million people,\\nand over the first six months of the epidemic.\\nHere's the plot.\\nThis is almost publication ready.\\nWe can compare different countries by looking for the traces\\nand hovering over them.\\nWe will recolor by continent.\\nWe will add the country name\\nto the tool tip that you get when you hover.\\nWe will change the color scheme\\nand the labels along the axis.\\n\\nWe can see here from the color\\nthat the number of cases grew very rapidly in Europe\\nand perhaps fastest in the US,\\nwhereas it was more slow in Africa.\\nAgain, this would be correct\\nfor countries in which testing was not too incomplete.\\nFrance here is strange.\\nMaybe cases were reported differently.\\nWe can provide more context by showing straight lines\\ncorresponding to the number of cases,\\ndoubling every two, five, and 10 days.\\nWe compute the data points for those lines manually\\nusing an exponential formula.\\n\\nFrom day zero to day 150,\\nthe initial number of cases, five, is multiplied by two\\nwith an exponent given by the number of doublings,\\nso that's 150 divided by the doubling times.\\nWe add those lines to the plot using add_scatter,\\nand we also annotate those lines using update_layout\\nwith the annotations keyword.\\nI will admit that I've picked\\nthe coordinates of these annotations manually.\\n\\nThis is beautiful.\\nWe write it out to an HTML file.\\nThen we can download the file,\\nwhich is pure HTML, and look at it in our browser.\\nThis is ready for inclusion on a website\\nor within a dashboard or anywhere you need it,\\nand interactivity is preserved.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2732052\",\"duration\":285,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Modeling COVID-19 data\",\"fileName\":\"4571000_en_US_10_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":374,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"ch10-covid19 > 10_04_modeling.ipynb\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11678016,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this video, we will try\\nto explain the number of total deaths from Covid\\nusing an ordinary linear regression on a variety\\nof explanatory variables.\\nHere's our dataset again.\\nAs we already discussed in this chapter,\\nit would be more representative to look at the number\\nof excess deaths, since differences in the reporting\\nof total deaths may distort the picture.\\nHowever, the total excess value is only available\\nfor a smaller number of countries, too few\\nto fit the model accurately.\\n\\nWe will augment the dataset\\nby adding the normalized number of deaths,\\nand look at all values at the end of the dataset in 2024.\\nThe final recorded incidence was about 1,000 per million.\\nWith a standard deviation of approximately another 1,000.\\nThis is the variation that we are looking to explain.\\nWe can also look at it in a histogram,\\nwhere we'll try a number of different explanatory variables.\\nMost of these are self-explanatory, in fact,\\nbut let me tell you that extreme poverty is the percentage\\nof humans with income or consumption per person\\nbelow about $2 in 2017 dollars,\\nand the human development index is a summary measure\\nof lifespan, health, education, and standard of living.\\n\\nTrying a couple of models randomly\\ngives us very high mean square residuals,\\nand low F value statistics.\\nLet me remind you that models here\\nare set using the tilde denotation,\\nwhere the outcome variable goes on the left,\\nand the explanatory variables that we are considering\\non the right.\\nAfter we fit, we can look at square residual,\\nMSC (indistinct), we can look at the R squared\\nand F statistics, and we use the F statistic\\nto find the best model.\\n\\nNot the model with the least residuals,\\nbut the model with the most explanatory power.\\nWe can't just start from one explanatory variables\\nand add all the others in sequence.\\nThis surely will keep producing the residuals,\\nbut it will weaken the explanatory power of the fit.\\nInstead, we will systematically try every combination\\nof the explanatory variables.\\nWe can do this with either tools combinations,\\nwhich returns all the combination, the subsets\\nof a given length.\\n\\nSo here's a function to fit the model given\\nas a list of variables.\\nWe build the tilde formula by joining strings with pluses.\\nThis works fine.\\nNext, we iterate over the number of variables\\nand all the combination of that length\\nusing a double (indistinct) comprehension.\\nIsn't it lovely?\\nFrom each fit, we collect the F statistic.\\nTo find the best model in the dictionary,\\nwe use max over the dictionary keys,\\nproviding a custom key function, get,\\nthat will access the value.\\n\\nMedian age.\\nThat's very interesting.\\nOlder countries had more deaths.\\nIn fact, from the summary of the fit,\\nwe see that one additional year in the median aged\\nincreased deaths per million, by almost 100.\\nMakes sense.\\nWe may ask if the explanation have changed\\nduring different phases of the pandemic.\\nTo find out, we'll divide up the dataset by the year.\\nThe date column can provide the year\\nif we use the method dot year in the data frame accessor DT.\\n\\nAnd here's a function to select values\\nfor each country at the end of a given year.\\nFinally, we iterate the fitting\\nover all the combinations of variables.\\nIt turns out that median age is a very predictive variable\\nthat works best alone,\\nat least within the limits of this dataset.\\nOnly the first year of the pandemic, we see\\nthat the human development index is a better predictor.\\nInteresting.\\nThe fit coefficient there is positive.\\nIn that first year, it was the countries\\nwith the highest development index that had the most deaths.\\n\\nCould it be just because of a bias in the reporting,\\nwhich was more accurate in more developed countries?\\nPerhaps that's the case.\\nIf we look instead at excess deaths,\\nalthough the data set is smaller,\\nthe sign of the regression is reversed.\\nMore developed countries had fewer deaths.\\nThis completes our exploration of the COVID-19 dataset,\\nbut you can certainly continue on your own.\\nThere's a lot here that can be learned and understood.\\n\"}],\"name\":\"10. Project: COVID-19 Data\",\"size\":41061738,\"urn\":\"urn:li:learningContentChapter:5994438\"},{\"duration\":101,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:6009031\",\"duration\":101,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Extending your Python data analysis skills\",\"fileName\":\"4571000_en_US_11_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":130,\"solutionVideo\":false,\"editingNotes\":\"Audio, B-roll, overlay at 1:05: \\\"Think Python: How to Think Like a Computer Scientist 3rd Edition\\nby Allen Downey\\\" and \\\"Python Data Science Handbook: Essential Tools for Working with Data 2nd Edition by Jake VanderPlas\\\"\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"There are many resources on the web that can help you master data analysis with Python and explore more advanced topics. This video suggests a few.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2560455,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We've reached the end of this course.\\nCongratulations on following along\\nthrough some rather challenging material.\\nMy goal in this course was to show you how standard Python,\\nNumPy, and pandas can be used as robust and flexible tools\\nfor many data analysis applications.\\nAnd now we can use them together to acquire, summarize,\\nvisualize, and model real-world data sets.\\nYou can continue on your road to mastery\\nwith the many excellent courses\\nin the LinkedIn Learning library.\\n\\nThe LinkedIn Learning AI coach will recommend the course\\nthat's most useful for you.\\nIf you like the style of this course,\\nyou may enjoy my other course\\non Python: Programming Efficiently.\\nAnd if you're an engineer or scientist\\nor a student of those disciplines,\\nyou could check out my course on Python\\nfor engineers and scientists.\\nIf you like to learn on books,\\nI recommend \\\"Think Python\\\" by Allen Downey.\\nnow in its third edition,\\nand the \\\"Python Data Science Handbook\\\" by Jake VanderPlas.\\n\\nIf you get stuck,\\nyou can get help in internet forums.\\nYou'll find that Python has a very helpful\\nand supportive community.\\nOr ask your favorite large language model,\\nbut be sure to check the answer with a critical eye.\\nAnd finally, be Pythonic.\\nPython is an expressive and elegant language,\\nand it's worth taking the time to find the most direct\\nand inspired solutions to your programming problems.\\nMany have said that what's best about Python\\nis that it's a language that makes you happy.\\n\\nI agree.\\nSo go out and have fun.\\n\"}],\"name\":\"Conclusion\",\"size\":2560455,\"urn\":\"urn:li:learningContentChapter:6009041\"}],\"size\":485325757,\"duration\":13399,\"zeroBased\":false}]}"