"{\"title\":\"Deep Learning with Python Professional Certificate by Anaconda\",\"courses\":[{\"course_title\":\"Hands-On PyTorch Machine Learning\",\"course_admin_id\":2884016,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":2884016,\"Project ID\":null,\"Course Name\":\"Hands-On PyTorch Machine Learning\",\"Course Name EN\":\"Hands-On PyTorch Machine Learning\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"Many of the world\u00e2\u20ac\u2122s most exciting and innovative new tech projects leverage the power of machine learning. But if you want to set yourself apart as a data scientist or machine learning engineer, you need to stay up to date with the current tools and best practices for creating effective, predictable models.&lt;br&gt;&lt;br&gt;In this course, instructor Helen Sun shows you how to get up and running with PyTorch, the open-source machine learning framework known for its simplicity, performance, and APIs. Explore the basic concepts of PyTorch, including tensors, operators, and conversion to and from NumPy, as well as how to utilize autograd, which tracks the history of every computation recorded by the framework. By the end of this course, you\u00e2\u20ac\u2122ll also be equipped with a new set of skills to get the most out of Torchvision, Torchaudio, and Torchtext.\",\"Course Short Description\":\"Discover the fundamentals of creating machine learning models with PyTorch, the open-source machine learning framework.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":1992748775,\"Instructor Name\":\"Helen Sun\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Technology Strategist and Thought Leader\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2022-12-13T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/hands-on-pytorch-machine-learning\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Artificial Intelligence for Technology\",\"Primary Software\":\"PyTorch\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":3412.0,\"Visible Video Count\":17.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":51,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2615032\",\"duration\":51,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Explore the capabilities of PyTorch\",\"fileName\":\"2884016_en_US_00_01_WX30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1591138,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Dr. Sun] PyTorch is one  \\n of the most widely adapted ML frameworks.  \\n If you know TansorFlow, learning PyTorch is essential  \\n to establish a foundation in AI and ML.  \\n And if you are just starting,  \\n PyTorch is an excellent place to start learning.  \\n Working the field of AI and ML,  \\n understanding the basics of PyTorch is imperative.  \\n In this course, I'll teach you the basics  \\n of computer vision, natural language processing,  \\n and audio processing through some hands-on experiences.  \\n Hello, I'm Dr. Helen Sun  \\n and I'm a senior engineering leader at Meta,  \\n working in the AI and ML space.  \\n Join me in this course to learn about the PyTorch platform  \\n that is used worldwide  \\n to support the work of AI research scientists  \\n and ML engineers.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":1591138,\"urn\":\"urn:li:learningContentChapter:2621017\"},{\"duration\":879,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2622022\",\"duration\":253,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"PyTorch overview\",\"fileName\":\"2884016_en_US_01_01_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"In this video, learn about PyTorch\u2019s tensor library and neural networks at a high level. This is a context-setting video and an overview of key concepts for the remainder of the videos.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7767736,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] PyTorch is one of the most popular  \\n deep learning frameworks,  \\n and it continues to gain adoption  \\n in both research communities and enterprises.  \\n It is an open source machine learning framework  \\n developed and maintained by Meta.  \\n The key benefits of PyTorch include  \\n quick iteration for research,  \\n seamless eager graph mode transition with TorchScript,  \\n and production ready with TorchServe.  \\n It has a backend called Torch.Distributed,  \\n which enables scalable distributed training  \\n and optimization.  \\n PyTorch also has a rich ecosystem of tools and libraries  \\n that extends development in computer vision,  \\n natural language processing, and more.  \\n Some featured projects in this ecosystem include Captum  \\n for model interpretability,  \\n geometric deep learning for irregular input data,  \\n such as graphs and Point Cloud,  \\n and Skorch that enables psychic learn compatibility.  \\n In this course, I'll be using PyTorch 1.10,  \\n and cover some of its top features,  \\n including tensors, autograd, APIs, and libraries.  \\n A PyTorch tensor is a multi-dimensional container of data.  \\n It is similar to a NumPy array.  \\n Torch defines 10 tensor types with CPU and GPU variants,  \\n including boolean, 8, 16, 32, 64 bit integer,  \\n 16, 32 and 64 bit floating point,  \\n 32, 64, and 128 bit complex.  \\n There are many ways to create a tensor,  \\n such as with pre-existing data.  \\n There are also many tensor operations in PyTorch,  \\n but they can be grouped into two categories,  \\n including slice and math.  \\n Autograd is what earns PyTorch it's popularity  \\n for fast and flexible iteration.  \\n PyTorch traces the computation dynamically at runtime  \\n to get correct gradients to drive learning.  \\n The key for model training is to minimize the loss  \\n through adjusting the model's learning weights.  \\n The gradients over the learning weights tells us  \\n what direction to change each weight  \\n to reduce the loss function.  \\n With deep learning, the number of derivatives  \\n goes up exponentially.  \\n Autograd tracks the history of every computation,  \\n and as a result, greatly speeds  \\n the local derivative computation.  \\n The primary API to PyTorch is Python.  \\n These APIs allow you to interact with PyTorch  \\n through tensors, views, CUDA, autograd,  \\n quantization, and storage.  \\n PyTorch also provides you with a C++ interface  \\n with access to PyTorch functionalities,  \\n including tensor and autograd, serializing PyTorch models,  \\n and building C++ extensions through TorchScript.  \\n The C++ APIs can also be used  \\n to write compact, performance-sensitive code  \\n with deep learning capabilities  \\n to perform ML inference on mobile platforms.  \\n There are five main PyTorch libraries.  \\n The first is Torchaudio for audio signal processing.  \\n Next, we have Torchtext containing data processing utilities  \\n and data sets for natural language processing.  \\n The third library, Torchvision  \\n with computer vision data sets, model architecture  \\n and common image transformation.  \\n Fourth is Torchserve, a highly performant and flexible  \\n serving tool for PyTorch eager modes and models.  \\n And finally, Torch_xla that runs PyTorch  \\n on xla devices such as TPU.  \\n Now that you have a high level overview,  \\n let's dig deep into the top features of PyTorch.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2617029\",\"duration\":230,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"PyTorch environment setup\",\"fileName\":\"2884016_en_US_01_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to install PyTorch and configure the environment. Get the environment set up so that you can start to work on the use cases.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9006947,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] To set up the PyTorch environment,  \\n we first download the PyTorch library  \\n from the PyTorch website.  \\n Click on the Install button here.  \\n And use the default tab here, Start Locally.  \\n Scroll down to go to the selection grid.  \\n You first select the build.  \\n You'll then choose the OS.  \\n The options are Mac, Windows, and Linux.  \\n And we'll choose Mac.  \\n For the package manager, we'll choose Pip.  \\n You can also select Conda, LibTorch,  \\n or build it using the source code from GitHub.  \\n There are two language options,  \\n Python and C++/Java.  \\n We'll choose Python.  \\n The compute platform is default here  \\n because we're using the Mac OS.  \\n Once you make all of the selections,  \\n the site will give you a command to install PyTorch:  \\n pip3 install torch torchvision torchaudio.  \\n And you just run this command to install PyTorch  \\n if you prefer to use the most recent stable version.  \\n The previous versions of PyTorch  \\n are available here on this website.  \\n Just click on this Previous PyTorch Versions tab.  \\n I'll be using the 1.10.1 for this course.  \\n And you copy and paste this command and run it in iTerm.  \\n So here is the command,  \\n I'll run it here,  \\n but I also added an optional parameter  \\n to suppress warnings here.  \\n So it says, \\\"no warning script location.\\\"  \\n If you receive a message saying \\\"Command Not Found,\\\"  \\n this is probably because you don't have  \\n Pip3 environment set up properly.  \\n You need to use a package manager on Mac OS called Homebrew  \\n to install Pip3.  \\n So here's the command.  \\n Now your first command to run PyTorch installation  \\n can be resumed right here.  \\n With this installation,  \\n it includes numpy, pillow, torch,  \\n torchaudio, torchvision, and typing-extensions.  \\n We'll use these packages in our later modules.  \\n Once you see your screen saying,  \\n \\\"Successfully installed those packages\\\"  \\n or \\\"These requirements already satisfied,\\\"  \\n then you're ready to go.  \\n A few other module installations  \\n that are useful for this course include:  \\n Matplotlib and Jupyter Notebook.  \\n To install Matplotlib,  \\n I'll use the command  \\n pip3  \\n install  \\n matplotlib.  \\n This command will install Matplotlib  \\n along with a number of packages, including:  \\n Pyparsing,  \\n python-dateutil,  \\n Packaging,  \\n Kiwisolver,  \\n Fonttools,  \\n and cycler.  \\n Once it is all and completed,  \\n you'll receive the following message here.  \\n To install the Jupyter Notebook,  \\n you can either use Anaconda or Pip3.  \\n Similar to installing PyTorch,  \\n we'll use Pip3 as follows.  \\n To verify the installation,  \\n try the following command:  \\n python3 -m notebook.  \\n You should see the console launching  \\n the Notebook environment in the browser here.  \\n There will be additional environmental setup  \\n in later modules,  \\n but this concludes the general setup.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2621016\",\"duration\":143,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"PyTorch use case description\",\"fileName\":\"2884016_en_US_01_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn about the use case, objectives, and problem space. This is an important context for model development and learning.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4163598,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] The use case I'll be using  \\n throughout this course is based on a project called Piraeus.  \\n The overall objective of Project Piraeus  \\n is to democratize knowledge.  \\n Piraeus is a portal that brings together learners and tutors  \\n in a digital portal.  \\n It then leverages machine learning and AI  \\n to find the best tutors for every learner  \\n and to help tutors and learners  \\n to devise a customized learning plan.  \\n Many students struggle in rigid linear learning systems.  \\n Education and learning is personal.  \\n Unfortunately, one-on-one learning, private tutoring,  \\n and custom-designed learning paths are just too expensive  \\n for many students and communities to access.  \\n AI may be able to help.  \\n Artificial intelligence, AI, can play a key role  \\n to achieve this objective and vision.  \\n AI provides the capability to mimic human judgment.  \\n In this case, it'll mimic the judgment  \\n of a tutor or a teacher with dedicated attention  \\n to a student's unique learning needs.  \\n The focus of our use case is to develop AI models  \\n to better match tutors with learners  \\n based on learning, characteristics, and learning objectives.  \\n We look to generate a guided learning path,  \\n select learning material,  \\n weave in checkpoints and assessment  \\n through feedback along the way,  \\n and continue to refine and adapt the next set of milestones.  \\n In this course, I'll demonstrate how to develop  \\n and train the machine-learning models,  \\n using as inputs the characteristics  \\n of the learners and tutors,  \\n the performance of the learner,  \\n along with the metrics associated with the tutoring sessions  \\n including number of sessions, duration of sessions,  \\n time between sessions.  \\n The inputs to the models  \\n and predictive methods must be consistently monitored  \\n to determine the most critical inputs  \\n that produce the best possible outcome  \\n for both learners and tutors.  \\n This type of rapid iteration of exploration,  \\n experimentation, and productization  \\n makes PyTorch the ideal choice for the AI and ML framework.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2615033\",\"duration\":253,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"PyTorch data exploration\",\"fileName\":\"2884016_en_US_01_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn about the dataset for this use case. Understanding the dimensionality, categories, and distributions of the data is fundamental to developing ML algorithms.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8855054,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - PyTorch can be used to explore a dataset  \\n for model development.  \\n PyTorch provides two data primitives  \\n DataLoader and Dataset.  \\n The Dataset class stores the samples  \\n and the corresponding annotation labels.  \\n The DataLoader class wraps in iterable around  \\n the Dataset class to enable easy access to the samples.  \\n These two classes allow you to use preloaded data sets  \\n and upload your own data.  \\n PyTorch comes with a set of domain libraries  \\n that provide many preloaded data sets.  \\n They are subclasses of torch utils: data, dataset.  \\n These libraries and subclasses implement  \\n specific functions to the particular data.  \\n One great example is Fashion MNIST  \\n which we will explore later.  \\n These preloaded data sets and functions  \\n enable ML engineers to prototype and benchmark their models.  \\n You can find these data sets and functions  \\n on the PyTorch website  \\n with the links included in this slide.  \\n For example, for image dataset  \\n go to pytorch.org/vision/stable/datasets.html.  \\n For text datasets  \\n go to pytorch.org/text/stable/datasets.html.  \\n And lastly, for audio data sets  \\n go to pytorch.org/audio/stable/datasets.html.  \\n Now let's take a look at the Fashion MNIST dataset.  \\n I'll first import the needed libraries here.  \\n I'll then create two data sets  \\n one for training  \\n and one for testing.  \\n The only difference is to set the parameter of train to true  \\n for training data set  \\n and false for testing data set.  \\n Next, I'll create a validation sample  \\n from the training data set.  \\n Next, I'll create three DataLoaders  \\n one for training, one for validation, and one for testing.  \\n Next, I'll create a data iterator  \\n to populate the image and label tensors.  \\n Lastly, I'll use the mat plot lib to plot the data.  \\n Next, let's upload and look at our own dataset.  \\n We'll go through four steps to load our data set.  \\n Step one, import all necessary libraries  \\n for loading our data.  \\n We will be uploading a CSV file that contains tutor's data.  \\n For that, we need to import the following libraries.  \\n Step two, find the file path.  \\n For this exercise  \\n I'll just upload our CSV file from a local drive.  \\n And here is the file path, here in the documents folder.  \\n And we'll put this path into a variable called data path.  \\n Step three, loading the data.  \\n We'll load the data using the pandas library  \\n we imported in step one.  \\n If you don't have pandas installed  \\n just run the following command: pip3 install pandas.  \\n Here we created a pandas dataframe called df  \\n to store the dataset.  \\n Now let's visualize the data  \\n using the pandas function display.  \\n And this is our dataset.  \\n Step four, convert a pandas dataframe into a PyTorch tensor.  \\n In order to use the data set and PyTorch for model training  \\n we'll need to convert it into a PyTorch tensor.  \\n Here's the code to do that.  \\n And we also printed out the tensor in this format.  \\n Now this wraps up our overview chapter.  \\n In the next chapters we'll dig into the details  \\n of the PyTorch libraries.  \\n \\n\\n\"}],\"name\":\"1. Preparation\",\"size\":29793335,\"urn\":\"urn:li:learningContentChapter:2615034\"},{\"duration\":891,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2622023\",\"duration\":213,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand PyTorch tensors\",\"fileName\":\"2884016_en_US_02_01_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"In this video, learn about the basic concepts of PyTorch tensors. Explore basic constructs that you are using for the rest of the use case implementation in PyTorch.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6395064,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Tensors are the building blocks  \\n of a neural network.  \\n A PyTorch tensor is the central data abstraction  \\n as a generalized form of arrays in n dimensions  \\n to run arbitrary computations.  \\n A tensor can be of any dimension.  \\n For example, zero dimensional tensors  \\n are a scale numbers like 0, 10, 100, et cetera.  \\n One dimensional tensors are array of numbers  \\n like an array of 9, 8, 7,  \\n or an array of 50, 30 10, et cetera.  \\n Tensors can also be two-dimensional.  \\n They are matrix of numbers like in 2x3 matrix  \\n with a two element array of an array.  \\n So for example, an array of 1, 2, 3 and a 7, 5, 3.  \\n So now let's create some tensors  \\n by importing torch and using the torch.Tensor methods.  \\n Here's our notebook, and here we are.  \\n We're creating some torch.Tensors,  \\n and here is the output of those tensors.  \\n The simplest way to set the underlying data type of a tensor  \\n is with an optional argument at creation time.  \\n Here is an example.  \\n We're setting the torch data type,  \\n the tensor data type, to be int16.  \\n There are 10 data types available.  \\n I have listed here boolean int8, uint8, int16, int32,  \\n int64, half, float, double, or bfloat.  \\n While training the model,  \\n you'll deal with higher dimensions.  \\n The neural network only accepts the dimension  \\n which is defined at the input layer  \\n while architecting the model.  \\n The dimension basically tells whether the tensor  \\n is zero-dimension, or one-dimension, or two-dimension,  \\n or even higher than that.  \\n Dimension of the tensor is also called  \\n the rank of the tensor.  \\n To get the size, you can use tensor.size method,  \\n or tensor.shape property,  \\n and to get the dimension of the tensor,  \\n use tensor.ndimension method, or tensor.ndim property.  \\n Let's take a look.  \\n There you have it.  \\n These are the examples.  \\n Sometimes working with a neural network,  \\n you need to change the dimension of the tensor.  \\n This can be done by using  \\n the tensor.view nrows, ncols method of the tensor.  \\n PyTorch tensor also provides flexibilities,  \\n such as in-place alterations, tensor copying,  \\n and changing the shape of the tensor,  \\n such as altering the dimensions of the tensor.  \\n One of the major advantages of PyTorch  \\n is its robust acceleration on CUDA-compatible NIVIDIA GPUs.  \\n CUDA stands for compute-unified device architecture,  \\n which is NIVIDIA's platform for parallel computing.  \\n By default, new tensors are created on the CPU,  \\n so we have to specify when we want to create our tensor  \\n on the GPU with the optional device argument.  \\n If you have an existing tensor living on one device,  \\n you can move it to another with a to method.  \\n It is important to know that,  \\n in order to do computation involving two or more tensors,  \\n all of the tensors must be on the same device.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2616025\",\"duration\":173,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand PyTorch basic operations\",\"fileName\":\"2884016_en_US_02_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn about the basic concepts of PyTorch operators. Explore basic constructs that you are using for the rest of the use case implementation in PyTorch.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5702499,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] PyTorch Tensors allow you to apply  \\n arithmetics on your tensors.  \\n PyTorch Tensors have over 300 operations  \\n that can be performed on them.  \\n You can apply these operations on Scala  \\n and multiple dimensions of tensors.  \\n The very basic operations on tensors are additions,  \\n subtractions, multiplications, and divisions.  \\n Here's an example for addition and subtraction.  \\n So we have two tensors  \\n and we are performing some additions and subtractions here,  \\n and you'll get those output.  \\n The major categories of operations  \\n include common functions such as abs, ceil, floor, clamp,  \\n trigonometric functions and their inverses  \\n such as pi, sin, asin, bitwise operations, comparisons,  \\n reductions such as max, min, standard, prod,  \\n unique, vector/matrices, and linear algebra operations.  \\n Now let's go over some more examples.  \\n First, here are some common functions  \\n like abs, ceil, floor, and clamp, and these are the output.  \\n Next, let's try a reduction function  \\n using the mean operator,  \\n and there you have the output.  \\n Finding maximum or minimum values in the Tensor  \\n can also be done by using tensor.max  \\n and tensor.min respectively.  \\n The expected output is right there.  \\n Now let's take a look at the trigonometric functions  \\n and their inverses.  \\n Supposed you have a tensor containing various values of pi  \\n and you want to apply the sine and cosine function on it.  \\n You can use the torch.sin tensor  \\n and torch.cos tensor as follows.  \\n And we import NumPy here as well.  \\n You can expect the output here.  \\n Sometimes you have to get an evenly spaced  \\n list of numbers between a range.  \\n You can use torch.linspace, start, end, and step to do that.  \\n Let's make it more interactive  \\n by plotting the sin minus pi to sin pi  \\n and cos minus pi to cos pi,  \\n making the dataset first here.  \\n Here we have the dataset.  \\n And last but not least,  \\n let's use Matplotlib to create the graph.  \\n Here you see the plotted output.  \\n The basic PyTorch operators offer a lot of functionalities  \\n and here is an example of that  \\n and hope you get the opportunity of exploring more.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2620024\",\"duration\":213,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand PyTorch NumPy Bridge\",\"fileName\":\"2884016_en_US_02_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to convert NumPy arrays to PyTorch tensors and vice versa. Explore the basic constructs that you are using for the rest of the use case implementation in PyTorch.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6786744,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Broadcasting is a way to perform an operation  \\n between tensors that have similarities in their shapes.  \\n This is an important operation in deep learning.  \\n The common example  \\n is multiplying a tensor of learning weights  \\n by a batch of input tensors,  \\n applying the operation to each instance  \\n in the batch separately  \\n and running a tester of identical shape.  \\n Here's an example with two by four  \\n multiplying one by four returns a tester of shape two,  \\n with dimension of one, with values of two and 16.  \\n If you're familiar with broadcasting semantics  \\n and NumPy ndarrays,  \\n you'll find the same rules apply with PyTorch.  \\n The exception to the same shape rule  \\n is tensor broadcasting.  \\n The rules for broadcastings are,  \\n one, each tensors must have at least one dimension,  \\n no empty tensors.  \\n Comparing the dimension sizes of two tensors,  \\n going from last to first, each dimension must be equal  \\n or one of the dimensions must be of size one  \\n or the dimension does not exist in one of the tensors.  \\n Tensors of identical shape,  \\n of course, are trivially broadcastable,  \\n as you saw earlier.  \\n Here are some examples of situations  \\n that honor the rules and allow broadcasting.  \\n We first have tensor A  \\n and tensor B, here, has the third and second dimensions  \\n identical to tensor A, with dimension one absent.  \\n Tensor C has third dimension equals to one,  \\n second dimension identical to A  \\n and first dimension empty.  \\n Tensor D has third dimension identical to A,  \\n second dimension equals to one  \\n and first dimension empty, and it is broadcastable as well.  \\n PyTorch's broadcast semantics are compatible with NumPy's,  \\n but the connection between PyTorch and NumPy  \\n goes even deeper than that.  \\n If you have existing ML or scientific code  \\n with data stored in NumPy ndarrays,  \\n you may wish to express the same data as PyTorch tensors.  \\n Whether to take advantage of PyTorch's GPU acceleration  \\n or its efficient abstractions for building ML models.  \\n It's easy to switch between ndarrays and PyTorch tensors.  \\n Let's take a look at some examples.  \\n Here's a conversion and here is the expected output.  \\n PyTorch creates a tensor of the same shape  \\n and containing the same data as NumPy array, going as far as  \\n to keep NumPy's default 64 bit float data type.  \\n The conversion can just as easily go the other way, here,  \\n as the example of a version.  \\n And the expected outcomes are here.  \\n It's also important to know that these converted objects  \\n are using the same underlying memory  \\n as their source objects,  \\n meaning that changes to one are reflected in the other.  \\n And here's an example,  \\n and the expected output is shown here.  \\n And this concludes the Intro to PyTorch NumPy Bridge.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2619025\",\"duration\":170,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understand PyTorch autograd\",\"fileName\":\"2884016_en_US_02_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to use autograd to automate the computation of backward passes in neural networks. Explore the basic component within PyTorch and the important-to-implement neural networks for the rest of the use cases.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5241187,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Narrator] Automatic differentiation is a building block  \\n of every deep learning library out there.  \\n PyTorch's automatic differentiation engine called Autograd  \\n is a tool to understand how automatic differentiation works.  \\n Modern neural network architectures  \\n can have millions of learnable parameters.  \\n From a computational point of view,  \\n training a neural network consists of two phases.  \\n The forward pass computes the value of the loss function.  \\n In forward prop, the neural network makes its best guess  \\n about the correct output.  \\n It runs the input data through each of its functions  \\n to make this guess.  \\n The backward pass computes the gradients  \\n of the learnable parameters.  \\n In backdrop, the neural network adjusts its parameters  \\n proportionate to the error and its guesses.  \\n It does this through three steps.  \\n First, it traverses backwards from the output,  \\n then it collects the derivatives of the error  \\n with respect to the parameters of the functions,  \\n aka gradients.  \\n And lastly, it optimizes the parameters  \\n using gradient dissent.  \\n The forward pass is pretty straightforward.  \\n The output of one layer  \\n is the input to the next and so forth.  \\n Backward pass is a bit more complicated  \\n since it requires us to use the chain rule  \\n to compute the gradients of the weights  \\n to the loss function.  \\n It is impractical to calculate gradients  \\n of such large composite functions  \\n by solving mathematical equations,  \\n especially because these curves exist  \\n in a large number of dimensions  \\n and are impossible to fathom.  \\n This is where PyTorch's Autograd comes in.  \\n It abstracts complicated mathematics  \\n and helps us magically calculate gradients  \\n of high-dimensional curves with only a few lines of code.  \\n Let's run through an example.  \\n First, we need to import the libraries  \\n using Torch and Torch Autograd.  \\n Next, we'll use the Autograd variable function  \\n to create a variable.  \\n Let's now perform an operation of the variable.  \\n Y was created as a result of an operation,  \\n so it has a grad function.  \\n Next, let's perform more operations on y.  \\n Time to calculate the gradients now.  \\n Let's back prop now using out.backward  \\n which is equivalent to doing the following.  \\n And once we print, you should have got a matrix of 4.5.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2620025\",\"duration\":122,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Advanced PyTorch autograd\",\"fileName\":\"2884016_en_US_02_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to use autograd to automate the computation of backward passes in neural networks. Explore an advanced component within PyTorch an important-to-implement neural network for the rest of the use cases.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3771747,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] There's a lot more to AutoGrad and PyTorch.  \\n Let's try something more complex.  \\n In this first example,  \\n we first generate three random numbers,  \\n assign them to a variable called X,  \\n and we require, in this case, gradients for X.  \\n We then provide some operations on X  \\n to generate a new variable called Y,  \\n which will have a gradient function attached to it.  \\n Now, let's print out the gradient for X.  \\n So in your Neural Network,  \\n parameters that don't compute gradients  \\n are usually called frozen parameters.  \\n It is useful to freeze part of your model  \\n if you know in advance  \\n that you won't need the gradients of those parameters.  \\n This offers some performance benefits  \\n by reducing AutoGrad computations.  \\n Let's walk through a small example to demonstrate this.  \\n We'll first import some libraries,  \\n including torchvision, in this case.  \\n We load a pre-trained resnet18 model, in this case,  \\n and then freeze all of the parameters.  \\n Assume we want to fine tune the model  \\n on a new dataset with 10 labels.  \\n In ResNet, the classifier is the last linear layer,  \\n model.fc.  \\n We can simply replace it with a new linear layer,  \\n unfrozen by default,  \\n that acts as our classifier.  \\n Now, all parameters in this model,  \\n except for the parameters of model.fc, are frozen.  \\n The only parameters that compute gradients  \\n are the weights and bias of model.fc.  \\n Notice that although we register all the parameters  \\n in the optimizer,  \\n the only parameters that are computing gradients  \\n and hence, updated in the gradient descent,  \\n are the weights and the bias of the classifier.  \\n The same exclusionary functionality  \\n is available as a context manager in torch.no_grad.  \\n \\n\\n\"}],\"name\":\"2. PyTorch Basics\",\"size\":27897241,\"urn\":\"urn:li:learningContentChapter:2617031\"},{\"duration\":603,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2617030\",\"duration\":521,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Torchvision introduction\",\"fileName\":\"2884016_en_US_03_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to install Torchvision and understand the basic usage and implementation of the package. Understanding the basics of Torchvision helps you to further implement the use case in future sections.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15422991,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] TorchVision is a library within PyTorch  \\n for image and video processing,  \\n it contains a number of important and useful data sets,  \\n model architecture,  \\n as well as models and transformation operations  \\n that are commonly used for computer vision projects.  \\n There are seven main packages for the TorchVision library:  \\n dataset, IO, models, feature extraction,  \\n ops, transforms, and utils.  \\n Let's look at each one of them.  \\n TorchVision dataset,  \\n this package, as the name suggests,  \\n contains some of the most popular computer vision data sets.  \\n All data sets are subset of torch.util.data.Dataset,  \\n they have getitem and lamp methods implemented,  \\n as a result, you can pass any of them  \\n to torch.utils.data.DataLoader,  \\n which can load multiple samples in parallel  \\n using multiple processing workers.  \\n We covered the PyTorch DataLoader in previous chapters.  \\n One thing to note is that all data sets  \\n have identical implementations and signatures.  \\n All of them have these two common arguments:  \\n transform, and target transform.  \\n The former will transform the input,  \\n and latter, to transform the target.  \\n Some of the most popular data sets are CalTech,  \\n which refers to the CalTech 101 project  \\n that contains pictures of objects  \\n belonging to 101 categories,  \\n each category has about 40 to 800 images.  \\n CIFAR.  \\n The CIFAR 10 and CIFAR 100 are labeled data sets  \\n of the 80 million TinyImage data set.  \\n Cityscapes.  \\n The Cityscapes data set focuses on semantic understanding  \\n of urban street scenes,  \\n with 50 cities across different seasons,  \\n with 5,000 images finely annotated,  \\n and 20,000 coarsely annotated.  \\n ImageNet contains 14,197,122 annotated images  \\n according to the WordNet hierarchy.  \\n Since 2010, the dataset is used  \\n in the ImageNet Large Scale Visual Recognition Challenge,  \\n a benchmark in image classification and object detection.  \\n Many of you have heard the MNIST dataset,  \\n which was developed by none other, Yann LeCun,  \\n the chief scientist at Meta  \\n who was dubbed as one of the god fathers of deep learning.  \\n The MNIST dataset has become a standard benchmark  \\n for learning, classification, and computer vision systems.  \\n Torch Vision Dataset also has a number of extended dataset  \\n from MNIST, including EMNIST,  \\n Extend MNIST that constitutes  \\n more challenging classification tasks  \\n involving letters and digits,  \\n and that shares the same image structure and parameters  \\n as the original MNIST tasks,  \\n allowing for direct compatibility  \\n with all existing classifiers and systems.  \\n Fashion-MNIST, consisting of a training set  \\n of 60,000 examples, and a test set of 10,000 examples.  \\n Each example is a 28 by 28 gray scale image  \\n associated with a label from 10 classes.  \\n KMNIST, it's a data set which is a replacement  \\n for the MNIST data set, 28 by 28 gray scale,  \\n with 70,000 images provided in the original MNIST format,  \\n as well as NumPy format.  \\n Torchvision.io,  \\n this package provides functions  \\n for performing IO operations,  \\n including reading and writing images and videos.  \\n Images, the Torch Vision IO read_image  \\n reads a JPEG or PNG image  \\n into a three-dimensional RGB tensor,  \\n optionally converts the image to the desired format.  \\n The values of the output tensor  \\n are unit 8, between 0 and 255.  \\n decode_image detects whether an image is a JPEG or PNG,  \\n and performs the appropriate operations  \\n to decode the image into a three-dimensional RGB tensor.  \\n encode_jpeg takes an input tensor and CHW layout,  \\n and returns a buffer with the content  \\n of its corresponding JPEG file.  \\n decode_jpeg decodes a JPEG image  \\n into a three-dimensional RGB tensor,  \\n optionally converts the image to the desired format.  \\n The value of the output tensor are unit 8  \\n between 0 and 255.  \\n write_jpeg takes an input tensor and CHW layout  \\n and saves it to a JPEG file.  \\n For each of the JPEG method above,  \\n there is an equivalent for PNG format.  \\n For videos,  \\n read_video reads a video from a file,  \\n returning both the video frames as well as the audio frames.  \\n read_video_timestamps list the video frames' timestamps.  \\n write_video writes a 4D tensor in THWC format,  \\n and a video file.  \\n VideoReader is a high performance, low-level API  \\n for more fine grain control video reading API,  \\n it supports frame by frame reading of various streams  \\n from a single video container.  \\n Models.  \\n The Models sub package contains definitions of models  \\n for addressing different tasks,  \\n including image classification,  \\n pixel-wise semantic segmentation,  \\n object detection, instant segmentation,  \\n person key point detection,  \\n and video classification.  \\n There are four model sub-packages,  \\n including Classification,  \\n Semantic segmentation,  \\n Object/keypoint detection,  \\n and Video classification.  \\n Operations.  \\n Feature extraction utilities enable ML engineers  \\n to access intermediate transformations of the model inputs.  \\n TorchVision provides great feature extractors  \\n for this purpose.  \\n The torch.fx.documentation  \\n provides a more general and detailed explanation  \\n of the above procedure  \\n and the inner workings of the symbolic tracing.  \\n TorchVision Ops,  \\n it implements operators  \\n that are specific for computer vision.  \\n Here are the key operators.  \\n batched_nms,  \\n it performs non-maximum suppression in a batched fashion.  \\n box_area, it computes the area of a set of bounding boxes.  \\n clip_boxes_to_image,  \\n clip boxes so that they lie inside an image of a size.  \\n mask_to_boxes,  \\n it computes the bounding boxes around the provided masks.  \\n remove_small_boxes,  \\n remove boxes which contain at least one size smaller  \\n than min size.  \\n sigmoid_focal_loss,  \\n loss used in retina net for dense detection.  \\n stochastic_depth implements the stochastic depth  \\n from deep networks  \\n with stochastic depth used  \\n for randomly dropping residual branches  \\n of residual architecture.  \\n Transforms, this library allows ML engineers  \\n to perform various transformation steps on images,  \\n including rotation, cropping, composing,  \\n adjusting color jitter, converting to gray scale,  \\n adjusting padding, resize,  \\n amongst many others.  \\n For details, check out the documentation in the link below.  \\n utils, this library provides  \\n a number of processing utilities for computer vision tasks,  \\n including making a grid of images,  \\n saving a tensor into an image file,  \\n and drawing segmentation masks on object detection.  \\n And the details and examples can be found  \\n at the PyTorch documentation site as well.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2620026\",\"duration\":82,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Torchvision for video and image understanding\",\"fileName\":\"2884016_en_US_03_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to use Torchvision to extract entities from various training and learning videos. Torchvision is a very powerful package to accomplish various computer vision tasks for object detection, entity extraction, and classification so similar training videos can be recommended.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3050507,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now that you have a good understanding  \\n of the TorchVision library,  \\n let's walk through a lab,  \\n continuing with the use case of Piraeus.  \\n When presenting a set of tutors to a student for selection,  \\n it'll be helpful to include some photos  \\n of the instructions in the lectures.  \\n We'll use the TorchVision image library  \\n and try a few transformation actions.  \\n First, we have to import some libraries.  \\n We then define a function to display images, here,  \\n called show.  \\n Say this is a Padre class.  \\n We'll load the images and show them here.  \\n Now, we define a function to plot various images  \\n through transformation,  \\n and the function is called plot,  \\n right here.  \\n Most transforms natively support tensors  \\n on top of the PIL images.  \\n Let's add some paddings to the image.  \\n We then try a few image transformations  \\n such as random crop,  \\n color jitter,  \\n random perspectives.  \\n And finally, let's crop to the center  \\n and find the best centered image.  \\n \\n\\n\"}],\"name\":\"3. Torchvision\",\"size\":18473498,\"urn\":\"urn:li:learningContentChapter:2617032\"},{\"duration\":448,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2616026\",\"duration\":205,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Torchaudio introduction\",\"fileName\":\"2884016_en_US_04_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to set up the environment and implement a basic model for audio processing using Torchaudio. Understanding the basics of Torchaudio helps you further implement the use case in the next section.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6476643,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] TorchAudio is a library  \\n for audio and signal processing with PyTorch.  \\n It provides IO, signal, and data processing functions,  \\n datasets, model implementations, and application components.  \\n TorchAudio offers a set of APIs,  \\n including backend, functional, transforms,  \\n datasets, models, pipelines,  \\n sox_effects, compliance.kaldi, kaldi_io, and utils.  \\n Similar to TorchVision,  \\n TorchAudio also provides a number of popular datasets  \\n out of the box.  \\n Examples include CMUDict,  \\n CMU pronouncing dictionary,  \\n Common Voice,  \\n GTZAN, which is music genre classification of audio signals,  \\n speech commands,  \\n and VCTK, which is speech data uttered  \\n by 110 English speakers with various accents.  \\n Details of these datasets can be found  \\n at PyTorch dataset documentation.  \\n Audio I/O package allow you to query audio file metadata,  \\n loading audio data into a tensor,  \\n and saving audio to files.  \\n Audio resampling.  \\n To resample an audio wave form  \\n from one frequency to another,  \\n you can use transforms.Resample or functional.resample.  \\n transforms.Resample pre-computes  \\n and caches the kernel used for resampling,  \\n while functional.resample computes it on the fly.  \\n TorchAudio's resample function can be used  \\n to produce results similar to that of Librosa,  \\n which is resampy's kaiser window resampling,  \\n with some noise.  \\n TorchAudio provides a variety of ways  \\n to augment audio data.  \\n TorchAudio SoX effects allows for directly applying filters,  \\n similar to those variables in SoX to tensor objects,  \\n and file object audio sources.  \\n Convolution reverb is a technique  \\n that's used to make clean audio sound  \\n as though it has been produced in a different environment.  \\n Using room impulse response, RIR, for instance,  \\n we can make a clean speech sound  \\n as though it has been uttered in a conference room.  \\n To add background noise to audio data,  \\n you can simply add a noise tensor  \\n to the tensor representing the audio data.  \\n A common method to adjust the intensity of noise  \\n is changing the signal-to-noise ratio.  \\n torchaudio.functional.apply_codec can apply codecs  \\n to a tensor object.  \\n Combining all of the above techniques,  \\n TorchAudio can simulate audio.  \\n that sounds like a person talking over a phone  \\n in an echoey room with people talking in the background.  \\n Similar to TorchVision,  \\n TorchAudio implements feature extractions  \\n commonly used in the audio domain.  \\n They are available in torchaudio.functional  \\n and torchaudio.transforms.  \\n There are also augmentation techniques available  \\n in TorchAudio, including TimeStretch, TimeMasking,  \\n and FrequencyMasking.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2620027\",\"duration\":243,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Torchaudio for audio understanding\",\"fileName\":\"2884016_en_US_04_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to implement audio characteristics of the training tutorials in this use case. Knowing how to extract language, intonation, and other audio characteristics of the video files can be a useful technique to generate more metadata for training material recommendations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8727736,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Now let's continue with a pyrees use case.  \\n We would like to transcribe the audio  \\n and video lectures into scripts.  \\n We'll go through an example using Pie Torch's  \\n Torch Audio Library to transcribe some audio speech.  \\n This example is provided by Hiro Moto and can be found  \\n at the Pie Torch website will be using pre-trained models  \\n from web to back 2.0 through Torch Audio.  \\n The high level steps  \\n of this pipeline includes extract the acoustic features  \\n from audio waveform and then estimate the class  \\n of the acoustic features framed by frame  \\n and then generate hypothesis  \\n from the sequence of the class probabilities.  \\n Torch audio provides an easy access  \\n to the pre-trained weights and associated information such  \\n as the expected sample rate and class labels.  \\n They're bundled together and available  \\n under the Torch Audio dot pipelines module.  \\n First, we import map plot lib.  \\n Next, we'll create a Wave two VAC two model  \\n that performs the feature extraction and the classification.  \\n We will use the torch audio dot pipelines  \\n dot wave two VAC two, ASR Base nine 60 H here.  \\n The bundle object provides the interface  \\n to instantiate model and other information.  \\n Sampling rate and the class labels  \\n are also found as follows.  \\n We'll create a model  \\n through automatically fetching the pre-trained weights  \\n and loaded into the Model.  \\n Importing S S L module is important to set the context  \\n so that the model can be downloaded  \\n through HTTPS via the secure sake layer.  \\n Next, we load data.  \\n We use the speech data from Voices dataset  \\n which is licensed under Creative Commons by 4.0.  \\n Let's listen to the output here.  \\n - [Male Computer Voice] I had that curiosity beside me  \\n at this moment.  \\n - Next, let's load the data into the model  \\n and run the resample if necessary  \\n for efficiency and performance.  \\n Next, we extract acoustic features frame by frame.  \\n The returned features is a set of tensors.  \\n Each tensor is the output of a transformer layer.  \\n Now let's visualize these features.  \\n All right. The result is back.  \\n As you can see, there are a total of 12 layers.  \\n Next, we'll classify these features into categories.  \\n Wave two, VAC two model provides method  \\n to perform the feature extraction  \\n and classification in one step.  \\n Let's visualize the classification.  \\n It's in the form of logics instead of probabilities.  \\n Next, we'll generate the transcript  \\n using a greedy decoding algorithm,  \\n meaning we'll simply pick up the best hypothesis  \\n at each timestamp  \\n and do not require external resources such as a dictionary.  \\n Now we will create the decoder object  \\n and decode the transcript.  \\n Lastly, let's check the result, and this is the output.  \\n I had that curiosity beside me at this moment.  \\n Let's listen to the audio  \\n and see if the transcription is correct.  \\n - [Male Computer Voice] I had that curiosity beside me  \\n at this moment.  \\n - Yes, it is correct.  \\n That wraps up the Torch Audio Lab.  \\n \\n\\n\"}],\"name\":\"4. Torchaudio\",\"size\":15204379,\"urn\":\"urn:li:learningContentChapter:2622024\"},{\"duration\":467,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2619026\",\"duration\":219,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Torchtext introduction\",\"fileName\":\"2884016_en_US_05_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to set up the environment and implement a basic model for text processing using Torchtext. Understanding the basics of Torchtext helps you to further implement the use case in the next section.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7261880,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - TorchText is PyTorch's Library  \\n for Natural Language Processing.  \\n It consists of data processing, utilities,  \\n and useful data sets for natural language processing.  \\n It also comes with pre-trained word embedding, dataset API.  \\n Iterator API ought to be used  \\n for training models with PyTorch.  \\n TorchText comes equipped with some popular data sets  \\n for text classification, language modeling,  \\n machine translation, sequence tagging, question and answers,  \\n and also unsupervised learning.  \\n Please note that these are actually data pipes  \\n from the Torch Data Project, which is still in beta status.  \\n So, be aware that APIs might change.  \\n Couple of recommendations while using these data sets.  \\n Use data loader for shuffling the data pipe.  \\n Use the built-in worker n-net function  \\n for multi-processing.  \\n And use drop last equals to true, to ensure that  \\n all batch sizes are equal.  \\n In addition to popular data sets,  \\n TorchText package also provides utilities  \\n for text data processing for training.  \\n Functional. The TorchText data functional module  \\n contains the following set of utilities.  \\n Generate SP model for training a sentence piece tokenizer.  \\n Load SP model for loading a sentence piece model for file.  \\n Sentence piece numericalizer is a sentence piece model  \\n that numericalizes a text sentence into a generator  \\n over the IDs.  \\n Sentence piece tokenizer is a sentence piece model to  \\n tokenize a text sentence into a generator over the tokens.  \\n Custom replace converts text strings.  \\n Simple space split splits text strings by spaces.  \\n Mericalize tokens from literator yields a list  \\n of IDs from a token iterator with a vocab.  \\n Filter Wikipedia XML filters Wikipedia XML lines.  \\n To map style dataset converts iterable style dataset  \\n to map style dataset.  \\n The metrics modules with blue score computes the blue score  \\n between a candidate translation corpus  \\n and a reference translation corpus.  \\n The data utils module contains get tokenizer function  \\n that generates tokenizer function for a string sentence.  \\n TorchText offers some common text transforms,  \\n including a few tokenizers, such as  \\n a sentence piece tokenizer, clip tokenizer,  \\n bert tokenizer, vocab transform,  \\n ToTnesor, label index, truncate, add token,  \\n sequential, pad transform, and string to int transform.  \\n They can also be trained together using  \\n torch.nn, sequential,  \\n or using TorchText, transforms, sequential  \\n to support torch's credibility.  \\n TorchText also contains a number  \\n of pre-trained models, including Roberta,  \\n that iterates on bert's pre-training procedure,  \\n including training the model longer  \\n with bigger batches over more data,  \\n removing the next sentence, prediction objective,  \\n training on longer sequences, and dynamically changing  \\n the masking pattern applied to the training data.  \\n We'll go through an example next.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2619027\",\"duration\":248,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Torchtext for translation\",\"fileName\":\"2884016_en_US_05_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to implement text transcription and translation from the video files. You need to understand how to use Torchtext for translation to make more learning content available to users.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9220109,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lab,  \\n we continue with the Piraeus use case.  \\n I'll walk you through the tutorial  \\n called \\\"Classifying Names with a Character-Level RNN\\\"  \\n by Sean Robertson.  \\n We will be building a text classifier  \\n to identify the language used in the course by the tutor.  \\n The character-level RNN  \\n reads words as a series of characters,  \\n outputting a prediction and hidden state at each step,  \\n feeding its previous hidden state into each next step.  \\n We take the final prediction to be the output,  \\n which class the word belongs to.  \\n Specifically, we'll train on a few thousand surnames  \\n from 18 languages of origin  \\n and predict which language a name is from,  \\n based on the spelling.  \\n First, we import the matplotlib.  \\n We then load the data files from the directory  \\n and build our category lines.  \\n Now we have category lines,  \\n a dictionary mapping each category, which is the language,  \\n to a list of lines, which are names.  \\n We also kept track of all categories,  \\n just a list of languages,  \\n and categories for later references.  \\n Let's print the first five lines of the Italian category.  \\n Now we have all the names organized.  \\n We need to turn them into tensors  \\n to make any use of them in our model.  \\n Next, let's create the network.  \\n To run a step of this network, we need to pass an input,  \\n in our case, the tensor for the current letter,  \\n and a previous hidden state,  \\n which we initialize as zeros at first.  \\n We'll get back the output probability of each language  \\n and a next hidden state, which we keep for the next step.  \\n We will use the lineToTensor  \\n to avoid creating a new tensor every step.  \\n This is more efficient.  \\n Before doing our training,  \\n let's create a few helper functions.  \\n First one is to get category from output.  \\n The second helper function  \\n is to quickly get training example.  \\n We will train the network using NLLLoss function.  \\n We set the learning rate accordingly,  \\n and define a function, train the model.  \\n Now we'll run the train function  \\n that we defined in the previous step with some examples.  \\n This will take a while.  \\n So after the training is done,  \\n we will plot the learning results.  \\n We are almost halfway through.  \\n Okay, now let's plot the learning results.  \\n Here we go.  \\n Next step, we will evaluate the results.  \\n We create a confusion matrix in order to do that,  \\n which indicate for every actual language, which are rows,  \\n which language the network guesses and predicts,  \\n which are the columns.  \\n To calculate the confusion matrix,  \\n a bunch of samples are run through the network  \\n with the evaluate function,  \\n which is the same as the train function  \\n is set for minusing the backprop.  \\n This is the evaluate function.  \\n As you can see, the rows are the actual language  \\n and the columns are what the model predicts.  \\n Lastly, let's see how the model work  \\n by giving it some examples through a predict function.  \\n We're giving it four examples towards the end,  \\n and this is the prediction.  \\n \\n\\n\"}],\"name\":\"5. Torchtext\",\"size\":16481989,\"urn\":\"urn:li:learningContentChapter:2618025\"},{\"duration\":73,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2616027\",\"duration\":73,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Continuing your PyTorch learning process\",\"fileName\":\"2884016_en_US_06_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, get a summary of the key concepts covered in this training and get ready to select the next set of learning from the available resources. This is a very elementary introduction to PyTorch. There are greater details that you need to delve into in order to be more proficient with the framework.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2076513,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Thanks for watching.  \\n Understanding the basics of computer vision,  \\n natural language processing, and audio processing  \\n through PyTorch can be a lot to get your head around.  \\n So if you need to go back  \\n and rewatch videos until you get it.  \\n It's very common for new learners to take a couple of passes  \\n through this material before it really sinks in.  \\n If you want to keep learning about PyTorch  \\n or Machine Learning, the next step is to dive  \\n into the documentation and build something,  \\n create an implementation for an idea you have.  \\n And if you want to learn more about AI,  \\n there is a 10 course learning series  \\n on LinkedIn called Master The Fundamentals of AI  \\n and Machine Learning.  \\n Courses such as Artificial Intelligence Foundations  \\n Neural Networks with Doug Rose,  \\n or AI Algorithms for Gaming with Eduardo Corpeno  \\n will both be super informative and fun.  \\n If you want to keep in touch with me,  \\n please follow me on Twitter or LinkedIn  \\n or look me up on Facebook and Instagram.  \\n Thank you and bye.  \\n \\n\\n\"}],\"name\":\"Conclusion\",\"size\":2076513,\"urn\":\"urn:li:learningContentChapter:2617033\"}],\"size\":111518093,\"duration\":3412,\"zeroBased\":false},{\"course_title\":\"PyTorch Essential Training: Deep Learning\",\"course_admin_id\":2706322,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":2706322,\"Project ID\":null,\"Course Name\":\"PyTorch Essential Training: Deep Learning\",\"Course Name EN\":\"PyTorch Essential Training: Deep Learning\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"&lt;p&gt;PyTorch is the most flexible and expressive library for deep learning, and offers simple Python API, GPU support, and flexibility. It\u00e2\u20ac\u2122s designed to load data, apply transforms, and build deep learning models with just a few lines of code. Many machine learning developers and researchers use PyTorch to accelerate deep learning research, experimentation, and prototyping. In this course, software developer Terezija Semenski teaches you the important features of PyTorch with a hands-on approach to help you develop the skills you need to dive into your deep learning projects.&lt;/p&gt;&lt;p&gt;This course includes Code Challenges powered by CoderPad. Code Challenges are interactive coding exercises with real-time feedback, so you can get hands-on coding practice alongside the course content to advance your programming skills.&lt;/p&gt;\",\"Course Short Description\":\"Explore the basics of deep learning using PyTorch and test your knowledge with hands-on challenges.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":20533014,\"Instructor Name\":\"Terezija  Semenski\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Software Developer, Mathematician, Writer, and Learner\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2024-04-15T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"No\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/pytorch-essential-training-deep-learning-23753149,https://www.linkedin.com/learning/pytorch-essential-training-deep-learning-revision-q1-2024-coderpad\",\"Series\":\"Essential Training\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Artificial Intelligence for Technology\",\"Primary Software\":\"PyTorch\",\"Media Type\":\"Interactive\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":4902.0,\"Visible Video Count\":29.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":296,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3890261\",\"duration\":31,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Deep learning with PyTorch\",\"fileName\":\"2706322_en_US_00_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Storyboard + WSC\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":false,\"rawDurationSeconds\":129,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Take a closer look at the advantages of PyTorch and explore the goals of this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1981176,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- PyTorch is one of the most popular deep learning\\nframeworks that allows us\\nto implement neural networks more efficiently.\\nIt is designed to load data, apply transforms,\\nand build deep learning models\\nwith just a few lines of code.\\nJoin me to learn the foundations of PyTorch\\nwith the hands-on approach for the skills you need\\nto dive into Deep Learning Project.\\nHi, I'm Terezija Semenski.\\nI'm a software developer, mathematician,\\nand a teacher with a passion for AI and machine learning.\\nLet's get started.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3886261\",\"duration\":77,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"2706322_en_US_00_02_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Overlay - colab.research.google.com Please mask sign in to Google account\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":86,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Before starting this course, explore what skills and knowledge you need to have to be successful in this course. This video also lists all installation requirements you need to set up before you proceed.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2591273,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Before starting this course,\\nlet's explore what skills and knowledge you will need.\\nTo truly be successful in this course,\\nit will be helpful\\nto have a basic Python programming knowledge.\\nI assume you're familiar\\nwith the concepts and technologies behind deep learning,\\nbut you don't need any experience building models\\nusing neural networks, or NNs.\\nIn order to run our code,\\nwe will be using the Google Colaboratory environment,\\ncommonly known as Google Colab,\\nwhich is available at colab.research.google.com.\\n\\nGoogle Colab, or short, Colab, is a free computing service\\nthat provides Jupyter Notebook instances\\nthat run on the cloud.\\nThe great thing is that notebooks can be saved\\non Google Drive or GitHub.\\nConveniently, all you need to start writing code\\nis a Google account.\\nGoogle provides free access to GPUs,\\nor graphical processing units,\\nwhich we will use for training our models.\\nThere are some limits on GPU usage,\\nbut it will be sufficient for our course.\\n\\nAnd that's about it.\\nSo let's get ready to jump into PyTorch in depth.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3888262\",\"duration\":188,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Tour of CoderPad\",\"fileName\":\"2706322_en_US_00_03_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Use callouts to highlight areas covered by the [in]structor.\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":236,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5703994,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] This course includes\\nautomated code challenges\\nthat appear when you click on the Challenge links\\nin the course's Table of Contents.\\nEach challenge includes instructions\\nand a couple of code editors\\nyou can use to create and test\\nyour own solutions to the challenge.\\nThese challenges are hosted by CoderPad,\\nand they appear in the same area of the course page\\nwhere you watch the course's videos.\\nWe recommend using a desktop browser\\nfor the best experience with the code challenges,\\nbut you can use the LinkedIn Learning mobile app\\nif you prefer.\\n\\nThe Code Challenges has four areas:\\ninstructions in the top left,\\na code editor for your answer in the top right,\\nanother code editor where you can see how your code is used\\nin the bottom right,\\nand a console for output in the bottom left.\\nYou can use these rectangles to allocate space as you like.\\nTo get even more horizontal space for the code editors,\\nyou can collapse the course's Table of Contents on the left.\\n\\nEach challenge has instructions\\nthat include a description of the challenge\\nand the challenge's parameters and desired result.\\nParameters are values that are passed into your code,\\nand they have to be of a particular data type.\\nThe return value also has to be of a particular type,\\nand you'll also see that noted in instructions.\\nThe Constraints section has useful information\\nabout the parameters that will be passed in.\\nThe examples show different parameter values\\nand what results would be returned\\nfor each of those test cases.\\n\\nCreate your answer in the top right code editor.\\nThere are comments in the starting code\\nshowing where to put your solution.\\nWhen you click Test My Code,\\nyou'll see a message\\nindicating whether your code returned a correct result.\\nCreate your answer in the top right code editor.\\nThere are comments in the starting code\\nshowing where to put your solution.\\nWhen you click Test My Code,\\nyou'll see a message\\nindicating whether your code returned a correct result.\\n\\nIf your code isn't successful, you can ask for help.\\nThe show_expected_result and show_hints variables\\ndetermine whether you see the expected outputs\\nand any hints.\\nChange them to a value of True to control the output.\\nThe code editor in the lower right\\nshows you how your solution is used.\\nYou can change that code\\nto experiment with different test cases.\\nRegardless of whether your answer is successful,\\nyou'll see messages in the console output in the lower left.\\n\\nIf any messages are too long to fit in that area,\\nyou can scroll sideways to see all of the text.\\nWhen you finish each code challenge,\\nreturn to the course's Table of Contents\\nand click the next window to see my solution.\\n\"}],\"name\":\"Introduction\",\"size\":10230942,\"urn\":\"urn:li:learningContentChapter:3891288\"},{\"duration\":588,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3885248\",\"duration\":260,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to deep learning\",\"fileName\":\"2706322_en_US_01_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"01:03 - audio only pickup to replace the missing word \\\"the\\\"\\nB-roll (seen on screen starting the video off is in Collected Assets\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":348,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":true},\"description\":\"Learn what deep learning is, and how it is different from ML and AI, and explore examples of its applications in real-life scenarios.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9170693,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Neural networks used to be\\na cool theoretical concept just a few decades ago,\\nand now they're part of our everyday lives\\ncarried around in our smartphones.\\nWe can use them to enhance their pictures,\\nreceive contact-sensitive email replies,\\nand have our voices recognized.\\nSmart speakers are a common household item\\nand self-driving cars are reality,\\nbut what is behind them?\\nIn the last few years,\\nartificial intelligence, or AI,\\nmachine learning, or ML,\\nand deep learning, or DL, have been buzzwords.\\n\\nEverybody talks about them,\\nbut only a limited number of people understand the meaning\\nand difference between them.\\nSo, is ML the same as AI, or is it different from it?\\nWhat about the DL?\\nWhere does it fit into picture?\\nFirst, artificial intelligence, or short, AI,\\nis challenging to define,\\nand there is no clear agreement among AI professionals.\\nSo, how can we define what AI is?\\nIn AI, the goal is to simulate human learning\\nso that application can adapt\\nto uncertain or unexpected conditions.\\n\\nArtificial intelligence is a concept of creating machines\\nthat can perform tasks requiring human cognitive functions\\nsuch as discovering, finding new information,\\nlearning, reasoning, problem solving, perception,\\nand language Understanding.\\nAI has many subsets,\\nsuch as machine learning, expert systems,\\nnatural language processing, or short, NLP, and robotics.\\n\\nMachine learning, or ML,\\nis just one of several subsets of AI.\\nTo build any learning system,\\nyou need three essential components, input data,\\nprocessing and output layer.\\nWhen we have a learning system\\nwith improved performance over time\\nby learning from a new examples or data,\\nwe call it machine learning, or ML.\\nML is a set of multiple techniques\\nthat enable a computer to learn from data\\nand use what it learns to provide an answer,\\noften in form of a prediction.\\n\\nWhen they have a huge data sets,\\nit is time consuming for ML algorithms\\nto process large scale matrix computations.\\nIn this case, deep learning or DL is more applicable.\\nWe can represent the relationship\\nbetween the AI, ML and DL\\nwith a simple Venn diagram like this.\\nSo, ML is a subset of AI,\\nand deep learning is a subset of ML.\\n\\nWait, but what is the difference\\nbetween machine learning and deep learning?\\nDeep learning is a subset of machine learning\\nthat uses multiple and numerous layers\\nof non-linear transforms\\nto progressively extract features from row input.\\nWait, what?\\nDeep learning is called deep\\nbecause it uses multiple layers of neural networks.\\nEvery layer is trained by algorithms.\\n\\nIt takes its inputs from previous layers\\nand progressively refines them in order\\nto minimize their errors and improve accuracy.\\nSo, machine learning is usually used\\nfor all dimensional data and for small volumes of data.\\nOn the other side,\\ndeep learning is used when the data dimension is huge\\nand the training data is also high in volume.\\nFinally, you may wonder where we can apply deep learning.\\n\\nThere are numerous use cases,\\nsome of them are speech recognition,\\nanomaly detection from videos,\\nmachine translation,\\nspeech to text conversion.\\nThis introduction leaves us with a good starting point\\nto discover why we should choose PyTorch for deep learning,\\nso let's discover that next.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3885247\",\"duration\":121,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Why should you use PyTorch\",\"fileName\":\"2706322_en_US_01_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Overlay -  https://github.com/pytorch/pytorch/blob/main/LICENSE\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":130,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"PyTorch provides Python with easily run array-based calculations, allows the building of dynamic neural networks, and performs auto differentiation with strong graphics processing unit (GPU) acceleration. All the above are the most important features required for deep learning development.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3693987,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] PyTorch is a Python library\\nthat facilitates building deep learning projects.\\nPyTorch provides Python\\nwith easily run array-based calculations,\\nallowing it to build dynamic neural networks\\nand perform auto differentiation\\nwith a strong graphics processing unit or GPU acceleration.\\nThese are the most important features required\\nfor deep learning development.\\nPyTorch was developed by researchers and engineers\\nfrom the Facebook AI Research, FAIR division,\\nto process large-scale image analysis,\\nincluding object detection,\\nsegmentation, and classification.\\n\\nIt was written using Python and C++ languages.\\nPyTorch was initially released in September, 2016,\\nand its free and open-source under the modified BSD license,\\nso its development involves many contributions\\nfrom the community.\\nBut what makes PyTorch useful to us?\\nPyTorch is the world's fastest growing deep learning library\\nfor three main reasons,\\nits simplicity, flexibility, and Python interface.\\n\\nThis is what makes PyTorch powerful.\\nIt is supported by all major cloud platforms,\\nsuch as Amazon Web Services, Google Cloud Platform,\\nand Microsoft Azure.\\nIt is mature and stable since it's regularly maintained.\\nIt supports CPU, GPU, TPU, and parallel processing.\\nIt supports distributed training,\\nmeaning you can train neural networks or multiple GPUs\\non multiple machines.\\n\\nNow that you're familiar with PyTorch,\\nlet's jump in and discover Google Colab.\\nIt is super easy for beginners\\nas you can run PyTorch code in a browser\\nwithout installation or configuration.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3890260\",\"duration\":207,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Google Colaboratory basics\",\"fileName\":\"2706322_en_US_01_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":287,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, explore Google Colab which allows you to write and execute Python and PyTorch code in your browser.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5374245,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's explore Google Collaboratory,\\nknown as Google Colab,\\nthat allows us to write and execute\\nPython and PyTorch code in our browser.\\nLet's take a look at where we'll be writing\\nand running our code.\\nTo get started, let's head on to colab.research.google.com.\\nTo use Colab, you only need to have a Google account.\\nIf you haven't signed into your Google account,\\nyou'll be prompted to sign in or to create one.\\n\\nIn case you already signed into your Google account,\\nyou'll see a default screen,\\nand it gives you a good set\\nof beginner-friendly instructions on how to use it.\\nThe very first thing you're going to do\\nis to create a new notebook.\\nClick on File, New notebook,\\nand it creates an empty blank template.\\nAnd we can rename it to 0103.\\nColab is great for beginners\\nor anyone who wants to start playing\\nwith machine learning and deep learning models,\\nbecause it allows us to have a GPU for absolutely free.\\n\\nWell, there are limits in GPU usage per week,\\nbut it will be sufficient for our needs.\\nTo verify our configuration,\\nyou're going to import the PyTorch library,\\nprint the installed version,\\nand lastly, check to see if you're using a GPU.\\nWe got false, as by default,\\nour Colab notebook does not use the GPU.\\n\\nWe are going to change the runtime type\\nso that we use a hardware accelerator.\\nWe want to run this code using GPUs,\\nbecause if you just use CPUs, it will consume too much time.\\nNow, let's get our environment set up.\\nTo change CPU to GPU, Let's select Runtime.\\nAnd under it change runtime type.\\nThen select T4 GPU\\nfrom the hardware accelerator dropdown menu, and click Save.\\n\\nLet's run the cell again by selecting the cell\\nand pressing Shift + Enter.\\nNow we have a true as the output. Great.\\nWe have verified that PyTorch is installed\\nand we have a GPU available.\\nOne more thing I want to set up\\nis line numbers for each code\\nso you can easily follow along.\\nClick on Tools, click on Settings,\\nand select the Editor tab.\\nThere is an option, right, that says Show line numbers.\\n\\nLet's select that option and click on Save.\\nThere are some other options like font and indentation\\nthat you can customize later if you want.\\nSome may wonder why we don't run PyTorch locally\\nor in the cloud.\\nSetting up your local machine or your cloud environment\\nis beyond the scope of this course,\\nand it would require much more time and even money\\nas GPU instances or cloud platforms like AWS, GCP,\\nor Azure have additional costs.\\n\\nNow that we have our playground ready,\\nlet's get to know tensors.\\n\"}],\"name\":\"1. PyTorch Overview and Introduction to Google Colaboratory\",\"size\":16664192,\"urn\":\"urn:li:learningContentChapter:3888263\"},{\"duration\":548,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3890259\",\"duration\":204,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to tensors\",\"fileName\":\"2706322_en_US_02_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":245,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn the fundamental data structure used in PyTorch called the tensors. It allows you to understand how PyTorch handles and stores the data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6158463,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] A neural network knows how to deal with data\\nstored as floating-point numbers.\\nInputs are real-world data in many forms.\\nLet's take for example, image recognition.\\nA deep neural network takes images\\nand codes them in a digestible way\\nand then decodes them back to some output,\\nfor example, text.\\nIt happens in multiple stages,\\nand partially transformed data between stages\\nis a sequence of intermediate representations.\\n\\nIn our case, for images,\\nlet's think of a picture of a person.\\nEarly representation can be edge detection.\\nIntermediate representation can capture ears, nose, or eyes.\\nEach intermediate representation\\nis a collection of a floating-point numbers\\nresulting from multiplying the input\\nwith the previous layer's weights.\\nTo handle and store the data\\nin all stages of deep learning,\\nPyTorch uses an essential data structure called a tensor.\\n\\nSo inputs, intermediate representations, and outputs\\nare all stored as tensors.\\nBut what is a tensor?\\nFrom mathematics, we can look at tensors\\nas generalizations of scalars, vectors, and matrices\\nto any dimension.\\nSo we can define the scalar as a zero dimensional,\\nor 0-d tensor,\\na vector as one dimensional, or 1-d tensor,\\na matrix as a 2-d tensor,\\nand a matrix stack in a third dimension,\\nfor example, when you need to process data\\nthat has a time element as 3-d tensor.\\n\\nGenerally, tensors can have an arbitrary number\\nof dimensions.\\nIn PyTorch, a tensor is a multi-dimensional array\\ncontaining elements of a single data type.\\nIf you have worked with a NumPy library,\\nthen tensor reminds you of the fundamental object in NumPy\\ncalled ndarray, which is defined as an n-dimensional array,\\nhomogenous array of fixed-size items.\\nYou may wonder,\\nwhat is the difference between tensor and ndarray?\\nTensors in PyTorch are similar to NumPy's ndarrays.\\n\\nBut tensors have additional advantages\\nthat make them more suitable for deep learning calculations.\\nSome of those advantages are\\ntensor operations are performed significantly faster\\nusing graphical processing units, or GPUs.\\nTensors can be stored and manipulated at scale\\nusing distributed processing on multiple CPUs and GPUs\\nand across multiple servers.\\n\\nAnd tensors keep track of the graph of computation\\nthat created them.\\nWith all those mentioned,\\nyou can see that tensors are much more than a special sort\\nof multidimensional arrays.\\nTensors interact with each other\\nsuch that transforming tensors as a whole\\nmeans that each tensor\\nfollows a particular transformation rule.\\nWith that in mind, let's head on to practical examples\\nand learn how to create tensors.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3887273\",\"duration\":108,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating a tensor CPU example\",\"fileName\":\"2706322_en_US_02_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"01:18 - Pause 4 action mogrt\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":144,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn a CPU example of how to create a tensor and perform a tensor operation so you can use a built-in method on the tensor itself.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2925364,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's create our first tensors\\nusing the PyTorch library.\\nWe're going to open a Google Colab notebook named 02_02.\\nFirst, we're going to import the PyTorch library.\\nNext, we're going to create two tensors\\nfrom two-dimensional lists,\\nand let's call them first_tens and second_tens.\\nTensor data type will by default be derived\\nfrom the input data type,\\nand the tensor is allocated to the CPU device.\\n\\nNow, let's apply a simple arithmetic operation.\\nIn our case, addition.\\nWe can do that using the plus operator\\nand store the result in the third tensor,\\nwhich we'll call add_tens.\\nLet's print the new tensor, add_tens, and print its size.\\nAs you can see,\\nit's the sum of tensors first_tens and second_tens,\\nand you can see its dimensions are two by four.\\nNow, let's try subtraction.\\n\\nHow would you calculate it?\\n(upbeat music)\\n(buzzer buzzing)\\nThat's right, by using the minus operator.\\nNow, let's do that and store the result in sub_tens.\\nFinally, let's print the latest tensor, sub_tens,\\nwhich we can see is the difference\\nof the first_tens and second_tens,\\nand we print the size of sub_tens.\\nAs you can see,\\nboth add_tens and sub_tens are tensor objects themselves,\\nand we can use size method to return its matrix dimensions,\\nnamely two by four.\\n\\nNow that we have seen how to create a tensor\\nthat is allocated to the CPU device,\\nwe'll explore the GPU example next.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3889244\",\"duration\":106,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating a tensor GPU example\",\"fileName\":\"2706322_en_US_02_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":113,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn a GPU example of how to create a tensor and perform a tensor operation so you can use a built-in method on the tensor itself.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2738242,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] GPUs were originally developed\\nfor rendering computer graphics.\\nSo when I was a kid, every gamer talked about them.\\nMeanwhile, with the need for speed\\nof computational processing involving neural networks,\\nthey now play a crucial role in deep learning.\\nIn PyTorch, we have the CUDA library\\nthat is instrumental in detecting, activating,\\nand harnessing the power of GPUs.\\nJust as for the CPU, we'll explore a simple example.\\n\\nFirst, before using CPUs,\\nwe are going to check if they are configured\\nand ready to use.\\nWe are going to import PyTorch\\nand print the PyTorch version.\\nNext, by calling the torch.cuda.is_available function,\\nwe'll move the tensors to the GPU device\\nif one is available.\\nSo, our device has a GPU support.\\nWe are going to create two tensors\\nand let's just call them tens_a and tens_b.\\n\\nNow let's apply a simple arithmetic operation,\\nin our case, multiplication.\\nWe can do that using the asterisk operator\\nand store the result in the third tensor,\\nwhich we'll call multi_tens.\\nNotice that the output tensor is also allocated to the GPU.\\nWhen we run the code,\\nwe get the output, in our case, device='cuda:0'\\nwhich indicates that the first GPU is being used.\\n\\nIn the case our device contains multiple GPUs,\\nthis way, we can control which GPU is being used.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3885246\",\"duration\":130,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Moving tensors between CPUs and GPUs\",\"fileName\":\"2706322_en_US_02_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":212,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Explore the torch.to() method that allows you to move an existing tensor from CPU to GPU device.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3628251,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] A few important reasons exist\\nfor moving the tenors between CPUs and GPUs.\\nLet's explore them and see how to transfer\\nthe data from the CPU to GPU.\\nBy default, in PyTorch, all the data are in the CPU.\\nIn case we are training neural network, which is huge,\\nwe prefer to use GPU for faster training.\\nFor example, if we have high dimensional tensors\\nthat represent images, their computation intents,\\nand take too much time if run over the CPU.\\n\\nSo we need to transfer the data from the CPU to the GPU.\\nAdditionally, after the training,\\nthe output tensors are produced in GPU.\\nSometimes the output data requires pre-processing.\\nSome pre-processing libraries don't support tensors,\\nand expect an NumPy array.\\nIn that case, NumPy supports only data in the CPU,\\nso there is a need to move the data from the CPU to the GPU.\\n\\nLuckily, tensors can be moved easily from the CPU\\nto GPU device with the torch to method.\\nWe can call this method in one of the three ways.\\nFirst way, tensor.cuda,\\nor second way, tensor to cuda,\\nor the third way, tensor to cuda 0.\\nWhen we need to move the tensors in the opposite direction\\nfrom the GPU to CPU, there are two possible cases.\\n\\nFirst one, tensor with required grad = false.\\nIn the first case, when required grad = false,\\nthen we use the methods tensor.cpu.\\nIn the second case, when required grad = true,\\nthen we use method, tensor.detach.cpu.\\nNow, that we have mastered methods to move the tensors,\\nlet's head on to explore how to create tensors.\\n\"}],\"name\":\"2. Tensors\",\"size\":15450320,\"urn\":\"urn:li:learningContentChapter:3887274\"},{\"duration\":941,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3884263\",\"duration\":219,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Different ways to create tensors\",\"fileName\":\"2706322_en_US_03_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Overlay - https://pytorch.org/docs/stable/torch.html\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":250,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Sometimes you may want to construct tensors from preexisting data stored in array-like structures such as lists, tuples, scalars, serialized data files, or NumPy arrays. Learn how to achieve that.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7708833,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Sometimes we want\\nto construct tensors directly from Python objects\\nlike lists, tuples or NumPy arrays.\\nWe can also create tensors\\nby using different functions in case we want\\nto generate a particular type of tensor.\\nLet's first learn how\\nto create a tensor from a Python list and a tuple.\\nTo do that, open notebook 0301,\\nwe have already imported PyTorch and numpy as np.\\nI will create a tensor from a list\\nand call it tensor from list by using a torch.tensor method,\\nand this list will contain integers from one to five.\\n\\nThis method is used\\nto create a tensor from an existing data structure.\\nNow to create a tensor from a tuple.\\nTuples are lists that are immutable,\\nmeaning that once defined,\\nthe individual elements of a tuple cannot be changed.\\nYou can easily spot a difference between a tuple and a list\\nbecause a tuple is written in a sequence of numbers\\nand closed in the round parenthesis.\\nLet's call this tensor tensor_from_tuple.\\n\\nWe will call the same torch.tensor method,\\nbut this time we'll pass a tuple instead of a list.\\nGo ahead and display these two tensors.\\nNow to create a tensor from a NumPy array,\\nwe will call torch.tensor\\nand pass a NumPy array instead of a list or a tuple.\\nLet's display our tensor.\\nPyTorch also has different functions that we can use\\nto create an initialized tensors.\\n\\nThe most useful ones are torch.empty(), torch.ones()\\nand torch.zeros().\\nAs you can see, each one of them is used\\nwith a torch namespace.\\nThese functions take integers as the first two arguments,\\nwhich specified the size of a tensor.\\nYou can easily guess what they do from their name.\\nAs you can see in our example using the empty function,\\nwe create a tensor from uninitialized elements.\\nIf you use a torch.zero function, it creates a tensor\\nwith elements initialized with zeros\\nand torch.ones function creates a tensor\\nwith all elements initialized to ones.\\n\\nSometimes you want to initialize the tensor\\nwith random values,\\nand PyTorch has a few useful functions called torch.rand,\\ntorch.randn and torch.randint.\\nAs you can see in our code, the difference between them\\nis that the first one returns a tensor filled\\nwith random numbers from uniform distribution.\\nThe second one used normal distribution,\\nand the third one returns the tensor filled\\nwith random integers using uniform distributions.\\n\\nThese functions take integers as first two arguments,\\nwhich specify the size of a tensor.\\nWe can also pass in an optional arguments data type\\nor dtype and device.\\nOften we want to create tensors that have similar properties\\nto another tensor, including the dtype device\\nand layout properties to facilitate calculations.\\nFor example, torch.ones_like will create a tensor\\nwith all ones with a dtype device\\nand layout properties of tensor X.\\n\\nYou can learn more about additional functions used\\nfor tensor creation\\nat the official PyTorch documentation page.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3891286\",\"duration\":131,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Tensor attributes\",\"fileName\":\"2706322_en_US_03_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":233,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to effortlessly find information about tensors by accessing their attributes.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3786168,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Knowing attributes\\nsuch as device location, data type, dimension, and rank\\nis very important\\nwhen you're doing computations with tensors.\\nAs you have learned, a tensor is a container\\nwith a fixed size that has elements of the same type.\\nYou can think of a tensor as a multidimensional array.\\nWe have already imported PyTorch,\\nand now let's create our one-dimensional tensor\\ncalled first_tensor.\\nWe'll call the device function\\nto find out the tensor's device location,\\nmeaning the device on which the tensor is stored.\\n\\nHere we run the cell\\nand see that our tensor is on the CPU.\\nTo check its data type, we'll call the dtype function.\\nAnd when we ran the cell we got int64,\\nmeaning 64-bit integer.\\nWhen we want to find out the shape of the tensor,\\nwe can always call its shape function\\nand it gives us back the dimensions.\\nAnother useful function is ndim,\\nwhich gives us the number of tensors, dimension, or rank.\\n\\nLet's go ahead and run the code.\\nAnd we get one, as we have expected,\\nsince we have a one-dimensional tensor.\\nWhat if we create a two-dimensional tensor?\\nLet's create one and name it second_tensor.\\nNow we can check its attributes by running the cells.\\nAs you can see, when accessing object attributes,\\nwe don't include parenthesis as we do with the class method.\\nThere are many more variable functions to access attributes,\\nsuch as requires_grad,\\nwhich is used in automatic differentiation\\nor short autograd.\\n\\nYou can find out more\\nby heading onto the documentation page\\non the following link.\\nNow that we have learned the tensor attributes,\\nit's time to move to the next section\\nand explore tensor data types.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3885245\",\"duration\":196,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Tensor data types\",\"fileName\":\"2706322_en_US_03_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"01:48 - Pause4Action Mogrt\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":298,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"When creating tensors, you can specify the data type. It is also possible to cast a tensor to a new data type. In this video, explore how to achieve that.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6074139,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] What sets PyTorch apart from other libraries\\nis the usage of tensors, multidimensional arrays\\nthat can seamlessly perform fast operations on GPUs\\nand distribute operations on multiple machines\\nwhile keeping track of the graphic computations\\nthat created them.\\nLet's look at the various tensor data types\\nthat are used in computations happening in neural networks.\\nWe'll start by importing PyTorch\\nand creating a couple of tensor objects.\\n\\nWe are going to create our first one-dimensional tensor\\nand call it int_tensor,\\nand specify the data type by using dtype as an argument\\nto the constructor.\\nWe want all tensor elements to be integer type\\nof eight-bit size,\\nso we'll type dtype=torch.int8.\\nLet's displace data type using dtype function.\\n\\nNow let's create a second one-dimensional tensor\\nand call it float_tensor.\\nWe want all tensor elements to be 32 floating points,\\nso we'll type dtype=torch.float32\\nand just display the data type.\\nWhat if we want to create a one-dimensional tensor\\nwith an integer type of 16 bits?\\nHow would you do it?\\n(light music)\\nThat's right, by specifying the data type\\nas dtype=torch.int16.\\n\\nAnd let's go ahead and create one.\\nSometimes we need to cast the tensor to new data type.\\nIt can be done in two different ways\\nby using the corresponding casting function\\nwhich is named after the data type you want to convert to.\\nFor example, in case you want to convert\\nint_tensor to float,\\nwe can do that by calling the float function.\\n\\nAnd when we display the data type,\\nwe can see that our short tensor\\nis the same type as the float tensor.\\nAnother way to cast a tensor to a new data type\\nis using two function; for example,\\nto cast short tensor from int 16 to int 8\\nand assign it to a new tensor called last_tensor,\\nwe'll type last_tensor = short_tensor.to(dtype=torch.int8).\\n\\nThe important thing to remember\\nis that when we are doing operations\\non and between tensors that have different data types,\\nPyTorch will automatically cast tensors\\nto a larger data type.\\nIt would be best if you always were mindful\\non which data type to choose.\\nThis can have a huge effect\\non memory consumption and precision,\\nwhich is especially relevant in deep learning algorithms.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3888261\",\"duration\":161,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating tensors from random samples\",\"fileName\":\"2706322_en_US_03_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":193,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Often, you need to initialize weights to random values or create random inputs with specified distributions. Learn functions that you can apply to create tensors from random data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5108856,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In deep learning,\\nwe often need to initialize weights to random values\\nor create random inputs with specified distributions.\\nPyTorch provides valuable functions\\nthat we can use for random sampling.\\nWith the power of these functions,\\nwe can generate values by drawing randomly\\nfor probability distributions, like normal distribution\\nand uniform distribution.\\nLet's explore the most useful ones.\\nWe have already imported PyTorch.\\n\\nLet's go ahead and create the two-dimensional tensors\\nusing the torch.rand function.\\nIt will create a tensor filled with random values\\nfrom a uniform distribution on the interval zero to one.\\nIt's going to be 3 by 3 shape,\\nwhich we define by passing it as an argument\\nto rand function and displaying it.\\nNow, if we rerun our code, you can see that, as expected,\\nwe get a different result each time.\\n\\nHowever, sometimes we want the same result.\\nTo achieve this, we can just call manual_seed function\\nbefore creating our tensor.\\nLet's add the line before.\\nGreat, now when we rerun the code,\\nwe obtain the same result.\\nIf we want to select random values\\nfrom a standard normal distribution\\nwith zero mean unit variance,\\nwe can use the torch.rand function.\\n\\nLet's create a 4 by 4 shape tensor using rand function.\\nWe use torch.randint function to create a tensor\\nfilled with random integers generated uniformly\\nbetween specified low and high values.\\nIt accepts three values as parameters.\\nThe first parameter is the lowest inclusive value,\\nthe second parameter is the highest exclusive value,\\nand the third parameter will pass the shape.\\nLet's provide the parameters minus 1, 10,\\nand it's going to be 3 by 3 shape.\\n\\nwhen creating tensors using sampling functions,\\nyou can also specify dtype and device and other parameters.\\nWhen creating tensors using sampling functions,\\nyou can also specify dtype and device and other parameters\\nin the case of using the more advanced distributions\\nlike multinomial or exponential distributions.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3887272\",\"duration\":125,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating tensors like other tensors\",\"fileName\":\"2706322_en_US_03_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":144,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn similarity functions so you can create\u2009and\u2009initialize\u2009a\u2009tensor\u2009that has similar properties to the existing tensor.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3751344,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Sometimes there is a need to create a tensor\\nthat has the same device, size, data type,\\nand layout properties as another preference tensor.\\nPyTorch has special functions that allow to achieve this.\\nYou can easily spot and memorize those functions\\nas they have the postfix like\\nas a part of the function name.\\nWe have explored the tensor creation functions,\\nempty, zero, ones, rand and rand int.\\nAnd all these functions have similar function,\\nwhich we call similarity functions with postfix like.\\n\\nSo empty has its pair empty like,\\nzeroes have a pair zeroes like and so on.\\nWe have already imported PyTorch.\\nWe'll start by creating a tensor using zeroes function,\\ncall it starting tensor and display it.\\nWe'll create a tensor with the same size\\nfilled with random values using torch.rand like function.\\n\\nNotice we are using manual seed\\nto have the same numbers generated\\neach time we rerun our code.\\nLet's create another two dimensional tensor.\\nThis time we want to have the same shape as starting tensor,\\nbut filled with ones.\\nWe will use ones like function.\\nAnd we get a tensor filled with ones.\\nNow, imagine you want to create a tensor\\nwith the same shape,\\nbut this time you want all elements\\nto be of a specific value.\\n\\nThis time we'll use a full like function.\\nProvide an input tensor\\nand pass number seven as a fill value.\\nGreat, now that we know what tensors are\\nand have seen different ways to create tensors,\\nlet's jump right into some practice\\nin a coder pad environment.\\nGive the challenge a go\\nand then come back to see how I solved it.\\nGood luck.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:6605e3623450c8a8f9db62b3\",\"duration\":600,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code challenge: Create tensors\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1006782\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:3890262\",\"duration\":109,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Create tensors\",\"fileName\":\"2706322_en_US_03_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":124,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn a solution for creating tensors from different data structures.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4100885,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(bright music)\\n- [Instructor] All right, so for this challenge,\\nwe needed to create four functions that return four tensors,\\nthe first two using the already provided Python list\\nand NumPy array,\\nand the next two by using PyTorch functions.\\nSo, here is how to solve this challenge.\\nSince Torch is not supported by CoderPad environment,\\nI have created a mock tensor class\\nthat has all required functions to solve this challenge.\\n\\nSo we have our first function,\\nwhich takes input list as parameter.\\nTo convert a Python list to a tensor,\\nwe'll use a built-in function\\nfrom the Torch Library called torch.tensor.\\nThis function creates a tensor\\nwith elements in the same data type as the Python list.\\nIn our case, integers.\\nNext, for a second function,\\nwhich takes an input array\\nand needs to return a two-dimensional tensor,\\nwe'll again use the torch.tensor function\\nand pass in an input array as a parameter.\\n\\nTo create a tensor of a shape three by four,\\nwith all its elements being one,\\nwe'll use torch.once function.\\nWe'll pass a tuple {3, 4} as a parameter.\\nFor the last task\\nwhere we need to create a shape four by five\\nwith all elements being five,\\nwe'll call full function.\\nWe use it by passing a tuple {4, 5} as a parameter\\nand 5 as a default value.\\nNow, go ahead and click on test my code.\\n\\nWe got the right answer,\\nand you can see a printout of all of our four tensors.\\n\"}],\"name\":\"3. Creating Tensors\",\"size\":30530225,\"urn\":\"urn:li:learningContentChapter:3886262\"},{\"duration\":1205,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3885244\",\"duration\":275,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Tensor operations\",\"fileName\":\"2706322_en_US_04_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Start with WSC, PUWSC - video and audio are on separate files. PU replaces content at 02:45 and start with WSC, separate audiofile\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":false,\"rawDurationSeconds\":939,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video explore a set of tensor operations that allow you to access and transform your tensor data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9202498,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Managing and transforming large data sets in deep learning\\ncan be frustrating.\\nYou're likely wasting time and computational power\\nif your tensor operations aren't optimized,\\nmaking the task of handling data\\ncumbersome and less effective.\\nLet's learn to efficiently split, combine,\\nand manipulate tensors,\\nwhich lead to more robust and scalable deep learning models.\\nLet's perform indexing and slicing on tensors.\\nIf you have used NumPy, I have great news.\\n\\nIndexing and slicing of tensors\\nis done the same way as per NumPy arrays.\\nIf you're new to this, don't worry.\\nIndexing means accessing a single element of a tensor,\\nand slicing means accessing a range of elements.\\nNow let's create a one-dimensional tensor,\\ncall it 1-dim-tensor.\\nIf we want to access and display\\na third element of our tensor,\\nwe just have to type print 1-dim-tensor two.\\nWhen we run the cell, notice we got tensor three.\\n\\nIn PyTorch, when we are passing the tensor element\\nto a function like print, we first have to convert it\\nto a Python value by calling the item function.\\nLet's do that and rerun our code.\\nAnd you can see, we got the value as a number.\\nNext, let's see how slicing is done.\\nThere is a format that we use start column and column step.\\nWe need to specify a start index\\nand the end index separated by column.\\n\\nThe step is optional,\\nand it defines the number of indexes to move forward\\nwhile slicing an object.\\nIf the step is not indicated,\\nit means moving without skipping any index.\\nThe output tensor will contain all the elements\\nfrom the starting index including,\\nto the ending index excluding.\\nSo, let's say we want to get elements\\nbetween the first and the fourth element.\\nWe will type 1-dim-tensor, one column four.\\n\\nNext, let's see how to do indexing and slicing\\non a two-dimensional tensor.\\nWe'll create a four by six tensor called 2-dim tensor.\\nTo access the fourth element of the second array,\\nwe'll type:\\n2-dim tensor one three.\\nIn the case when we want to access two or more elements\\nat the same time, we'll perform slicing.\\nLet's say we want to access\\nthe first three elements of the first row,\\nand the first four elements of the second row.\\n\\nWe'll type print first three elements on the first row,\\n2-dim tensor, zero zero three.\\nAnd we'll print first four elements of the second row,\\n2-dim tensor one zero four.\\nSometimes we want to use indexing\\nto extract the data that meets some criteria.\\nFor example, if we would like to keep only elements\\nthat are less than 10, we would type 2-dim tensor,\\n2-dim tensor less than 11, and display it.\\n\\nPyTorch has many different built-in functions\\nthat we can use to access, combine, and split tensors.\\nLet's use the two most common ones in our example.\\nIf we want to combine tensors,\\nthere is a function called torch stack.\\nIt concatenates a sequence of tensors along a new dimension.\\nLet's create a new tensor called combined tensor.\\nLet's combine our two-dimensional tensor\\nusing torch stack function.\\n\\nThe second useful function is for splitting tensors,\\nand it is called torch and bind.\\nIf we want to split our two-dimensional tensor\\ninto four tensors, let's call them first tensor,\\nsecond tensor, and so on.\\nWe will type: first tensor, second tensor, third tensor,\\nfourth tensor equals torch unbind, 2-dim tensor.\\nAnd display it.\\nAs you can see, torch and bind function splits the tensor\\naccordingly to the number of rows,\\nbut we can select along which dimension we want it to work.\\n\\nNow, if we put dim equals one,\\nwhich corresponds to the column dimension,\\nthe tensor will split into six smaller tensors,\\nwith the corresponding values of each column.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3891287\",\"duration\":216,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Mathematical functions\",\"fileName\":\"2706322_en_US_04_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":259,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn a set of built-in math functions. It is extremely important to know them as deep learning is based on mathematical computations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6007871,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Mathematical functions\\nare one of the most useful functions in PyTorch.\\nLet's explore five types of built-in math functions,\\npointwise operations, reduction functions,\\ncomparison functions, linear algebra operations,\\nand spectral and other math computations.\\nDon't be overwhelmed with these fancy names.\\nLet's figure them out together.\\nPointwise operations are named after their function\\nas they perform an operation\\non each point in the tensor individually\\nand return a new tensor.\\n\\nSome commonly used pointwise operation\\ninclude some basic math functions,\\nadd(), mul(), div(), neg(), and true_divide(),\\nfunctions for truncation,\\nceil(), clamp(), floor(), et cetera,\\nlogical functions, trigonometry functions, and so on.\\nThe second type of mathematical functions\\nare reduction operations.\\nThey're also named after their role\\nas they reduce numbers down to a single number\\nor a smaller set of numbers\\nthat results in reducing the dimensionality\\nor rank of the tensor.\\n\\nReduction operations include statistical functions\\nsuch as mean, median, mode.\\nComparison functions, as the name suggests,\\ncompare all the values between the tensor\\nor compare values of two different tensors.\\nThey also include functions\\nto find the minimum or maximum value, sort tensor values,\\ntest sensor status or condition, or similar.\\nLinear algebra functions enable matrix operations\\nand are essential for deep-learning computations.\\n\\nThese include functions for matrix computations\\nand tensor computations.\\nThe last type of mathematical function\\nis a spectral and other math operations.\\nThese functions are useful\\nfor data transformations or analysis.\\nAs there are so many math functions for different purposes,\\nwe won't be covering all of them with code examples.\\nLet's explore some of them\\nby heading on to the Colab notebook.\\nWe have already imported PyTorch.\\n\\nLet's create two one-dimensional tensors\\nand call them a and b.\\nWe are going to add, multiply,\\nand divide tensor a with tensor b\\nby using add(), mul(), div() functions,\\nand display the results.\\nNext, let's see the most useful reduction functions.\\nThese are torch.mean(), torch.median(),\\ntorch.mode(), and torch.std().\\nAs their name suggests,\\nthey're used for calculating the mean, the median, the mode,\\nand the standard deviations\\nof all elements between the tensor.\\n\\nLet's create a tensor c that contains floating points.\\nWe are using floating points\\nas torch.mean() accepts only tensors\\nwith floating points data type.\\nNext, let's calculate these reduction functions\\nand display them.\\nAs you can see when we run the code,\\ntorch.mode() function returns a namedtuple\\ncontaining mode values and indices,\\nrepresenting the index occurrence of mode in each row.\\n\\nKeep in mind,\\nwe have only scratched the surface of math functions\\nas there are many more valuable functions.\\nYou can discover them later on your own.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3887271\",\"duration\":236,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Linear algebra operations\",\"fileName\":\"2706322_en_US_04_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"00:35 - Overlay - https://pytorch.org/docs/stable/linalg.html\\n01:04 - overlay - NumPy Essential Training\\n02:39 - PAUSE4ACTION\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":288,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Many computations, including optimization algorithms and gradient descent, use linear algebra to implement the calculations. Explore the most important built-in linear algebra operations which you can use.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7828922,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Linear algebra is used heavily\\nin deep learning computations,\\nfrom different optimization algorithms to gradient descent.\\nLet's explore the most important ones.\\nPyTorch has a module called torch lineal that contains a set\\nof built-in linear algebra functions\\nthat are mostly based on the basic linear algebra,\\nsubprograms or Blast and Linear algebra package\\nor APEC standardized libraries.\\nYou can find a complete list\\nof functions on the following PyTorch linear algebra\\ndocumentation page.\\n\\nIf you have already mastered Numpy,\\nyou will quickly learn these functions as most\\nof the functions from Numpy's linear algebra module are\\nthe same, but they're extended with accelerator\\nand L2 grad support.\\nOnly a few of the functions will be completely new.\\nIf you haven't explored the linear algebra capabilities\\nof Numpy, you can take my course, Numpy Essential Training.\\nWe have already imported PyTorch.\\n\\nThe first useful function performs matrix products\\nof two tensors, and it's called Torch Matmul.\\nWe'll create two tensors,\\nand then compute the dot product\\nof them using torch matmul and display it.\\nWhen we run our code, we see\\nthat the result is a zero dimensional tensor or scaler.\\nFor the next example, let's compute a product of two,\\ntwo dimensional tensors or matrix matrix products.\\n\\nLet's create a first tensor that has a shape, two by three\\nand a second tensor that has a shape three by two,\\nand after we perform a matrix product\\nand display the output, you can see we got a two\\nby two two dimensional tensor or matrix.\\nWhen performing matrix multiplication, it is important\\nthat matrices have compatible dimensions.\\nThis means that the number\\nof columns in the first matrix is the same as the number\\nof rows in the second matrix, so in our case,\\nwe multiplied two by three with three by two matrix\\nand got two by two matrix.\\n\\nWe could have also multiplied the two by three matrix\\nwith a three by five matrix.\\nWhat would be the dimension\\nof the result matrix in this case?\\n(upbeat music)\\nYou are right.\\nThe result matrix would be two by five matrix.\\nThere is a second function\\nfor calculating the matrix product of two tensors,\\ncalled torch mm.\\nUnlike torch matmul, it doesn't support broadcasting.\\n\\nIn deep learning, we often have\\nto calculate the matrix product\\nof in two dimensional tensors.\\nTo do that, we can use torch.linalg.multi_dot function.\\nLet's create five two dimensional tensors using torch render\\nand then call torch linalg.multi_dot function.\\nWhen we run our code, we see we got two by seven matrix\\nas a result.\\nOne of the most useful types\\nof matrix decompositions in deep learning is the eigen\\ncomposition, which decomposes a square matrix into eigen\\nvectors and eigen values.\\n\\nComputing eigen values\\nand eigen vectors is simply done\\nby calling the torch ike function.\\nLet's create a four by four square matrix A,\\ndisplayed and compute the Eigen values\\nand eigen vectors of the matrix.\\nNow that you have a basic foundation on tensors,\\nlet's head on to explore how to use them\\nto perform deep learning development.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3890258\",\"duration\":365,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Automatic differentiation (Autograd)\",\"fileName\":\"2706322_en_US_04_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"01:55. - Overlay - Machine Learning Foundations: Calculus\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":472,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn the backward() function. It is important because it is used to differentiate and compute gradients of tensors based on the Chain rule.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10276293,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] When we train a neural network,\\nwe follow two simple steps: forward propagation\\nand backward propagation.\\nIn forward propagation, we input data into the network.\\nIt runs the input data through each of these functions\\nand makes the best guess about the correct output.\\nThen we compare the predicted output with the actual output.\\nWe calculate the lowest function to determine the difference\\nbetween the predicted output and actual output.\\nIn the next step called backward propagation,\\nthe neural network adjusts its parameters\\nproportionate to the error in its guess.\\n\\nSo after we find the loss function,\\nwe take the derivative of the loss function\\nin terms of the parameters of our neural network.\\nLastly, we iteratively update\\nthe weight parameters accordingly\\nso that the loss function\\nreturns the smallest possible loss.\\nWe call this step iterative optimization\\nas we use an optimizer to perform the update\\nof the parameters.\\nWe call this process gradient-based optimization.\\nAutomatic differentiation, or auto diff,\\nis a set of techniques that allows to compute gradients\\nfor arbitrary complex loss functions efficiently.\\n\\nIf this sounds terrifying,\\nremember, we are just calculating partial derivatives.\\nIf you still need help with the concept of derivatives,\\nI highly recommend\\nthe Machine Learning Foundation calculus course.\\nWe can calculate derivatives in a few different ways.\\nThe first one is numerical differentiation,\\nwhich follows the definition of a derivative.\\nA derivative of Y with respect to X\\ndefines the rate of change of Y with respect to X.\\n\\nWe can write it down using the following formula.\\nThe cons are the computation costs,\\nwhich increase as we increase the number of parameters\\nin the loss function, the truncation errors,\\nand the round-off errors.\\nAnother method is symbolic differentiation\\nwhich is done in calculus.\\nBasically, we are using a set of rules,\\nmeaning a set of formulas that we can apply\\nto the loss function to get the gradients.\\n\\nFor example, if we want to calculate\\nthe derivative of a function, f of x equals 3x squared\\nminus 4x plus 5.\\nWhen we apply the symbolic rules,\\nwe get derivative of f equals 6x minus 4.\\nThe cons are it is limited\\nto the already defined symbolic differentiation rules,\\nso it cannot be used\\nfor differentiating a given computational procedure\\nand the computation costs,\\nas it can lead to an explosion of symbolic terms.\\n\\nThe way to avoid the above issues\\nis to use automatic differentiation.\\nEvery complex function that we want to differentiate\\ncan be expressed as a composition of elementary functions.\\nFor those elementary functions,\\nwe could apply symbolic differentiation,\\nwhich would mean storing and manipulating\\nsymbolic forms of derivatives.\\nBy using automatic differentiation,\\nwe don't have to go through the tedious process\\nof simplifying the expressions.\\n\\nInstead, we can just evaluate a given set of values.\\nAnother benefit of automatic differentiation\\nis that our function can contain if-else statements,\\nfor loops, or recursion.\\nTo understand automatic differentiation,\\nwe can represent the flow using a computational graph.\\nIt's a directed graph,\\nand nodes represent mathematical operations.\\nFor example, if you have a function f of x, y, z\\nequals x minus y multiplied by z,\\nand the variables x, y, and z are x equals 2,\\ny equals 1, and z equals 5.\\n\\nOn our graph, we can label an intermediate variable 'a'\\nthat will store the computed value of x minus y,\\nand final variable f which will store the final value:\\nx minus y multiply by z.\\nAutograd package provides automatic differentiation\\nfor all operations on tensors.\\nWhen we want to train or optimize the neural network,\\nthis requires computing the gradient.\\nIn the forward pass step, we substitute the variables\\nand get the final value f:\\n2 minus 1 multiplied by 5 equals 5.\\n\\nIn the next step, using automatic differentiation,\\nwe find gradients of f with regards to the input variables.\\nWe'll calculate gradients of the loss function\\nwith respect to the weights by using chain rule.\\nNow, enough math.\\nLet's see this simple implementation\\nof automatic differentiation using autograd package.\\nPyTorch autograd package can automatically differentiate\\nall tensor operations.\\nIt's super helpful for backpropagation computations\\nbetween our neural networks.\\n\\nPlus, we can easily access individual gradients\\nthrough a variables grad attribute.\\nLet's head on to Colab notebook.\\nWe'll define tensors and set the required gradient\\nequal to true to enable gradients to be computed.\\nWe can do that by assigning the keyword\\nfor requires grad as true.\\nNow we are computing the intermediate variable 'a'\\nthat is equal to x minus y,\\nand then computing to function f.\\n\\nNext, let's go ahead and call backward function,\\nand then the module computes\\nall the backpropagation gradients automatically.\\nAnd finally, let's print gradients\\nby accessing them using a variables grad attribute.\\n\"},{\"urn\":\"urn:li:learningContentAssessment:6605e382498e6fccaed114c3\",\"duration\":600,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"</> Code challenge: Split tensors to form new tensors\",\"externalAsset\":{\"coderPadCampaign\":\"urn:li:coderPadCampaign:1006784\"},\"coderPadCampaign\":null},{\"urn\":\"urn:li:learningContentVideo:3888259\",\"duration\":113,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Split tensors to form new tensors\",\"fileName\":\"2706322_en_US_04_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":145,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, learn one solution for splitting tensors to form new tensors.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4212748,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Lecturer] For this challenge,\\nwe need to implement four functions\\nusing PyTorch building functions.\\nHere is how we solve this challenge.\\nOur task involves splitting two tensors, x and y,\\nusing PyTorch chunk and split functions.\\nWe use the chunk function when we want to divide the tensor\\ninto specified number of smaller tensors.\\nIt attempts to divide them as evenly as possible.\\n\\nFor the first implementation, I have called chunk function\\nwith parameter four and a dimension value equal to zero.\\nThis means we are dividing tensor x\\ninto four smaller tensors along the first dimension or rows.\\nYou can think of it as splitting the matrix\\ninto four smaller matrices,\\neach with roughly the same number of rows.\\nFor the second task, I've used chunk four,\\nwhich splits one dimensional tensor y\\ninto four smaller 1d tensors.\\n\\nSince y has 16 elements, each chunk will have four elements.\\nFor third task, we use split\\nsince we need more control over the sizes\\nof the resulting tensors.\\nIn this case, we specified the exact size of each chunk.\\nWe split x into two parts, the first with five rows\\nand second with three rows along first dimension.\\nLastly, we split y into three chunks\\nwith the specified lengths.\\n\\nThe first chunk has four elements\\nand the next two have six elements each.\\nFinally, if you go ahead and click on Test My Code,\\nyou can see that that is the right answer,\\nand we can scroll through\\nand see all of our modified tensors.\\n\"}],\"name\":\"4. Manipulate Tensors\",\"size\":37528332,\"urn\":\"urn:li:learningContentChapter:3885249\"},{\"duration\":1280,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3887270\",\"duration\":151,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to the DL training process\",\"fileName\":\"2706322_en_US_05_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Append root with part 2\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":178,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In this video, explore the basic pipeline used to train, test, and deploy your deep learning models.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4345518,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] There are different ways\\nto build your deep learning model.\\nYou can achieve it using supervised learning,\\nunsupervised learning, or semi-supervised learning.\\nEither way, you decide you're going to use the same pipeline\\nto train, test, and deploy your deep learning model.\\nThe process begins with the data preparation stage.\\nAs it's name suggests, we load the generic data,\\nwhich can be in many different formats,\\nsuch as text, images, videos, audio files, et cetera,\\nfrom an external source,\\nand we convert it to numeric values\\nsuitable for model training.\\n\\nThese numeric values are in form of tensors.\\nThen tensors need to be pre-processed during transforms,\\nand we group them with batches\\nthat can be passed into the model.\\nThe second stage is the model development stage\\nthat consists of three parts,\\ndesigning the model,\\ntraining the model using training data,\\nand testing its performance.\\nWe take the data set and split it into three data sets,\\ntraining data, validation data, and testing data.\\n\\nWhen we design the model,\\nwe use training data to train its parameters.\\nThe next step is the testing step\\nwhen we perform back propagation\\nand validate the model by passing invalidation data,\\nmeaning we measure model's performance against unseen data\\nand tune in hyperparameters.\\nBut what is the difference\\nbetween training data and validation data?\\nOne of the common problems in deep learning is overfitting.\\nBasically, model becomes really good\\nat recognizing what it has been trained on,\\nbut cannot recognize examples it hasn't seen.\\n\\nIn order to prevent that, we use the validation set.\\nValidation is a crucial step\\nin measuring your model's performance.\\nDuring this process,\\nyou can evaluate training data against validation data\\nthat has never been used.\\nIn the last step called model deployment,\\nyou can save the model to the file\\nor deploy the model to a product or service.\\nThe model is usually deployed to a production environment\\non a cloud server or to an edge device.\\n\\nNow that we are familiar with the deep learning process,\\nwe can expand on the data preparation step in the next video\\nand see how it's done on a simple network.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3889243\",\"duration\":97,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data preparation\",\"fileName\":\"2706322_en_US_05_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":111,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn the steps of data preparations and why they are important.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2701886,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Data preparation is the first step\\nin developing a deep learning model.\\nThis step consists of loading the data, applying transforms,\\nand batching the data using PyTorch built-in capabilities.\\nWe won't carry how to generate a good dataset,\\nas we use an existing popular academic dataset\\ncalled CIFAR-10, developed by researchers from the Canadian\\nInstitute for Advanced Research, or short, CIFAR.\\n\\nCIFAR-10 dataset is a subset\\nof a much larger dataset with 80 million images in it.\\nIt consists of 60,000 small color photographs of objects\\nfrom 10 classes divided into 50,000 training images\\nand 10,000 test images.\\nHere is the table with class labels\\nand their associated integer values.\\nWe use a Python library called Torchvision,\\nas it has classes that support computer vision.\\n\\nThe Torchvision datasets module provides several\\nsubclasses to load image data from standard data sets,\\nsuch as our CIFAR-10 dataset.\\nTo create a training dataset using the existing CIFAR-10\\ndataset we'll import the torch and then import the CIFAR-10\\ndataset by typing from Torchvision dataset import CIFAR-10.\\nI'll show you how to load, summarize,\\nand display the training and testing dataset.\\n\\nSo let's dive into a mini deep learning project.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3888260\",\"duration\":157,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data loading\",\"fileName\":\"2706322_en_US_05_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Append with PU\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":false,\"rawDurationSeconds\":201,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Explore built-in classes and utilities for loading various types of data and learn how to apply them to pull the data from its source and create dataset objects.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4792123,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We can load and convert different types\\nof data into format that are ready for training\\nand if it's often the most time consuming task.\\nLuckily, PyTorch has two important standard conventions\\nfor interacting with data called datasets and data loaders.\\nA dataset is a Python class that stores the samples\\nand their corresponding labels,\\nand the data loader feeds the data from the dataset\\ninto the network.\\nLet's take a look at our dataset\\ncalled the CIFAR-10 dataset.\\n\\nAs our first baby steps into deep learning,\\nwe'll run a deep neural network\\nthat was pre-trained on the object recognition task,\\nmeaning we'll be using it\\nto recognize the subject of an image.\\nWe have imported PyTorch as well as our dataset\\nin Matplotlib.\\nWe are going to use Matplotlib\\nto display some of the images.\\nHere is an example of how to load the training\\nand test data sets and print their shape.\\n\\nWhen we run the code, you can see\\nthat we have 50,000 images in the training set\\nand 10,000 images in the testing set,\\nand their dimensions are only 32 by 32 pixels,\\nso we have very small images.\\nNow, we also want to display the first few images,\\nso we'll do that by plotting the first 16 images.\\nHere we iterate over them and then call pyplot,\\nimshow and show functions.\\nFinally, let's go ahead and run our code.\\n\\nWe can see Matplotlib has created a plot\\nwith the first 16 images,\\nwhich are 32 by 32 pixel color images.\\nNumber three in our tensors output represents\\nthat the color image has three channels, RGB,\\nwhich represent red, green, and blue color.\\nOne more thing that we would like\\nis to examine the training data set.\\nWhen we print this shape,\\nwe see we have 50,000 images divided into 10 classes\\nand corresponding labels.\\n\\nThe label is an integer value\\nthat represents the class of the image.\\nFor example, airplane zero, automobile one, birds two, etc.\\nTo check the class labels,\\nwe literate over the pictures.\\nWhen we run the code,\\nwe see that on the first photo is a frog,\\non the second two is a truck, and on the fourth is a deer.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3886260\",\"duration\":160,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data transforms\",\"fileName\":\"2706322_en_US_05_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":262,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Data often needs to be adjusted before it is passed into the model for training and testing. Learn different ways to adjust the data by applying transforms.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5242735,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Before passing the data into the model,\\nwe usually need to adjust it.\\nFor instance, data values need to be converted\\nfrom one type of object to a tensor.\\nAnother example of adjustment is data values\\nthat may be augmented to create a larger dataset.\\nWe can achieve this adjustment by applying transforms.\\nPyTorch has a torchvision library\\nthat supports common computer vision transformations\\nin the torchvision.transforms\\nand torchvision.transforms.v2 models.\\n\\nTransforms are common image transformations\\nthat we can use to transform or augment data\\nfor training or inference of different tasks.\\nFor example, image classification, detection, segmentation,\\nand video classification.\\nHere we have important transforms from torchvision.\\nWe'll then define a list of transforms\\nusing transforms.Compose function,\\nwhich composes several transforms together.\\nWe'll scale image data to 64 by 64 resolution,\\nthen turn it into a tensor and normalize the tensor\\naround the specific set of mean\\nand standard deviation points.\\n\\nYou may wonder where do these values\\nfor mean and standard deviation come from?\\nYou can calculate these statistics yourself\\nby iterating through the training set\\nand computing the mean and standard deviation\\nof each channel across all images.\\nHowever, since these values are standard for CIFAR-10,\\nthey're often provided in tutorials, documentation,\\nor widely used libraries like torchvision\\nto simplify the pre-processing step for users.\\nWithout normalization, these values would typically range\\nfrom zero to 255 for each channel,\\nrepresenting the intensity of RGB component of each pixel.\\n\\nAs the input passes through the layers of the network,\\nthere is a lot of multiplication,\\nso maintaining values between zero and one\\nprevents these values from increasing\\nduring the testing phase,\\nwhich is called the exploding gradient problem.\\nFinally, we use transforms\\nto create a training data dataset.\\nTo take a look at the transforms,\\nwe can simply print the training dataset\\nfor the first image.\\nOne more thing to add\\nis that we can also define a different set of transforms\\nfor testing dataset and apply them to our test data.\\n\\nLet's do that.\\nFor example, we may not want to resize;\\nwe want to convert the image to tensors and normalize them,\\nas you can see here in our code.\\nLet's go ahead and display it.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3890256\",\"duration\":175,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data batching\",\"fileName\":\"2706322_en_US_05_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Append with PU\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":260,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In the process of model training data is passed in small batches at each iteration. That way training is more efficient training and accelerated.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5953046,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] A data loader feeds the data\\nfrom the dataset into the neural network.\\nAt the core of PyTorch data loading utility\\nis a torch.utils.data.DataLoader class.\\nIt represents a Python iterable over a dataset,\\nwith support for map-style and iterable-style datasets,\\ncustomizing data loading order, automatic batching,\\nand single and multi-process data loading.\\nYou can find more about it\\non the PyTorch documentation page here.\\n\\nThe neural network trains best with batches of data,\\nmeaning rather than using the complete dataset\\nin one training pass,\\nwe've used mini batches,\\nusually 64 or 128 samples.\\n(upbeat music)\\n(buzzer buzzing)\\nThat's right.\\nSmaller batches require less memory than the entire dataset,\\nand it results in more efficient and accelerated training.\\nDataLoader has, by default, a batch_size of one,\\nso we will want to change that.\\n\\nSo DataLoader has a parameter called batch_size\\nwhich represents a number of images\\nthat go through the network before we train and update it.\\nNow, let's head on to the Colab notebook.\\nWe are using the same example and introducing DataLoader.\\nOne more thing we have to do\\nis import DataLoader from torch.utils.data.\\nAs we can see in our code, we have two dataset.\\nThe training set,\\nwhich is used in the training pass to update the model,\\nand the testing set,\\nwhich is the final dataset that we use\\nto evaluate the model's performance.\\n\\nTo build our data loaders,\\nwe'll set the batch_size to 16\\nand create training_data_loader and test_data_loader\\nby calling DataLoader and passing two parameters,\\ntraining_data or test_data and batch_size.\\nThe most commonly used parameters\\nare the dataset, batch_size, shuffle, sample,\\nand num_workers parameters.\\nNow in our example,\\nwe could also pass in the third parameter called shuffle\\nand set it to true if we want to shuffle our dataset,\\nso the data loader returns a random sample of data.\\n\\nLet's go ahead and do that for the training_data dataset.\\nAnd for the test data_dataset,\\nwe'll set it to false\\nas we want to have repeatable test results.\\nNow in our example,\\nwe could also pass in the third parameter called shuffle\\nand set it to true if we want to shuffle our dataset\\nso the data loader returns a random sample of data.\\nLet's go ahead and do that for the training_data dataset\\nand for the test_data dataset, we'll set it to false\\nas we want to have repeatable test results.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3884260\",\"duration\":286,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Model development and training\",\"fileName\":\"2706322_en_US_05_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Append with PU\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":false,\"rawDurationSeconds\":382,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn different steps of model development and importance of the each step. Explore fundamental processes used for training models.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9549213,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] After taking care of preparing data sets,\\nwe can finally explore model development.\\nIt consists of a few steps,\\nmodel design, training, validation, and testing.\\nIn the model design step,\\nwe can design one or more model architectures\\nand initialize weights and biases.\\nUsually we take an existing design and modify it.\\nWe won't be covering this step in more detail\\nas we are taking care to understand the basics.\\nIn model training, we feed the training data into the model,\\ncalculate the error, and then adjust the parameters\\nto improve the model's performance.\\n\\nAfter the training, in the validation step,\\nwe measure the model's performance\\nagainst the data that wasn't used in training.\\nLet's explore how to build our first neural network.\\nWe have already imported libraries and data set.\\nHere we define the neural network and we call it net.\\nThen we fill out the init and forward functions.\\nIn init, we are adding layers to our network.\\nYou can think of them as filters and lenses\\nthat help our model to focus\\non important parts of the image.\\n\\nFor example, the first layer called self.conv1\\nis the first convolutional layer.\\nIt takes three input channels for free channel color images,\\noutputs six channels and uses five by five kernel.\\nSelf pool is the max pooling layer\\nwith two by two window and a stride of two.\\nIt is used to reduce the spatial dimensions of the output\\nfrom the convolutional layers.\\nSelf.conv2 is the second convolutional layer.\\n\\nIt takes the six output channels from conv1 as input\\nand output 16 channels.\\nSelf.fc1 is the first fully connected layer.\\nThe input size is 16 by 5 by 5,\\nwhich is derived from the output dimensions\\nof the last convolutional layer.\\nSelf.fc2, as the name suggests,\\nis the second fully connected layer\\nand self.fc3 is the final fully connected layer\\nthat outputs 10 channels,\\nwhich correspond to the 10 classes in our cipher 10 dataset.\\n\\nThe forward function defines\\nhow data flows through the network\\nin both training and making predictions.\\nIt takes two parameters, self and x.\\nX is passed through these layers sequentially.\\nHere we have a view function which reshapes the tensor\\nbefore feeding it to fully connected layers,\\nand F.relu,\\nwhich for example, when applied to the output of conv1\\nresults in the tensor where all negative values\\nare set to zero and positive values are left unchanged.\\n\\nNext, we instantiate the model\\nby setting up the necessary tools\\nfor training the network, loss function and optimizer,\\nas you can see in the code.\\nThen we load and transform the data.\\nAnd finally, in the training step,\\nour model learns by adjusting its weights\\nbased on the computed loss and gradients.\\nIt involves iterating over a data set multiple times,\\nwhich we call epochs.\\nIn our case, we are looping 10 times.\\nNow we initialize a variable\\nto accumulate the loss over each batch.\\n\\nThis inner loop iterates over the training data set\\nand the train loader provides batches of data,\\nmeaning images and labels.\\nThen we unpack the data into images\\nand their corresponding labels.\\nHere we use the optimizer to zero all the gradients\\nfrom weights and biases.\\nThis prevents the accumulation of gradients\\nfrom multiple passes.\\nAfter we pass the input data through the network\\nto get outputs, we compute the loss\\nby comparing the model's outputs with the actual labels.\\n\\nBy calling loss.backward function,\\nwe perform a backward pass\\nto compute the gradient of the loss\\nwith respect to the network parameters.\\nFinally, we update the weights\\nbased on the computed gradients\\nand accumulate the loss over the batches for reporting.\\nIn the if statement, we use print inside it\\nto log the average loss every 2000 mini batches\\nso we can monitor the training.\\nI have already executed this upfront\\nas usually it takes around 10 minutes\\nfor this code to finish execution.\\n\\nOnce training is finished,\\nwe can see finished training message,\\nwhich we will use to indicate\\nthe end of the training process.\\n\"},{\"urn\":\"urn:li:learningContentVideo:3884259\",\"duration\":250,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Validation and testing\",\"fileName\":\"2706322_en_US_05_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Append with PU\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":false,\"rawDurationSeconds\":295,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Validation and testing are important parts of model development as they take care that overfitting does not occur and that the model performs well against unseen data. Learn the key steps of each of them.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10043035,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Validation and testing are important parts\\nof model development as they take care\\nthat overfitting does not occur\\nand that the model performs well against unseen data.\\nLearn the key steps of each of them.\\nIn the validation step, we use a separate set of data,\\nwhich we haven't used previously in training\\nand call this set validation set.\\nThe main goal is tuning hip parameters such as learning rate\\nor number of epochs,\\nso we can provide an early indication\\nof how our model is performing.\\n\\nWe have already imported libraries and dataset.\\nNext, we define a neural network with init\\nand forward function.\\nAfter that, we instantiate the model\\nand define the loss function and optimizer.\\nTo load and transform the data,\\nwe will create transformations for the training data.\\nWe'll create validation set from our training data\\nby using torch.utils.data.randomsplit,\\nwhich splits the training data into the training set\\nof 40,000 images and validation set of 10,000 images.\\n\\nThen we set up loaders for the training set\\nand validation set with 16 images per batch\\nand enable shuffling.\\nA data loader for the test set is created\\nwith a batch size of four,\\nand we don't use shuffling.\\nAt the end of each epoch,\\nwe are going to evaluate our model on the validation set.\\nWe will usually check the loss and accuracy.\\nThe testing step is crucial\\nfor evaluating the model's performance on unseen data.\\nThe test set is a separate data set\\nthat the model has never seen during training.\\n\\nIt provides us with a final evaluation\\nof the model's performance.\\nWe are looping 10 times to train the model\\nas we have 10 epochs.\\nWe switch the model net to training mode.\\nThen we initialize the running loss to zero.\\nIn this inner loop,\\nwe iterate over our training data loader.\\nTrain loader batches the training data and enumerate,\\nwill give us the index and the data for each iteration.\\nThe data table is unpacked into inputs\\nthat represent features and labels.\\n\\nThen we zero out any gradients from the previous batch\\nbefore calculating the gradients.\\nThe input passes through the network\\nand it performs forward pass and returns the outputs.\\nLastly, we calculate the loss\\nusing the loss function criterion.\\nWe compare the network outputs with labels\\nand compute the gradient of the loss.\\nThen the optimizer updates the models parameters,\\nand we add the loss for this batch to running loss\\nso we can keep track of the total loss of epoch.\\n\\nAfter each training epoch, the model is switched\\nto evaluation mode using net.eval function,\\nwhich deactivates dropout\\nand normalizes batch normalization layers\\nfor consistent behavior during inference.\\nThen we iterate over the validation data set,\\nand for each batch of data, it computes the model's output\\nwithout updating the model's weights\\nas gradients are not needed for validation.\\n\\nWe calculate accuracy and loss on this validation data,\\nproviding insight into how well the model performs\\non data it hasn't been trained on.\\nAfter looping through all test data,\\nwe calculate the overall accuracy by dividing the number\\nof correct predictions by the total number\\nof samples in the test set.\\nAt the end of the epoch, we print the average training loss,\\nthe average validation loss,\\nand the validation accuracy percentage.\\n\\nThese averages are calculated by dividing the total loss\\nby the number of batches in the loader.\\nFor example, for average training loss,\\nwe divide the training loss by the number\\nof batches in the train loader.\\n\"}],\"name\":\"5. Developing a Deep Learning Model\",\"size\":42732224,\"urn\":\"urn:li:learningContentChapter:3890263\"},{\"duration\":48,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3890257\",\"duration\":48,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"2706322_en_US_06_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Twitter handle @Tsemenski\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":50,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"This video explains where to go from here.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1493818,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Terezija] Congratulations on completing this course.\\nNow that you have seen the basics of using PyTorch,\\nyou can try applying it to your own projects.\\nI encourage you to play around with the different datasets\\nthat you can find on Kaggle.\\nYou can look for more PyTorch courses\\ncoming from me in the future.\\nYou can also search our library\\nfor more deep learning courses to continue your journey.\\nI want to wrap up by saying thank you.\\nYou have devoted time to learning this material with me,\\nand I want you to know that I don't take that for granted.\\n\\nI really do appreciate your engagement\\nas well as your feedback on this course.\\nIf you have any questions, please feel free\\nto get in touch on LinkedIn and on Twitter.\\nIf you enjoyed this course, I'd love to know.\\n\"}],\"name\":\"Conclusion\",\"size\":1475670,\"urn\":\"urn:li:learningContentChapter:3885250\"}],\"size\":154611905,\"duration\":4906,\"zeroBased\":false},{\"course_title\":\"PyTorch Essential Training: Working with Images\",\"course_admin_id\":3968391,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":3968391,\"Project ID\":null,\"Course Name\":\"PyTorch Essential Training: Working with Images\",\"Course Name EN\":\"PyTorch Essential Training: Working with Images\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"Machine learning developers and data scientists can feel overwhelmed by the complexity of convolutional neural networks (CNNs) and their applications. This course provides a hands-on, project-based approach to image classification. Join instructor Terezija Semenski to gain practical experience in preprocessing data, training, and evaluating a pre-trained model. Plus, explore transfer learning, model fine-tuning, and evaluation metrics.\",\"Course Short Description\":\"This course provides hands-on learning for preprocessing data, training, and evaluating a pretrained model for image classification using PyTorch.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":20533014,\"Instructor Name\":\"Terezija  Semenski\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Software Developer, Mathematician, Writer, and Learner\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2025-04-04T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"No\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/pytorch-essential-training-working-with-images\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Artificial Intelligence for Technology\",\"Primary Software\":\"PyTorch\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":3714.0,\"Visible Video Count\":22.0,\"Learning Objectives\":\"Apply a pre-trained deep learning model and to a binary classification task.,Preprocess image data and fine-tune the ResNet model.,Evaluate and improve the performance of the model using accuracy and loss.\",\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":129,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4017667\",\"duration\":41,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Image classification with PyTorch\",\"fileName\":\"3968391_en_US_00_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":234,\"solutionVideo\":false,\"editingNotes\":\"WSC\\nTwo files - pu is audio\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":true,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Take a closer look on how to use PyTorch for image classification and explore the goals of this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2539906,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- How would you approach writing a program\\nthat could tell a dog from a cat?\\nYou could write a set of rules describing their appearance\\nand apply those rules to an image.\\nHowever, it would be a tedious task\\nto code all these rules manually.\\nIn this course,\\nwe'll discover how PyTorch simplifies image classification,\\nallowing us to solve this challenge efficiently.\\nHi, I'm Terezija Semenski.\\nI'm a software developer, mathematician,\\nand a teacher with a passion for AI and machine learning.\\n\\nJoin me into hands-on journey\\ninto image classification with PyTorch,\\nwhere you can gain practical skills\\nthat you can apply to your next computer vision project.\\nLet's get started.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2030651\",\"duration\":88,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"3968391_en_US_00_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":103,\"solutionVideo\":false,\"editingNotes\":\"00:35 - Overlay - https://www.linkedin.com/learning/pytorch-essential-training-deep-learning\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Before starting this course, let's explore what skills and knowledge you will need to have to be successful in this course. Learn all installation requirements we need to set up before we proceed.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2934879,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Before starting this course,\\nlet's explore what skills and knowledge you will need.\\nTo truly be successful in this course,\\nit'll be helpful to have some experience\\nprogramming in Python.\\nAt least you should be familiar with Python syntax\\nand the operative environment,\\nand experience with some other programming language,\\nsuch as C++ plus or Java.\\nI assume you're familiar with the basics of neural networks\\nand how you can train them using PyTorch.\\nIf you aren't familiar, I encourage you\\nto take my previous course,\\n\\\"PyTorch Essential Training: Deep Learning,\\\"\\nso you can easily follow along.\\n\\nIn order to run our code,\\nwe will be using the Google Collaboratory environment,\\ncommonly known as Google Colab,\\nwhich is available at colab.research.google.com.\\nGoogle Colab, or short, Colab, is a free computing service\\nthat provides Jupyter Notebook instances\\nthat run on the cloud.\\nThe great thing is that notebooks\\ncan be saved on Google Drive or GitHub.\\nConveniently, all you need\\nto start writing code is a Google account.\\n\\nGoogle provides free access to GPUs\\nor graphical processing units,\\nwhich we will use for training our models.\\nThere are some limits to GPU usage,\\nbut it'll be sufficient for our course.\\nAnd that's about it.\\nSo let's get ready to jump into PyTorch,\\nworking with images in depth.\\n\"}],\"name\":\"Introduction\",\"size\":5474785,\"urn\":\"urn:li:learningContentChapter:2032691\"},{\"duration\":973,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2032690\",\"duration\":249,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"What is image classification?\",\"fileName\":\"3968391_en_US_01_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":313,\"solutionVideo\":false,\"editingNotes\":\"00:00 - begin with b-roll not slides.\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":true},\"description\":\"Understand what image classification is and its common use cases.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8519009,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Presenter] You just started a new job a few months ago\\nworking at a help desk in a e-commerce company.\\nThe emails are filled with customer complaints\\nand you often struggle to sign them\\nto different departments:\\nsales, accounting, and IT.\\nYou could develop an ML model that predicts\\nwhich of these three departments those complaints should go.\\nThis is an example of a classification problem.\\nThe most common types of machine learning\\nare regression and classification.\\n\\nThe regression model outputs a continuous variable.\\nSome examples of regression tasks are forecasting trends.\\nWe can use a regression model\\nto forecast the sales of a product\\nbased on historical data and economic indicators.\\nPredicting prices:\\nto predict the price of a house based on its size, location,\\nand other features.\\nHealth:\\nto identify risk factors for diabetes\\nor heart disease based on patient's data.\\n\\nThe classification model classifies inputs\\ninto different categories.\\nThe most common use cases are email spam filtering:\\nto classify every email as spam\\nor not spam.\\nMedical diagnosis:\\nto classify as positive or negative.\\nAnd image classification,\\nwhich I will explain in detail a little later in this video.\\nWe can transform a classification model\\ninto regression model and vice versa.\\n\\nFor example, email classification\\ncan become a regression model\\nif we put values 0 and 1\\nand decide a threshold value for spam.\\nWe can divide classification problem\\ninto binary classification and multi-class classification.\\nIn binary classification, we classify the input\\ninto one or two classes or categories.\\nFor example, given health data,\\nwe determine whether the person\\nhas a certain disease or not.\\n\\nIn multi-class classification,\\nwe classify the input\\ninto one of several classes or categories.\\nFor example, given different dog breeds,\\nwe determine which dog breed our dog belongs to.\\nSimple as that.\\nBut what is an image classification?\\nImage classification is a fundamental task\\nin computer vision that involves assigning a label\\nor category to an image based on its visual content.\\nImage classification can sometimes get mixed up\\nwith other computer vision problems or tasks.\\n\\nSo let's understand the difference.\\nObject detection.\\nImage classification tells you this is a dog,\\nbut object detection also tells you where the dog is\\nby drawing bounding boxes around objects.\\nImage segmentation.\\nUnlike image classification\\nwhere the whole image gets one label,\\nimage segmentation classifies each pixel in the image.\\nIt's about dividing the image into regions or segments.\\nFor example, in a picture of a dog in a park,\\nimage segmentation would label the pixels\\nbelonging to a dog as dog,\\nthe grass is grass, and so on.\\n\\nAction classification.\\nThis involves identifying activities\\nbetween a sequence of images or videos.\\nInstead of labeling the image as dog,\\nit might predict a dog is running.\\nLastly, image captioning.\\nIt combines object recognition\\nwith natural language processing.\\nSo given an image,\\nthe system generates a natural language sentence\\ndescribing an image.\\nFor example, a child is playing in the park.\\nThese problems may seem challenging to you,\\nbut they're already driving transformation\\nacross industries.\\n\\nCompanies like Tesla, Google, and Meta use them\\nto build intelligent systems impacting millions of people.\\nTo explore image classification,\\nwe'll start by experimenting\\nwith a pre-trained model using PyTorch.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4019676\",\"duration\":231,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Binary image classification\",\"fileName\":\"3968391_en_US_01_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":254,\"solutionVideo\":false,\"editingNotes\":\"Append root with pu\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":true,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Describe what binary image classification, its difference from multiclass classification, and its common use cases.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7697470,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Presenter] Have you ever uploaded a photo\\nto your phone or computer\\nand it magically identified your friends, family, pets,\\nor even the location?\\nOr maybe you wondered how car's dashboard camera\\nrecognizes road signs and pedestrians in real time.\\nWell, it may seem like magic, but it's image classification.\\nComputers don't see like we do.\\nTo them, an image isn't cute puppy or a tasty cake.\\nIt's a grid of numbers, just raw data.\\n\\nTeaching machines to go from crunching numbers\\nto actually understanding what's in the image\\nis one of my favorite challenges.\\nImage classification is a core task in computer vision,\\na field of AI focused on enabling computers to interact\\nand make decisions based on visual data.\\nIt is super useful\\nand has some applications in real-world scenarios.\\nFrom facial recognition on your phone\\nto sell driving cars recognizing road signs,\\nimage classification is everywhere.\\n\\nIt can be used to help medical professionals\\nidentify diseases in X-rays,\\nenable retailers to recommend visually similar products,\\nand even in agriculture by identifying crop health.\\nHow can we define image classification?\\nImage classification is a task in computer vision\\nthat deals with automatically understanding the content\\nof an image.\\nThe algorithm looks at the image\\nand then picks up a category\\nfrom among a fixed set of categories\\nor labels to classify that image.\\n\\nIn its simplest form, image classification assigns a label\\nto an entire image.\\nFor example, if we feed a model a picture of a cake,\\nit outputs a cake.\\nTo achieve this, we train machine learning models\\nusually convolutional neural networks\\nor CNNs on large datasets of labeled images.\\nThe model learns to detect patterns\\nsuch as textures, shapes, or colors\\nto classify the image into predefined categories.\\n\\nThere are few different types of image classification:\\nbinary classification, multi-class classification,\\nmulti-label classification, and hierarchical classification.\\nThere are a few more,\\nbut we will focus on the two most common ones:\\nbinary classification and multi-class classification.\\nBinary image classification is the simplest type\\nof image classification.\\nAs its name suggests,\\nit involves classifying images into one of two categories.\\n\\nFor example, figuring out\\nwhether an image contains a dog or not.\\nA binary image classification problem assigns an image\\nto one of two classes.\\nMost image classifications\\nuse convolutional neural networks or CNNs.\\nThey are different from standard neural networks\\nbecause they take inputs in the form of images,\\nso they are more efficient.\\nCNNs are built out of many convolutional layers\\nthat act as features extractors.\\n\\nAnd just like neural networks,\\nthey contain neurons that learn parameters\\nlike weights and biases.\\nMulti-class image classification\\ninvolves categorizing images into more than two classes.\\nFor example, we can classify images\\nof different types of animals:\\ncats, dogs, rabbits, horses, et cetera.\\nWe assign every image to only one category.\\nNow that we understand what is the difference\\nbetween the two most common types of image classification,\\nwe'll go on to discover multi-class classification.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2031635\",\"duration\":154,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Multiclass image classification\",\"fileName\":\"3968391_en_US_01_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":171,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Understand what multiclass image classification is and differentiate between binary image classification and multiclass image classification.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5949827,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Presenter] I love animals,\\nso I have both a cat and a dog.\\nLet's check out the images below.\\nWhat do you see in the first image\\nand what do you see in the second, third, and fourth one?\\nThe first image is of a dog.\\nThe second one is of a cat.\\nThe third one is of a bird.\\nAnd the fourth image has a group of animals:\\na dog, a cat, and a bird.\\nWe have learned that when we have only two classes\\nin which we can classify images,\\nthis type of image classification\\nis called binary image classification.\\n\\nSo we could apply this type of classification\\non our first two images.\\nHow about if you want to classify the first three images\\nof the corresponding categories: cat, dog, bird?\\nThen we would use multi-class classification.\\nWhat about the fourth image?\\nWe have three different animals in the same picture,\\nmeaning we can classify this image\\ninto three different classes.\\nSo in this case,\\nwe will call this multi-label image classification.\\n\\nI know those two,\\nmulti-class classification and multi-label classification,\\nsounds similar, and it's probably confusing.\\nIt took me a few days to figure out the difference.\\nLet's understand them together.\\nMulti-class image classification\\nis when a model has to choose one label,\\neven if there are few different animals in the picture.\\nSo we could have 100 images\\nand each one could have a different animals on it.\\nFor example, the first image would contain a cat and a dog,\\nand the model would say it's a picture of a dog.\\n\\nThe second image could have a dog, a cat, and a fish,\\nand it would say it's a picture of a cat, etc.\\nFor simplicity,\\nlet's say we have only five possible categories,\\na cat, a dog, a fish, a hamster, a bird,\\nand then we would classify each of the 100 images\\nin one of those five categories.\\nIn the case of multi-label classification,\\nwe would have 100 images,\\nand each image would contain two or more objects.\\n\\nFor example, the first image has a dog and a cat.\\nThe second image has a cat and a fish\\nand a hamster, and so on.\\nSo each image belongs to more than one class,\\nand each object in the image would get its own label.\\nNow that we understand\\nwhat multi-class image classification is\\nand how it differs from multi-label image classification,\\nwe'll head on to explore how CNNs process images.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4017666\",\"duration\":339,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understanding convolutional neural networks (CNNs)\",\"fileName\":\"3968391_en_US_01_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":406,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Explain how convolutional neural networks process images.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9590871,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The biggest challenge\\nwhen working with neural networks is overfitting.\\nThe more powerful a neural network is,\\nmeaning the more layers and weights it has,\\nthe more it's prone to overfitting.\\nIt usually happens when a neural network\\nlearns the noise in the dataset\\ninstead of making decisions based on the true signal.\\nWait, what does it mean?\\nLet's take a look at these two images.\\nObviously, both images represent a cat.\\nIn the first picture, the cat is sitting on a blanket\\non a nice chair in the room.\\n\\nIn the second picture, we have a silhouette of a cat.\\nSo everything surrounding the cat in the first picture,\\nblanket, chair, background, is the noise.\\nIn the second picture,\\nthe middle black part of the cat is also noise.\\nIt doesn't describe a cat, it just fills the middle.\\nA signal in these two pictures is the essence of the cat.\\nOverfitting also happens when we have more parameters\\nthan necessary to learn a specific dataset\\nand not enough training examples.\\n\\nWe would like to figure out how to reduce overfitting,\\nmeaning how to get the neural network\\nto train only on signals and ignore the noise.\\nIt could be done using a few different techniques.\\nOne way to reduce overfitting is to selectively choose\\nto reduce weights for multiple purposes in a neural network,\\nbecause we assume the same pattern\\nhas to be detected in multiple places.\\nUsing this approach, you can reduce the amount\\nof actual parameters to store so the model will be smaller.\\n\\nThe most useful structure in neural networks\\nthat uses this approach is called a convolution,\\nand in this case, we call each layer\\na convolutional layer.\\nA convolutional neural network, or CNN,\\nis a special type of neural network that is built\\nwith the ability to extract unique features\\nfrom the image data.\\nInstead of having a large linear network\\nthat has a connection from every input to every output,\\nCNNs' individual neurons do not have their unique weights,\\nso they use the same weights\\nas other neurons in the same layer.\\n\\nA CNN is similar to fully connected feedforward networks,\\nbut it has fewer connections than a fully connected network.\\nInstead of being fully connected, it is sparsely connected.\\nCNNs consist of multiple layers,\\nincluding an input layer for the image,\\nconvolutional layers, pooling layers,\\nfully connected layers,\\nand an output layer used for classification.\\nThe images fed to CNNs\\nmust be converted to a numerical representation\\nbecause images are made up of pixels.\\n\\nEach pixel is represented by an intensity value.\\nFor black and white images, this value ranges from 0-255,\\nwhere zero represents black, and 255 represents white.\\nFor color images, each pixel has three intensity values,\\none for each channel, red, green, and blue, or short, RGB.\\nThese numerical values are then normalized,\\nusually scaled to a range of zero to one,\\nto make training more efficient and stable.\\n\\nThe resulting array of numbers\\nis what gets passed into the CNN as input.\\nThe most important part of a CNN is the convolutional layer.\\nIn the convolutional layer,\\nwe have a lot of very small linear inputs\\nthat are often smaller than 25 inputs\\nand only a single output.\\nThat one output is used in every input position.\\nWe call each of these mini layers a convolutional kernel.\\nFor example, for the sake of simplicity,\\nhere we have a three-by-three convolutional kernel\\nthat we use to process the same eight-by-eight image.\\n\\nEach kernel is just a grid of weights,\\nlike a three-by-three, or a five-by-five matrix.\\nThe kernel or filter slides or convolutes over the image,\\nperforming an element-wise multiplication\\nbetween the filter's values\\nand the pixel values it overlaps.\\nThe result of this operation is called a feature map.\\nFor example, a filter might detect horizontal edges\\nby responding strongly to the regions of the image\\nwhere there's a sudden change in pixel intensity.\\n\\nEach filter learns to detect a specific pattern,\\nlike edges, textures, or corners.\\nAs the image passes through multiple layers,\\nthe CNN extracts various features.\\nThese features capture low-level patterns\\nin the early layers, and more complex patterns,\\nlike shapes or objects, in the deeper layers.\\nNext, it goes through pooling layers.\\nIt includes compressing a portion of the image.\\nThese layers reduce the dimensions of the feature maps\\nby taking the maximum value from small regions,\\nlike two-by-two blocks.\\n\\nPooling not only makes computations more efficient,\\nbut also provides translation invariance,\\nmeaning the network can recognize features\\nregardless of their exact position in the image.\\nFinally, after convolutional and pooling layers,\\nthe feature maps are flattened into a one-dimensional array\\nand passed to fully connected layers.\\nThese layers take the high-level features\\nand map them to the output classes.\\nIn this example, the network will output probabilities,\\nin our example, the probability of an image being a cat.\\n\\n\"}],\"name\":\"1. Introduction to Image Classification\",\"size\":31757177,\"urn\":\"urn:li:learningContentChapter:4017668\"},{\"duration\":666,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2031634\",\"duration\":161,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Importing the packages\",\"fileName\":\"3968391_en_US_02_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":202,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Branch > 02_01\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Indentify the required libraries for data loading and preprocessing.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5163923,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's head over to Google Colab\\nand grab our first notebook from the exercise files.\\nWe are going to use the dataset\\nfrom Kaggle called Muffin versus Chihuahua.\\nI have already preloaded the data set.\\nWe have organized the images into two folders called\\ntrain and val.\\nThe train folder has two subfolders\\nwith two categories called muffin and chihuahua,\\nand the val folder has also two subfolders\\ncalled muffin and chihuahua.\\nWe are going to use the train folder to train the model,\\nmeaning to feed the parameters\\nand the val folder to define the model\\nand test how well our model is doing.\\n\\nThroughout this course, we will be using packages required\\nto train our model.\\nLet's import them once here\\nand use them in all other exercises.\\nFor now, don't worry what each package does\\nas we'll cover this later.\\nHowever, I'll give you the short overview of each.\\nWe need NumPy to turn images into numbers.\\nNext we need Matplotlib for plotting.\\nWe are going to use Torch,\\nwhich is the Core PyTorch Library\\nand Torch.nn, which contains all specific functions related\\nto neural networks.\\n\\nWe also need touch.optim, which we will use\\nto define our optimizer so we can adjust weights.\\nAnother thing we need from touch.optim is\\nlearningrateschedule for scheduling the learning rate,\\nwhich means the speed of our model's training.\\nWe also need cudnn, which we'll use to optimize our code\\nto run faster.\\nWe are going to use touchvision package.\\nFrom torchvision, we'll need data sets\\nwhich provide various data sets models\\nwhich include pre-trained models we'll use\\nand transforms, which will help us pre-process\\nand transform images.\\n\\nWe also need time, a library for time related functions,\\nos for interacting with our operating system\\nwhen we want to change or delete files in similarm\\nand copy for copying objects.\\nTo run the code, press shift plus enter,\\nor you can use the run button as well.\\nWe are setting the seed so we both get the same results.\\nSince we want to train our model faster,\\nwe want to check if we have access to the CPU\\nor a GPU for this notebook.\\n\\nFor now, we won't bother about which type\\nof device we'll get here.\\nWe'll cover this later.\\nNow that we have covered imports, we can head on to discover\\nhow we have organized our data set.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4018652\",\"duration\":100,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Organizing the dataset\",\"fileName\":\"3968391_en_US_02_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":137,\"solutionVideo\":false,\"editingNotes\":\"00:35 - Overlay - https://www.kaggle.com/datasets/samuelcortinhas/muffin-vs-chihuahua-image-classification\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Branch > 02_02\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Describe the structure of the muffin versus chihuahua dataset and folder organization. Demonstrate how to structure datasets for binary classification.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3794086,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] I love animals and I love pastries,\\nand I suppose that you love at least one of those two.\\nSo, when I was considering data sets for this course,\\nI had a hard time choosing,\\nbut finally, I found a data set that consists\\nof around 6,000 images of muffins and chihuahuas.\\nImages were scraped from Google Images\\nand duplicates have been removed.\\nTo see some of these images, let's head onto Kaggle.\\nThis data set called Muffin vs Chihuahua\\nwas inspired by the following meme.\\n\\nAs you can see, some of the dog's faces look very similar\\nto muffins and vice versa.\\nImages have already been divided into a training set\\nwhere we have around 80% of the images\\nand a test set where we have the rest of 20% images.\\nLet's click on the few images of chihuahuas.\\nAnd now muffins.\\nAnd we are going to have so much fun\\nwith this binary image classification.\\n\\nLet's head on to Google Collab\\nand explore how you can set up your data folder.\\nYou can download the image folders from exercise files,\\nand the easiest way for you\\nis to separate the classes into folders.\\nI have created the folder train,\\nwhich we are going to use for predicting,\\nand the folder wall, which will have the images for testing.\\nYou can see that for both folders,\\nI created the subfolders chihuahua and muffin,\\nwhich correspond to their different classes.\\n\\nNow that we have our dataset ready,\\nlet's learn how to transform the data in the next video.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4018651\",\"duration\":289,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Transforming the data\",\"fileName\":\"3968391_en_US_02_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":599,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Branch > 02_03\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"When we are loading images they are in different sizes, so we have to apply the torchvision.transforms library to preprocess images. Explore how to seperately perform transforms on the training set and on the validation set.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9897818,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] If you ask any machine learning engineer\\nor data scientist about what part\\nof their job they find exhausting,\\nthere is a high probability\\nthat their answer will be loading\\nand converting data into formats\\nthat are ready for training.\\nThere is a valid reason for this.\\nIt consumes too much time and doesn't spark joy.\\nLuckily, PyTorch is here to the rescue\\nas it has developed two standard conventions of interacting\\nwith data called datasets and dataloaders.\\n\\nA dataset is a Python class that allows us\\nto get the data we are supplying to the neural network.\\nA dataloader feeds data from the dataset into the network.\\nWhen we load our images,\\nthey're usually in different sizes.\\nMeaning different dimensions.\\nSo we want to standardize our images\\nso that when our model sees them,\\nthey're represented in a consistent format\\nand we want to transform them to help the model to learn.\\nBefore we feed the data into the model,\\nwe have to format it into\\npre-processed floating point tensors.\\n\\nOur data consists of a bunch of JPEG files\\nof different sizes, and some of them look pretty funny.\\nTo get our data into the model,\\nwe have to perform these steps.\\nRead the pictures, meaning JPEG files,\\ndecode the JPEG files to RGB grids of pixels.\\nConvert these into floating point tensors.\\nResize them to shared size.\\nPack them into batches.\\nWe'll use batches of four images.\\n\\nIt may seem a bit overwhelming,\\nbut fortunately the torchvision package has a class\\nnamed ImageFolder that will do almost everything for us.\\nSo let's get started.\\nWe are going to transform our training\\nand validation set separately\\nbecause we want to transform them differently\\nfor the purpose of training and validation.\\nWe are going to create a dictionary called data-transforms,\\nand there is a key, an item in our dictionary for training,\\nand we'll call it train.\\nAnd values are going to be transforms.Compose,\\nand then we'll create a list inside.\\n\\nWe'll use a function called RandomResizeCrop\\nthat randomly crops every image\\nto the same resolution of 224 by 224.\\nRandom means it'll sometimes crop the top-left\\nor top-right square of the picture,\\nsometimes square from the center of the picture,\\nand sometimes square from the bottom.\\nNext function we need is a RandomHorizontalFlip,\\nwhich we will sometimes flip the images horizontally.\\nThen we convert the images to tensor.\\nAnd lastly, we normalize the tensor around the specific set\\nof mean and standard deviation points.\\n\\nNormalization is crucial\\nbecause we want to keep\\nincoming values between zero and one.\\nAs we are going to have a lot\\nof multiplication while inputs pass through the layers\\nof the neural network.\\nWhat happens in normalization?\\nWe take the values, subtract the mean,\\nand divide by standard deviation.\\nI won't cover how we got these values for mean\\nand standard deviation.\\nIt would take too much of our time.\\nWe are going to use ResNet model,\\nand the mean and standard deviation\\nare from the image United Status set.\\n\\nAs this model has been trained using\\nthis mean and this standard deviation.\\nFor the validation set,\\nwe are going to use similar transforms.\\nWe are going to resize it to 256\\nby 256 using resize function.\\nAnd then use center crop function to crop the image\\nto see the center of our image.\\nFinally, as for our training set,\\nwe are going to use two tensor and normalized function\\nand use the same values for the mean.\\n\\nBefore we head on to visualizing our data,\\nwe'll create a data directory path containing our data set.\\nThen we are going to pass our directory\\nto dataset.image folder.\\nTo create a data order called image data sets\\nwhere the images are arranged.\\nIn the same way our folders are currently structured.\\nWe are going to use image data sets for our training\\nand validation dataset, sizes, and class names.\\nWe'll use a function called image folder\\nand here we have joining a data directory we have\\nprovided with lend to function X.\\n\\nAnd forming a data transforms\\nand creating two separate directories for training\\nand validation and applying data transforms\\nseparately on these images.\\nWe'll find the sizes of each data set,\\nget the class names, and use data loaders to read the data.\\nOur page size will be four.\\nMeaning we'll use four images per batch\\nand by setting shuffle to true,\\nwe are going to have four different images.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2030650\",\"duration\":116,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualizing the data\",\"fileName\":\"3968391_en_US_02_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":169,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Branch > 02_04\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to code and define a function imshow() for visualizing the images from the dataset to ensure data has been loaded and preprocessed correctly.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4033117,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] To visualize our data,\\nwe are going to define a function called imshow\\nso we can explore what we have done.\\nThis function will take use\\nof Matplotlib's imshow function for tensors.\\nFirst, we are going to transpose the images\\ninto the shape 1, 2, 0 by using NumPy's transpose function.\\nThen we are going to multiply the images\\nby the standard deviation and add the mean.\\nWe are doing this because in normalization,\\nwe had to subtract the mean\\nand divide by the standard deviation.\\n\\nSo now, in order to visualize the correct images,\\nwe have to do the inverse mathematical operations.\\nTo get a clipped image,\\nwe are going to call NumPy's function called clip.\\nAnd next, to visualize our images,\\nwe are going to call function plt.imshow,\\nand function plt.title to get the plot title.\\nOkay, now let's run the code.\\nWe'll get a batch of four images from training data,\\nand we'll call a function, utils.make_grid,\\nto make a grid from the batch.\\n\\nFinally, we are going to plot our images,\\nmeaning, we are going to plot a grid with a title\\nthat concatenates all the class labels.\\nLet's run our code.\\nAnd we get cropped, transformed,\\nand de-normalized images from our training dataset.\\nGreat. Now we have four images from our training data\\nand each image has a class label displayed above it.\\nWe can copy paste this code\\nand just take validation images\\ninstead of training images.\\n\\nWhen we run the code,\\nwe get a batch of four cropped,\\ntransformed, and de-normalized images\\nfrom the validation set.\\nAfter playing with our data,\\nwe'll finally move to model training in our next chapter.\\n\"}],\"name\":\"2. Data Preparation\",\"size\":22888944,\"urn\":\"urn:li:learningContentChapter:2031636\"},{\"duration\":695,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2031633\",\"duration\":154,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to transfer learning\",\"fileName\":\"3968391_en_US_03_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":174,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Describe what transfer learning is and how it benefits model training.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5345557,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] As a mathematician, I love music.\\nWhat I particularly like is listening to classical music,\\nespecially one played on a piano.\\nImagine you have learned to play synthesizer\\nand one day you decide to learn to play a piano.\\nThat would be quite an easy task to handle\\nbecause those two instruments are similar,\\nand you can transfer your skill of playing the piano\\nto a new task of playing a synthesizer.\\nYou wouldn't have to learn notes, keys,\\nand timing from scratch.\\n\\nWe just have to reapply our knowledge\\nto a little bit different keyboard.\\nThere's a similar concept applicable to deep learning,\\nand it is called transfer learning.\\nIf you have a model that has been trained\\non a large amount of images representing\\ndifferent animals and objects,\\nthen you can take this model\\nand instead of building a new model\\nstarting from random initializations,\\nwe need to focus on the last few layers.\\nWe can use the power of a deep model\\nthat has been trained on a large set of images\\nand repurpose it to detect some specific images,\\nin our case, chihuahuas and muffins.\\n\\nSo by applying transfer learning,\\nwe make use of a pretrained model to build a model\\nthat is further trained for a different use case.\\nYou may wonder what other benefits transfer learning offers.\\nIf we would like to build effective model,\\nwe would have to train it on a very large data set\\nof diverse images.\\nThe first challenge is that the data set is often hard\\nto assemble and expensive.\\nAnother issue is training,\\nwhich requires significant computing resources.\\n\\nWhile training, CNN learns\\nto extract general features from images like edges,\\ncolor lines, and simple shapes,\\nand even combinations of shapes\\nand textures as well parts of objects.\\nWhen it is trained on a diverse set of images\\nand is sufficiently deep,\\nthese feature maps consists of a large library\\nof visual elements that can be assembled\\nand combined to form nearly every possible image.\\nTransfer learning uses this library of visual elements\\nthat are inside of the feature maps\\nover retrained neural network and applies them\\nto become specialized in identifying new classes of objects.\\n\\nSo by using transfer learning, we can save time and money\\nand improve sample efficiency, allowing a model\\nto learn the same behavior with fewer examples.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4021671\",\"duration\":199,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"ResNet model\",\"fileName\":\"3968391_en_US_03_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":240,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Explore the ResNet model and explain why ResNet is suitable for image classification.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5377393,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Back when I was a kid,\\nmore than three decades ago.\\nYes, and that all, we used to play\\na game called \\\"Telephone.\\\"\\nChildren would sit in the circle,\\nand first kid would whisper a message\\nin the ear of the kid sitting next to him.\\nThen the second kid whispers\\nwhat he or she has heard,\\nto the third kid, and so on.\\nThen the last player,\\nmeaning the last child in the circle,\\nhas to say the message out loud.\\nYou can probably guess\\nthat the last message is usually\\ncompletely different from the regional message,\\nand it often brings a lot of laughter.\\n\\nWhen training a deep learning model,\\na similar thing happens.\\nWe have a chain functions.\\nFor example, we could write it as Y equals F four,\\nof F three, of F two, of F one of X.\\nBackpropagation is adjusting the parameters\\nof each function in the chain\\nbased on the error recording,\\non the output of the last function, F four.\\nFor example, if we want to adjust the function F one,\\nwe have to see the error information\\nthrough the functions F two, F three, and F four.\\n\\nWhat happens when we try to accomplish that,\\nis each following function,\\nthe chain introduces some amount of noise.\\nWhen we have a deep chain of functions,\\nit overwhelms gradient information.\\nAnd backpropagation doesn't work anymore.\\nMeaning our model stops training.\\nWe call this kind of challenge\\na vanishing gradients problem.\\nTo solve this,\\nwe need to make each function in the chain\\nto be non-destructive,\\nmeaning to get back to a noiseless version\\nof the information contained in the previous input.\\n\\nWe can do that by using something called\\na residual connection.\\nHow do we do that?\\nBy adding the input of a layer of a block of layers\\nback to its output,\\nthe residual connection acts\\nas an information shortcut\\naround destructive or noisy blocks,\\nand enables error gradient information\\nfrom early layers,\\nto propagate noiselessly through a deep network.\\nThis solution was first presented\\nwith ResNet architecture just a decade ago in 2015.\\n\\nWe could pick up architecture for\\na model out of a few standard architectures\\nthat work most of the time.\\nI decided to go with ResNet because it is both fast\\nand accurate for many data sets and problems.\\nIf you go into PyTorch official documentation page,\\nyou can see we can choose a few model builders.\\nFor example, in ResNet 18,\\nthe number 18 refers to the number of layers\\nin this variant of architecture.\\n\\nOther options are 34, 50, 101, and 152.\\nWhat it means for us,\\nis if we decide to use a model\\nthat has an architecture with mold layers,\\nit will take longer to train,\\nand it's prone to our fitting.\\nMeaning you cannot train it for as many outputs\\nbefore the accuracy of validation set starts getting worse.\\nOn the other hand, in the case, use more data,\\nit can be quite a bit more accurate.\\n\\nNow that we understand the basics,\\nlet's head on to training our model.\\nI can't wait to get started.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4017665\",\"duration\":187,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Fine-tuning ResNet for binary classification\",\"fileName\":\"3968391_en_US_03_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":357,\"solutionVideo\":false,\"editingNotes\":\"00:40 - Overlay -  https://pytorch.org/docs/stable/torchvision/models.html\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Branch > 03_03\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Apply transfer learning to modify a pretrained ResNet model for binary classification.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6420696,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] After learning enough theory\\nabout the ResNet model,\\nwe can finally move back to coding.\\nAnd here's where things get exciting,\\nas we are going to train our model.\\nThe starting point for any binary image classification task\\nshould be looking for pretrained networks.\\nWe can go to PyTorch's official documentation site\\nand choose from plenty of pretrained networks\\nthat were trained for several hours\\nor days on many datasets\\nthat represent the most common objects.\\n\\nWe can directly leverage a pretrained network\\ninstead of training our own network from scratch.\\nIf you want to learn more about the available list\\nof pretrained models, you can head onto PyTorch\\nand visit their documentation page.\\nWe are going to use a pretrained model called ResNet18,\\nwhich has 18 layers.\\nWe'll use two functions called train_model\\nto train our model\\nand another function called visualize_model\\nso we can visualize it.\\nThe sample is a single image or a single row data.\\n\\nA batch is a group of images.\\nWe are going to run our training with four images at a time.\\nEpoch is the number of times a model will encounter\\nall the samples in the dataset.\\nOur model will only see four images at once,\\nso if we have 2,000 images,\\nthen we are going to run it 500 times to see all the images,\\nand that's one epoch.\\nEach epoch has a training and a validation phase.\\nWe are going to run the model with eight epochs\\nbecause it takes us around half an hour.\\n\\nIf we would like to achieve better results,\\nwe would run it at least for 20 epochs.\\nAs you progress, you can play around and increase\\nor decrease the number of batches in epochs\\nto achieve better performance of your model.\\nWe have loaded our ResNet18 model.\\nWe'll define a variable to store the number\\nof input features for the final layer.\\nThis way, we set the size of each epoch.\\nOkay, now we'll move the model to the device\\nto optimize the speed\\nand call a loss function called CrossEntropyLoss.\\n\\nThis loss function is usually used\\nwhen you have a classification task.\\nWe'll define an optimizer to optimize all parameters\\nof the model and a learning rate that will store\\nhow fast our model is running,\\nand it will decay after seven epochs.\\nFinally, we just need to call the train_model function\\nwith the ResNet model\\nand pass the model, the criterion, the optimizer,\\nthe schedule, and the number of epochs as parameters.\\n\\nWe have run our code,\\nso our model is running with eight epochs.\\nKeep in mind it can take up to even a few hours\\nfor the model to run, so I have already done it\\nand we can see the results.\\nWe are training the model with each epoch\\nand we are trying to minimize the loss.\\nAs you can see, the loss is decreasing\\nand accuracy is increasing for both testing and validation.\\nThose are the things we are looking\\nfor when training our model.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4022661\",\"duration\":155,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Freezing layers and training from specific blocks\",\"fileName\":\"3968391_en_US_03_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":162,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Explain why and how to freeze the last one or two blocks of ResNet.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4831453,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] pretrained models can classify\\nimages into categories, like objects, vehicles,\\nanimals, and plants.\\nWe can adapt these models to classify specific types\\nof animals such as dog breeds.\\nAs these are similar tasks,\\nwe should apply transfer learning\\nand reuse the pretrained network,\\nbut the pretrained network might not give us\\nthe best performance.\\nThe idea is to replace some layers.\\nThis process is called fine-tuning.\\nWe can define it as,\\nfine-tuning is a transfer learning technique\\nthat updates the parameters of a pretrained model\\nby training for additional epochs using a different task\\nfrom that used for pre-training.\\n\\nWe would like to reuse most of the training layers,\\nbut which layers should be replaced and why?\\nIn this case, tasks are very similar.\\nWe would likely keep all the hidden layers\\nand replace just the output layers\\nbecause they were designed\\nto classify the categories in the regional\\npre-training data set.\\nIt could also be useful to replace the upper hidden layers\\nof the regional model as high-level features\\nfor our task usually defer from the regional features.\\n\\nWhen we start training our model,\\nthe layers from the pretrained model\\nhave already been trained for many epochs\\non a large data set.\\nAs we replace the output layers,\\nthe weights in the output layers are now entirely random.\\nThe first few layers, which stay the same,\\nencode general concepts,\\nsuch as finding gradients and edges,\\nand output layers encode concepts\\nthat are specifically useful for finding dog breeds,\\nsuch as finding eyeballs and fur.\\n\\nIn this case, when we replace the random weights\\nin our added linear layers with weights\\nthat correctly achieve our desired task,\\nwe have a significant risk\\nthat the learning algorithm would break\\npretrained weights and other layers.\\nTo address this, we instruct the optimizer\\nto update the weights in only new\\nrandomly added output layers,\\nleaving the pretrained layers untouched.\\nThis technique is called freezing pretrained layers.\\nThe benefit of it is that the training process\\nwill be faster because the number\\nof adjustable parameters is significantly smaller.\\n\\nAfter we train the model with the pretrained layers frozen,\\nwe can try fine-tuning our model by unfreezing those layers\\nand training for another few epochs\\nwith a smaller learning rate.\\n\"}],\"name\":\"3. Transfer Learning with Pretrained Model\",\"size\":21975099,\"urn\":\"urn:li:learningContentChapter:2030652\"},{\"duration\":1214,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4019675\",\"duration\":271,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Setting up the training loop\",\"fileName\":\"3968391_en_US_04_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":402,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Branch > 04_01\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Describe the components of a training loop and define functions for training the model.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9415643,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's dive into understanding the function\\nfor training our model.\\nFirst, we define a variable called since that we will use\\nto store the total time it takes\\nto complete the training process.\\nWe set it to the current time\\nby using the time.time function.\\nLet's create the backup of the model starting state\\nand call it best_model_vts.\\nWe'll do that by calling the two functions, copy.deepcopy\\nand model_state_dict.\\nSo we can return a dictionary containing all the parameters\\nof the model, meaning weights\\nand biases for each layer at the exact moment.\\n\\nWe are doing this to save the model\\nwhen it's performing the best.\\nNext, we'll also create a variable called best_accuracy\\nso we can track the best validation accuracy\\nduring the training.\\nLet's initialize it to zero.\\nAfter each epoch,\\nwe'll update this value with the best accuracy achieved\\nwith the model during the training process.\\nLet's understand the training loop.\\nAs you can see, we have three, four loops.\\nTraining happens in the cycles we called Epoch.\\n\\nOne epoch is when the model sees all the training data once.\\nWe loop through the number of epochs,\\nin our case we will set it to eight\\nand we will print the current epoch\\nso we can track our progress.\\nWe need the for loop to run for as many epochs.\\nWe are set it in the number of epochs.\\nEach time it runs,\\nit represents one pass through the data to train the model,\\nevaluate performance, and update the models parameters.\\nNow we want to display the current epoch number\\nand the total number of epochs to trace the progress.\\n\\nTo do that, we'll use the print function\\nand we'll also print a separate line\\njust to make our output easier to read.\\nWe'll use the next for loop to iterate through the training\\nand validation phase.\\nDuring the training phase, the model is learning\\nand we update the model's weights\\nso that certain layers act differently.\\nSo we have a condition to check if the current phase\\nis training.\\nIf the phase is not training,\\nthen we switch the model to evaluation mode.\\n\\nFinally, we use our third for loop to iterate\\nthrough batches of data provided\\nby the data loader dictionary,\\nwhich contains train\\nand validate data sets.\\nEach batch contains inputs, meaning images,\\nand corresponding class labels.\\nThen we create two variables called inputs\\nand labels, so we can move\\nand store the input images to a specified device,\\nand we can also move\\nand store labels to the same devices model.\\nThe next step in the model training is called Forward Pass.\\n\\nOptimizer.zero_grad function resets the gradients\\nof all models parameters to zero\\nto ensure they don't accumulate from previous batches.\\nThen we enable\\nor disable gradient calculation based on the current phase.\\nIf it is training, we set it to true.\\nIf it's validation, we set it to false.\\nThe input data, meaning inputs is passed through the model\\nto get prediction, meaning outputs,\\nand the loss function calculates the difference\\nbetween these predictions and the actual labels.\\n\\nIn the training phase,\\nwe calculate gradients using loss.backward function.\\nIt tells the model how to adjust the parameters to improve.\\nThe optimizer.step function updates the parameters.\\nYou can think of this step\\nas fine tuning the model's brain to learn better.\\nIn the next two lines, we track the model's progress\\nby computing loss statistics.\\nRunning_loss, adds up the errors of the current phase.\\nRunning_correct, counts how many predictions are correct,\\nand then we use the print function to display them.\\n\\nAfter each epoch,\\nwe compare the validation accuracy with the best accuracy.\\nIf the model performs better, we save its weights.\\nDuring the validation phase,\\nif the current epoch accuracy\\nis better than the best accuracy so far,\\nwe save the model's ways using copy.deepcopy.\\nAfter training is done,\\nwe print the total time it took for training,\\nalong with the best validation accuracy.\\nFinally, we update the model with the saved best weights\\nand return it.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4021670\",\"duration\":237,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Loss function and optimizer\",\"fileName\":\"3968391_en_US_04_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":301,\"solutionVideo\":false,\"editingNotes\":\"03:19 - Overlay - https://pytorch.org/docs/stable/optim.html\\n04:45 - Overlay - https://pytorch.org/docs/stable/nn.html#loss-functions\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Identify appropriate loss functions and optimizers.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6807923,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The loss function is one of the crucial parts\\nof an effective deep learning model.\\nPyTorch uses the loss function\\nto quantify not just whether a prediction is right or wrong,\\nbut just how wrong or right it is.\\nBased on the loss function,\\nit determines how to update the network\\nto get the desired results.\\nThe loss function is sometimes also called an error function\\nor objective function.\\nLet's understand the whole process in general\\nbefore moving on to which loss function should we pick.\\n\\nWhen we begin with training,\\nwe can approach it in two ways.\\nBegin training from scratch,\\nin which case the weights of our model would be random.\\nIn this approach,\\nthe output that we get\\nwon't be anything close to what we want.\\nWe can use a pre-trained model.\\nThis is what we are doing,\\nand it's called transfer learning.\\nIn this approach,\\nthe model will also need to adjust the weights.\\nWe use a loss function\\nto compare the model's output with our targets.\\nWe know what results we should expect\\nbecause we are using label data.\\n\\nThe loss function returns the number\\nthat we want to lower as much as we can,\\nand we can do that by improving the weights.\\nHow do we do that?\\nBy taking the images from the training set\\nand feeding them to our model,\\nthen we compare the outputs with the targets\\nby using loss function.\\nIt tells us how good or wrong our predictions are.\\nThen we need to tweak the weights to make them better.\\nWe do that by using something called gradients.\\nYou can think of gradients using the simple analogy.\\n\\nImagine you are hiking in the mountains\\nand you want to find the river to get some fresh water.\\nThe river is placed at the lowest point,\\nso you would go downhill\\nand always take a step\\nin direction of the steepest downward slope\\nto get to the river.\\nThe steepness of the slope is the magnitude of the gradient,\\nand it shows us how big step we should take.\\nThe learning rate is a key parameter\\nthat needs to be tweaked\\nto get our network learning properly and efficiently.\\n\\nIn our case,\\nwe we'll multiply the gradient by the learning rate,\\nwhich is a number representing step size.\\nAfter that, we iterate until we reach the lowest point.\\nA gradient is a measure of how that loss function changes\\nwith small changes to the weights.\\nWe are going to use one of the most popular gradients\\ncalled stochastic gradient descent, or SGC,\\nand it is the traditional approach\\nto optimizing neural networks.\\n\\nHowever, other optimizers are available for deep learning.\\nYou can find out more about them\\non PyTorch's official page.\\nConstructing or choosing loss function is important\\nbecause it determines the network's knowledge.\\nYou can have two neural networks\\nwith identical starting weights,\\nand you can train them over the same data set.\\nAnd if you choose a different loss function,\\nthey will learn different patterns.\\nFor example, in the case the model isn't learning properly,\\nyou can transform the loss function\\nby choosing smaller layer sizes,\\nuse larger data sets,\\nor different regularization techniques.\\n\\nEach of these choices will have an effect\\non the loss function\\nand ultimately on the behavior of your network.\\nA great thing to know\\nis that we don't have to write our own loss function\\nas PyTorch already has a variety of built-in loss functions\\nto pick from.\\nThey're provided in the torch.nn package.\\nWe are going to use a PyTorch loss function\\ncalled CrossEntropyLoss,\\nwhich is usually used for single-label classification.\\n\\nYou can find out more about the loss functions\\non the PyTorch's official documentation page.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2030649\",\"duration\":183,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Evaluating model performance\",\"fileName\":\"3968391_en_US_04_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":242,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":true,\"exerciseFileDisplayText\":\"Branch > 04_03\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Analyze the model's performance and identify key evaluation metrics such as accuracy, precision, and recall.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5424763,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We have trained our model\\nand done everything we could to minimize the loss.\\nSo we are done with development, right?\\nNot exactly, because that would be similar\\nto concluding that we are done\\nwith developing a feature and then skipping writing unit\\nand integration tests.\\nHere we should apply the same approach\\nas for developing any application.\\nSo model development includes validation and testing loops.\\nWhy do we need those to be confident\\nthat overfeeding doesn't occur\\nand that our model will perform well\\nagainst data it hasn't seen before?\\nOverfeeding usually occurs when the model gives accurate\\npredictions for training data, but not for the new data.\\n\\nIt can happen because of few reasons.\\nThe training data has a large amounts\\nof irrelevant information called noisy data.\\nThe training data is too small\\nand does not contain enough data samples\\nto accurately represent all possible inputs.\\nThe model is training for a long period\\nof time on a single sample set of data.\\nThe complexity is high, so the model learns\\nthe noise between the training data.\\nAs we have seen at the beginning\\nof our binary classification journey,\\nwe use as part of this initial data for validation.\\n\\nSo we are using training data to train the neural network,\\nand we are using validation data\\nto test the model's performance at the end of each epoch.\\nIn our example, we have a data set\\nthat has been split into training data set\\nand a validation data set.\\nHowever, sometimes we don't have it prepared in advance.\\nIn that case, we can use the random_split() function\\nfrom torch.utils.data.\\nFor example, if we have 10,000 images in the data set,\\nthen we would split them into 8,000 training images\\nand 2,000 validation images and create their training\\nand validation sets by typing training_set,\\nvalidation_set = random_split (training_data, [8000, 2000]).\\n\\nWhen we perform validation, the model passes the data\\nthat seized for the first time,\\nmeaning that data wasn't used during training.\\nAs you can see, we have displayed the training\\nand validation loss and accuracy after each epoch.\\nYou can easily spot that both training loss\\nand validation loss have decreased,\\nand training and validation accuracy have increased,\\nmeaning our model is performing great.\\nHowever, sometimes we have a situation\\nwhere the training loss decreases,\\nbut the validation loss doesn't.\\n\\nWe call this overfeeding.\\nSometimes we can still have overfeeding despite\\nthe fact We have good validation results.\\nFor example, maybe the training data contains too much\\nirrelevant information that has affected this result,\\nor the training data is too small\\nor there's some other reason.\\nThat's why we need to test the model.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2030648\",\"duration\":115,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Saving the model\",\"fileName\":\"3968391_en_US_04_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":163,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Branch > 04_04\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Demonstrate how to save the trained model in the case when we are content with the performance.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3881309,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that we have the model\\nthat is performing well,\\nwe would like to save the current state of the model\\nand reload the saved iteration of the model\\nwith our trained parameters when we need it.\\nLet's learn how to do that with PyTorch.\\nThere are two functions that can help us called torch.save,\\nwhich saves the current state\\nof the model in Python's pickle format.\\nAnd torch.load function\\nthat loads the previously saved iteration of the model.\\nTo save our model, we'll use torch.save function\\nthat saves a dictionary\\nthat contains the model state dictionary,\\noptimizer state dictionary if there is one,\\nthe current epoch number,\\nand the path that indicates the file path\\nwhere the dictionary will be saved.\\n\\nWe have called our model model_ft,\\nso to save it we'll call torch.save,\\nand as the parameters will pass the path to our model.\\nThis way, we store the parameters\\nand the structure of the model to file.\\nGreat, to load the model, we'll call the load function\\nand pass as a parameter the path.\\nHowever, sometimes we have to\\nchange the structure of the model.\\nIn this case, it is better to save a model\\nstate_dict instead.\\n\\nState_dict is a standard Python dict,\\nso it contains the maps\\nof each layer's parameters in the model.\\nTo save it we'll call torch.save function again\\nand pass this parameter model state_dict and the path.\\nTo restore our model, we first have to create an instance\\nof the model and then use the load state_dict function.\\nThe advantage of using this function is the case\\nwhen we want to extend the model.\\nThis function load state_dict assigns parameters\\nto layers of the model that are present\\nbut doesn't fail if some layers are missing or are added.\\n\\nIt's especially useful when we are pulling in the parameters\\nfrom a different model.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4017664\",\"duration\":149,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualizing predictions\",\"fileName\":\"3968391_en_US_04_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":222,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Branch > 04_05\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Apply techniques to visualize predictions.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4921125,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's create a function\\nto visualize the results of our model.\\nWe'll call it Visualize Model.\\nAs parameters will take our trained model\\nand the number of images we'll put six.\\nNow create a variable called was training\\nthat is equal to model.training.\\nWe'll use it to restore the model later.\\nNext, we'll set the model to evaluation mode\\nby calling model eval function.\\nDysfunction will disable behaviors like dropout\\nthat are only used during training.\\n\\nWe are going to create a variable called images so far\\nto keep the track of how many images\\nwe have visualized so far, and another variable fig,\\nwhich is equal to PLT figure function.\\nSo we have a figure where images will be plotted.\\nThere is one more thing we have to do before looping\\nover the validation data.\\nWe have to disable gradient calculations\\nbecause we are not training the model.\\nWe'll use a for loop to iterate our validation dataset\\nand data loader as well to get the images\\nand their corresponding labels.\\n\\nLet's create a variable called outputs\\nto store the predictions.\\nTo find the class with the highest score for each image,\\nwe'll call a function torch.max\\nand store it in a variable.\\nWe'll also move the images\\nand inputs from the CPU to the GPU for faster processing.\\nFinally, we'll loop through each image in the batch\\nand called plt.subplot function\\nto create a subplot for each image.\\nWe'll hide access for a cleaner look\\nand set the title to display the predicted class name.\\n\\nTo display our images,\\nwe'll call Imshow function,\\nand then convert the image back to CPU Tensor\\nby calling inputs.cpu data.\\nIn the end, we just need to check if the number\\nof the images we have displayed is equal to num images.\\nIf it is, we'll stop the model\\nand restore the model to its original training mode.\\nNow that we understand\\nhow our visualized model function works,\\nwe just need to call it and pass in our model\\nand display it using the PLT show function.\\n\\nI have already run the code so we don't have to wait\\nfor the model to finish training.\\nAs you can see, we got six images\\nand our model predicted all of them correctly except\\nthe last one,\\nwhich seems more like a palm tree.\\nSo it obviously got it wrong.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2032689\",\"duration\":103,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Evaluate and test the model\",\"fileName\":\"3968391_en_US_04_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":102,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this challenge, evaluate a model using different performance metrics.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3001731,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(bright lively music)\\n- [Instructor] Are you ready for a challenge\\nto test your knowledge?\\nFor this challenge,\\nyour task is to change the loss function,\\nand evaluate the performance of your model.\\nInstead of using the cross-entropy loss function,\\nyou're going to use the binary cross-entropy\\nwith logits loss, or short, BCEWithLogitsLoss\\nYou can find more about it.\\non the official PyTorch documentation page.\\n\\nIt is a loss function in PyTorch\\nthat combines a sigmoid layer,\\nand the binary cross-entropy loss into one single class.\\nThis is variable for binary classification tasks\\nwhere the model outputs logits instead of probabilities.\\nThe BCEWthLogitsLoss function\\nhelps avoid issues with floating point precision\\nthat can occur when using separate sigmoid\\nand binary cross-entropy functions.\\nTo use this function,\\nyou will have to make a few changes in your model\\nthat are connected with labels and output.\\n\\nFirst, call the BCEWithLogitsLoss function\\ninstead of cross-entropy loss function.\\nIn the train_model function\\nyou have to change labels to different shape.\\nThree, use torch.sigmoid outputs to calculate predictions.\\nAnd four, set a size which output to one instead of two.\\nRun the model after applying those changes.\\nDoes the training behavior change?\\nHow does the model perform on validation set?\\nThis challenge should take about 15 minutes.\\n\\nWhen you're done, or you get stuck,\\nyou can check out the solution video\\nto see how I solved the challenge.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4018650\",\"duration\":156,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Evaluate and test the model\",\"fileName\":\"3968391_en_US_04_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":222,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"exerciseFileDisplayText\":\"Branch 04_07\",\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn one solution for evaluating the model using different performance metrics.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6212965,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] We have to change the loss function\\nand evaluate the performance of our model.\\nInstead of using the cross-entropy loss function,\\nyou are going to use the binary cross-entropy\\nwith logits loss, or short, BCEWithLogitsLoss.\\nTo use this function,\\nyou will have to make a few changes in our model\\nthat are connected with labels and output.\\nYou could use everything from our previous project\\nand just change a few lines of code.\\n\\nLet's change the criterion and call our new loss function.\\nSo we'll set a criterion equal to nn.BCEWithLogitsLoss.\\nOkay, we also have to set the size\\nof each output sample to 1 instead of 2.\\nWe'll do that when defining model_ft.fc\\nby calling nn.Linear function\\nand passing 1 as our second parameter.\\nGreat. We still have some work to do\\nin our train_model function.\\n\\nWe have to make sure labels are float\\nand have the correct shape.\\nBy calling to(device) method,\\nwe move labels to GPU if it's available,\\nand then call the float function,\\nwhich will convert them from int64 to float32,\\nand then call the unsqueeze function\\nto reshape them to being the same size as the model output.\\nOkay, now that we have set the size\\nof each output to 1 instead of 2,\\nour model now outputs a single login.\\n\\nWe still have one challenge.\\nOur new loss function outputs raw logits\\ninstead of probabilities.\\nTo fix that, we'll call the sigmoid function\\nthat squashes these logits into a probability range\\nbetween 0 and 1.\\nThe model will predict class 1\\nif the probability is greater than 0.5,\\nand 0 otherwise.\\nSo let's make it greater than 0.5\\nand call float to convert our boolean.\\n\\nOkay, let's run our code and see how our model performs.\\nI have already done model training to save us the time.\\nGreat. We see we have no errors.\\nNot only that, but by looking at the results,\\nwe see that both training and validation loss are decreasing\\nand accuracy for both the training set\\nand the validation set are increasing,\\nso our model is performing great.\\n\"}],\"name\":\"4. Training and Testing the Model\",\"size\":39665459,\"urn\":\"urn:li:learningContentChapter:2032692\"},{\"duration\":1800,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2031632\",\"duration\":0,\"visible\":false,\"requiredForCertificateOfCompletion\":null,\"name\":\"Fake Video: Capstone project requirements\",\"fileName\":\"3968391_en_US_05_01_FAKEVIDEO_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":900,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":true,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video you're explained the purpose and the requirements of the capstone project.\",\"captionsStatus\":\"REQUESTED\",\"cdnStatus\":\"NOT_ATTEMPTED\",\"size\":0,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"NEW\",\"transcript\":null},{\"urn\":\"urn:li:learningContentArticle:1730001\",\"duration\":1800,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Capstone Project\"}],\"name\":\"5. Capstone Project\",\"size\":0,\"urn\":\"urn:li:learningContentChapter:2031637\"},{\"duration\":37,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4017663\",\"duration\":37,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"3968391_en_US_06_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":41,\"solutionVideo\":false,\"editingNotes\":\"x overlay - @Tsemenski\",\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn where to go from here: define a road map for further learning.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":912193,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Terezija] I hope you enjoyed the course\\nand now have a solid foundation of PyTorch\\nfor image classification.\\nA good next step would be to look into our library\\nfor more PyTorch and deep learning courses.\\nNow, if you're keen to improve your PyTorch skills,\\nthen go ahead and use Kaggle\\nto explore other image datasets.\\nI want to wrap up by saying thank you.\\nI really appreciate your engagement\\nand would love to hear your feedback on this course.\\nIf you enjoyed this course, please let me know.\\n\\nYou can find me LinkedIn and Twitter,\\nand can message me there to let me know\\nhow you're using PyTorch.\\n\"}],\"name\":\"Conclusion\",\"size\":912193,\"urn\":\"urn:li:learningContentChapter:2032693\"}],\"size\":122673657,\"duration\":5514,\"zeroBased\":false},{\"course_title\":\"Natural Language Processing with PyTorch\",\"course_admin_id\":3004335,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":3004335,\"Project ID\":null,\"Course Name\":\"Natural Language Processing with PyTorch\",\"Course Name EN\":\"Natural Language Processing with PyTorch\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"Learn about natural language processing with PyTorch, the popular deep learning tool used by tech giants like OpenAI and Microsoft. In this course, Zhongyu Pan guides you through the basics of using PyTorch in natural language processing (NLP). She explains how to transform text into datasets that you can feed into deep learning models. Zhongyu walks you through a text classification project with two frequently used deep learning models for NLP: RNN and CNN. She also shows you how to tune hyperparameters and construct model layers to get more robust and accurate results, as well as the differences between the two models for NLP tasks.\",\"Course Short Description\":\"Learn the basics of using PyTorch, a powerful deep learning tool, for natural language processing.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":20160017,\"Instructor Name\":\"Zhongyu Pan\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Content Creator at LinkedIn\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2022-04-15T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/natural-language-processing-with-pytorch\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Artificial Intelligence for Technology\",\"Primary Software\":\"PyTorch\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":2464.0,\"Visible Video Count\":12.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":161,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3070068\",\"duration\":77,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Natural language processing\",\"fileName\":\"3004335_en_US_00_01_WX30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"In this video, explore the course overview and learn the prerequisites for the course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2854751,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Pan] We interact with machines  \\n most commonly through natural languages  \\n like English, Chinese, or Spanish, you name it.  \\n We ask our smartphones to open our favorite playlist  \\n on the music app.  \\n We type a sentence in Google Translate  \\n to be able to communicate with people  \\n from a different culture.  \\n Natural language processing, or NLP for short,  \\n gives machines the ability to understand human languages  \\n that we use in daily life accurately and efficiently.  \\n It has also undergone a massive transformation  \\n with the introduction of Deep Learning.  \\n PyTorch, which is one of the most popular  \\n Deep Learning libraries, developed by Meta,  \\n makes NLP even easier to learn and work with.  \\n Hi, my name is Pan Zhongyu.  \\n I work as a data scientist and software developer.  \\n I have built NLP-related systems  \\n for companies with knowledge of Deep Learning.  \\n Join me on my LinkedIn Learning course  \\n as we explore the exciting field  \\n of natural language processing with PyTorch.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3068059\",\"duration\":84,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"3004335_en_US_00_02_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"In this video, learn about the benefits of PyTorch and whether it is the best fit for you.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2810381,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this course,  \\n I will be giving you brief introductions  \\n to natural language processing, deep learning and PyTorch.  \\n You will learn how to pre-process  \\n and transform the text data into numeric datasets  \\n that can be fed into deep learning models.  \\n Then, you will be guided through  \\n a text classification project  \\n where you build and train a deep learning model  \\n for classification predictions.  \\n If you have never worked  \\n with natural language processing before, don't worry.  \\n This course is designed for anyone  \\n with a basic Python programming background  \\n who wants to dive into NLP  \\n with the powerful tool of PyTorch.  \\n You will need to have knowledge of key concepts in Python,  \\n like variables, functions, classes and so on.  \\n It will also help if you are familiar  \\n with the use of Python packages,  \\n like Numpy, pandas, NLTK.  \\n We assume that you have some deep learning exposure,  \\n you understand the basics of neural networks  \\n so that when we are introducing the PyTorch way  \\n of building neural nets,  \\n it will be easier for you to follow.  \\n We are going to use Google Colab throughout the coding part  \\n and I will help you set this tool up before we start coding.  \\n Let's dive right in.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":5665132,\"urn\":\"urn:li:learningContentChapter:3069059\"},{\"duration\":353,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3068060\",\"duration\":134,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Popular topics in NLP\",\"fileName\":\"3004335_en_US_01_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn the basic definition of NPL, its popular fields, and why you should learn it.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4490083,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] So, what is natural language processing?  \\n There can be multiple interpretations,  \\n but one way of defining it  \\n is that natural language processing  \\n is a subfield of artificial intelligence  \\n that helps machines better understand  \\n or generate human languages.  \\n NLP is therefore divided into two major components,  \\n natural language understanding  \\n and natural language generation.  \\n As the names imply, natural language understanding  \\n enables machines to understand the meaning of the text  \\n while natural language generation instead  \\n produces text responses based on specific text input.  \\n We will mainly focus on natural language understanding  \\n in this course.  \\n Let's take a look at some popular applications of NLP.  \\n One of the most important applications  \\n is sentiment analysis.  \\n It is a type of text classification task  \\n and it has been largely implemented by enterprises  \\n for understanding customer feedback,  \\n monitoring social media,  \\n conducting market research, et cetera.  \\n The second application is chatbot,  \\n which has been usually applied as a tool of customer support  \\n by various companies that need to interact with customers.  \\n It is also used for marketing or sales purposes.  \\n Speech recognition allows us to communicate  \\n with Apple's Siri or Amazon's Alexa  \\n just like the way we talk to a real assistant.  \\n Machine translation is also a common use case of NLP.  \\n One popular application is Google Translate,  \\n which translates one language to another  \\n in a very short period of time.  \\n One of the coolest applications is advertisement matching,  \\n which you can see probably every day on YouTube.  \\n The most relevant ads are displayed  \\n based on your personal preferences.  \\n The NLP application we're going to build in this course  \\n is text classification, stay tuned.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3073044\",\"duration\":219,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introducing deep learning\",\"fileName\":\"3004335_en_US_01_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to describe deep learning at a high level and intuitively explain how deep learning works under NLP concepts.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7773368,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Deep learning is a subset  \\n of machine learning,  \\n and machine learning is a subset of artificial intelligence.  \\n In this video, we are going to introduce three types  \\n of deep learning models:  \\n artificial neural networks,  \\n convolutional neural networks,  \\n and recurrent neural networks,  \\n about how they work at a high level  \\n and compare their differences.  \\n Note that CNN and RNN are both ANN models.  \\n Let's first look at artificial neural networks  \\n or ANN for short.  \\n It normally has one input layer,  \\n one or more hidden layers  \\n and one output layer.  \\n The data is fed into the input layer,  \\n then calculated and extracted by hidden layers.  \\n And finally, the output layer delivers the result  \\n based on the information simplified  \\n by previous layers.  \\n However, regular ANN does not work so well  \\n in natural language processing or image processing tasks.  \\n So let's look at convolutional neural networks, or CNN,  \\n which performs better in both tasks.  \\n But why does CNN work better?  \\n Because it works spatially along the data  \\n and it focuses more on the feature  \\n rather than on the position of the data.  \\n The unique layer structures of CNN  \\n are convolutional layers and pooling layers,  \\n compared with a regular ANN.  \\n The world convole refers to the filtering process.  \\n This filtering process simplifies the information included  \\n within each layer  \\n so that it is easier for the next layer  \\n to process the output as shown in the picture below.  \\n This way it is convenient  \\n to find important features and ignore the trivial ones  \\n so that we get more solid results.  \\n CNN is also the deep learning model we're going to build  \\n for solving a natural language processing task  \\n in this course.  \\n Another major type of deep learning model  \\n is called recurrent neural networks or RNN.  \\n The structure of RNN is similar to a regular ANN,  \\n which contains one input layer,  \\n one or more hidden layers  \\n and one output layer.  \\n The difference between RNN and CNN  \\n is that RNN not only passes data forward  \\n throughout the layers  \\n but also feeds the data back into itself  \\n while CNN passes data only one way forward.  \\n That means RNN can remember the context before  \\n and after the current word in a sentence.  \\n An RNN is therefore well-suited for sequential data,  \\n such as text or audio.  \\n Another thing to keep in mind  \\n is that CNN is on average faster than RNN  \\n in processing text data.  \\n I hope you now have a general understanding  \\n of how these deep learning models work differently.  \\n In the next chapter,  \\n we're going to get our hands dirty  \\n by writing code and we will introduce PyTorch,  \\n the essential framework  \\n for learning natural language processing.  \\n So you in the next chapter.  \\n \\n\\n\"}],\"name\":\"1. NLP with Deep Learning Introduction\",\"size\":12263451,\"urn\":\"urn:li:learningContentChapter:3071047\"},{\"duration\":390,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3067062\",\"duration\":102,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Why PyTorch?\",\"fileName\":\"3004335_en_US_02_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, explore which tech giants are using PyTorch for NLP and its most popular uses and functionalities within the tech sector.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3747172,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Speaking of deep learning,  \\n PyTorch, developed by Meta,  \\n and TensorFlow, developed by Google Brain,  \\n are two open source Python libraries  \\n that are extensively used nowadays  \\n and they do share some similar features.  \\n So why do we use PyTorch instead of TensorFlow  \\n in this course?  \\n PyTorch has a reputation for being more applied  \\n in research than in production  \\n and most of the top AI papers use PyTorch  \\n as their deep learning framework  \\n while TensorFlow is widely used in industry  \\n by tech companies for deploying AI products.  \\n But for people who are just getting started with NLP,  \\n it is recommended to learn it with PyTorch.  \\n The reasons are;  \\n first, PyTorch was created to make  \\n the models easier to write.  \\n The code in PyTorch is relatively more concise,  \\n which makes it beginner-friendly.  \\n Second, the statements in PyTorch are designed  \\n to mimic the ones in Python.  \\n So if you have previous working knowledge of Python,  \\n you will feel pretty natural coding in PyTorch.  \\n Third, although PyTorch is more geared toward research,  \\n it has been used by large enterprises  \\n for building NLP-related services.  \\n For instance, Airbnb used to build dialogue assistant  \\n using PyTorch for improving the customer service experience.  \\n PyTorch has also been used by Microsoft  \\n for its language modeling service.  \\n Without further ado, let's start coding.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3073045\",\"duration\":288,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"PyTorch tensor\",\"fileName\":\"3004335_en_US_02_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn what a PyTorch tensor is and how to implement the basic calculations and functions of PyTorch Tensor.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10976137,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this video,  \\n we will introduce the PyTorch tensor  \\n and the common methods of tensors  \\n but before that, let's first set up Google Colab.  \\n Make sure you have a Google account  \\n and you're able to access your Google Drive.  \\n Then visit colab.research.google.com.  \\n There is this documentation of welcome to Colab.  \\n You can read it briefly if you're interested  \\n but we will cover everything you need to know  \\n for learning this course.  \\n Now please open your exercise file  \\n of this video in Google Colab  \\n and code along with me.  \\n We can think of a tensor as a data container  \\n or data structure  \\n that carries arrays of numbers in PyTorch.  \\n Let's first look at how to create a PyTorch tensor.  \\n If you don't have Torch library yet,  \\n type pip install torch  \\n and run the cell to install the package.  \\n I have already installed the package,  \\n so it says requirement already satisfied.  \\n We then import Torch library and NumPy.  \\n Now, let's create a PyTorch tensor with an array.  \\n We already had an array here.  \\n So we type torch.tensor array  \\n and name it tensor0.  \\n And then we print out tensor0  \\n as well as its data structure type,  \\n which is torch.tensor,  \\n and data type, which is the type  \\n of the data carried by the tensor.  \\n In this case, it's integer.  \\n We also printed out the shape of the tensor  \\n where the first number in the case, the number of rows  \\n and second number in the case, the number of columns.  \\n Another way of constructing a tensor  \\n is from a NumPy array.  \\n This is basically the same as what we just did  \\n but instead of a regular array,  \\n we put a NumPy array  \\n inside of torch.tensor container, like this.  \\n Now let's take a look at some common methods in PyTorch.  \\n The first one is slicing.  \\n Slicing of tensors is basically the same  \\n as the slicing of NumPy arrays.  \\n We have tensorA and B here.  \\n And we can slice the first two rows of tensorA like this  \\n where index one is inclusive  \\n and index two exclusive.  \\n If we want to slice the first two columns,  \\n we will need to first take all the rows  \\n and then select from columns, just like before.  \\n The second common method is concatenation.  \\n Concatenation is extremely useful  \\n when we're building deep learning models.  \\n By using torch.cat method,  \\n we can concatenate two tensors vertically  \\n when both tensors have the same amount of columns.  \\n Don't forget to wrap the two tensors in one array.  \\n We can also concatenate them horizontally  \\n and specify dimension as one.  \\n Let's print out the results  \\n to see what the concatenated tensors look like.  \\n Okay, the first one is the vertical concatenation  \\n of the two tensors  \\n and the second one is the horizontal concatenation.  \\n Those are exactly what we want.  \\n One thing to point out is that torch.cat performs  \\n vertical concatenation by default.  \\n So if you want to apply horizontal concatenation,  \\n you will need to add an argument.  \\n Then equals one to get the desired tensor.  \\n Congratulations, you just finished learning the basics  \\n of PyTorch tensor.  \\n In the next chapter, we will start our guided project  \\n by building a convolutional neural network  \\n for solving a tax classification task.  \\n See you in the next chapter.  \\n \\n\\n\"}],\"name\":\"2. PyTorch Basics\",\"size\":14723309,\"urn\":\"urn:li:learningContentChapter:3068062\"},{\"duration\":1513,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3073046\",\"duration\":557,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Preprocessing text dataset\",\"fileName\":\"3004335_en_US_03_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to preprocess text data by learning how to load and transform text into a format that can be fed into a DL model. Explore different ways of preprocessing and learn the difference between them.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17603256,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] From now on,  \\n we will be working on the guided project  \\n which is text classification  \\n using convolutional neural networks.  \\n The first step of the project  \\n is to pre-process text dataset,  \\n which is the goal of this video.  \\n Please download the exercise file of this video  \\n and open it in Google CoLab.  \\n Before we start coding,  \\n let's change runtime type to GPU  \\n to ensure the code runs as fast as possible.  \\n Simply click on Runtime.  \\n Then change the runtime type to GPU.  \\n Please keep in mind that when you change runtime type,  \\n your Jupyter Notebook will be restarted,  \\n and you will have to rerun all the code.  \\n So make sure you change runtime type  \\n before you start typing any code.  \\n Let's first import the required libraries.  \\n Here we import torch from torchtext dot legacy,  \\n import data and datasets.  \\n Lastly, we import random.  \\n Then in the second cell at line number two,  \\n we call torch dot manual seed here  \\n to set the seed of the random number generator,  \\n so the random values generated by torch  \\n will be reproducible the next time we run the code.  \\n You can also set the seed value  \\n to any number you want.  \\n To check if you actually switched your runtime type  \\n to GPU successfully,  \\n you can run line number four and five  \\n to print out your current device.  \\n If it prints out cuda,  \\n then congratulations.  \\n That means you have your GPU ready.  \\n If not,  \\n it means you are still using CPU,  \\n which will make your model run slower.  \\n But don't worry.  \\n Even with CPU,  \\n you can still complete our project,  \\n since CNN is by itself a lot faster  \\n than most of the deep learning models.  \\n Now let's define the fields,  \\n text field and label field.  \\n We are using data dot Field and data dot LabelField here.  \\n And you can read more about them in this documentation.  \\n So field and label field are basically the same things,  \\n except that label field doesn't take sequential data  \\n like text,  \\n which is reasonable,  \\n since label field takes in classification labels  \\n rather than the text.  \\n And for field,  \\n it basically models our text data  \\n to numerical representations carried by tensors.  \\n We set tokenize method to spacy.  \\n And we will lowercase all the text.  \\n Now you must be wondering,  \\n what does the data actually look like?  \\n Let me show you.  \\n PyTorch has some built-in datasets  \\n that we can import directly.  \\n And the datasets we are using here  \\n is called Text Retrieval Conference or TREC dataset.  \\n It's question classification dataset.  \\n So each piece of text in this dataset is a question.  \\n And each question is labeled as one of the six categories.  \\n The table here gives us an idea  \\n of what the TREC dataset actually looks like.  \\n Each data example in a dataset  \\n contains a question and a label.  \\n Each question in the first column of the table  \\n is labeled as one of the six categories,  \\n which are in the second column.  \\n And the third column is just the full name of those labels.  \\n The six categories are abbreviation,  \\n numeric, human, entity, description and location.  \\n These names are pretty self-explanatory.  \\n Let's take a look at a few examples here.  \\n The answer to the question,  \\n what is the date of boxing day,  \\n is clearly a date,  \\n so this question is labeled as numeric.  \\n If the question is asking about the city's name,  \\n like what is California's capital,  \\n then it is labeled as location.  \\n As we mentioned,  \\n the text or the question  \\n might be labeled as any of the six categories,  \\n so our task is to build a CNN model  \\n to successfully predict the labels of input questions.  \\n For example, if I feed the question,  \\n who discovered electricity,  \\n into the model,  \\n since the answer to it must be a human name,  \\n we would expect the model to output the correct label,  \\n which is human.  \\n I hope the mission of this project is now clear to you.  \\n Let's split TREC dataset into train and test datasets.  \\n We are passing the text and label fields we just built  \\n here at line number one.  \\n Now we got our train and test data ready,  \\n but we also need a validation dataset  \\n for the sake of evaluation.  \\n So let's split a part of the training data  \\n as validation dataset.  \\n And don't forget to save a random seed here as well  \\n to save the randomness here at line number two.  \\n Now we can run this line of code  \\n to check a sample of the training data.  \\n The text or the question is  \\n how fast does the fastest car go?  \\n And the label is categorized as numeric.  \\n That makes sense.  \\n Since our data is ready,  \\n we can start building vocabulary for text and label fields.  \\n To reduce the number of unique words in our vocab object,  \\n we set the minimum frequency to two.  \\n That means a word has to appear at least twice  \\n in a train data  \\n to be included in a vocab object of the text field.  \\n Please also notice that we are building vocabularies  \\n based on only training data,  \\n so make sure you split train validation and test datasets  \\n before you build vocab.  \\n Let's print out the vocab object of label field.  \\n We can see the six labels  \\n as well as their numerical representations.  \\n Similarly, in a vocab object of text field,  \\n we also have words with their numerical representations.  \\n Now let's check how big those vocabs are.  \\n There are 2,679 words in text field,  \\n and of course, six words in label field,  \\n which represent the six categories.  \\n Amazing.  \\n We just got our train validation and test set  \\n and successfully built the vocabs  \\n with their numerical representations.  \\n We only have one thing left to do,  \\n which is to construct the iterators  \\n for each set in the last cell.  \\n But why do we need iterators?  \\n The bucket iterator transforms  \\n the train validation and test datasets into batches  \\n at line number two.  \\n The batch size is set to 64 at line number three,  \\n which means the number of training examples  \\n in one batch is 64.  \\n Then in a sort key argument at line number four,  \\n we are sorting based on the length of each sentence,  \\n which means it batches the text of length together.  \\n Finally, we set the device to GPU  \\n for an even faster training process at line number five.  \\n That's all for pre-proccessing the text dataset  \\n with PyTorch.  \\n See you in the next video  \\n where we are going to build a CNN model.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3067063\",\"duration\":386,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Building a simple CNN model\",\"fileName\":\"3004335_en_US_03_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to build a basic deep learning model using PyTorch, as well as how to fine-tune model parameters and make the model more solid.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14679394,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] I hope you are excited.  \\n In this video,  \\n we are building CNN model using PyTorch.  \\n What we are building is a very simple CNN  \\n that helps you consolidate your understanding  \\n of CNN architecture.  \\n We first import required libraries in the first cell.  \\n Then in a second cell,  \\n we define the class called CNN at line number one,  \\n and pass in nn.Module.  \\n All the PyTorch models  \\n inherit from the subclass of nn.Module.  \\n As you can see here, we are going to define  \\n init and forward functions,  \\n which are two of the most essential functions  \\n in a neural network model.  \\n Init is the constructor where we define the model layers.  \\n And when you instantiate a model object,  \\n init will be called.  \\n Forward method is where we actually add layers to the model  \\n and build the model architecture.  \\n In the init function,  \\n we are passing in the parameters we need  \\n for the constructor.  \\n There are vocabulary size, embedding size,  \\n number of kernels, kernel sizes,  \\n output size, and dropout rate.  \\n We use super.init to call the entire nn.Module class.  \\n Then we define the embedding layer  \\n with the required arguments.  \\n And then, the convolutional layers.  \\n Now, the dropout layer.  \\n And finally, the fully connected layer.  \\n Please notice there are actually three convolutional layers  \\n with three different kernel sizes we are going to define,  \\n namely, two, three, and four.  \\n The number of convolutional layers is the same  \\n as the number of different kernel sizes.  \\n I will leave you a little task here.  \\n Please try searching for the name of those nn. layers online  \\n and figure out what those layers do,  \\n as well as what their arguments mean.  \\n For example,  \\n when searching for nn.Embedding layer at line number five,  \\n take a look at the meanings of arguments,  \\n vocabulary size, and embedding size.  \\n Keep doing this until you understand all the layers  \\n and their arguments in the CNN model.  \\n I ensure this will help you a lot  \\n with understanding any CNN model  \\n you are going to work with in the future.  \\n Now, it's time to move on to the forward function.  \\n The forward function  \\n is where we build the model's architecture  \\n using the layers we defined in the init function.  \\n As we mentioned earlier in the course,  \\n the forward function should take in a piece of text  \\n and output the prediction of one of the six categories.  \\n We need to first permute the size of the text  \\n so that our embedding layer can properly carry the data.  \\n nn.Embedding will automatically build  \\n the embedding of the text.  \\n Then we pass the result to convolutional layers,  \\n max pooling layers, and dropout layer.  \\n By the way,  \\n the purpose of using dropout is to prevent overfitting.  \\n And finally, we return the result  \\n of the fully connected layer.  \\n The architecture looks the same  \\n as what we discussed in theory,  \\n but the shape of the output of each layer  \\n needs to be taken care of  \\n to be able to fit in the next layer.  \\n That is also why we are using  \\n functions permute, unsqueeze, and squeeze  \\n in a forward function.  \\n I encourage you to print out  \\n the shape of the output of each layer  \\n and think about why we use those functions  \\n to modify the shape of the data  \\n when we are passing them through different layers in CNN.  \\n After building the model,  \\n we need to instantiate it.  \\n And to do that,  \\n we will need to pass in all the arguments of the model  \\n from the init function.  \\n We give each argument a value,  \\n but you can play with those parameters  \\n to see if you can get a better result.  \\n This is actually one way of building a more solid model  \\n to achieve better accuracy.  \\n Now that the model is instantiated,  \\n we can print the model out  \\n to see if it has the architecture we wanted.  \\n We see an embedding layer,  \\n three convolution layers  \\n with kernel sizes two, three, and four,  \\n then a dropout layer, and a fully connected layer.  \\n Exactly what we wanted.  \\n Finally, in the last cell,  \\n let's move our model to GPU  \\n so we can train a lot faster.  \\n We say model.to(device)  \\n where our device is cuda, or cpu.  \\n That's it for building the CNN model.  \\n Our next step is to complete  \\n the train and evaluate functions for the training process.  \\n See you very soon.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3069057\",\"duration\":226,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Train and evaluate functions\",\"fileName\":\"3004335_en_US_03_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn about CNN at a high level, how CNN works in text classification, and how CNN and RNN perform different text classification tasks.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8297142,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] For a complete training process,  \\n we need to have a train function  \\n and evaluate function.  \\n The train function makes predictions based  \\n on training data  \\n and calculates how inaccurate the predictions are  \\n by comparing them with true labels  \\n of the training dataset.  \\n The train function then updates the parameters  \\n of the network to minimize the error  \\n or we call it minimizing the loss.  \\n So the model performs better.  \\n The evaluate function acts very similarly  \\n to the train function.  \\n It makes predictions based on validation data  \\n or test data and calculates  \\n how inaccurate the predictions are by comparing them  \\n to true labels of the dataset.  \\n But it doesn't update the parameters of the model.  \\n It simply evaluates the model based  \\n on the model's current state.  \\n Let's code the train function together.  \\n There are three essential parameters we need to define  \\n for the training process:  \\n the accuracy, the criterion and the optimizer.  \\n For criterion, we simply use a loss function  \\n call CrossEntropyLoss.  \\n We also move the criterion to GPU  \\n for a faster training process.  \\n You can check out PyTorch documentation  \\n and change the loss function as needed.  \\n For the optimizer,  \\n we are using Adam optimizer here  \\n and passing in the model's parameters.  \\n You can experiment with different optimizers as well.  \\n For accuracy, we prepared a function  \\n to calculate it in the second cell.  \\n As you can see here,  \\n it is basically calculating the percentage  \\n of correct predictions out of all the true labels.  \\n We also prepared the train function.  \\n As we mentioned before,  \\n we iterate through batches in the iterator.  \\n The optimizer helps us update the parameters  \\n of the model to minimize the loss.  \\n We use the model to make predictions  \\n of the text data  \\n and calculate the loss and accuracy.  \\n Then we add in total the loss  \\n and accuracy of each batch for the loss  \\n and accuracy of the entire epoch at line number 20 and 21.  \\n Finally, we return average loss and accuracy  \\n at line number 23.  \\n The length of the iterator is just the number  \\n of data examples in the iterator.  \\n In the evaluate function,  \\n we are doing something similar to the train function  \\n where we use the model to make predictions,  \\n then calculate and return the average loss and accuracy.  \\n The only thing we didn't do in evaluate function  \\n is that we didn't update model parameters  \\n based on the loss of each batch.  \\n We have pre-processed text data,  \\n built CNN model,  \\n completed train and evaluate functions,  \\n it is time to train the model.  \\n I am going to make it a challenge for you  \\n so you can work on it on your own  \\n but don't worry, in the next video,  \\n you will get enough hints  \\n on how to solve the challenge.  \\n Let's jump right into the challenge.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3068061\",\"duration\":108,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Training process\",\"fileName\":\"3004335_en_US_03_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to clean text data as the first essential step in building a model.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4570464,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Welcome to the challenge.  \\n This is also the last section of our guided project  \\n which is the model training process.  \\n When you open the exercise file of this video,  \\n you can see that we wrap up everything we did together  \\n in one file,  \\n including text preprocessing,  \\n model building,  \\n train and evaluate functions.  \\n You can take your time to review the above sections,  \\n but we will go to the training section right now.  \\n The challenge for you is to fill in all the code  \\n under the Python comments,  \\n where it asks you to write code.  \\n As you can see,  \\n there are only three lines of code you need to fill in  \\n at line number eight, number 10, and number 14.  \\n For line number eight and 10,  \\n you need to figure out how to calculate  \\n the training loss, training accuracy,  \\n validation loss, and validation accuracy  \\n using the functions we built in previous sections  \\n and passing the correct arguments.  \\n The purpose of the code at line number 14  \\n is to update the best accuracy  \\n which we initialized to negative infinity  \\n at the beginning of the training process  \\n here at line number three.  \\n Try to complete the code on your own  \\n and please do review the code in the previous sections  \\n as it does help with solving this challenge.  \\n See you in the solution video.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3069058\",\"duration\":236,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Training process\",\"fileName\":\"3004335_en_US_03_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, explore the solution to Challenge: Training process.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9207649,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Let's complete the training process together.  \\n First, we define the number of epochs we want to train  \\n at line number one.  \\n You can change 20 to a smaller or bigger number  \\n to get the desired accuracy and loss.  \\n Then we loop through each epoch.  \\n And for each epoch, we calculate accuracy and loss  \\n for the training and validation set  \\n using the train and evaluate functions  \\n at line number 8 and 10.  \\n Like this, we can easily pass in the model, train,  \\n or validation iterator, optimizer, and criterion.  \\n I hope you got the answer right.  \\n Now, let's look at how we can update the best accuracy.  \\n When we find that the validation accuracy  \\n is larger than the best accuracy,  \\n that is, of course, a good thing,  \\n because we just found a better model.  \\n This is the time when we should update the best accuracy.  \\n As it is so far the best model,  \\n we will save it at this epoch at line number 15.  \\n And finally, we print out the accuracies and losses  \\n for each epoch from line 17 to line 19.  \\n Let's start training right now.  \\n We can see that both training and validation accuracies  \\n are going up.  \\n Our training set might have overfitting issues  \\n since it has quite a bit of difference  \\n compared to the validation accuracies.  \\n But we are training a very small CNN model,  \\n so we will ignore that for now.  \\n After the training, we need to test the actual performance  \\n of the model when it takes in real data.  \\n We load the best model,  \\n which has the highest accuracy at line number one  \\n and evaluate the model using our test set  \\n at line number three.  \\n This is also the last step in the project,  \\n which is to get the final test accuracy.  \\n The accuracy is around 85%,  \\n which is pretty decent for a CNN model sample like this.  \\n As we mentioned before, we can play with the parameters  \\n that we passed into the model  \\n to get hopefully a better accuracy.  \\n Just make sure to change only one parameter at a time  \\n if you want to explore the impact  \\n of one particular parameter on model accuracy.  \\n Let me show you how to do it.  \\n Let's change the dropout_rate to 0.6.  \\n Rerun this cell as well as all the cells after it  \\n and wait for the test accuracy to be calculated  \\n at the end of the file.  \\n We can see that the test accuracy  \\n is lower than last time,  \\n which means playing with the parameters  \\n can actually influence the model accuracy.  \\n I encourage you to keep exploring  \\n using this Jupyter Notebook.  \\n Congratulations, you just completed the entire project  \\n on text classification with PyTorch.  \\n You have successfully preprocessed text data,  \\n built a CNN model,  \\n trained and evaluate the model.  \\n I hope you find this project informative,  \\n and see you in the last chapter.  \\n \\n\\n\"}],\"name\":\"3. Guided Project: CNN Text Classification with PyTorch\",\"size\":54357905,\"urn\":\"urn:li:learningContentChapter:3066067\"},{\"duration\":47,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3073047\",\"duration\":47,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Keep learning and connect\",\"fileName\":\"3004335_en_US_04_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"This video celebrates you for completing the course and encourages you to keep learning through more advanced courses on NLP using PyTorch.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1610716,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Zhongyu] We've reached the end of the course.  \\n At this point, you have successfully built  \\n and trained a deep learning model  \\n for solving a tax classification task  \\n using the popular framework, PyTorch,  \\n but this isn't the end.  \\n I encourage you to learn more about deep learning models  \\n and natural language processing related tasks  \\n and continue to expand your knowledge of NLP,  \\n as we were only able to scratch the surface of it.  \\n Don't forget to connect with me on LinkedIn  \\n and check back often, as we are adding more advanced courses  \\n on NLP with PyTorch soon.  \\n I hope you enjoyed learning my course.  \\n Until next time, my name is Pan Zhongyu  \\n and thank you for watching.  \\n \\n\\n\"}],\"name\":\"Conclusion\",\"size\":1610716,\"urn\":\"urn:li:learningContentChapter:3066068\"}],\"size\":88620513,\"duration\":2464,\"zeroBased\":false},{\"course_title\":\"AI Workshop: Build a Neural Network with PyTorch Lightning\",\"course_admin_id\":2750013,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":2750013,\"Project ID\":null,\"Course Name\":\"AI Workshop: Build a Neural Network with PyTorch Lightning\",\"Course Name EN\":\"AI Workshop: Build a Neural Network with PyTorch Lightning\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"&lt;p&gt;In this interactive workshop, Janani Ravi\u2014a certified Google cloud architect and data engineer\u2014explores the fundamentals of building neural networks using PyTorch and PyTorch Lightning. Learn the basics of neural networks and setting up your virtual environment. Find out how to load and preprocess data, creating simple yet powerful neural networks to tackle regression tasks. Step through the conversion of PyTorch code to PyTorch Lightning and encapsulate data and models with LightningDataModule and LightningModule. Discover how to use PyTorch Lightning Trainer to streamline the training process and evaluate models efficiently. Explore building classification models to further hone your skills in data preparation, model creation, and performance evaluation. When you complete this course, you will be well-equipped to construct and deploy efficient neural network models, adding significant value to your AI projects. &lt;/p&gt;&lt;p&gt;This course was created by Janani Ravi. We are pleased to host this training in our library.&lt;/p&gt;\",\"Course Short Description\":\"Learn how to create, train, and evaluate both regression and classification models effectively with this hands-on workshop.\",\"Content Type\":\"SKILLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":\"20161004\",\"Instructor Name\":\"Janani  Ravi\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Certified Google Cloud Architect and Data Engineer\",\"Author Payment Category\":\"LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2025-09-04\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"Yes\",\"LIL URL\":\"https://www.linkedin.com/learning/ai-workshop-build-a-neural-network-with-pytorch-lightning-26906172,https://www.linkedin.com/learning/ai-workshop-build-a-neural-network-with-pytorch-lightning-revision-fy26q2-licensed\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Artificial Intelligence, Artificial Intelligence for Technology\",\"Primary Software\":\"PyTorch\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":5779.0,\"Visible Video Count\":19.0,\"Learning Objectives\":null,\"Contract Type\":\"LICENSED\",\"Certifications\":null,\"Framework Topic\":\"Level 4: Training and Maintaining Models, AI Upskilling Framework Level 4: Training and Maintaining Models, AI Upskilling Framework, AI Framework Level 4: Training and Maintaining Models\",\"Automatic Caption Translations\":\"Global Captions\",\"Automatic Metadata Translations\":\"LiL Libraries, Global Metadata\",\"Gen AI Feature Flag\":null,\"Hands-On Practice\":null,\"Hands-On Practice Library\":null,\"Unlocked for Viva Learning\":\"Global Captions\",\"Free Course\":null,\"Certification Library\":null,\"Github Codespace\":null,\"Skills Count\":2,\"Skills\":\"Artificial Neural Networks,PyTorch\",\"Skills EN\":\"Artificial Neural Networks,PyTorch\",\"Content Manager\":\"Kim Norbuta\",\"Acquisition Manager\":\"Kim Norbuta\",\"Framework Subject\":\"AI Upskilling Framework, AI Framework Level 4: Training and Maintaining Models\",\"Suppress Upsells\":null},\"sections\":[{\"duration\":719,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5487086\",\"duration\":339,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"AI workshop build a neural network with PyTorch Lightning\",\"fileName\":\"2750013_en_US_00_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":339,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn about the features of PyTorch and PyTorch Lightning.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8241807,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Presenter] Hi, and welcome to this course AI Workshop:\\nBuild a Neural Network with PyTorch Lightning.\\nSince this course is an AI workshop,\\nfor most of this course,\\nwe'll be performing hands-on coding.\\nWe'll build a neural network with PyTorch,\\nand we'll see how we can write cleaner,\\nmore modular reusable code with PyTorch Lightning.\\nNow, before we get to the demos,\\njust a little bit of an overview of PyTorch\\nand PyTorch Lightning.\\nFirst, what exactly is PyTorch?\\nHere is a definition from the PyTorch documentation.\\n\\nIt's an optimized tensor library\\nfor deep learning using GPUs and CPUs.\\nThe tensors here refer to multidimensional arrays\\nthat can be trained in a distributed manner.\\nAnother way to look at PyTorch also from the documentation,\\nit's an open-source machine learning framework\\nbased on the Python programming language.\\nIt has simple and intuitive APIs,\\nwhich accelerate the path from research prototyping\\nto production deployment.\\nThe PyTorch framework is primarily used\\nto build deep learning neural network models,\\nand its APIs are so simple\\nthat with just basic knowledge of Python,\\nyou should be able to work in PyTorch right away.\\n\\nNow, hopefully you've worked with neural networks before,\\nand this is not your first neural network course.\\nNeural network models, you know,\\nare just directed acyclic graphs.\\nPyTorch uses something known as dynamic computation graphs.\\nThis means you can build the graph for the model\\nand execute it right away.\\nThis makes it easier\\nand more flexible to build complex neural networks.\\nNow, PyTorch is deeply integrated with NumPy.\\nYou can set up your data in the form of NumPy arrays\\nand convert those to PyTorch tensors\\nand vice versa very easily.\\n\\nPyTorch has native support for training on GPUs.\\nYou can have your model parameters\\nand data all moved to the GPU available on the machine\\nthat you're running training,\\nand the entire training process will run there.\\nPyTorch also uses a powerful library called Autograd\\nfor automatic differentiation.\\nAutomatic differentiation is an important part\\nof training a neural network model.\\nThis involves computing partial derivatives\\nof the loss function with respect to every model parameters,\\nand then using that information to tweak model parameters\\nto minimize the loss of a network.\\n\\nThe PyTorch library contains everything that you need\\nto build neural networks.\\nYou have classes for the layers of a neural network;\\ndifferent kinds of layers optimizers\\nthat you use to train neural networks;\\nloss functions for different kinds of models;\\nserializers to serialize the model autodisk.\\nThe PyTorch framework is flexible and easy to use.\\nWhen you use PyTorch directly\\nto build and train neural networks,\\nyou get access to a low-level API for model training.\\n\\nAnd this is great when you really want to configure\\nand customize your model\\nand want very granular control over the training process.\\nBut if you want to be abstracted away from the details\\nof model training,\\nusing the PyTorch framework directly is not a great choice.\\nPyTorch has many repetitive tasks\\nand a lot of boilerplate code.\\nSo it's often very tedious to write code in PyTorch\\nto train your model.\\nIf you want to avoid working\\nwith all of these nitty gritty details\\nand want your model code to be cleaner and more modular,\\nwell, you use PyTorch Lightning.\\n\\nPyTorch Lightning is an open-source lightweight wrapper\\nor framework built on top of PyTorch\\nthat simplifies the training\\nand research process for deep learning models.\\nSo this is something important.\\nPyTorch Lightning is just a wrapper,\\nso you can't do anything in Lightning\\nthat you can't do with PyTorch.\\nThe fact is it's just much easier to work\\nwith PyTorch Lightning.\\nPyTorch Lightning abstracts away\\nall of the nitty gritty details\\nand really reduces the boilerplate code\\nthat you have to write for training models.\\nWhatever you need to do to build\\nand train neural networks,\\nPyTorch Lightning will give you\\na high-level interface for this.\\n\\nYou can define datasets, define models,\\nset up training loops,\\nand log your experiments\\nall using this high-level interface.\\nWhat are some of the advantages of using PyTorch Lightning?\\nWell, your code is much cleaner\\nbecause most of the repetitive code\\nand training loops that you have to use in PyTorch\\nis abstracted away.\\nYour code is also more modular with Lightning.\\nIt encourages a modular design\\nby separating the different parts of the training process\\ninto well-defined components\\nsuch as the model, data loaders, and training logic.\\n\\nCleaner modular code results in better reproducibility\\nof your model and data.\\nLightning provides built-in support for experiment logging\\nand tracking as well.\\nLightning support for distributed training\\nis more straightforward\\nbecause you do not need to move your model parameters\\nand data to specific devices\\nto actually train on that device.\\nLightning offers a callback system,\\nallowing you to add custom functionality at various points\\nduring the training process\\nwithout modifying the core training loop.\\n\\nInstead of writing complex four loops to train your model,\\nthe trainer class in Lightning abstracts away\\nmany training loop details.\\nIf you feel that PyTorch Lightning does not offer you\\nthe flexibility that you need for model training,\\nwell, you can use Lightning along with the PyTorch API,\\nso they're interoperable, giving you experiment flexibility.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5493052\",\"duration\":51,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Prerequisites\",\"fileName\":\"2750013_en_US_00_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":52,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Review what you should know beforehand to get the most out of this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1184891,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Before you get hands-on,\\nand that will be very soon,\\nlet's quickly look at some of the prereqs\\nyou need to have to make the most of your learning.\\nThe first thing here is you need\\nto have a basic understanding of machine learning.\\nThis is not a beginner machine learning course.\\nInstead, it's an AI workshop,\\nwhich means we'll get hands-on with demos right away.\\nA basic understanding of machine learning,\\nregression, and classification models will really help you.\\nAlso, you need to have a basic understanding\\nof how neural networks work.\\nIn the next movie, we'll quickly go through\\nhow neural networks function,\\nbut that's more of revision,\\nrather than explaining all of the nitty gritty\\nof neural network training.\\n\\nSo basic understanding of neural networks would really help.\\nAnd finally, because we're going to be coding\\na lot using Python,\\nyou should be comfortable programming in Python.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5487085\",\"duration\":329,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Quick overview of neural networks\",\"fileName\":\"2750013_en_US_00_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":330,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Discover how neural networks work.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8650752,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's do a quick revision\\nof how neural networks work before we get down to the demos.\\nRemember, this is not a comprehensive overview\\nor a look at neural network training,\\nbut a quick overview to give you the main points to remember\\nas you dive into the code.\\nNeural network models are made up of layers\\nand how these layers are arranged and connected\\nmake up the architecture of the model.\\nYou can think of every layer in a neural network\\nas being connected to other layers in the neural network.\\n\\nThe way neural networks function\\nis that every layer in the neural network\\nis responsible for extracting a different detail\\nfrom the underlying data,\\nand all of the layers put together make predictions.\\nNow, the first layer here in our neural network,\\nthat's the input layer.\\nThis is where we feed in the input data,\\nwhether during the training process or for predictions.\\nThe final predictions of the neural network\\nare available from the last layer,\\nthat is the output layer.\\n\\nBetween the input and output layers,\\nyou have one or more hidden layers,\\nand these hidden layers transform the data.\\nThese transformations are applied\\nas the data flows through the layers of the model.\\nThe operation of each hidden layer\\nis to extract a different bit of information\\nfrom the data that passes through.\\nIn a neural network, every layer is made up\\nof active learning units called neurons.\\nThey're called active learning units\\nbecause it's these neurons that are identifying patterns\\nand making generalizations\\nfrom the data that passes through the network.\\n\\nNeurons are fed inputs and they produce outputs,\\nand these inputs and outputs\\nare essentially interconnections in the model.\\nThe output of every neuron may be connected\\nto one or more neurons in the layer after it.\\nAnd how these connections are set up?\\nWell, that's a part of the neural network architecture.\\nNow, we've said that neurons are active learning units,\\nbut what exactly is a neuron?\\nEach neuron is nothing but a mathematical function.\\nEach neuron applies this function\\nthat you see here at the bottom to its inputs.\\n\\nIt computes the weighted X values.\\nX values are the input, adds a bias,\\nand applies an activation function on Wx + b\\nto compute the final output Y.\\nThe first of these mathematical functions\\nthat the neural network applies is responsible\\nfor learning linear relationships that exist in the data.\\nA neuron receives a vector of inputs.\\nYou can think of these as X1 through Xn,\\nand it basically applies a weight value\\nto each element of the vector.\\n\\nThese weights are associated with the connections\\nthat flow into the neuron.\\nWx + b is the first mathematical operation of the neuron,\\nand this operation is responsible for learning\\nlinear relationships that exist in data.\\nThe second mathematical function that a neuron\\napplies to its inputs is the activation function.\\nThe activation function is responsible\\nfor learning non-linear relationships that exist in data.\\nPopular activation functions\\ninclude the rectified linear unit or ReLU.\\n\\nThere's the sigmoid activation function and many others.\\nThe choice of activation function\\nis a part of the neural network design.\\nThe weights and biases of all of the neurons\\nin your neural network make up\\nthe trainable parameters of the model.\\nThese weights and biases are what are found\\nduring the training process.\\nYou can think of the objective of the training process\\nof a neural network is to find the weights and biases\\nfor all of the interconnections\\nthat minimizes the loss of the model.\\n\\nThe loss here is essentially a measure\\nof how far the predictions of the model\\nare from the actual values in the training data.\\nWe know that model parameters are found\\nduring the training process of the neural network,\\nbut how does training work?\\nHere is a very high level explanation.\\nDuring the training process,\\nwe feed training data in batches\\nthrough the network and get predictions\\nusing the current parameters of the model.\\nThese predictions, at least to start off with,\\nwill not be very good ones.\\n\\nWe'll measure how good the predictions of the model are\\nby computing the loss.\\nThe loss represents how far is the prediction of the model\\nfrom the actual labels in the training data.\\nOnce we have the loss function, we'll compute gradients.\\nThe gradients are just the partial derivatives of the loss\\nwith respect to each parameter in model training.\\nThese gradients give us a sense\\nof how to tweak the model parameters\\nto minimize the loss of the model.\\n\\nWe then make a backward pass through the model\\nto update parameters to minimize the loss.\\nAnd this forward pass to get predictions\\nand then backward pass to update the model parameters\\ncontinues through the entire training process\\nuntil the model parameters converge.\\nThe entire objective of the training process\\nis to minimize the loss of the network,\\nand thus improve the predictions of the model.\\nThis minimization of the loss of the network\\nis done using an optimization algorithm\\ncalled gradient descent.\\n\\n\"}],\"name\":\"Introduction\",\"size\":18077450,\"urn\":\"urn:li:learningContentChapter:2177062\"},{\"duration\":2353,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5489057\",\"duration\":311,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Setting up the virtual environment\",\"fileName\":\"2750013_en_US_01_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":312,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to set up a virtual environment.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7927235,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] In this course, we'll first build\\na simple neural network model for regression using PyTorch,\\nand you'll see that there are a lot of granular details\\nthat we need to know about model building\\nin order to work with PyTorch directly.\\nThere'll be a lot of boilerplate code.\\nThen we'll basically build the same neural network\\nusing PyTorch Lightning.\\nWith PyTorch Lightning we'll eliminate\\na lot of the boilerplate code\\nand create reusable components,\\nyou'll see how much cleaner the code is\\nwith PyTorch Lightning.\\nBefore we do any of that, let's set up a virtual environment\\nwithin which we'll install PyTorch\\nand build and train our neural network models.\\n\\nNow, for much of the code that we'll write in the course,\\nwe'll make use of some digital assistance.\\nChatGPT, Google Gemini, and Claude.\\nNow I can't remember the exact set of instructions\\nthat I need to follow\\nto set up a virtual environment in Python,\\nso I'm going to ask ChatGPT\\nand follow what ChatGPT has to say.\\nWe use the VENV package to create our virtual environment,\\nand then we activate our virtual environment.\\nNow, I'm working on a MacOS machine\\nbut if you're working on Windows,\\nplease use the highlighted command.\\n\\nI'll now follow these instructions\\nto set up a virtual environment.\\nHere I am on my local machine, and I have Python installed.\\nMake sure you have a recent version of Python.\\nYou can see that I'm working with Python 3.10.9.\\nMy working directory is going to be a projects folder\\nthat I've created earlier.\\nI'll just cd into that folder.\\nI'll now create a Python virtual environment.\\nA virtual environment is just an isolated environment\\nfor Python projects, ensuring that each project\\ncan have its own dependencies\\nregardless of what dependencies other projects may have.\\n\\nThis command creates a virtual environment\\ncalled pytorch_venv, using the VENV module.\\nWhen you create a virtual environment,\\nthis sets up a directory\\nunder your current working directory\\nwith the name of your virtual environment,\\nyou can see the PyTorch VENV directory here.\\nThis is where all of the packages\\nthat we install in the virtual environment\\nwill be set up and stored.\\nNow, to activate the virtual environment\\nyou'll run the source command, pytorch_venv/bin/activate.\\n\\nThis will activate the virtual environment.\\nNotice that my prompt has changed.\\nIf you're running on a Windows machine,\\nthe activation command is a little bit different\\nand you can go back to the output of ChatGPT\\nto see exactly what it is.\\nThe name of the virtual environment\\nis now part of the prompt.\\nThe same version of Python\\nthat I had installed on my local machine\\nshould now be available here in my virtual environment.\\nIt's possible to create virtual environments\\nusing different versions of Python,\\nbut I'm happy with the Python version that I have.\\n\\nTo ensure that I'm using the Python\\ninstalled within my virtual environment,\\nI'm going to run which Python,\\nand you can see this is the Python that belongs to\\nthe pytorch_venv folder.\\nThat is my virtual environment's folder.\\nIn order to be able to work within this virtual environment\\non a Jupyter Notebook, let's install the\\nIPykernel module in Python.\\nUse pip install to install the latest version of IPykernel.\\nThis is the Python package that provides the kernel\\nfor Jupyter Notebook and Jupyter Lab.\\n\\nThe kernel is just the computational engine\\nthat executes the code on the notebook.\\nNow, once we have this installed,\\nrun Jupyter kernelspec list\\nto see what Python kernels you have available.\\nYou can see I have just the one, Python 3.\\nI now install the kernel\\nassociated with my virtual environment\\nusing this command here, python -m ipykernel install,\\nthe kernel with name pytorch_venv.\\nRemember, pytorch_venv is a folder\\nin our current working directory,\\nand it's that folder that will be used to set up the kernel.\\n\\nThus, we have a kernel\\nwith our virtual environment available\\nwhen we use Jupyter Notebooks.\\nRun Jupyter kernelspec list once again,\\nand you can see the pytorch_venv kernel.\\nThat's the kernel that we use to run our code.\\nWithin our virtual environment\\nwe've installed the pytorch_venv kernel,\\nlet's bring up the Jupyter Notebook server\\nso that we can work within Jupyter Notebook\\nto build and train models\\nusing PyTorch and PyTorch Lightning.\\nHere, let's open up a new notebook.\\n\\nClick on new, and make sure that you choose\\npytorch_venv as the kernel\\nthat you want to run your code on.\\nIf we choose this kernel,\\nwe'll be running within our virtual environment.\\nOn the top right, notice the kernel, it's pytorch_venv.\\nIf you happen to be in some other kernel\\nand you need to switch,\\nsimply select the kernel dropdown menu here on this page,\\nand there you'll find an option to switch your kernel.\\nMeanwhile, let's rename this notebook\\nand give it a meaningful name.\\n\\nLet's call this TrainingRegressionModelUsingPytorch,\\nbecause first we'll work with PyTorch,\\nwhich is much more low level\\nand involves much more boilerplate\\nin order to understand the different components\\nthat you'll use to build a neural network model,\\nand then we'll switch to PyTorch Lightning.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5490067\",\"duration\":346,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Loading and exploring regression data\",\"fileName\":\"2750013_en_US_01_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":347,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to load data for regression.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11083448,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We first going to train a regression model\\nusing raw PyTorch before we switch over\\nto using PyTorch Lightning.\\nWe'll be using the Scikit learn module in Python\\nin order to pre-process our data.\\nSo go ahead and pip install Scikit learn.\\nIn addition, we need the standard Python packages\\nto work with data, Pandas, Numpy,\\nand Seaborn for visualization.\\nI'm just going to pip install all of these\\nwithin my virtual environment.\\nNext, of course, we are going to be using PyTorch\\nto train our model, so you'll need\\nto pip install the PyTorch framework as well.\\n\\nAnother PyTorch-related module\\nthat you'll need is Torch Metrics.\\nThis is the library that contains metrics\\nto evaluate our models, mean squared error for regression,\\nand accuracy, precision, and recall for classification.\\nSo make sure you pip install\\nand have this module ready within your virtual environment.\\nNow that we have the libraries that we need,\\nlet's confirm the PyTorch version that we are using.\\nI'm running PyTorch version 2.7.1,\\nthe latest version at the time of this recording,\\ngreater than or equal to two\\nin order to be able to run these demos.\\n\\nI'll now set up a bunch of input statements\\nfor all of the libraries and classes\\nthat we'll need for this demo.\\nI'll use Numpy, Panda, Seaborn, Matplotlib,\\nTorch, Scikit learn.\\nWe won't go through these import statements right now.\\nWe'll discuss each class or a function as we use it.\\nThe dataset that we'll be using\\nto train our regression model\\nis an insurance charges data set\\nand it's present here within this datasets directory\\nunder my current working directory.\\n\\nYou can see this insurance.csv file.\\nThis is the file that I'm going\\nto read into my notebook using Pandas.\\nThe dataset is fairly simple.\\nThe records contain details of insurance customers,\\nage, gender, BMI, number of children,\\nwhether they smoke or not,\\nand the region in which they live.\\nThese are all the features of the data.\\nWe'll try to use this information\\nto predict how much they've been charged for insurance.\\nThe label column is charges.\\n\\nLike I said, this is a fairly simple and small dataset,\\nperfect for training a simple regression model\\nusing PyTorch.\\nThe shape of the data shows us\\nthat there are a total of 1338 records.\\nLet's make sure every column in this data\\nis of the right type.\\nFor that, I run the info method on a Pandas data frame\\nand you can see that the types are correct.\\nEach BMI number of children and charges are numeric columns.\\nThe remaining are string columns or categorical columns.\\n\\nNow, I'd like to do some exploratory data analysis\\nbefore we turn to actually training\\nour regression model using PyTorch.\\nNow, here are the columns in my data.\\nI'm going to make use of ChatGPT to get help\\nwith some Seaborn visualization.\\nYou can see that I'm giving ChatGPT some context here.\\nI have a Pandas data frame insurance data\\nwith the following columns.\\nHow do I use Seaborn to view a histogram\\nof the charges field?\\nChatGPT very helpfully gives me code\\nand also explanation for that code.\\n\\nLet's copy this code over, paste it to our notebook\\nin order to visualize a histogram of charges.\\nI'm going to get rid of those two import statements\\nbecause I already have the imports up top\\nand we're left with simple code which calls sns.histplot\\nto view a histogram of the charges information.\\nYou can see that for a vast majority\\nof customers, the insurance charges\\ntend to be under $15,000.\\nYou can see that on the x-axis.\\nHowever, there are a few customers\\nfor whom insurance charges tend to be very high\\nin the order of 40 to $50,000.\\n\\nIn this dataset, all of the feature variables\\nare relevant to predict the insurance charges for customers.\\nBut one of the most significant features\\nis this smoker feature.\\nWhether you are a smoker or not\\nheavily influences your insurance charges.\\nYou can see that for non-smokers, the insurance charges\\ntend to be much lower than for smokers.\\nThis box plot makes that very clear.\\nAlso, how old the customer is influences insurance charges.\\n\\nYou can see a scatterplot of insurance charges versus age,\\nand there is a linear relationship.\\nYou can see that insurance charges tend to increase\\nwith age, but for each age,\\nthey seem to be different bands of charges.\\nI found that digital assistance like ChatGPT, Gemini,\\net cetera, are great at interpreting data.\\nI often use their help to verify whether my interpretations\\nof data are correct.\\nI've taken a screenshot of this box plot\\nand I'm going to drag this screenshot into ChatGPT\\nand I'm going to ask it a question.\\n\\nWhat can you interpret about insurance charges\\nusing this box plot?\\nAnd here you can see ChatGPT very accurately tells me\\nthat smokers are charged significantly more.\\nNot only is the median insurance charge for smokers higher,\\nbut there's also greater spread of charges for smokers.\\nIn this manner, you can use generative AI\\nto validate the findings from your data.\\nNow that we've understood the data that we are working with,\\nlet's quickly split the data into training and test data\\nusing train test split.\\n\\nx features include all columns except charges.\\nThe y values that we are trying to predict\\nare the insurance charges.\\nTrain test split will spread the features and labels\\nso that we have 80% of the data\\nto train our neural network model\\nand 20% of the data to validate the model.\\n1070 records for training and 268 records for validation.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5493051\",\"duration\":459,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Preprocessing data for training\",\"fileName\":\"2750013_en_US_01_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":460,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to preprocess data for model training.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15246295,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that we have training\\nand validation data,\\nthe next step is to pre-process the data\\nso that we can feed that into a machine learning model.\\nNow, neural network models and all machine learning models\\nonly understand numeric values.\\nYou can't feed in strings to those models,\\nwhich means you need to numerically encode all\\nof your categorical variables,\\nwhich may be represented as strings.\\nIn addition, you need to scale your numeric features\\nso that all of the features are represented\\nusing the same range.\\n\\nAnd I'm going to take ChatGPT's help for this.\\nNow, I asked ChatGPT a very specific question.\\n\\\"I have these categorical features for my data.\\nCan you set up a pre-processor\\nin scikit-learn to encode this data?\\\"\\nNow, ChatGPT knows all of the columns in my data set.\\nIt has the context from my previous question,\\nso it goes above and beyond here.\\nIn its response, not only does it give me code\\nto encode the categorical features,\\nbut it also gives me code to scale the numeric features.\\n\\nThis has basically given me code\\nfor not just the current step,\\nbut also the next step of my processing.\\nIf you see the categorical_transformer pipeline,\\nyou can see that it first imputes missing values\\nin our data using the most frequent value in a category,\\nand then it performs one-hot encoding\\nof the categorical variables.\\nThe numerical_transformer, again,\\nperforms imputation for the missing data\\nby replacing missing values\\nwith the average values in the columns,\\nthe strategy is mean,\\nand then it standard scales the data.\\n\\nYou can see on top, it has correctly identified\\nthe categorical and numerical features in our data.\\nAnd here at the bottom,\\nencoding the categorical columns\\nand scaling the numeric columns.\\nNow this code is great, but I'd like to tweak it just a bit.\\nSo I'm going to ask ChatGPT to fix the code.\\nI want to fix the one-hot encoding code\\nso that we drop one of the categories\\nto avoid the dummy variable trap.\\nYou can see that the one-hot encoder has been instantiated\\nwith additional parameters: drop equal to first,\\nand handle_unknown equal to ignore.\\n\\nI'll explain these in more detail once we copy this code\\nover to use in our notebook.\\nThere are three categorical columns in our data,\\nsex, smoker, and region.\\nAll of these are nominal categorical values.\\nThat means that there is no inherent ordering\\nacross categories for any of these columns.\\nAnd so one-hot encoding is perfect in such a situation.\\nWe instantiate the one-hot encoder categorical transformer,\\nhandle_unknown is ignore,\\nthat is, if we encounter unknown values,\\nthis transformer will simply ignore them.\\n\\ndrop is equal to first specifies the methodology to use\\nto drop one of the categories in the feature.\\nLet's say the smoker column\\ncan have two possible values, yes or no.\\nIn the final output, you'll have just one column,\\neither smoker yes or smoker no\\nwith zero one values indicating whether the customer\\nis a smoker or not.\\nThis avoids something known as the dummy variable trap,\\nwhich occurs when one or more one-hot encoded variables\\nare perfectly multicollinear with others,\\nmeaning one variable can be predicted exactly from others.\\n\\nIt's in order to avoid this\\nthat we specify drop equal to first\\nto drop the column corresponding to one category value\\nin each categorical variable.\\nThe numerical features are age, bmi, and children,\\nand you can see the numerical_transformer pipeline.\\nAfter imputation, it standardizes the data,\\nthat is, expresses the features in terms\\nof number of standard deviations away from the mean.\\nWhen you feed in numeric values to neural networks,\\nneural networks perform much better\\nwith smaller numeric values.\\n\\nAnd also when the individual features\\ndo not have widely different ranges,\\nand standard scaling is one way\\nto preserve the information in individual features,\\nbut also have them all centered around zero\\nand expressed using small numeric values.\\nThe ColumnTransformer is our pre-processor\\nthat works on both the numeric data\\nas well as the categorical data.\\nOnce we've instantiated the transformer,\\nlet's pre-process our data.\\nI call preprocessor.fit_transform on the training data,\\nand using the computer values on the training data,\\nwe simply call transform on the validation data.\\n\\nThis ensures that the properties computed\\non the training data\\nare what we use to transform the validation data.\\nAnd we are left with eight features in our data\\nafter one-hot encoding.\\nLet's take a look at the training data,\\nand you'll find that it's just a NumPy array.\\nWe don't have the corresponding columns.\\nIf you want to understand this data,\\nlet's convert it to a temporary data frame\\nand see how the data has been transformed.\\nSo we have one column for sex_male yes, no,\\none column for smoker yes, no,\\nand then columns for northwest, southeast, and southwest.\\n\\nThe column corresponding\\nto region Northeast has been dropped.\\nA value of all zeros for northwest, southeast,\\nand southwest essentially indicates\\nthat the region is northeast.\\nThe numeric columns, age, bmi, and children\\nhave been standard scaled.\\nWe've divided every value in the column by the mean\\nand then subtracted the standard deviation.\\nThe y values are still in the data frame format.\\nLet's convert those to the NumPy format as well,\\nand from NumPy arrays later on,\\nwe'll convert these to Torch Tensors.\\n\\nNow, the y values that we need to train our model\\nare currently in the form\\nof a single dimensional array or a vector.\\nIn order to feed them into a neural network,\\nthey need be in the form of a multidimensional array.\\nSo instead of a vector of length size,\\nwe'll have a multidimensional array of dimensions,\\nsize, comma, one.\\nAnd that's what this reshape operation accomplishes.\\nIt's the same y values, the charges,\\nin the form of a multidimensional array.\\n\\nAs I mentioned before, neural networks work better\\nwhen you're dealing with small numeric values.\\nNow, our insurance charges vary from 0\\nto about 50,000, $60,000.\\nThese are not small numeric values, and in order\\nto make our neural network training more robust,\\nmore likely to converge,\\nI'm going to use the MinMaxScaler\\nto scale all insurance charges\\nto be expressed in the range zero to one.\\nThat's what the MinMaxScaler does by default.\\n\\nNow, all insurance charges are expressed\\nas values between zero and one.\\nWe now have our input features\\nand labels that we'll use to train our neural network.\\nHowever, they're in the NumPy format,\\nI'm now going to convert them to Torch Tensors.\\ntorch.from_numpy will convert NumPy arrays\\nto Torch Tensors.\\nNow, Torch Tensors are the primary data structures used\\nin PyTorch for building neural networks and other ML models.\\n\\nTorch Tensors are multidimensional arrays like NumPy,\\nbut they support distributed training using GPUs.\\nThey're used in neural network training\\nbecause they support automatic differentiation\\nfor gradient computation, an essential part\\nof the training process of a neural network.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5491066\",\"duration\":307,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating a simple neural network\",\"fileName\":\"2750013_en_US_01_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":307,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to create a simple neural network in PyTorch.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10548675,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The next step is for us\\nto set up a simple neural network model in PyTorch,\\nand here I'm going to take Gemini's help.\\nI want a dense, fully connected neural network.\\nI'm going to describe the neural network that I'm looking for,\\nhow many layers, how many activation units in each layer,\\nand I'll ask Gemini to generate the code for me.\\nI want a simple PyTorch neural network module\\ncalled SimpleNeuralNet.\\nIt should have three layers with 16, 32, and then 16 units.\\nIt should use ReLU as the activation function,\\nand I wanted to include the init forward\\nand predict method implementations.\\n\\nIn PyTorch, a module is a subclass of torch.nn.module,\\nand that can represent either a neural network layer\\nor a complete model.\\nHere I'm looking for a complete model,\\nand let's look at the code that Gemini gives us.\\nNow it turns out this code is perfect.\\nWhat I'm going to do is I'm just going to copy this code over\\nand then explain it within the notebook.\\nGemini has generated code not just for the neural network,\\nbut also for training the neural network.\\nWe'll worry about training when we come to it.\\n\\nFor now, I'll just copy the code for the architecture\\nof the neural network and paste it into my notebook.\\nOur neural network is called SimpleNeuralNet.\\nIt inherits from the nn.module base class,\\nand the layers of the neural network are defined\\nwithin the init method.\\nNn.module is the base class\\nfor all neural network modules in PyTorch.\\nIt provides a framework for defining layers like linear\\nor convolutional layers\\nand organizing them into a forward computation graph.\\n\\nIt tracks your model parameters and buffers\\nand makes it easy for you\\nto save, load, and train your models.\\nWe'll first look at the linear layers in this network,\\nand then we'll turn our attention\\nto the activation functions\\nfor these linear layers.\\nWe take in the number of features\\nas an input argument.\\nThat is the input size variable\\nand layer one comprises of 16 neurons.\\nNotice I instantiate nn.linear.\\nThe input size that comes from the input\\nand number of neurons equal to 16\\nLayer one is the first layer in our neural network.\\n\\nThe output of this linear layer will be fed\\nto the second linear layer.\\nOur first fully connected layer is fc1,\\nand the output of this is passed\\nto the second linear layer fc2.\\nThe dimensionality of the inputs to the second layer is 16,\\nand this needs to match the number\\nof neurons in the previous layer.\\nSo layer one has 16 neurons.\\nYou can see this on line 10,\\nand this matches the number\\nof input features fed into layer two.\\n\\nThat is input features equal to 16 on line 12.\\nRemember, this is\\nbecause the output of layer one is fed into layer two.\\nThe number of neurons in layer two is 32,\\nand then we have layer three,\\nthe third linear layer in our neural network.\\nThe input features to the third linear layer is 32,\\nand this should match the number\\nof neurons in the previous layer, which is 32.\\nLayer three has just 16 neurons.\\nAnd then finally, we have the last linear layer\\nthat is the output layer.\\n\\nIt accepts 16 input features as input,\\nand produces just one output.\\nThe single output is of course,\\nthe prediction from the regression model.\\nThe 16 features as input corresponds\\nto the 16 neurons in the previous layer.\\nEach of the three layers, layer one, layer two,\\nlayer three have an activation function,\\nand the activation function\\nthat I've chosen here is ReLU activation.\\nThere are three variables for the ReLU activation,\\nrelu1, relu2, and relu3.\\n\\nActivation functions are what allow us\\nto learn non-linear relationships\\nthat might exist in the data.\\nNow the forward function defines the forward pass\\nthrough the neural network.\\nThis forward pass gives us the prediction from the model.\\nWe receive the inputs as an input argument\\nto this forward function.\\nWe apply layer one and then the relu activation function.\\nThen the outputs are then passed through to layer two,\\nand then relu activation again,\\nand the outputs are passed through to layer three,\\nand we have the relu activation yet again.\\n\\nThe transformed data\\nafter passing through three layers is finally passed\\nthrough the output layer\\nand the output layer's prediction is\\nwhat we return from the forward pass.\\nThe predict method is invoked when you use this model\\nto get predictions,\\nand the predict method makes the same forward pass\\nthrough the neural network.\\nThe one thing that we do different here,\\nwe call torch.no_grad\\nto disable gradient computation for the network\\nduring the inference phase.\\nNow that we've set up our neural network,\\nlet's explore and understand it.\\n\\nLet's instantiate the neural network.\\nThe number of features that we have is eight.\\nRemember, that's the number of columns in the X train data.\\nPrinting out the neural network will give us\\na string representation of the layers in the net.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5491065\",\"duration\":225,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Setting up the Dataset and DataLoader\",\"fileName\":\"2750013_en_US_01_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":225,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to create a PyTorch Dataset and DataLoader.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7224689,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now before we set up our neural network,\\nwe have pre-processed our data and set up the features\\nand labels in the form of Torch Tensors.\\nNow, in order to organize and feed our data efficiently\\ninto our machine learning model,\\nwe'll make use of two PyTorch objects,\\nthe dataset and the DataLoader.\\nThe dataset abstracts access to individual data samples\\nin our training and test data.\\nAnd the DataLoader will automate batching, shuffling,\\nand multi-threaded loading.\\nLet's get Gemini's help in setting these up.\\n\\nSo I have train inputs and train targets\\nthat are both to Tensors.\\nHow do I create a dataset and a DataLoader\\nto train a neural network?\\nNow, I've found that Gemini's code\\ntends to be overly complex.\\nWhat I'm going to do is take a look at this code\\nand only use the stuff that I need.\\nNotice how it uses the Tensor dataset class\\nto instantiate the train dataset.\\nAnd then if you scroll down,\\nit sets up a train loader using the DataLoader class.\\nNow these are the bits that I'm going to copy over\\nto set up our code.\\n\\nYou can see the explanations that Gemini has provided\\nin order to understand why these objects are needed.\\nLet's head back to our notebook\\nand set up our code.\\nIn order to feed our training data\\ninto our PyTorch neural network in batches,\\nwe are going to be using a Tensor dataset\\nand a Tensor DataLoader.\\nIn PyTorch, the dataset and DataLoader\\nare foundational classes provided\\nby the torch.util.data module.\\nThese facilitate the loading, processing\\nand batching of data.\\n\\nDataset is an abstract base class in PyTorch,\\nand we'll be using the derived Tensor dataset class.\\nYou can think of a dataset as representing a collection\\nof data items, and this Tensor dataset\\nthat we've instantiated here holds our training data.\\nThe instantiated Tensor dataset using the train inputs\\nand train targets, and here are the first five records\\nin the Tensor dataset.\\nJust a heads up that here we've used one\\nof the built in dataset classes, the Tensor dataset,\\nbut it's also possible for you to create\\nyour own custom dataset by deriving\\nfrom the dataset base class.\\n\\nOnce we've set up a dataset,\\nthen we actually access the training data\\nto train our model, we'll want to load the data in batches.\\nWe may want to shuffle the data\\nor use multiple workers to speed up data loading.\\nAll of that is done via the DataLoader.\\nI've instantiated a DataLoader here\\nand specified a batch size of eight.\\nFor the training data, I've also set shuffle equal to true,\\nso when we feed the data into our model for training,\\nit'll be shuffled.\\nIt'll not come in any predictable pattern.\\n\\nThe DataLoader is an iterable\\nthat allows us to iterate over the data in batches.\\nOn line five, you can see I create an iterator\\nand call next on it,\\nand this will allow us to see the first batch\\nof training data.\\nNotice that we have eight records here.\\nBecause we specified batch size eight.\\nWe've now created a dataset\\nand DataLoader for our training data.\\nLet's do the same for validation data.\\nFirst, we convert x_val and y_val to the Tensor format,\\nand once that's done, we'll instantiate a Tensor dataset\\nfor our validation data and then we'll instantiate\\na data loader using this Tensor dataset.\\n\\nNotice that when we instantiate a DataLoader\\nfor the validation data, I haven't specified\\nshuffle equal to true.\\nValidation data is only used to evaluate the model\\nand does not need shuffling.\\nHere is the first patch of records from the validation data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5487084\",\"duration\":480,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Training a neural network using PyTorch\",\"fileName\":\"2750013_en_US_01_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":481,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to train a neural network in PyTorch.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14931722,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We are now ready\\nI'm going to set up a dictionary called loss_stats,\\nwhich will hold the values of the training loss\\nand validation loss for each epoch.\\nWe'll run 100 epochs of training\\nand epoch, as you likely already know,\\nis one pass through the entire training data.\\nThe next thing we need to do is figure out\\non what device we'll run this training.\\nI happen to be running on a new M2 Mac,\\nand this has a GPU available.\\nMaybe you're running on a similar machine.\\nMaybe you're running on Google Colab,\\nand you too have a GPU.\\n\\nOtherwise, just run training on a CPU.\\nThat's totally fine as well.\\nWhat we are doing here is a check\\nto see whether a GPU is available.\\nIf it is, we'll use the GPU.\\nOtherwise, we'll use the CPU.\\nSo if torch.cuda.is_available returns true,\\nthat is we have a GPU.\\nThen the device will be cuda.\\nIf torch.backends.mps is available is true.\\nthis means that the new Metal Performance Shaders backend\\nfor GPU training\\nand acceleration is available on your machine.\\n\\nSo the device will be mps.\\nThis is likely to be available\\nif you're working on a new Mac device,\\nor if neither of these options is true,\\nwe'll just go with CPU training and device will be CPU.\\nYou can see that my device says mps\\nbecause I'm on a new Mac.\\nWe'll now instantiate\\nand train our neural network using PyTorch.\\nRemember, PyTorch is a lower level API,\\nand it does not abstract us away\\nfrom the details of neural network training,\\nand there'll be a lot of boilerplate code involved.\\n\\nI instantiate the neural network, input_size=8,\\nand I call to(device)\\nin order to move the model parameters of the neural network\\nto whatever device we are using for training,\\neither CPU or GPU.\\nMoving to the device that you're using\\nfor training is a part of PyTorch's boilerplate.\\nFor all of your training data\\nand your model,\\nyou have to move them to the right device\\nso that the training occurs on the right device.\\nTraining a neural network involves using a loss function.\\n\\nThis loss determines\\nhow good the neural network is at any point in time.\\nYou can think of the loss as representing\\nhow far away the predictions of the neural network are\\nfrom the actual target values.\\nSince this is a regression model,\\nthe loss function that we'll use\\nis going to be the means square error loss.\\nThe objective of training a neural network\\nis to update your model parameters\\nfor every iteration of training\\nso as to minimize the loss function.\\n\\nIn PyTorch, it's the optimizer\\nthat actually updates the model parameters\\nusing gradient values.\\nSo I've instantiated an optimizer for that purpose.\\nThe optimizer I've used here is the SGD\\nor the Stochastic Gradient Descent optimizer.\\nThere are several different optimizers available\\nas a part of the PyTorch framework.\\nSGD is a commonly used straightforward optimizer.\\nThe optimizer takes in the model parameters\\nthat need to be updated,\\nas well as the learning rate,\\nwhich I've set to 10 to the power -2.\\n\\nThe learning rate determines the step size\\nfor how the model parameters converge\\nto their optimal values.\\nToo a large a step size,\\nyour model may not converge.\\nToo small a learning rate,\\nand your model may take too long to converge.\\nThis 0.01 works well for this particular model,\\nand that's why I've selected it.\\nNext, let's set up the training loop for our model,\\nand here is where you'll really see the boilerplate code,\\nand you'll find that when we use PyTorch Lightning\\nin the next demo,\\nmost of this code will be eliminated.\\n\\nFirst, I have a for loop to iterate\\nover the number of epochs of training that will run.\\nI initialize the training epoch loss to 0.\\nWe'll reset this for every epoch,\\nand then we make sure\\nthat the model is in training mode by calling model.train.\\nIn the training mode,\\ngradients will be computed\\nso that model parameters can be updated\\nusing those gradients.\\nGradients are just partial derivatives of the loss function\\nwith respect to individual model parameters,\\nand these partial derivatives are used to determine\\nhow model parameters should be tweaked\\nto minimize the loss function.\\n\\nThis is all you need to understand\\nconceptually about gradients.\\nFor every epoch,\\nwe run another for loop iterating over each batch\\nof the training data.\\nThis is on line 7.\\nOn lines 10 and 11,\\nwe move our training data\\nto the device that we are using for training,\\nwhether it's a GPU or a CPU.\\nNotice the boilerplate to device code here.\\nNext, we make a forward pass through the model,\\nthis is on line 16,\\nfor the first batch of training data\\nand get predictions.\\n\\nThis forward pass uses the current value\\nof the model parameters.\\nWe then compute the loss of the model at this stage\\nby invoking the loss function,\\nwhich take in the prediction and the Y values.\\nNow for each batch of training,\\nwe zero out the optimizer's gradients\\nso that gradients that were computed previously\\ndo not affect this particular batch.\\nAnd then we perform gradient descent.\\nWe make a backward pass through the model\\nby calling train_loss.backward.\\nThis is where gradients are computed\\nat partial derivatives,\\nand optimizer.step will then use those gradients\\nto tweak our model parameter values.\\n\\nAnd then on line 25,\\nwe add the current training loss for this batch\\nto the training loss of the epoch as a whole.\\nAll of the steps you see from line 9\\nthrough line 25 is repeated\\nfor every batch of data in each epoch.\\nModel parameters will be updated\\nfor every batch in each epoch,\\nand the loss function will be minimized\\nto improve the model.\\nAfter each epoch of training,\\nwe'll run the validation data through the model\\nand evaluate the model's performance.\\n\\nYou can see the with torch.no_grad(): on line 28.\\nThat's within the outer for loop for the epoch,\\nbut outside of the for loop\\nfor the individual batches of training.\\nThe torch_no.grad method turns off gradient computation\\nfor the model,\\nwhen we pass through the validation data.\\nI initialize the validation epoch loss to 0 on line 30,\\nand on line 32, we switch the model over\\nto the evaluation state by calling model.eval.\\n\\nThis is the state for evaluating the model,\\nand once again, I have a nested for loop,\\nwhere we iterate over the batches of validation data.\\nOn lines 36 and 37,\\nWe move the validation data to the device that we are using\\nfor training CPU or GPU.\\nWe get the predictions on the validation data on line 39,\\ncompute the loss on line 41,\\nand we add the loss of this batch\\nto the validation epoch loss.\\nOn lines 45 and 46,\\nwe compute the training loss\\nand validation loss for the entire epoch\\nand then upend that information\\nto our loss_stats dictionary.\\n\\nThis is on lines 48 and 49.\\nOn line 51, for each epoch,\\nwe print out the training loss,\\nand the validation loss,\\nand that's it.\\nThis is the training process.\\nIt's simple, but there is really\\na lot of boilerplate code involved.\\nLet's look at the output of training.\\nBecause this is a very simple neural network,\\ntraining runs through very quickly.\\nYou can see that after the first epoch of training,\\ntraining loss was at 0.68,\\nvalidation at 0.33.\\n\\nBy the end of training,\\ntraining loss is at 0.092\\nand validation loss at 0.088.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5489056\",\"duration\":225,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualizing losses and evaluating models\",\"fileName\":\"2750013_en_US_01_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":225,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to evaluate a regression model using PyTorch metric libraries.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7056916,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that we have a trained model,\\nlet's visualize how the training loss\\nand validation loss change over epochs of training.\\nAnd for that, I'm going to set up a DataFrame\\nwith the training loss\\nand validation loss along with the epochs.\\nThis information is available in the loss statistics\\nthat we've manually populated in the training process,\\nand we now have this in the form of a DataFrame.\\nWe have the epochs, then whether it's training\\nor validation loss, and the corresponding value.\\nThe head shows us all of the training losses.\\n\\nAnd the tail of this DataFrame contains all\\nof the validation losses.\\nNow with this information in the DataFrame format,\\nthe next step is to plot this in the form\\nof a nice line chart.\\nAnd I'm going to take Gemini's help for this.\\nNow I'm going to tell Gemini exactly what I have.\\nI have a train_val_loss DataFrame,\\nand then I have the columns in that DataFrame\\nvalue that holds the loss.\\nVariable tells us whether it's a training\\nor validation loss,\\nand epochs tracks the number of epochs.\\nAnd then I asked Gemini, can you plot a line chart\\nusing Seaborn to display this information?\\nAnd Gemini has a brain feed.\\n\\nInstead of giving me the code, it just gives me the chart.\\nIt basically made up some data\\nand just plotted this line chart.\\nGen AI models can have brain feeds, they can hallucinate,\\nthey can be plain wrong.\\nI know this is not what I want.\\nSo here is the follow-up question.\\nI asked Gemini to generate the code for the line chart,\\nand now Gemini corrects itself and gives me accurate code.\\nIt's also set up some fake data\\nbefore it gives me the plotting code.\\nI'm only interested in the plotting code,\\nso I'll only copy that bit over.\\n\\nVisualizing this using a Seaborn line plot\\nis very straightforward.\\nWe'll have epochs on the x axis and training\\nand validation losses on the y axis.\\nAnd here's what the line chart looks like.\\nFrom the start,\\nyou can see that for about the first 40 epochs of training,\\ntraining and validation losses fall pretty drastically.\\nBut after that, they kind of stabilize\\nand don't really fall much further.\\nAbout 40 epochs of training\\nwould've been sufficient for this model.\\nLet's now compute the R square score\\nof the model on the validation data.\\n\\nIdeally, we should have a separate test dataset\\nfor this purpose, but because the dataset was fairly small,\\nlet's just do it with the validation data.\\nWe turn off gradients with torch.no_grad,\\nswitch the model to eval mode, model.eval,\\nand we iterate over every batch in the validation data.\\nMove the features to the device, this is on line nine.\\nGet the predictions from the model.\\ny_pred will hold the predictions from the model,\\ny_true will hold the labels from the actual data.\\nLet's quickly look at the format\\nof the actual data that is in y_true.\\n\\nWhat we have here is a list of tensors\\nwhere each tensor contains a prediction\\nfor one batch of data.\\nYou can check this out on your own.\\nThe prediction data will also be in the same format.\\nI'll now perform a torch.stack operation\\nthat will give us the actual values in the form\\nof a single tensor rather than a list of tensors.\\nThis is what torch.stack outputs.\\nI now have a single tensor with all actual values.\\nI'll use torch.stack on the predicted values\\nso that we now have a single tensor\\nwith all predicted values as well.\\n\\nNow that the data is in this form,\\nlet's compute the mean square error and R square score\\nfor this model on the validation data.\\nI instantiate MeanSquaredError and R2Scored\\nand move those to the device\\nas well and compute the two values.\\nYou can see that the R square of this model is 0.893,\\nwhich is a very good score.\\n\"}],\"name\":\"1. Building a Neural Network with PyTorch\",\"size\":74018980,\"urn\":\"urn:li:learningContentChapter:5493053\"},{\"duration\":1593,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5487083\",\"duration\":242,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Prompt engineering to convert PyTorch to PyTorch Lightning\",\"fileName\":\"2750013_en_US_02_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":243,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to use prompt engineering techniques to help convert raw PyTorch code to use the PyTorch Lightning framework.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7895777,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] At this point, we've successfully built\\nand trained a neural network to perform regression,\\nbut we used PyTorch and not PyTorch Lightning.\\nIt was important that you see\\nhow the model is built using PyTorch first\\nso that you can see how much cleaner our code is\\nwhen we use PyTorch Lightning.\\nWe'll eliminate most\\nof the boilerplate code that you saw for training loops\\nand feeding data in batches, iterating through number\\nof epochs, moving model parameters\\nand data to the right device.\\n\\nAll of that code will just disappear,\\nand we'll build our model in a very clean manner\\nwith PyTorch Lightning.\\nLet's say you really want to understand\\nhow PyTorch Lightning can help you.\\nYou can turn to digital assistance.\\nHere I am asking Claude, what's the difference\\nbetween PyTorch and PyTorch lightning?\\nMaybe I haven't understood it completely.\\nClaude gives me a very clean breakdown.\\nPyTorch is the foundational deep learning framework,\\nand this is what we are basically going to be using\\nfor all our low level tensor operations,\\nexcept that when we use PyTorch Lightning,\\nit gives us a high level wrapper\\nthat makes our code more readable and more organized.\\n\\nIt's a structured framework that organizes our code.\\nIt abstracts away boilerplate code.\\nYou automatically end up following the best practices\\nfor training, validation, and testing,\\nand everything else just gets easier.\\nLet's try and understand when we would choose\\nto use PyTorch Lightning over PyTorch.\\nAnd that's my next question to Claude.\\nAnd here it has a very clear recommendation.\\nWhen you are working with multiple developers\\nin a production environments,\\nPyTorch Lightning is the right choice.\\n\\nPyTorch Lightning is useful for complex training scenarios\\nand research with standard patterns.\\nYou can clearly see here that raw PyTorch is\\nwhat you'll choose when you want\\nto understand what's happening\\nunder the hood, like we did earlier.\\nRaw PyTorch is great for one-off experiments\\nor for custom loops that have unusual requirements,\\nmaybe unique architectures.\\nWhen you're coding with the help\\nof generative AI models like ChatGPT, Gemini,\\nor Claude, you'll find that you often have\\nto write very little code, but you may need to reorganize\\nor review a lot of code.\\n\\nNow, for the simple regression model that we've built\\nso far, you can actually just drag that notebook to Gemini\\nand then basically ask Gemini to convert the code\\nto use PyTorch Lightning, and it often works very well.\\nYou can try this with Claude.\\nYou can try this with ChatGPT,\\nbut I got the best results with Gemini.\\nThis is the notebook where we wrote our code earlier.\\nIt's my original code using PyTorch.\\nCan you generate code for this using PyTorch Lightning\\nand make it clear that I want the answer\\nin the chat response and not in the form of a notebook,\\nso that I can quickly review it.\\n\\nAnd this gives me great results.\\nNot only does it use the lightning module for data loading\\nand pre-processing, but it also sets up\\nthe lightning module\\nfor my training, test, and validation steps.\\nYou can see here that Gemini has given me two classes,\\nthe insurance data module\\nthat inherits from lightning data module.\\nThis is the pre-processing data pipeline,\\nand then there is the insurance regression model\\nthat inherits from lightning module.\\n\\nNow, I have to admit that I asked Gemini, ChatGPT,\\nand Claude this prompt several times.\\nThe answers were always good, but not always this thorough.\\nSo that's something to keep in mind.\\nGen AI models always set up the PyTorch Lightning module\\nfor my neural network correctly,\\nbut it often forgot the data processing module.\\nNow, copying this code over will not allow us to learn\\nand understand how PyTorch Lightning works.\\nSo what I'm going to do is break this code down\\nand explain how you would convert our regression model\\nthat we trained earlier to PyTorch Lightning.\\n\\nI thought it's important that you know\\nthat if you're migrating old code to PyTorch Lightning,\\nyou can make use of digital assistants.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2177061\",\"duration\":464,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Encapsulating data using a LightningDataModule\",\"fileName\":\"2750013_en_US_02_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":464,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to encapsulate data with a LightningDataModule.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15812526,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] !pip install lightning\\nto get PyTorch Lightning installed on your local machine.\\nYou need the Lightning module\\nin addition to the PyTorch framework\\nthat we've already installed.\\nLet's take a look at the version of PyTorch Lightning\\nthat I'm using here, lightning._version_,\\nand this tells us that I'm using v2.5.2,\\nthe latest at the time of this recording.\\nLet's go ahead and set up the import statements\\nfor the various libraries that we'll be using.\\nThe imports here are the same as in the previous demo,\\nbut I have a few additional imports for PyTorch Lightning,\\nthe one on line nine, you can see,\\nimport lightning.pytorch as pl.\\n\\nIn addition to abstracting away the training process\\nof a neural network, the training loop,\\nand other code associated with it,\\nPyTorch Lightning also makes available\\nthe LightningDataModule.\\npl.LightningDataModule allows us to abstract and organize\\nthe data related aspects of our deep learning model.\\nIt's a way to decouple the data processing steps,\\nthe loading, pre-processing, and splitting of data\\nfrom the model training logic.\\nWhen we structure all of our data processing tasks\\nto be encapsulated within a LightningDataModule class,\\nwe are centralizing all data related operations in one place\\nand encapsulating all of our code\\nto make it more modular and reusable.\\n\\nHere I have specified the skeleton\\nof the InsuranceDataModule class\\nthat inherits from pl.LightningDataModule.\\nAnd you can see that there are several functions\\nof the base class that I'm about to override.\\nThe functions have all been named in very meaningful ways,\\nso you know exactly what goes in each of these functions.\\nHowever, it's important that you understand\\nwhat goes in each of these methods\\nwithin the skeleton structure,\\nand this is where you can take help from generative AI.\\n\\nI've pasted in my skeleton structure\\nand I've asked for an explanation of the methods\\nthat I have in the LightningDataModule.\\nHere is Gemini's response.\\nIn the init method is the constructor\\nwhere you initialize parameters for your data module.\\nThe prepare_data method is invoked exactly once\\nacross the entire training process,\\nand is one that does not require an accelerator.\\nThis is where you might access\\nand download data to shared storage,\\nprocess data and store it in shared storage once again,\\nwhere it's accessible by the workers\\nrunning in a distributed manner.\\n\\nIt's important that you not have self.attribute assignments\\nin prepare_data because this is run on just one worker,\\nit's not run in a distributed manner.\\nThe setup method is invoked on every process\\nin distributed training,\\nand this is where you perform data loading,\\nsplitting, transformations, and other pre-processing.\\ntrain_dataloader, as the name suggests,\\nreturns the data loader for the training data,\\nand val_dataloader returns the data loader\\nfor the validation data.\\nThe LightningDataModule offers other methods\\nthat you can override,\\nbut these are the basic ones that we look at.\\n\\nLet's start by adding in the code for the init method\\nhere in this InsuranceDataModule,\\nthis is where we'll initialize various parameters\\nwe may want to use for the data.\\nThe only one we have is the batch_size.\\nIn the prepare_data function is where you access the data\\nwherever it's stored,\\nmaybe you'll need to download the data.\\nKeep in mind though that this is a method\\nthat is called only once, even in distributed training.\\nThis is not where you pre-process the data.\\nThis is where you store whatever you do\\nin a shared repository,\\nwhere the distributed processes can access it.\\n\\nSince my data is available on the local machine,\\nI don't really need to do anything here in prepare_data,\\nso I just pass.\\nNext, we have setup.\\nThis is where you'll split the dataset\\nand apply whatever transformations and pre-processing\\nthat you need.\\nThis will be called on every GPU separately.\\nAn input argument to the setup function\\nis what stage the model is currently running.\\nThis can be the fit stage, that is training,\\nor it can be the validation or test stages.\\n\\nHere I use pd.read_csv to read in the dataset\\nand store it in a member variable for this object.\\nHere we are keeping things simple,\\nso I'll apply the data transformations\\nall in the fit stage or the training stage of the model.\\nThe data transformation operations that you see here\\nshould all be very familiar to you\\nbecause these are the steps we carried out\\nin the previous demo.\\nOn lines 15 and 16, we extract the X features and y values.\\nOn line 18, I check whether the stage is fit\\nor stage is None.\\n\\nSo in the training phase,\\nwe split the data into training and validation,\\nthis is on lines 19 and 20.\\nThe code on lines 22 through 34 is where we encode\\nthe categorical features of our data.\\nOne thing I do differently here from the previous demo\\nis that I encode the categorical features first\\nand then scale the numeric features.\\nOn lines 36 and 37, we convert the y values to NumPy arrays.\\nOn lines 40 through 42, we standard scale our features.\\n\\nOn lines 45 through 47, we min-max scale our targets,\\nthat is the y values.\\nAnd on lines 50 through 53,\\nwe convert all our NumPy arrays into tensors.\\nWe'll feed in data to our model in the form of tensors.\\nThe training and validation data\\nare available as member variables of this class,\\ntrain_inputs, train_targets, val_inputs, and val_targets.\\nThe feature and target tensors\\nneed to be instantiated as data loaders.\\n\\nThe train_dataloader method returns the data loader\\nfor the training data.\\nThe steps here are, again, familiar.\\nWe instantiate a tensor dataset,\\nand using that, we instantiate a data loader.\\nNotice that I've specified num_workers = 4\\nfor the data loader.\\nThis is because I have four cores on my machine,\\nand this will allow me to load data in parallel\\nusing those four cores.\\nThe train_dataloader function returns an instance\\nof the data loader for the training data.\\n\\nIn exactly the same way,\\nI've overridden the val_dataloader function.\\nThis is just a data loader for the validation data.\\nWe instantiate a TensorDataset\\nand use that to instantiate a validation data loader,\\nnum_workers again set to four\\nso that four workers running on four cores\\ncan be used to load this data.\\nWhen we actually train our model,\\nthis InsuranceDataModule will be passed in\\nas an input argument to the trainer object that we'll use,\\nand the individual methods of this data module\\nwill be invoked at the right point in time\\nto get access to the right bits of data\\nneeded for training and validation.\\n\\nLet's just make sure\\nthat the InsuranceDataModule works as expected.\\nI'm going to create an object of the InsuranceDataModule.\\nLet's now call prepare_data and setup manually\\nso that the data is available.\\nAnd now I'm going to invoke the train_dataloader\\nand I'll print out one batch of the training data.\\nAnd you can see that there are eight records in this batch.\\nIn a similar way, let me access the val_dataloader\\nand I'll print out one batch of the validation data as well.\\n\\nAnd we have eight records in this batch.\\nObserve how by using theta module\\nto manage all of the data preparation\\nand processing operations,\\nwe've created a modular bit of code\\nthat can be reused anywhere.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2176052\",\"duration\":437,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Encapsulating a model using a LightningModule\",\"fileName\":\"2750013_en_US_02_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":437,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to encapsulate a model with a LightningModule.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14784922,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In the previous demo,\\nwe built a simple regression model using PyTorch.\\nNow, in this demo,\\nI'm going to build the same neural network\\nthat we used before,\\nbut this time, I'm going to use PyTorch Lightning.\\nWhat I've defined here on screen\\nis the skeleton of a class\\nthat derives from pl.LightningModule.\\nJust like the LightningDataModule\\nencapsulates all of the data-related operations,\\na LightningModule is a fundamental class\\nin PyTorch Lightning that encapsulates\\neverything related to our deep learning model.\\n\\nIt extends the functionality of the nn.Module class\\nthat we used in PyTorch to build up our neural network.\\nThe LightningModule adds additional methods and structures\\nthat streamline the training, validation,\\ntesting, and prediction processes of the model.\\nA LightningModule organizes your PyTorch code\\ninto different sections,\\nand each section is a different method\\nthat you override from the LightningModule base class.\\nYou can see that I have six different methods here.\\n\\nThese correspond to the six different sections\\ninto which LightningModule organizes your code.\\nLet's get help from gen AI\\nto understand the basic structure\\nof a LightningModule.\\nI've pasted in the skeleton code with all of the methods\\nand here is my prompt,\\n\\\"Can you explain each of these methods\\nin the PyTorch LightningModule?\\\"\\nAnd then, let's quickly go through\\nwhat each of these methods are meant for.\\n__init__ method, that's the constructor.\\nThat's where we'll define the architecture\\nfor our neural network.\\n\\nGemini reminds us that it's important\\nthat we call self.save_hyperparameters\\nwithin the __init__ method.\\nWe'll do that when we actually fill out the stubs.\\nsave_hyperparameters automatically assigns\\nthe input parameters to the __init__ method\\nas hyperparameters that you can access later.\\nconfigure_optimizers is where we set up the optimizer\\nfor our model and the learning rate.\\nThis is where we make a forward pass\\nthrough the neural network\\nwhere we pass the input data\\nthrough the layers of our regression model.\\n\\nAnd then, we have train step.\\nThis defines what happens in a single training step\\nwith a single batch of data.\\nPyTorch Lightning will automatically call this in a loop\\nfor all of the batches for every epoch.\\nThe validation step defines the code\\nfor a single batch of validation data.\\nAnd finally, predict_step defines what happens\\nin a single prediction step.\\nLet's look at and understand each of these step by step,\\nstarting with __init__.\\nThe __init__ method\\nand in addition, there is also a setup method\\nthat I've not overridden,\\nthis is where you'll define your model.\\n\\nWhatever we had specified in the nn.Module __init__ method,\\nI've essentially moved that code in here\\nto the __init_ method of LightningModule.\\nThe __init__ method takes in a number of parameters\\nfor the model.\\nnum_features is the number of input features\\nand the learning_rate, which I've set to 0.01.\\nThis learning_rate parameter\\nwill be used by the optimizer that we'll instantiate.\\nNow, within the __init__ method,\\nyou can see I've set up the layers of the neural network.\\nThere are three linear layers,\\nand then there is the final output layer.\\n\\nAnd the activation function\\nthat we'll use for the three layers\\nis the ReLU activation.\\nThis network is exactly the same network that we've used\\nin our earlier demo.\\nGemini had highlighted the importance\\nof calling self.save_hyperparameters within __init__.\\nsave_hyperparameters\\nis a method in the LightningModule base class.\\nWhen you invoke self.save_hyperparameters\\nhere in the __init__ method,\\nall of the input arguments to __init__,\\nhere we have two, num_features and learning_rate,\\nwill be saved as hyperparameters\\nand will be accessible by other self.hparams object.\\n\\nLet me explain what I mean\\nby adding the code here in the configure_optimizers method.\\nThis is the method where you'll instantiate\\nand set up any optimizers\\nand schedulers that you use to train your model.\\nI once again use\\nthe stochastic gradient descent optimizer, SGD.\\nI pass in the parameters of the model\\navailable in self.parameters,\\nand I pass in the learning rate of the optimizer\\nusing self.hparams.learning_rate.\\n\\nThe learning_rate that we passed in\\nas an input argument to __init__ on line three\\nhas been saved in this hparams object\\nthat we access here on line 17,\\nbecause we invoked self.save_hyperparameters on line 14.\\nself.save_hyperparameters thus saves all input arguments\\nthat you pass into __init__\\nThe forward function\\nis where you define the forward pass\\nthrough the neural network,\\nand here we define the same forward pass\\nas we did before in PyTorch.\\n\\nWe pass the inputs through the three linear layers.\\nEach layer has the ReLU activation,\\nand then finally we pass through the last output layer\\nand return the final value.\\nNow, if you remember in PyTorch,\\nwe set up a training loop and within that,\\nwe defined the training process of a model.\\nThis is what you'll define here in training_step.\\nThis training_step function will be invoked in a loop.\\nWhat you define here\\nare the operations that need to be performed\\non a single batch of training data.\\n\\nYou can see the input arguments.\\nA batch of data is passed in.\\nWe access the x features and y values from this batch.\\nWe make a forward pass through the model\\nby calling self.forward on the x features.\\nOn lines 32 and 33,\\nwe instantiate the mean squared error loss function\\nand compute the loss for this batch of predictions,\\nand then we simply call self.log\\nand log the loss out.\\nActually displaying the loss on screen and logging it out?\\nWell, PyTorch Lightning\\nwill take care of that automatically.\\n\\nMake sure you return the loss from this function.\\nThis is what the Lightning framework will use\\nto compute gradients and update model parameters.\\nTo define the validation process of your model,\\nyou'll simply specify validation on one batch of data\\nhere in the validation step method.\\nWell, validation is straightforward once again.\\nWe make a forward pass with a batch of data\\nusing self.forward.\\nWe compute the mean square error loss on the validation data\\nusing the loss function,\\nand we log out the validation loss.\\n\\nAnd we have a predict step that you can override here\\nin order to make predictions on the data.\\nWe access the x variables and y values from the batch\\nand simply make a forward pass through the model\\nto get predictions.\\nYou can see here that the entire model training code\\nwithout the additional boilerplate\\nof moving the model to a device or the training loop\\nhas been encapsulated here in one class.\\nLet me just instantiate\\nand print out a string representation\\nof our model defined in this LightingModule class.\\n\\nAnd in the next movie,\\nwe'll see how we'll actually train a model\\nusing PyTorch Lightning.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5486079\",\"duration\":450,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Training the model using the PyTorch Lightning Trainer\",\"fileName\":\"2750013_en_US_02_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":450,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to train a model with the PyTorch Lightning Trainer.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16043213,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- We've defined our model, our training step\\nand validation step nicely encapsulated\\nin a PyTorch lightning module,\\nand we are now ready to train our model.\\nIf you remember in PyTorch, you had to write a lot of code\\nto actually train your model\\nand get validation metrics for your model after each epoch.\\nIn addition, in order to ensure\\nthat your model trains on a GPU, if A GPU is present,\\nyou had to also move the model parameters as well as\\nthe X and Y values that you're using to train your model\\nto the right device for training.\\n\\nNow, keep all of that in mind while we see how easy it is\\nto train a model using PyTorch Lightning.\\nYou see those five lines of code here on screen,\\nincluding the import statement?\\nWell, that's all the code you need to run a training loop,\\nrun validation at the end of every epoch\\nand display all of that nicely to screen.\\nNo nested for loops, no moving the model\\nto the right device, no loss computation on training\\nand validation data.\\nNo optimizer step, no loss.backward, nothing.\\n\\nSo what's the code that we write?\\nFirst, let's take a look at the import statement\\nwhere I import a CSV logger that will log the details\\nof the training process\\nto our local machine in a CSV format.\\nNext, I instantiate the insurance data module class,\\nwhich encapsulates all of the data preparation\\nand processing operations for the data\\nthat I plan to use to train the model.\\nNext on line 5 I instantiate a CSV logger class\\nto write the logs out\\nto the logs sub folder under my current working directory.\\n\\nThe actual training process will be taken care of\\nby the pl.trainer class.\\nThis is a central class that manages\\nand automates the entire training process\\nand abstracts away a lot of the boilerplate code\\ntypically associated with training loops,\\ndistributed training and evaluation.\\nI instantiate the trainer, specify I want to run a maximum\\nof 50 epochs of training passing in the CSV logger\\nso that logs are generated in my current working directory\\nand that's it.\\n\\nI call trainer.fit.\\nTrainer.fit takes on an instance of a lightning module\\nthat is the model that we want to train\\nand a data module that is a dataset\\nwe should use for training.\\nTrainer.fit will invoke the right methods on the model class\\nas well as the data class to get the right data\\nfor training and validation,\\nand it'll run the training process for 50 epochs.\\nOur training process has begun.\\nObserve that the trainer automatically checks\\nto see whether a GPU or a TPU is available.\\n\\nIf not, it'll just run training on the CPU.\\nYou can see that I do have a GPU available\\nso that GPU will be used for training.\\nThe trainer also shows you\\nhow many trainable parameters your model has.\\nWe have about 1.2K or 1200 parameters.\\nAs the training process continues,\\nyou'll see for every epoch of training,\\nthe trainer prints out the epoch.\\nSo you can see epoch 16 here on screen.\\nThe training loss at this epoch and the validation loss\\nat this epoch.\\n\\nWe had set show progress bar to TRUE when we logged out\\nour training loss and validation loss,\\nand that's why you see these progress bars\\non screen as well.\\nAt epoch 49, our 50th epoch,\\nthe training process is complete.\\nThe training loss at this point is 0.00889\\nand validation loss is 0.00525.\\nNow that we have a trained model,\\nlet's see how easy it is to get predictions\\nfrom the model.\\nI call trainer.predict.\\n\\nPass the model in\\nand specify the validation data loader as the data loader\\nfor the data for which I want predictions,\\nand you can see the predictions from the model\\noutput here on screen.\\nNow, in addition to that,\\nyou can see a warning here on screen.\\nIt's good practice to pay attention to these warnings.\\nYou may want to ignore certain warnings, but not others.\\nI found generative AI incredibly useful in helping me\\ndebug warnings, letting me know\\nwhether I can ignore this or not.\\nBefore I switch over to get Gemini's help with this warning,\\nlet's just take a look at the result.\\n\\nThe output format of the predictions\\ngives us the predictions for every batch of the input\\nin a separate tensor,\\nand this is something we'll have to deal with in a bit.\\nNow about the warning, I'm going to copy the warning over\\nand I head over to Gemini to ask it\\nwhat exactly this is about.\\nNow this is actually interesting.\\nThe warning has to do with the performance\\nof the predictions on your data.\\nWhen you have multiple workers in your data loader,\\nremember, we had set this to four,\\nyou need to set an additional parameters.\\nPersistent workers equal to TRUE for faster loading of data.\\n\\nWithout this additional parameter,\\nthe workers are reinitialized for each batch\\nof data loaded, but when you set persistent workers\\nequal to TRUE, the worker processes do not need\\nto be reinitialized each time\\nmaking your data loading more performant.\\nI'm not going to rerun my code,\\nbut let me show you the change that you need to make.\\nHere we have two data loaders,\\nthe trained data loader and the VAL data loader.\\nI've set persistent workers equal to true\\nfor both of these data loaders.\\nSince I'm not going to rerun the code,\\nyou won't see that these warnings have disappeared,\\nbut I'll leave it to you to try this out on your own.\\n\\nLet's go back to our data where we have predictions\\nfor each batch in a separate tensor.\\nLet's get all of these in a stacked format\\nby using torch.cat to concatenate them.\\nThis will give us a single answer\\nwith all the predictions from our model\\nfor the validation data.\\nIn order to compute metrics for the model,\\nI need the actual values\\nor the labels from the validation data as well,\\nand I extract this by using a for loop.\\nNow that I have this, let's stack the labels as well\\nso we get a single tensor\\nwith all of the labels from our data.\\n\\nNow that we have the predictions as well as the labels,\\nall that's left to do is to compute the mean square error\\nand the R square score for this model.\\nAnd here's the R square score for this model.\\nIn the same ranges before, 0.87.\\nIf you remember, we passed in a CSV logger\\nto the pl.trainer object that we had instantiated\\nto log out the metrics during the training process.\\nWe can now access these metrics.\\nThere'll be in a metrics.csv file in your log directory.\\n\\nThe log directory is accessible via other trainer instance\\ntrainer.logger.log_dir/metrics.csv.\\nThis is a data frame\\nthat I read in which gives us the training loss,\\nvalidation loss for every epoch.\\nWe do a little bit of pre-processing\\nwith this metrics data frame.\\nFor instance, I drop the step column and then I go ahead\\nand display the metrics as a line plot.\\nAlong the X-axis, we have the epoch of training.\\nAlong the Y-axis we have the training and validation losses.\\n\\nThe blue line represents the training loss\\nand the orange dotted line is the validation loss.\\nNow you've trained the same model using PyTorch Lightning\\nand you can see how much simpler\\nand more intuitive the code was.\\n\"}],\"name\":\"2. Using PyTorch Lightning to Build a Regression Model\",\"size\":54536438,\"urn\":\"urn:li:learningContentChapter:2176053\"},{\"duration\":1036,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5487082\",\"duration\":238,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Loading and exploring classification data\",\"fileName\":\"2750013_en_US_03_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":238,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to load data for classification.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7486833,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We've seen how easy it is to build\\nand train a neural network, using PyTorch Lightning.\\nLet's get some more practice with this\\nand this time, we'll build and train a classification model.\\nHere I am on a new Jupyter Notebook.\\nI'll first set up the import statements for all\\nof the libraries that we use.\\nThis involves data access, data processing, libraries.\\nYou can see that I've also imported lightning.pytorch as pl\\non line 10 because this is a classification model.\\nWe'll evaluate this model with a different set of metrics.\\n\\nOn line 13, you can see that I import binary F1 score\\nand binary accuracy.\\nThese are the metrics we'll use to evaluate the model.\\nI have the data that we'll use\\nto train the model in the datasets folder here.\\nThis is a churn modeling dataset.\\nWe'll have information for a number of customers,\\nand we'll use that information\\nto predict whether the customer churned or not.\\nLet's read in and take a look at the data\\nbefore we actually set up a lightning data module\\nto encapsulate all our data operations.\\n\\nI'm going to read in from the churn modeling .CSV file\\nunder datasets.\\nHere in this dataset,\\nwe have information for bank, customers, row number,\\ncustomer ID, last name, credit score, gender, age, tenure,\\nwhether the customer has a credit card or not.\\nAnd finally, the last column here is exited.\\nThat column contains the labels that we are trying\\nto predict, exited equal to one,\\nmeaning the customer churned,\\nzero means the customer did not churn.\\nWhen you take a look at the columns here in this dataset,\\nit's pretty clear that there are certain columns\\nwhich are not really relevant\\nin predicting whether the customer churned or not.\\n\\nNow, when you look at the columns here in this dataset,\\none question you might ask yourself,\\nare all of these columns relevant,\\ncolumns like row number, customer ID?\\nWill they really help us predicting churn?\\nWe'll take generative AI's help to figure out.\\nNext, let's take a look at the data types\\nfor the various columns to make sure they're all\\nof the right type.\\nA quick glance over the column shows me\\nthat numeric values are numeric types, either integer\\nor floats, and categorical values are\\nof type objects or strings.\\n\\nThings look good so far.\\nI'm going to copy these columns over,\\nand I'm going to ask ChatGPT, what columns are relevant\\nfor training our classification model?\\nSo I give ChatGPT to the columns, and I ask it,\\ndo you think all of the features are relevant\\nin predicting customer churn?\\nAnd ChatGPT gives me a very nice breakdown\\nof what features might be useful and what features are not.\\nThese are the features that ChatGPT thinks are relevant,\\nand it also tells you why those features may be relevant\\nand less useful or redundant features include row number,\\ncustomer ID, and surname.\\n\\nWhen we process our data,\\nthese are the columns that we'll drop.\\nThe classification model that we are trying\\nto train is a binary classification model.\\nWe are trying to predict\\nwhether a bank customer exited the relationship or not.\\nWe are trying to predict churn.\\nIf you look at the value counts for the exited field,\\nyou can see that this data set is very skewed.\\n8,000 out of the 10,000 customers did not churn\\nand only 2,000 did.\\nYou can actually view the same information,\\nusing a nice C bone count plot.\\n\\nBased on the exited values,\\nyou can see that there are many more customers\\nwho haven't exited as compared with customers\\nwho have exited or churned.\\nOnce again, this is a skewed dataset,\\nand I'll give you a heads up right now,\\nbut we won't actually be mitigating the skewness\\nof this data.\\nSo I won't perform any sampling to balance out the customers\\nwho have churned and who've not churned.\\nSo there is a limit to\\nhow good our binary classification model can be.\\nIf your training data is skewed, it's hard\\nto build a great classification model.\\n\\nBut since our focus is primarily on learning\\nto use PyTorch Lightning to build a neural network model,\\nwe'll work with this queued data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2177060\",\"duration\":298,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating a LightningDataModule\",\"fileName\":\"2750013_en_US_03_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":299,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to create a LightningDataModule for data preparation.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11576279,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] If you remember the previous regression demo\\nwhere we used PyTorch Lightning,\\nthe first thing we set up was a data module class.\\nThis data module encapsulated all of the data preparation,\\nprocessing, and other operations to work\\nwith our training and validation data.\\nHere is our bank customer churn data module, which inherits\\nfrom the PL dot lightning data module based class.\\nNow in the init method is\\nwhere we initialize the parameters of the data.\\nWe just have the batch size,\\nand that's the only input argument to the init method.\\n\\nIn repair data is\\nwhere we load the data from wherever it's available\\nand download it and make it available\\nto a shared repository.\\nSince our dataset is available on our local machine\\nand we are not running distributed training,\\nI don't really need to write any code in here.\\nRemember that prepare data is not run\\nin a distributed manner, so it's not run on every machine\\nwhere distributed training runs.\\nIt's run on a single machine.\\nThe setup function is where we perform data transformations\\nand pre-processing and data splitting.\\n\\nIt takes in the stage as an input argument.\\nThe first thing that we do within setup is read\\nin the contents of the churn modeling CSV file\\nand assign it to a self-attribute,\\nself-taught bank customer churn data.\\nNow this data contains missing values.\\nI'll use drop init to drop records with missing values,\\nand I'll use drop duplicates to get rid of duplicate rows.\\nNow, ChatGPT had suggested\\nthat certain columns are not relevant,\\nsuch as row number, customer ID, and surname.\\nLet's drop those.\\n\\nThe X features will not include exited.\\nThat's of course the target.\\nWe have to drop that,\\nbut we'll also not include room number,\\ncustomer ID and surname,\\nbecause these fields do not have any predictive power\\nto figure out whether a customer churned or not.\\nThe Y value, or the target\\nof the classification is the exited\\nthat I assign on line 21.\\nNow, I'll pre-process the data only if stage is equal to fit\\nor stage is equal to none.\\nThat is only in the training phase.\\n\\nThe training phase, we'll get the training as well\\nas validation data and pre-process them both.\\nIn the training phase on lines 24 and 25,\\nI first use train test split from psychic learn\\nto split the data 80% for training\\nand the remaining 20% to evaluate the model.\\nOn lines 28 through 37,\\nwe one hot encode the categorical values present\\nin the data.\\nThe only categorical features include geography and gender,\\nand we instantiate the one hot encoder\\nand then use a column transformer\\nto one hot encode these values.\\n\\nOn lines 39 and 40 is\\nwhere we actually perform the one hot end coding\\nby calling fit transform on the training data\\nand transform on the validation data.\\nOn line 42, we convert the Y values to number array\\nand on lines 45 through 48, we use the standard scaler\\nto standardize all numeric values.\\nAnd finally, on lines 51 through 54, we take our training\\nas well as validation data and convert them all to PyTorch.\\n\\nAnd then of course, we have the train data loader function,\\nwhich returns the data loader for the training data.\\nWe instantiate a tensor data set,\\nand then we use that to instantiate a data loader.\\nWe load the data using four workers,\\nsince I have four cores on my machine.\\nAnd keeping in mind, the performance related warning\\nfrom the previous demo, I've set persistent workers to true.\\nBecause we are using multiple workers,\\nI don't want the workers to be reinitialized\\nfor every batch of data loaded.\\nThe worker should be persistent and the code\\nand the validation data loader is identical.\\n\\nWe instantiate a tensile data set with the validation data\\nand use that to instantiate a data loader.\\nLet's quickly check that our data module works just fine.\\nI instantiate the bank customer churn data module,\\ncall, prepare data, and then set up.\\nI now access the train data loader,\\nand let's take a look at one batch of training data.\\nOnce again, we've used a batch size of eight.\\nAfter all of the pre-processing that we've performed,\\nlet's see the number of features\\nthat we have in the input data.\\n\\nWe access one record of the training data,\\nuse the shape property, and get the second dimension,\\nwhich will give us a number of features.\\nIt's equal to 11.\\nRemember, we need this information to set up the layers\\nof our neural network model.\\nNext, let's look at one batch of the validation data\\nto make sure that the validation data loader is\\nalso working fine, and you can see that indeed it is.\\nAll that's left for us is to set up a lightning module\\nwith our model and train the model,\\nusing a lightning trainer object.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5492065\",\"duration\":259,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating a LightningModule\",\"fileName\":\"2750013_en_US_03_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":259,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to create a model with a LightningModule.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9133097,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] We are now ready to define\\nour PyTorch lightning module,\\nwhich will hold our model as well as have the steps required\\nfor training and validating our model.\\nHere's a class that I've defined,\\nbinary classification module.\\nWe are going to be performing binary classification,\\npredicting whether customers churned or not.\\nWe inherit from the lightning module base class.\\nThe init method is where we set up our model.\\nThis is where we define the model architecture\\nand its layers.\\n\\nNow, init takes into input arguments,\\nthe number of features in the input data\\nand the learning rate that I've set to 0.001.\\nI'm using a smaller learning rate here.\\nMy model comprises of three linear layers.\\nAgain, layer one, layer two, and layer three.\\nAnd each of the three linear layers has a ReLu activation,\\nand these are initialized separately\\nas activation one, activation two, and activation three.\\nThe final output is also a linear layer.\\n\\nThis is the output layer on line 12.\\nThe output of this layer will be a probability score,\\nand this is the probability score that we use to determine\\nwhether a particular customer churned or not.\\nOnce again, I call self.save hyper parameters on line 16.\\nThis will cause num features and learning rate\\nboth to be saved as members of the H params object\\nthat can then be accessed by the optimizer.\\n\\nThe forward function is where we define\\nwhat a forward pass through the model looks like.\\nYou can see we pass the input through the three layers,\\neach layer has the ReLu activation.\\nAnd finally, we pass the data through the output layer.\\nThis will give us the final predictions that we return.\\nThe training step defines the operations\\nfor a forward pass through the model\\nfor a single batch of data.\\nThe batch is passed in as an input argument.\\n\\nWe access the X features and the Y variables,\\nand then we do a forward pass through the model\\nby invoking self on the input features.\\nA forward pass through the model\\nwill output logits for classification.\\nThe term logits refers to the raw, unnormalized\\nprediction scores output by a model\\nfor each class or category.\\nSince it's binary classification,\\nwe'll have just one logits scores.\\nNow, the loss function that we use\\nfor our binary classification model\\nis the BCE with logits loss.\\n\\nBCE here stands for binary cross entropy.\\nThe BCE with logits loss\\nconverts the raw logic scores into probabilities.\\nIt squashes the output logits\\nto be probability values in the range zero to one.\\nIt then computes the binary cross entropy laws,\\nwhich is essentially computing the difference\\nbetween two probability distributions,\\nthe probability distributions of the actual value\\nversus the predicted values.\\n\\nOn line 33, we compute the BCE with logits\\nloss for one batch of data,\\nand we log that out as a training loss\\nalong with a progress bar.\\nIn the validation step,\\nwe perform predictions on one batch of validation data.\\nYou can see that the code on 40 through 44\\nis the same as in the training step,\\nwe compute logits and then compute the loss.\\nHowever, here in the validation data,\\nwe want the actual predictions from the model,\\nand we compute that by converting the raw logits scores\\nto probability scores\\nusing the torch.sigmoid activation.\\n\\nThis will give us probability scores\\nin the range zero to one.\\ntorch.round of this probability score\\nwill give us the model prediction,\\nzero for not exited, one for exited.\\nOn line 48, we print out the loss on the validation data\\nduring the training process.\\nThe predict step just gives us\\nthe predictions from the model.\\nHere we just make a forward ask through the model.\\nIn configure optimizers,\\nyou can see that we've set up an Adam optimizer\\nThe Adam optimizer is an adaptive\\nlearning rate optimization algorithm.\\n\\nVery popular and widely used in the real world.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5486078\",\"duration\":241,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Training a classification model and evaluating metrics\",\"fileName\":\"2750013_en_US_03_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":241,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"Learn how to evaluate a classification model using PyTorch metric libraries.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8682209,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that we've defined our lightning module,\\nlet's instantiate our classification module\\nwe need to pass in the number of features.\\nHere is what the layers of the model look like.\\nWe know that the training process\\nof this model is a very straightforward\\nusing a PyTorch trainer.\\nOnce again, I use a CSV logger\\nto log out the training metrics in a CSV file.\\nI instantiate the data module on line three,\\nthe logger on line five and the trainer,\\nwhich will actually run the training\\nand validation process on line seven.\\n\\nWe'll run for a maximum of 20 epochs\\nof training and start the training process\\nby calling trainer.fit.\\nThe trainer rightly identifies\\nthat I do have a GPU on my machine\\nand it'll train the model using the GPU.\\nNotice, GPU available is true.\\nNow, the training process took about six\\nor seven minutes to run.\\nAt the end of one epoch of training,\\nboth training and validation losses are rather high.\\nValidation loss, 0.436, train loss, 0.48.\\nI let this run through again for all 20 epochs.\\n\\nAnd at epoch 19, let's take a look at the various scores.\\nValidation loss is 0.351, it's fallen a bit.\\nTrain loss is 0.331.\\nBut the real question is how well does the model perform?\\nMeanwhile, let's access the metrics for the training process\\nof this model in the metrics.csv file\\nunder the training logs directory.\\nLet's now go ahead\\nand plot all of these metrics in a nice visualization,\\na Seaborn line plot.\\n\\nAnd here you get a nice plot of how the training\\nand validation losses change over the course of the epochs\\nof training that we ran.\\nNext, I'll call model.predict on the validation data\\nso we get predictions from the model\\nand we get them as a stacked answer.\\nSo we have one tensor with all the predictions.\\nThe predictions here are in the form\\nof raw unnormalized logic scores.\\nIn order to get these scores in the form\\nof probability values between zero and one,\\nyou need to apply the torch. sigmoid function,\\nand then torch round will give us the actual predictions\\nin the form of zero one values.\\n\\nI'll now get the ground truth labels\\nfrom the validation data.\\nWe'll have to compare the predictions from the model\\nagainst these actual values.\\nWe need to call torch.stack on these labels as well\\nin order to get them all in a single tensor.\\nNow that we have the predictions\\nand the actual values, let's compute some evaluation metrics\\nfor the validation data.\\nWe'll compute accuracy, precision, recall, and the F1 score.\\nThe accuracy tells us\\nhow many predictions the model got, right?\\nBut because we have a skewed data set,\\nthe accuracy is not a great measure\\nof how good this model is.\\n\\nPrecision is the proportion\\nof positive identification the model got and recall measures\\nof the positive identifications in the dataset.\\nHow many was the model able to correctly identify?\\nThe F1 score here represents a trade off between precision\\nand recall metrics that are better suited\\nto evaluate models trained on skewed data.\\nLet's go ahead and hit + shift enter\\nand we'll see how the model is.\\nAccuracy is quite good at 0.857.\\n\\nRecession, which is the proportion\\nof positive identifications of exited customers\\nthat the model got right is quite high, 0.77.\\nThe recall score is on the lower side, 0.466.\\nOf the customers identified as exited\\nor churned by the model, how many were actually right?\\nThis is what recall measures.\\nAnd finally, we have the F1 score, which is the trade off\\nbetween precision and a recall, and that's at 0.58.\\nThat's great.\\nAt this point, you've successfully built\\nand trained a classification model using PyTorch Lightning.\\n\\n\"}],\"name\":\"3. Using PyTorch Lightning to Build a Classification Model\",\"size\":36878418,\"urn\":\"urn:li:learningContentChapter:5487087\"},{\"duration\":78,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5486077\",\"duration\":78,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Summary and next steps\",\"fileName\":\"2750013_en_US_04_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":78,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1866465,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] And this demo brings us\\nto the very end of this AI workshop\\non building a neural network with PyTorch Lightning.\\nNow, we started this course off\\nwith a quick overview of PyTorch and PyTorch Lightning,\\nand we discussed how PyTorch Lightning allows us\\nto write cleaner and more modular code for model training.\\nWe then got hands-on\\nand we trained a regression model using the PyTorch APIs.\\nWe identified that there was a lot of boilerplate code here\\nin the model building and training process\\nand we eliminated a lot of this boilerplate repetitive code\\nin the next demo,\\nwhere we trained the same regression model,\\nbut this time we used PyTorch Lightning.\\n\\nAnd then in order to get some more practice\\nwith PyTorch Lightning for model training,\\nwe built and trained a classification model\\nusing PyTorch Lightning.\\nWell, this brings us to the very end of this AI workshop.\\nIf you're interested in neural networks\\nand you want to study further,\\nhere are some other courses on LinkedIn Learning\\nyou might want to watch.\\nTensorFlow: Neural Networks and Working with Tables\\nand Hands-on PyTorch Machine Learning are both great courses\\nthat might be a good fit for you.\\n\\nWell, that's it from me here today.\\nI hope you had fun in this AI workshop.\\nThank you for listening.\\n\"}],\"name\":\"Conclusion\",\"size\":1866465,\"urn\":\"urn:li:learningContentChapter:5487088\"}],\"size\":185377751,\"duration\":5779,\"zeroBased\":false},{\"course_title\":\"Full-Stack Deep Learning with Python\",\"course_admin_id\":3095447,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":3095447,\"Project ID\":null,\"Course Name\":\"Full-Stack Deep Learning with Python\",\"Course Name EN\":\"Full-Stack Deep Learning with Python\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"If you seek a more in-depth understanding of deep learning and Python, this hands-on course can help you. In this course, certified Google cloud architect and data engineer Janani Ravi guides you through the intricacies of full-stack deep learning with Python. After a review of full stack deep learning, MLOps, and MLflow, dive into setting up your environment on Google Colab and running MLflow. Learn how to load and explore a dataset, as well as how to log metrics, parameters, and artifacts. Explore model training, evaluation, and hyperparameter tuning. Plus, go over model deployment and predictions.\",\"Course Short Description\":\"Increase your knowledge and get a hands-on understanding of full-stack deep learning with Python.\",\"Content Type\":\"SKILLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":20161004,\"Instructor Name\":\"Janani  Ravi\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Certified Google Cloud Architect and Data Engineer\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2024-02-06T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/full-stack-deep-learning-with-python\",\"Series\":\"Advanced\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Advanced\",\"LI Level EN\":\"Advanced\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Artificial Intelligence for Technology\",\"Primary Software\":\"Python\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":7081.0,\"Visible Video Count\":25.0,\"Contract Type\":\"INSTRUCTOR_PRODUCTION\"},\"sections\":[{\"duration\":443,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3854102\",\"duration\":405,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Full-stack deep learning, MLOps, and MLflow\",\"fileName\":\"3095447_en_US_00_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":405,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, discover the steps involved in full-stack deep learning and how MLOps and an integral part of this workflow.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9849076,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Hi, and welcome to this course  \\n on full-stack deep learning.  \\n In this movie,  \\n the role of MLOps in full-stack deep learning,  \\n and how we can implement MLOps using  \\n the MLflow framework.  \\n But first, what is full-stack deep learning?  \\n Full-stack deep learning refers  \\n to the comprehensive understanding,  \\n and expertise in all components  \\n and stages of building and deploying deep learning systems.  \\n These components include everything  \\n in the deep learning lifecycle,  \\n data collection and pre-processing, model development,  \\n training, hyperparameter tuning,  \\n deployment, and monitoring.  \\n A full-stack deep learning practitioner  \\n is proficient in both the technical,  \\n and practical aspects of each of these stages.  \\n Everything that's involved in getting deep learning systems  \\n from prototype to production.  \\n As you might imagine,  \\n full-stack deep learning is a very vast topic  \\n and really not something that you would cover  \\n in a single course.  \\n Full-stack deep learning is something  \\n that you would typically cover over the course of a semester  \\n at a university.  \\n Here are the steps involved in full-stack deep learning.  \\n The first step is planning and project setup.  \\n This is where you figure out what model you want to build.  \\n Data collection and labeling,  \\n getting the data to train your model.  \\n Then we have model training and debugging.  \\n And then finally, deploying, testing,  \\n and maintenance of models.  \\n Let's quickly discuss what's involved  \\n in each of these phases.  \\n Project planning and setup is the initial phase  \\n where we define the project's goals, allocate resources,  \\n and choose the necessary tools  \\n and frameworks that we are going to use in every step.  \\n This is where you'll establish the project environment,  \\n set up version control,  \\n and ensure that you have the infrastructure in place  \\n for all of the steps coming up next.  \\n Once you've figured out what kind of problem  \\n you're going to solve with your deep learning model,  \\n you move on to data collection and labeling.  \\n This is where you'll gather  \\n and pre-process the data required for training your model.  \\n You'll clean and format the data, annotate it if needed,  \\n label it, and get things ready for model development.  \\n Maybe you'll even split your data into training, validation,  \\n and test sets.  \\n You'll then pick the right kind of model for your data,  \\n whether it's regression, classification,  \\n or some other kind of model.  \\n You'll design the architecture of your neural network  \\n and its layers train the model on the data,  \\n monitor the performance of the model,  \\n maybe perform hyperparameter tuning  \\n to get just the right structure for your model,  \\n fine-tune the model,  \\n adjust hyper parameters  \\n and essentially get the model ready for deployment.  \\n Once your model is deployed, well,  \\n that's really just the beginning.  \\n This is where you're constantly evaluating your model  \\n in production, monitoring it and maintaining it,  \\n and possibly retraining it on new data.  \\n Machine learning models require constant updates  \\n so that they continue to perform well in the real world.  \\n Full-stack deep learning is not just about taking models  \\n from planning and project setup through to deployment.  \\n It's an iterative process.  \\n You may need to constantly go back to previous stages  \\n in this process in order to refine and improve your model.  \\n As you can see, full-stack deep learning involves  \\n the entire lifecycle of a deep learning model.  \\n It's a very vast topic,  \\n and it's hard to cover everything in a single course.  \\n A really integral part of full-stack deep learning is MLOps,  \\n and that's the part that we are  \\n going to focus on in this course.  \\n MLOps, short for machine learning operations,  \\n is a set of practices and tools that aims to streamline  \\n and automate the end-to-end machine learning life cycle.  \\n It bridges the gap between the development  \\n of machine learning models  \\n and their deployment into production environments.  \\n The term MLOps comes from the term DevOps  \\n because MLOps borrows several concepts  \\n from development operations  \\n and applies them specifically  \\n to the machine learning workflow.  \\n Here are some important components and concepts in MLOps.  \\n The first is version control.  \\n Just like in software development,  \\n MLOps emphasizes the importance of version control  \\n for machine learning code, data, and even models.  \\n MLOps incorporates continuous integration  \\n to automatically build, test,  \\n and validate machine learning models  \\n and code changes whenever new code is committed,  \\n or new data is available.  \\n MLOps also incorporates continuous delivery,  \\n or continuous deployment,  \\n automating the deployment of machine learning models  \\n into production or staging environments  \\n without manual intervention.  \\n MLOps includes techniques  \\n for packaging machine learning models  \\n into containers or other deployment-ready formats,  \\n making it easier to deploy  \\n and manage models consistently across environments.  \\n Ongoing monitoring and logging of deployed models are also  \\n very important for detecting performance issues, data drift,  \\n and model degradation.  \\n Another key concept in MLOps is model versioning.  \\n Keeping track of different versions of ML models,  \\n along with their associated data and configurations,  \\n is important for reproducibility and auditing.  \\n There are several tools that come together to enable  \\n the MLOps workflow,  \\n and an important tool amongst these is MLflow.  \\n MLflow is an open-source platform  \\n for managing the machine learning lifecycle,  \\n designed to simplify and streamline the end-to-end process  \\n of developing, training,  \\n and deploying machine learning models.  \\n MLflow was developed by Databricks  \\n and is now part of the Linux Foundation.  \\n MLflow covers two important steps  \\n in the full-stack deep learning workflow,  \\n model training and hyperparameter tuning,  \\n and model deployment and serving.  \\n MLflow helps you with model tracking.  \\n You can log and track experiments and model runs.  \\n You can record parameters, metrics,  \\n and artifacts associated  \\n with your different model executions.  \\n MLflow enables you to package your machine learning code  \\n into projects, which is just a directory containing code,  \\n data, and specification files, defining dependencies  \\n and entry points,  \\n making it easier to organize and manage your ML code.  \\n MLflow also allows you to package  \\n and share machine learning models in a standardized format.  \\n It offers a centralized model registry,  \\n making model versioning and management easy.  \\n And finally, it contains everything  \\n that you need for deploying  \\n your model locally or on the cloud.  \\n In this course,  \\n we'll focus on understanding  \\n and we'll get hands-on with MLflow for tracking experiments  \\n runs packaging models,  \\n and deploying models on our local machine.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3852088\",\"duration\":38,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Prerequisites\",\"fileName\":\"3095447_en_US_00_02_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":38,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, discover the prerequisites that you need for this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":884753,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Before we get into the course content,  \\n let's take a quick look at the pre-reqs that you need  \\n to have for the hands-on demos in this course.  \\n This course is not one for beginners.  \\n I expect that you fully understand the fundamentals  \\n of machine learning.  \\n I also expect that you're comfortable programming in Python  \\n and using Python libraries.  \\n And finally, I expect that you have some experience  \\n with building and training neural network models in PyTorch.  \\n The hands-on demos in this course will use  \\n cloud-hosted notebooks on Google Colab  \\n and will be training a deep learning model  \\n using PyTorch Lightning.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":10733829,\"urn\":\"urn:li:learningContentChapter:3859100\"},{\"duration\":1538,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3855187\",\"duration\":365,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introducing full-stack deep learning\",\"fileName\":\"3095447_en_US_01_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":365,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, gain a big-picture understanding of the full-stack deep learning workflow.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7982571,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In the earlier movie,  \\n we've already discussed a little bit about  \\n what Full Stack Deep Learning is all about.  \\n In this movie, I'll just go a little bit deeper.  \\n Full Stack Deep Learning covers the entire lifecycle  \\n of a deep learning model,  \\n right from its conceptualization,  \\n its prototyping, its development,  \\n all the way through to deployment and maintenance.  \\n The steps involved here are iterative in nature,  \\n which means you might come to a step  \\n such as model training and debugging,  \\n realize that your project planning  \\n and setup maybe wasn't correct,  \\n you need to change the technologies that you're using.  \\n You'd go back to a previous step  \\n and iterate and improve upon the model.  \\n The very first step is, of course,  \\n figuring out what you're going to build,  \\n that is in the project planning and setup phase.  \\n This is, of course, an entire complex process of its own,  \\n and I've tried to break this down into several substeps.  \\n You'll first define the goals of the project.  \\n What is it that you are trying to achieve?  \\n You'll then use metrics that you can use to evaluate  \\n whether the project was a success or failure.  \\n Let's say you're building a recommendation systems  \\n in order to upsell some of the products  \\n on your e-commerce site.  \\n Well, what is the uptick that you expect  \\n from customers because of the system?  \\n You need to choose your metrics wisely  \\n so that you know whether the model that you're building  \\n is actually worth it.  \\n You'll need to evaluate your baselines.  \\n What is the current uptake in customer sales  \\n because of the existing recommendation system,  \\n or do you not have one at all?  \\n You'll need to figure out the code base,  \\n and figure out what technologies you're going to use  \\n across the entire model development lifecycle.  \\n What deep learning framework you're going to be using,  \\n what programming language,  \\n what cloud platform or environment  \\n will you use to train your model?  \\n Where are you going to deploy and serve the model?  \\n All of that thought process, comes here.  \\n Now, a very important part of project planning and setup  \\n is to determine the impact of your project.  \\n A deep learning project may not be worth taking on  \\n unless it's impactful.  \\n Is the investment in this project going to be worth it?  \\n Is the predictions made by our model  \\n significantly going to improve business performance?  \\n Can you automate the entire complicated software pipeline  \\n in order to improve the product for users?  \\n These are some of the details  \\n you should consider for impact.  \\n The next thing you need to assess  \\n is the feasibility of your project.  \\n Deep learning projects require a lot of investment  \\n in terms of infrastructure, resources, data,  \\n and so much more.  \\n You need to ensure that all of the effort  \\n that you're putting in will result  \\n in a feasible maintainable model.  \\n Machine learning costs are often non-linear in nature,  \\n which means if you need a very high level  \\n of accuracy in your predictions,  \\n you might find that the investment is very, very high.  \\n The project costs scales non-linearly  \\n for greater accuracy requirements in predictions.  \\n These are the trade offs you need to think of  \\n in the project planning and setup phase.  \\n Also, machine learning models  \\n are probabilistic and not deterministic,  \\n and you should think to see  \\n whether probabilistic predictions are okay  \\n for whatever product it is that you're trying to build.  \\n Once you've figured out the basics  \\n of your deep learning project,  \\n that is project planning and setup is complete,  \\n you'll move on to data collection and labeling.  \\n This is where you get access to the data  \\n that you need to train your model.  \\n This is where you'll figure out  \\n the strategy for collecting the various data,  \\n where are your data located?  \\n Is the data already available in the organization,  \\n or do you need to look to external sources?  \\n You need to clearly state  \\n the data requirements of your project,  \\n identify data sources,  \\n ensure data quality in terms of accuracy and relevance.  \\n Make sure that your data is private and protected,  \\n and then set up the ingestion pipeline for your data.  \\n Depending on the kind of model that you choose to build,  \\n you may need to label your data.  \\n Data labeling is where you specify categories  \\n or classes for your prediction tasks.  \\n Now, data labeling is often an onerous and manual process.  \\n If you're doing manual labeling,  \\n you need to think about what the annotation guidelines are  \\n and you need to have processes in place  \\n for quality control of the labels,  \\n or there are several data labeling tools available  \\n that you can use to automate the labeling process.  \\n Once you have label data,  \\n you're ready to move on to model training and debugging.  \\n Model training and debugging is an iterative process,  \\n and this is where you'd follow the steps  \\n of the machine learning workflow.  \\n You'll start off with a simple model  \\n that you implement and debug.  \\n You then evaluate the model to see how it performs  \\n in a real world scenario.  \\n You may tune the models hyper parameters  \\n to improve its performance on your data.  \\n You may rearchitect the model, use different kinds of data,  \\n and you may tweak the model in multiple ways  \\n before you get a model that you're satisfied with.  \\n It's in the model training and debugging phase  \\n where you'll set up your ML Ops workflows.  \\n And finally, when you're happy with the model  \\n that you'll have, you'll deploy it to production.  \\n You'll test it and maintain it in production,  \\n and this maintenance might require constant retraining  \\n of your model on new data as it becomes available.  \\n Again, there are several substeps in this particular step  \\n of the Full Stack Deep Learning pipeline.  \\n You may first set up a small pilot in production  \\n to see how your model performs.  \\n Maybe you'll roll out your recommendations engine  \\n to only a subset of users of your e-commerce site.  \\n You'll then ask to see  \\n whether the new recommendation systems  \\n is an improvement over the old one.  \\n Are more customers buying products  \\n that they see recommended?  \\n If yes, you might go for full deployment  \\n where you'll launch the system for your entire user base,  \\n and then you'll constantly be monitoring  \\n and tracking the system to see, that it performs  \\n the way that you expect over the course of its lifetime.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3853103\",\"duration\":268,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introducing MLOps\",\"fileName\":\"3095447_en_US_01_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":268,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how MLOps is crucial to helping automate the ML workflow.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6654740,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] An important part  \\n is MLOps or machine learning operations.  \\n This is a set of practices and tools  \\n that combines machine learning systems  \\n with DevOps practices to automate, streamline,  \\n and manage the end-to-end machine learning lifecycle.  \\n You don't really use MLOps in the first two stages  \\n of full stack development, in project planning and setup  \\n and in data collection and labeling.  \\n MLOps really comes into play in the last two steps  \\n where you train and debug your model,  \\n tune your model using hyper-parameter tuning,  \\n and then deploy, test, and maintain your model.  \\n MLOps serves to automate the machine learning workflow,  \\n as expressed here in this very simple diagram.  \\n You start with a model, you implement and debug that model,  \\n evaluate that model, tune hyper-parameters,  \\n improve the model or the data that you use,  \\n and when the model meets requirements,  \\n you deploy to production.  \\n This entire workflow can be automated using MLOps tools  \\n and practices.  \\n MLOps is simply an extension of DevOps for data and models.  \\n DevOps is a set of practices to bridge the gap  \\n between development and operations.  \\n Development involves creating software  \\n and operations involved managing and deploying the software  \\n in production environments.  \\n DevOps practices have been shown  \\n to promote collaboration automation  \\n and a cultural shift to enable faster  \\n and more reliable software delivery.  \\n Now, MLOps is simply DevOps applied to building  \\n and training machine learning models.  \\n MLOps is just a set of practices and tools focused  \\n on automating and streamlining  \\n the machine learning lifecycle, extending DevOps practices  \\n to building and deploying machine learning models.  \\n Building and training machine learning models  \\n thus involve software, but there is a lot more  \\n to machine learning than just plain software development.  \\n MLOps is geared to address the unique challenge  \\n where data and models are equally important artifacts.  \\n So MLOps has to deal with a data pipeline  \\n as well as a code pipeline.  \\n The building and training machine learning models involve  \\n the use of software.  \\n Software development is fundamentally different  \\n from building and training ML models.  \\n You can think of a machine learning model  \\n as having two separate, very different inputs,  \\n data and code, and the characteristics of data  \\n and code are code very, very different.  \\n The code or the software that you use  \\n to train a machine learning model comes  \\n from a controlled environment, you have complete control  \\n over the version of the code that you use  \\n and the exact nuances of the software.  \\n In addition to code that is predictable,  \\n there is another factor involved in an ML model,  \\n and this is data, and data is often ever-changing  \\n and unpredictable.  \\n Data comes from an uncontrolled environment.  \\n When you build a machine learning pipeline, you have  \\n to handle two very disparate variables, data and code.  \\n The lifecycle and characteristics of both  \\n of these inputs are very, very different.  \\n MLOps thus seeks to productionize systems,  \\n which have inputs that have different evolutions  \\n and lifecycles.  \\n When you set up a continuous integration,  \\n continuous deployment pipeline for a machine learning model,  \\n this pipeline should include both data pipelines  \\n as well as code pipelines.  \\n The model will need to be updated  \\n if the data that is used to train it changes  \\n or if the code that is used to train it changes.  \\n Training and deploying a machine learning model  \\n is not a one-way process, machine learning  \\n is intrinsically iterative in nature.  \\n As new data comes in, the model will need to be retrained  \\n on that data and this, of course, means redeployment.  \\n If the machine learning algorithm changes,  \\n that is the code changes, the model will need  \\n to be retrained using the new code.  \\n If you find that your model isn't performing well,  \\n data relabeling may be required  \\n or you may need to choose a different kind of model.  \\n MLOps is extremely important  \\n in the machine learning workflow because deploying models  \\n to production is just the beginning.  \\n Models have to be constantly monitored  \\n and retrained on new data, and this is only scalable  \\n if the entire process is automated using MLOps.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3855188\",\"duration\":261,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introducing MLflow\",\"fileName\":\"3095447_en_US_01_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":260,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, discover how MLflow helps simplify the machine learning lifecycle.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6378816,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now if you were to set up an MLOps workflow  \\n for your deep learning system,  \\n there are many different tools  \\n and technologies that you could use.  \\n In this course, however, we'll focus on one, MLflow.  \\n MLflow is an open source platform  \\n that allows you to manage  \\n the entire machine learning lifecycle.  \\n MLflow is explicitly designed to simplify  \\n and streamline the end-to-end process of developing,  \\n training, tuning, and deploying machine learning models.  \\n MLflow is designed to be language-agnostic,  \\n meaning you can use it with different programming languages  \\n and machine learning frameworks.  \\n It supports popular languages such as Python and R,  \\n as well as machine learning frameworks,  \\n such as TensorFlow, PyTorch, and scikit-learn.  \\n Let's discuss the components that make up MLflow  \\n before we get hands on.  \\n MLflow has features that enable tracking  \\n of machine learning experiments and runs.  \\n MLflow allows you to deal  \\n with model artifacts in a standardized package and format.  \\n MLflow allows you to set up projects  \\n where you can organize your machine learning workflows,  \\n code, and structure.  \\n MLflow offers a model registry  \\n where you can register and manage the model versions.  \\n And finally, MLflow can also be used  \\n for model deployment and serving.  \\n Let's first talk about the features that MLflow offers  \\n for model tracking.  \\n MLflow uses experiments and runs to organize  \\n and track ML experiments and executions.  \\n Each time you train an MLflow model,  \\n that is a run within an experiment.  \\n The experiment is just a logical container used  \\n for organizing a set of runs.  \\n Think of the experiment as a high level grouping mechanism  \\n for model executions.  \\n Each experiment has a unique name  \\n and can contain multiple runs  \\n corresponding to multiple executions of the model.  \\n A run is a single execution of a machine learning workflow.  \\n A workflow might involve training a model  \\n with specific hyperparameters and data.  \\n Every run is associated with an experiment,  \\n which helps you categorize  \\n and organize your runs.  \\n Within a run,  \\n you can log various details,  \\n including hyperparameters, metrics, and model artifacts.  \\n All model artifacts are packaged in a standard format  \\n that can be used by deployment tools.  \\n Every model in MLflow is a directory  \\n that contains several files.  \\n At the root of the model directory  \\n is a special file called MLmodel,  \\n and this is what defines the various flavors  \\n in which the model is available.  \\n This tells you whether you can access a model  \\n as a PyTorch artifact, a Python function,  \\n or a TensorFlow artifact.  \\n In addition to the MLmodel file,  \\n a model contains several environment files  \\n that define the packages  \\n and versions the model needs to run.  \\n We'll be working with MLflow Tracking and MLflow Models.  \\n We won't be working with projects,  \\n but let me tell you what they are.  \\n MLflow Projects are a core feature  \\n of the MLflow platform.  \\n They provide a standardized  \\n and organized way to package, share,  \\n and execute machine learning code and workflows.  \\n MLflow Projects are very useful for model reproducibility  \\n and collaboration in ML projects.  \\n Once you have a model that you're satisfied with  \\n and which you may want to choose  \\n to deploy in a certain environment,  \\n you can use the MLflow Model Registry  \\n to register your model.  \\n The Model Registry is a centralized repository  \\n and management system for models,  \\n and it facilitates organization, versioning,  \\n tracking, and collaboration on ML models.  \\n This is where you'll deploy multiple versions of a model  \\n and all of the versions are accessible  \\n within the Model Registry.  \\n You can also move your model  \\n between different environments,  \\n such as staging, production, and archive.  \\n The Model Registry makes it very easy  \\n to manage models in a systematic and controlled manner.  \\n And finally, MLflow can also be used  \\n for model serving and deployment.  \\n MLflow, in fact, supports deploying to cloud platforms,  \\n such as Azure and GCP.  \\n MLflow has integrations with several cloud platforms,  \\n but you can also deploy  \\n and serve your models locally.  \\n With MLflow model serving, once you deploy your model,  \\n inferencing will be available via REST endpoints  \\n that the serve model exposes.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3859099\",\"duration\":322,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Setting up the environment on Google Colab\",\"fileName\":\"3095447_en_US_01_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":322,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to set up the Google Colab environment to run code.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12086840,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this demo, we'll see how we  \\n can use MLflow to log our model parameters  \\n and metrics while we train a dense neural network  \\n to perform image classification.  \\n Now, if you're familiar with neural networks, you know  \\n that typically you would not perform image classification  \\n using dense neural networks.  \\n You use convolutional neural networks.  \\n However, here we'll start off with a dense neural network,  \\n so you know how we can use MLflow,  \\n and then we'll move on to a convolutional neural network  \\n in the next demo.  \\n Now we'll be performing our model training  \\n and even deploying MLflow on Colab.  \\n So head over to colab.research.google.com.  \\n Colab is a Google project that gives you free access  \\n to a cloud hosted Jupyter Notebook.  \\n The Colab runtime comes pre-installed  \\n with many useful Python packages that we'll use  \\n for model building and training.  \\n And most importantly, Colab allows you  \\n to access GPUs entirely free of charge, at least  \\n for a limited duration.  \\n And that's the feature that we are going to be using.  \\n Now, sign into Colab with your Google account,  \\n and if you've never used Colab  \\n before, you need to sign up for Colab.  \\n We're going to be using Colab  \\n because we need access to GPUs  \\n to train our image classification model,  \\n whether it's using dense neural networks  \\n or convolutional neural networks.  \\n Click on new notebook here  \\n and let's start coding on a brand new notebook.  \\n In order to have the code in this notebook have access  \\n to a GPU, you'll need to head over to Runtime  \\n and go to change a runtime type.  \\n This will bring up a dialogue which will show you  \\n what options you have for your runtime.  \\n Notice there is CPU  \\n and there is a T4 GPU that is available.  \\n You also have access to TPUs  \\n or tensor flow processing units.  \\n Select the T4 GPU as an option.  \\n You'll be writing code in PyTorch to run on a GPU.  \\n Go ahead and hit Save. Our runtime now uses a GPU.  \\n Next, let's give this notebook a meaningful name.  \\n Click on the title that you see here,  \\n and let's change of the notebook  \\n to EMNISTClassification using DNNs or dense neural networks.  \\n Now this EMNIST data set that we are going to be working  \\n with is a short form for extended MNIST, an extension  \\n of the well-known MNIST dataset.  \\n The MNIST dataset, you're likely aware, is a dataset  \\n of hundred digits 0 through 9 comprising of 28  \\n by 28 gray scale images.  \\n The extended MNIST dataset  \\n includes handwritten characters from the English alphabet  \\n and is designed to be a more comprehensive dataset  \\n for character recognition tasks.  \\n Here in the Google Drive associated  \\n with my same Google account, I have a folder  \\n called EMNIST_data under My Drive.  \\n And here I have two CSV files, emnist-letters-test  \\n and emnist-letters-train containing the test  \\n and training data for my model.  \\n Back to our notebook, let's install the libraries  \\n that we need to write our code for our model.  \\n We'll install torch matplotlib numpy and pandas.  \\n Torch because we're going to be building a neural network  \\n using PyTorch Lightning.  \\n Go ahead and run install to install these libraries.  \\n Many of these will already be available  \\n within your Colab environment,  \\n so there's nothing to update here.  \\n Now, MLflow tracking  \\n and logging does not work with plain Vanilla PyTorch.  \\n You need to use PyTorch Lightning,  \\n which is an open source lightweight PyTorch wrapper  \\n that simplifies the training  \\n and research process for deep learning models.  \\n So pip install pytorch_lightning as well.  \\n This is not available in the Colab environment, so you need  \\n to explicitly install this.  \\n I'm going to install the latest version  \\n of MLflow available at the time of this recording,  \\n which is MLflow 2.91.  \\n If there is a newer version of MLflow available,  \\n you might want to explicitly install MLflow 2.91 to ensure  \\n that all of your demos work.  \\n And this is simple to do with pip,  \\n simply run pip install mlflow==2.9.1.  \\n We're going to be running MLflow in the Colab environment,  \\n which means that the MLflow UI  \\n and server will be hosted on the runtime of Colab.  \\n Now we do not have direct access to Colab's runtime.  \\n Our Colab notebook is hosted on some virtual machine  \\n in Google's Colab environment.  \\n Now, whatever virtual machine hosts our Colab notebook  \\n will be running our MLflow development server  \\n on that virtual machine,  \\n but we do not have direct access to the ports  \\n and other details of that pm.  \\n So if you run MLflow within Colab,  \\n how do we access MLflow?  \\n Well, we're going to be using Ngrok.  \\n Ngrok is a cross-platform application  \\n that allows developers to expose a local development server  \\n to the internet in practically no time at all.  \\n You have to do very little work to actually expose a server  \\n that's running locally to the internet complete with a URL.  \\n So whatever MLflow development server will be running  \\n in the Colab runtime, we exposed to the internet  \\n using Ngrok.  \\n And we are installing PyNgrok, the package that allows us  \\n to interface with Ngrok using Python code.  \\n We talk about Ngrok in more detail in the next movie.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3852089\",\"duration\":322,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Running MLflow and using ngrok to access the MLflow UI\",\"fileName\":\"3095447_en_US_01_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":322,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, discover how to use ngrok to expose local development servers to the internet.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10126449,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Now, in the earlier movie I'd mentioned  \\n that we are going to be using ngrok  \\n to expose our locally running MLflow to the internet  \\n so that we can view the MLflow UI  \\n and view the tracking parameters and metrics  \\n for our model training process.  \\n Now, we wouldn't actually need ngrok,  \\n had we been running this  \\n DNN training for image classification on our local machine.  \\n We'd simply install and run MLflow locally.  \\n However, because we're using colab for training  \\n and we are going to be integrating  \\n with MLflow running on colab,  \\n we need to expose MLflow running on the colab  \\n runtime to the internet.  \\n And that's why we are using ngrok.  \\n Now in order to use ngrok,  \\n we need an authentication key from ngrok.com  \\n and that's where I'm headed.  \\n You can see here that  \\n is a lot more than exposing your locally running server  \\n to the internet.  \\n But that's the use case  \\n that we are going to be using it for.  \\n Let's sign up for an ngrok account here.  \\n I'll use the same email ID I did for colab.  \\n I'll specify a new password.  \\n I'll indicate that I'm not a robot  \\n and I'll accept the terms of service of ngrok.  \\n At this point, if you want to verify your account,  \\n you can head over to Gmail.  \\n You should have received an email  \\n that'll allow you to verify your account.  \\n But in a few moments,  \\n you'll be taken to the main page for ngrok.  \\n Now here you'll find instructions  \\n for downloading and installing the ngrok stk  \\n on your local machine,  \\n whether it's macOS, windows, or a Linux machine.  \\n Since I'm running on a macOS machine,  \\n I get the instructions here for macOS.  \\n But we don't really need all this.  \\n All we need is an Authtoken.  \\n Click on Your Authtoken  \\n off to the left hand side of your screen  \\n and this will display to you the Authtoken  \\n for your ngrok account.  \\n And this is the Authtoken  \\n that we'll use to authenticate our ngrok agent  \\n that we used to run ngrok.  \\n And the ngrok agent that we'll be using is by ngrok.  \\n That is the Python agent.  \\n Copy the authentication token over  \\n and we'll use this in our code right now.  \\n Here is the Python code  \\n that I execute here in my colab notebook  \\n to run an MLflow server  \\n on the local host of the colab runtime.  \\n And then expose that endpoint to the internet using ngrok.  \\n The code that you see online  \\n to runs the MLflow tracking user interface  \\n in the background on the colab runtime.  \\n So notice I run the command \\\"mlflow ui --port 5000 &\\\"  \\n to run the tracking UI in the background.  \\n This will get MLflow running  \\n on the colab runtime for this notebook.  \\n Next, we need to create a remote tunnel  \\n to the current environment that we are on  \\n to allow ngrok local port access on our colab runtime.  \\n Ngrok.kill will terminate any open tunnels if they exist.  \\n We then specify our NGROK_AUTH_TOKEN.  \\n This is the Auth token that I've copied over  \\n from my ngrok account that we just created.  \\n I call ngrok.set_auth_token  \\n to set the auth token to my Python client  \\n to authenticate the Python client.  \\n On line 17, I call ngrok.connect  \\n to set up the remote tunnel to port \\\"5000\\\"  \\n on the local environment.  \\n This port on our local machine  \\n will be exposed to the internet via an HTTPS tunnel.  \\n And then we'll print out the public_url  \\n where we'll be able to access the MLflow UI.  \\n Now, if I were to click on this public_url,  \\n you'll see that this brings up  \\n whatever is running on local host 5000 in my colab runtime,  \\n and that is the MLflow UI.  \\n You should see a warning here about accessing this URL.  \\n We trust this URL.  \\n We know that we've exposed this  \\n particular URL to the internet.  \\n Let's visit the site  \\n and here is what the MLflow UI looks like.  \\n The colab runtime is an ephemeral runtime.  \\n So this UI and the MLflow that's running on that UI  \\n will only be accessible  \\n so long as my colab runtime environment is available.  \\n So if you kind of close the notebook  \\n or terminate your runtime,  \\n well, you'll find that MLflow will be killed as well.  \\n So it's ephemeral in nature,  \\n but it works perfectly well for the purposes of our demo.  \\n Of course, in a production environment,  \\n you'll have a permanent setup with your MLflow UI  \\n and your training runs for your ML models.  \\n Now that you've understood how we've used ngrok  \\n to access MLflow running on colab,  \\n let's explore the MLflow UI.  \\n Notice on the left, we have a default experiment  \\n that has already been created.  \\n An experiment is just a container  \\n for all of your different model executions or runs.  \\n We have no runs at this point in time  \\n and that's why the middle pane is empty.  \\n Once you have runs logged,  \\n you can use these configuration options  \\n to sort based on active runs,  \\n to sort based on different columns  \\n that you have displayed in the runs  \\n and essentially configure your view.  \\n We'll primarily work with  \\n experiments and runs in this course.  \\n However, you can also register your models  \\n using the model registry.  \\n And once you do that,  \\n the models will show up here in the models tab.  \\n This is where you'll track  \\n the different versions of your model,  \\n tag them as being in staging, archive  \\n or production environments and so on.  \\n \\n\\n\"}],\"name\":\"1. An Overview of Full-Stack Deep Learning\",\"size\":43229416,\"urn\":\"urn:li:learningContentChapter:3852093\"},{\"duration\":2008,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3852090\",\"duration\":293,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Loading and exploring the EMNIST dataset\",\"fileName\":\"3095447_en_US_02_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"rawDurationSeconds\":293,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to access and load the EMNIST dataset for model training.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9786139,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now that we have ML flow running on Colab  \\n and we have access to our ML flow environment,  \\n using Ngrok, let's get started coding.  \\n First, I set up the import statements  \\n for all of the Python libraries that we need.  \\n We have the ML flow library,  \\n torch, matplotlib, numpy, pandas to work with data  \\n and to set up our deep learning model.  \\n The data set that we'll be using for classification  \\n is on Google Drive,  \\n so let's mount Google Drive onto our Colab notebook  \\n using drive.mount.  \\n This will bring up a dialogue asking you  \\n to authenticate the user drive  \\n from within your Colab notebook.  \\n Continue to Google Drive and make sure you log in  \\n with the account that you're using for Google Drive,  \\n which should be the same as the account  \\n that you're using for Colab  \\n and give your Colab notebook  \\n access to your Google Drive folder.  \\n Once you've successfully given access,  \\n you'll see that the contents of Drive  \\n will be available at /content/drive.  \\n And if you open up this folder on the left side bar,  \\n you'll see a drive folder here in your local runtime.  \\n Under MyDrive, you'll find that all of your drive folders  \\n will be available to your Colab notebook,  \\n and this will allow us  \\n to access our training data under MyDrive.  \\n Observe there is also an ML runs folder  \\n that has been created here.  \\n This folder did not exist before,  \\n this folder was created when we launched ML Flow  \\n in our Colab runtime.  \\n ML Flow uses this ML Runs folder  \\n to track all of the experiments that we create,  \\n all of the runs or executions of our model,  \\n the model parameters, artifacts,  \\n everything that ML flow tracks  \\n will be locked to this folder by default.  \\n Let's get back to the code and let's access and load in  \\n our training and test data.  \\n I use Pandas for this pd.read_csv  \\n and I point to the folder on the mounted Google Drive  \\n where my training and test data lives.  \\n Both of these files have no headers  \\n and that's why I've set header equal to none.  \\n Read in this data and let's print out  \\n a sample of the training data so you can see  \\n what the data looks like.  \\n Emnist_images are gray scale images,  \\n 28 pixels by 28 pixels,  \\n so total of 784 pixels.  \\n You can see the column header start with zero,  \\n end at 784.  \\n The zero column is the label associated with each image.  \\n Notice that the labels range from one through 26.  \\n Each image represents a single character in the alphabet.  \\n You'll see that the test data  \\n is set up in a similar structure as well.  \\n We have the zeroth column  \\n containing the labels for the test images  \\n and columns one through 784,  \\n those are the actual pixel values of the images.  \\n In order to quickly understand the numbers in the CSV file  \\n representing the images and the labels,  \\n let's run describe on this data frame.  \\n Observe that we have a total of 88,800 records  \\n in the training data.  \\n For column zero, the min value is one,  \\n and the max value is 26.  \\n So you can see that these numbers represent letters  \\n from A through Z.  \\n You can see that the pixel values are just intensities  \\n ranging from zero through 255.  \\n The training data for the Emnist dataset  \\n is randomly shuffled,  \\n but the test dataset is not.  \\n So I call emnist_test_data.sample(frac = 1)  \\n in order to shuffle the test data as well.  \\n Next, I'll set up a Python list  \\n representing the classes or categories in this data  \\n from A through Z.  \\n Now, one thing to note here,  \\n that the labels in this dataset go from one through 26,  \\n but the indexes of the elements in a Python list  \\n go from zero through 25.  \\n Just something to keep in mind  \\n because we need to make an adjustment for this  \\n when we view and work with this data.  \\n Now let's take a look at the data that we are working with  \\n before we start setting up our experiment in ML flow.  \\n On lines two and three,  \\n I access the labels and the training data  \\n and the actual image values.  \\n Remember, the first column in the data frame  \\n comprises of the labels.  \\n The training images are in the flattened form,  \\n and in order to visualize these images,  \\n we need to reshape them  \\n to the original dimensions, 28 by 28,  \\n this is what I do on line six.  \\n On lines nine through 13,  \\n I sample 18 different images at random  \\n from the training set.  \\n And on lines 15 through 24,  \\n I display these images using matplotlib  \\n along with the corresponding labels.  \\n Observe on line 20 that I have a  \\n train_labels[idx] - 1.  \\n This is an adjustment I have to make  \\n before we can access the right label in the classes list.  \\n Let's run this code  \\n and take a look at the data that we are working with.  \\n You can see that all of these images  \\n are of letters in the English alphabet,  \\n both lowercase and uppercase.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3855189\",\"duration\":366,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Logging metrics, parameters, and artifacts in MLflow\",\"fileName\":\"3095447_en_US_02_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":366,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, discover how you can create MLflow runs for tracking.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10997653,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now that we have our data set up  \\n for training our model,  \\n let's play around with MLflow and understand how it works.  \\n The first thing we need to do is  \\n create an MLflow experiment,  \\n and you can do this programmatically  \\n by calling mlflow.create_experiment,  \\n and specifying a name for that experiment.  \\n You can do this using the UI as well.  \\n An experiment is made up of runs.  \\n A run is an execution of a model  \\n and a run has all of the metrics  \\n and parameters that are logged when the model is run.  \\n Once you've created an experiment, you can head over  \\n to the MLflow UI  \\n and under experiments you should find our newly  \\n created experiment.  \\n You'll find the test experiment over on the left  \\n and if you were to select it, you'll find  \\n that it's completely empty.  \\n That's because we haven't created any runs  \\n yet within this experiment.  \\n Back in our notebook,  \\n let me open up the left sidebar  \\n and show you the contents of the mlruns folder.  \\n If you expand mlruns, you'll find  \\n that there is a sub folder for each experiment  \\n that we've created in MLflow.  \\n We had the default experiment with ID 0  \\n and our newly created experiment  \\n with an ID starting with 61.  \\n There's also models folder, which contains the models  \\n registered with the model registry.  \\n This is not something we'll be working  \\n with much in this course.  \\n Now that we've created an experiment,  \\n let's access this experiment  \\n and look at the details of this experiment,  \\n mlflow.get_experiments accepts an experiment ID  \\n and returns an experiment object, which you can use  \\n to access the name, the ID, the artifact location, tags,  \\n lifecycle stage, and the creation time  \\n for this experiment.  \\n Notice the artifact location is in the mlruns folder.  \\n This is where the artifacts of our model training,  \\n that is the serialized model and other images  \\n and any other artifacts that we store will be located.  \\n The next thing I'm going to do is create runs  \\n within this experiment.  \\n For that, you need to set the current experiment  \\n within this notebook, mlflow.set_experiment.  \\n That's why the name of the experiment will set that up  \\n as the experiment to use for all of the code that follows.  \\n Now you can create a run within this experiment  \\n by calling mlflow.start_run.  \\n This will be created within the currently active experiment.  \\n Mlflow.end_run will end the run.  \\n Every start run has to be accompanied  \\n by a corresponding end run.  \\n Then all of your model executions  \\n and logging of your model parameters  \\n and metrics should be done within this run.  \\n Execute this and now head back to the MLflow UI  \\n and refresh the page for the test experiment.  \\n You should see a newly created run there.  \\n Notice that MLflow has picked the name  \\n for the run at random.  \\n Let's look at this awesome-slug run.  \\n If you click through, there are no details  \\n because this is just an empty run.  \\n We just started and ended the run right away.  \\n We did not actually log any metrics, artifacts,  \\n or model parameters within the run.  \\n When we actually create a run, let me show you  \\n how it's tracked on a locally running MLflow server.  \\n Open up this folder icon off to the left  \\n and notice under our experiment folder,  \\n there is another sub folder for our run.  \\n The folder starting with 61 is the folder  \\n for our text experiment  \\n and the folder starting with c5 is the folder  \\n for our current run.  \\n And under a run you can see the different folders  \\n tracking the artifacts, metrics, params and tags.  \\n These will be populated when we actually train our model  \\n using an MLflow run.  \\n Let me show you how you can log metrics, parameters,  \\n artifacts and figures within a run.  \\n The first thing I'm going to do is set up a simple text file  \\n that's going to be an artifact that I want logged.  \\n Data info is a string which contains information  \\n about the MNIST dataset,  \\n and I write that out to a data_info.txt file.  \\n This is the code on lines 9 and 10.  \\n Now I have an artifact that can be logged.  \\n What I'm going to do now is start an MLflow run  \\n using the width keyword.  \\n When you're using MLflow  \\n using the width keyword to start a run is a best practice  \\n because this will automatically close  \\n or end the run for you.  \\n So with MLflows, start run, the name  \\n of the run is test_experiment_run,  \\n and this is stored in the current run variable.  \\n And then I explicitly log some metrics,  \\n params, artifacts and tags.  \\n Mlflow.log_metric. I set the accuracy to 0.67.  \\n Even though we are not actually training a model.  \\n I set up some fake params for the model on lines 5  \\n through 7 and log those parmas on line 8.  \\n Notice how I call log_metric to log metrics,  \\n log_params to log parameters.  \\n And mlflow.log_figure can be used to log images.  \\n Figure refers to the mapplotlib figure that we use  \\n to display the images earlier on in this demo.  \\n Log_artifact is used to log artifacts.  \\n That is a data info text file that I just saved now.  \\n And I've also specified a tag for this run in the form  \\n of a key value pair EMNIST classification using DNNs.  \\n Execute this code.  \\n And now that we have a run  \\n where we logged different parameters, metrics, et cetera,  \\n let's head over to our experiments  \\n and take a look at how this is displayed.  \\n Observe under test_experiment, we have two runs.  \\n The first one, awesome-slug is the one  \\n that we created earlier.  \\n The second one is test_experiment_run  \\n where we specified our own run name.  \\n Let's click through to the test_experiment_run  \\n and see how the metrics and params have been logged.  \\n Notice there are sections for different parts.  \\n If you expand parameters, you can see num_nn_layers  \\n are parameters has been logged there.  \\n Under metrics you can see accuracy.  \\n And under tags you can see our tag, EMNIST.  \\n At the bottom here we have artifacts  \\n where we have our test artifact that we logged out.  \\n Also, the image artifact, sample_images.png .  \\n Artifacts can be files, images,  \\n and of course when we train a model,  \\n we'll have our model artifacts here.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3853104\",\"duration\":227,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Set up the dataset and data loader\",\"fileName\":\"3095447_en_US_02_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":227,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to create a PyTorch dataset and dataloader to work with training, validation, and test data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6936420,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now that we know how ML flow tracks  \\n runs within experiments, we are ready to train  \\n our dense neural network  \\n for image classification.  \\n Once again, A DNN is not the right model for this,  \\n but it'll give us a handle on how ML flow works  \\n and will work well as a preparatory step  \\n before convolutional neural network training.  \\n The first thing I'm going to to do is set up a dataset  \\n to access our emnistS data.  \\n A dataset is just an abstraction that represents your data  \\n and it typically provides access  \\n to individual data samples one at a time.  \\n This emnist dataset class inherits from the dataset base  \\n class in PyTorch.  \\n Within the init method, we initialize the images  \\n and labels as torch tensors.  \\n Observe that the init method takes in the data frame  \\n for our training or test data as an input argument.  \\n The Len Method returns the length of the dataset.  \\n That's just the length of the images get item  \\n allows us to access the data samples one at a time.  \\n It takes in an index as an input argument  \\n and returns the images and labels at that index.  \\n Notice on line 14, we divide all of the pixel values  \\n of the images by 255 so that every pixel value  \\n is represented in the range zero to one.  \\n This will make training of our neural network more robust  \\n and allow our neural network to converge.  \\n Having set up this dataset class,  \\n let's instantiate our training and test data.  \\n We have the trained dataset  \\n and the test dataset, both of the kind emnist dataset.  \\n Make sure you pass in the right data frames in.  \\n We have 88,800 records for training  \\n and 14,800 records to test and evaluate our model.  \\n I'm going to carve out some samples from the training dataset  \\n to use for validating our model as training progresses.  \\n So I import the random split method from torch utils data.  \\n We use 10% of the training records.  \\n For validations I've set  \\n val percentage should be well proportion 2.1.  \\n I compute the length of the validation data  \\n on line four, that is val length.  \\n And then I use the random split to get the training data set  \\n and the validation data set.  \\n And if you look at the length of both of these data sets,  \\n 79,920 records for training.  \\n 8,880 records for validation.  \\n Before we use this data to train our model,  \\n let's instantiate data loaders for the training, test,  \\n and validation data.  \\n A data loader is a utility that wraps around a data set  \\n and provides batching, shuffling,  \\n and parallel loading of data.  \\n I've gone with a batch size of 64.  \\n This is something of course that you can tweak  \\n and I've instantiated three different data loaders.  \\n And then at the bottom I print out the length  \\n of the data loader, which will tell us the number  \\n of batches in each of our data sets.  \\n Notice for the train dataloader I've set shuffle  \\n to true because I want the training data to be shuffled.  \\n Drop last is true, so that last batch, which may not  \\n be the same size  \\n as the other batches will be dropped.  \\n And for all data loaders, I've set number workers equal  \\n to one because we have just one GPU  \\n available in our runtime.  \\n PyTorch data loaders are used  \\n with iterators giving you access to one batch  \\n of data at a time.  \\n Here is one batch of images from the training data,  \\n and here you can see corresponding batch of labels  \\n for the first batch of images.  \\n Let's take a look at the shape of one batch of data  \\n that will be feeding into our dense neural network.  \\n The batch size that we are chosen was 64,  \\n so you can see one batch of images is 64 by 784,  \\n where 784 is the flattened representation of each emnist  \\n image, and of course the length correspond to.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3852091\",\"duration\":296,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Configuring the image classification DNN model\",\"fileName\":\"3095447_en_US_02_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":296,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to configure a simple DNN for image classification.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10192550,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's set up the MLflow experiment  \\n to track our dense neural network training.  \\n I'm going to call mlflow.create_experiment.  \\n The name of the experiment  \\n is emnist_letters_prediction_using_dnn.  \\n And I'm going to set that experiment  \\n as the active experiment for this notebook.  \\n So any runs we create will be created  \\n within this emnist_letters_prediction_using_dnn experiment.  \\n Next, I'm going to import some Pytorch-related libraries  \\n and classes that we'll need to set up our neural network.  \\n We're going to be using a dense neural network  \\n for image classification.  \\n The size of the input fed into this dense network  \\n will be 28 by 28, that is 784,  \\n and the size of the output will be 26  \\n since each image can be classified  \\n into one of 26 categories.  \\n The output will be a probability score  \\n for each class or category,  \\n and the category with the highest probability  \\n will be the predicted label of the model.  \\n We are now ready to set up our dense neural network  \\n for image classification.  \\n Now, I'm defining this as a class  \\n that inherits from pytorch_lightning.LightningModule.  \\n A LightningModule is designed to encapsulate  \\n the entire machine learning workflow,  \\n including model architecture, the training loop,  \\n validation loop, and any test functionality.  \\n You should have your class representing your model  \\n inherit from LightningModule  \\n and implement several hooks or placeholders  \\n or required methods in your derived class.  \\n The Pytorch Lightning trainer  \\n can work with the Pytorch LightningModule  \\n to train, validate, and test your model.  \\n Now, within the init method here,  \\n I set up the structure of my dense neural network.  \\n Because this is a classification model,  \\n we'll use the cross-entropy loss that measures the distance  \\n between two probability distributions.  \\n We have the initial linear layer  \\n which takes in our input and has 512 neurons,  \\n and we have linear layers two, three, and four.  \\n The output of each layer  \\n feeds as an input into the next layer.  \\n And notice our last linear layer  \\n has the output size equal to the number of classes  \\n or categories that the data can be classified into.  \\n The forward function in this LightningModule  \\n defines a forward pass through the neural network.  \\n Xb is one batch of data fed into the model,  \\n which is then passed through the first three linear layers.  \\n Each of these linear layers have ReLU activation.  \\n The data is then passed through the last linear layer,  \\n linear4, which has no activation.  \\n In order to specify the optimizer to use  \\n for your neural network training,  \\n you override the configure_optimizers function.  \\n And here, you've seen that I've initialized  \\n an Adam optimizer with a learning rate of .0001.  \\n The training_step will be invoked  \\n during the training of your neural network.  \\n Observe that the training_step takes in one batch of data  \\n along with the indexes of the batch.  \\n We access the x, features, and y, labels, of the batch  \\n on line 28.  \\n On line 29, I subtract the y, labels, by one  \\n so that they're indexed starting at zero.  \\n So zero will represent the letter A,  \\n one will represent the letter B, and so on.  \\n The original dataset uses one to represent the label A,  \\n two to represent B.  \\n And we want the categories to start at zero,  \\n that's why I have the y -= 1.  \\n On line 31, we make a forward pass through the model  \\n to get the predictions  \\n with the current model parameters, y_hat.  \\n Invoking self on the input data  \\n will automatically invoke the forward method  \\n in this LightningModule.  \\n On line 32, we compute the current loss of the model.  \\n And on line 33, we get the actual predicted labels  \\n by computing the category or class  \\n with the highest probability score using argmax.  \\n On line 35, we compute the current accuracy of the model.  \\n And on lines 37 and 38, we log the current training loss  \\n and the current training accuracy.  \\n Observe on line 40, we return the loss  \\n from the training_step.  \\n This loss will be used by the PyTorch trainer that we'll use  \\n with this LightningModule  \\n to make a backward pass through the model.  \\n The validation_step is invoked on validation data.  \\n Once again, it takes in a batch of data  \\n and the batch indexes as input,  \\n and the code within the validation  \\n is exactly the same as a code within the training_step.  \\n At the end of the validation_step,  \\n on line 55 we return the current accuracy  \\n on the validation data.  \\n Next, we have the test_step,  \\n which is invoked on the test dataset.  \\n The code on lines 58 through 70 for the test_step  \\n is identical to the code in the validation_step,  \\n including the fact that we return the accuracy.  \\n And then we have the final predict_step,  \\n which is invoked when we use the trained model  \\n for predictions.  \\n That just makes a forward pass on the data  \\n by invoking self on the x input.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3858093\",\"duration\":246,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Training a model within an MLflow run\",\"fileName\":\"3095447_en_US_02_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":246,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to track model training parameters automatically with MLflow.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10850401,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] With our model configured,  \\n now it's time for us to train the model  \\n and track the metrics, parameters,  \\n and artifacts of this model using ML Flow.  \\n Notice the imports on lines one, two, and three.  \\n These are imports that have to do with logging  \\n all of the details of the model,  \\n including the model signature, the schema,  \\n and tensor specifications for the tensors  \\n that we feed into the model.  \\n I instantiate the model on line five.  \\n That is our emnist_model.  \\n And on line seven, I instantiate the CSVLogger.  \\n The CSVLogger is what we use to log our details  \\n of the training process to our local machine.  \\n The actual training process will be run  \\n using the pytorch lightning trainer,  \\n that I instantiate on line nine.  \\n We'll train for a maximum of 10 epochs.  \\n And notice, I pass in the CSV logger  \\n as an input argument, so that the trainer  \\n can use that logger to log out details during training.  \\n Now, ML Flow can track parameters and metrics  \\n in a very granular and manual fashion,  \\n by calling log metrics log params,  \\n as you saw earlier in our test example.  \\n However, an easier thing to do  \\n when you're working with PyTorch, TensorFlow,  \\n or any of the standard frameworks  \\n is to use ML Flow Auto Log.  \\n On line 11, I enable auto log for pytorch,  \\n mlflow.pytorch.autolog.  \\n I'm going to ask auto log to log  \\n all of the details it usually does,  \\n parameters, metrics, tags, and so on,  \\n except for the actual model artifacts.  \\n I've set log models to false, indicating that I  \\n do not want model artifacts to be logged.  \\n I'm going to be logging them myself,  \\n manually in an explicit fashion.  \\n This is so I can explicitly log the model's signature.  \\n On line 13, I kickstart an ML Flow run,  \\n using mlflow.start_run.  \\n Now all of the training that you'd run  \\n using PyTorch Lightning, within this start run with block,  \\n will be instrumented and tracked automatically by ML Flow.  \\n ML Flow will plug into the training process  \\n of your PyTorch Lightning module  \\n and it will track metrics and parameters,  \\n as model training is going on.  \\n On line 15, we start the model training process  \\n by calling trainer.fit.  \\n The model to be trained is the emnist model.  \\n The training data is in the train_dataloader.  \\n And the validation data is in the val_dataloader.  \\n Once training is complete, we'll evaluate the model  \\n on the test data, I call trainer.test  \\n and pass in the test data loader for evaluation.  \\n Now I want to explicitly specify the schema  \\n of the input data fed into the model  \\n and the output predictions received from the model.  \\n I instantiate those schemas on lines 18 and 19.  \\n The shape of the input data is a tensor.  \\n The mat size can be anything, that's the minus one.  \\n But each image is 784 pixels.  \\n The shape of the output schema is 26 different outputs,  \\n with probability scores for each of the 26 categories.  \\n On line 21, I instantiate a model signature  \\n with the input and output schema.  \\n And I explicitly call log_model on line 23,  \\n to log out the model artifacts,  \\n along with the model signature.  \\n The name of the model is  \\n emnist-letters-classifier-dnn-model.  \\n I've noticed that for PyTorch, auto log  \\n logs out the model artifacts fine,  \\n but it doesn't explicitly specify  \\n the signature of the model,  \\n that is the input schema and the output schema.  \\n Because I want that,  \\n I explicitly logged out the model myself.  \\n Let's kickstart the training process.  \\n Now the training process took about 15  \\n to 20 minutes, to run on the GPU.  \\n There is a lot of data that we have to train with.  \\n Observe the first line, GPU available, true  \\n and used is true, so training ran on the GPU.  \\n And if you scroll down, you'll see  \\n the final results of training.  \\n The test accuracy is almost 85%, which is quite good  \\n considering we have 26 classes or categories  \\n into which the input images can be classified.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3854103\",\"duration\":259,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Exploring parameters and metrics in MLflow\",\"fileName\":\"3095447_en_US_02_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":259,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to understand and interpret results displayed in the MLflow UI.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8821854,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] With model training complete,  \\n let's head over to the MLflow UI,  \\n and if you hit Refresher,  \\n you'll find that our new experiment  \\n containing our PyTorch model run, is available.  \\n Here we have the  \\n emnist_letters_prediction_using_DNN experiment.  \\n And within this experiment, we have a single run,  \\n that is the training of our PyTorch model  \\n that we just completed.  \\n Now let's take a look at the charts first,  \\n because they're very interesting.  \\n You'll see that MLflow automatically logs the charts,  \\n for all of the metrics that were tracked during training.  \\n Here you can see first a bar chart,  \\n tracking the accuracy on the test data.  \\n That's 0.85.  \\n If you scroll down, you'll find other useful charts.  \\n Let me show you a few.  \\n Here is a chart of the training loss  \\n over the epoch of training.  \\n You can see how the loss falls as we train for longer.  \\n Here is a chart for the validation accuracy,  \\n which grows as we train for longer,  \\n and here is another chart for the validation loss.  \\n All of these charts were automatically logged  \\n when we used MLflow's auto logging feature.  \\n Now let's click through, and take a look at the run.  \\n This is where we'll be able to see all of the metrics  \\n and parameters that were logged.  \\n Let's expand the parameters section,  \\n so you can look at that first.  \\n Observe that all of the parameters of the model,  \\n including neural network layers, number of neurons,  \\n number of model parameters, learning rate of the optimizer,  \\n the optimizer used, everything is tracked here.  \\n Let's take a look at the metrics next.  \\n You can see that we had three data sets:  \\n test, validation, and training.  \\n And we get the loss and accuracy  \\n for each of these datasets.  \\n You can see here that the accuracy on the training data  \\n was 0.8797, that's the train accuracy.  \\n and on the test data, it was 0.853.  \\n That is the best accuracy up top.  \\n Mlflow has also auto logged some tags for our training run.  \\n Notice the tag here says \\\"Mode is testing,\\\"  \\n because we had some test data,  \\n and here at the bottom we have the model artifacts  \\n that we logged explicitly.  \\n There's a lot going on here,  \\n and I'll break this down for you.  \\n The model artifacts are in the form of a folder,  \\n which contains the actual model that has been serialized  \\n and saved out, in the data sub folder,  \\n and it also defines the model's environment,  \\n those are the remaining files,  \\n The MLmodel file, the conda.yaml, the Python_env.yaml,  \\n and the requirements.txt.  \\n Here on the right in the center pane,  \\n you can see the model schema  \\n that we explicitly logged the input schema,  \\n so you know what kind of data to feed in,  \\n and the output schema,  \\n so you know what kind of output to expect.  \\n And over to the right,  \\n how you can load in a trained model, and make predictions.  \\n We'll get to that in a bit.  \\n We'll do that in the next movie.  \\n In the meanwhile,  \\n let's quickly understand this model artifact structure.  \\n In the data sub folder, under the main model folder,  \\n notice we have the model path file,  \\n which contains the actual serialized model.  \\n The model has been serialized using the Pickle format,  \\n that's what the second file indicates.  \\n The MLmodel file is a special file that MLflow uses  \\n to package and serialize machine learning models,  \\n along with their associated metadata.  \\n This MLmodel file is primarily used  \\n when we deploy and serve machine learning models,  \\n using Mlflow.  \\n And we'll do that in the very last demo of this course.  \\n You can see the Python version that was used  \\n to train the model 3.10.12.  \\n You can see the PyTorch version 2.1.0.  \\n You can see the MLflow version 2.9.1.  \\n because we explicitly logged out the model signature,  \\n the MLmodel file also contains the signature  \\n of the input data to be fed into the model,  \\n and the outputs to be received from the model.  \\n The other files here, such as conda.yaml,  \\n is used to define the conda environment  \\n that can be used to train this model.  \\n Similarly, python_env.yaml defines the Python environment  \\n used to train the model:  \\n the build dependencies, and the requirements.txt file,  \\n contains the versions of all of the libraries needed  \\n for this model training.  \\n And model_summary.txt gives you an overview of the model  \\n that was trained, the layers in the model,  \\n and the number of trainable parameters in the model.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3857096\",\"duration\":321,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Making predictions using MLflow artifacts\",\"fileName\":\"3095447_en_US_02_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":321,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to load model artifacts and make predictions using Python code.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11260287,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's visualize some  \\n there is a metrics.csv file under the log directory  \\n of our PyTorch Lightning model trainer.  \\n I'm going to read the contents of this metrics.csv file,  \\n and this is a data frame that gives you all  \\n of the training accuracy, the various steps,  \\n and other details of training.  \\n I'm going to use Seaborn to visualize some of these training  \\n and validation metrics as a line plot.  \\n If you take a look at this line plot,  \\n you see the validation accuracy, validation loss,  \\n and training accuracy and loss per epoch.  \\n Observe the two lines for accuracy,  \\n the straight blue and the dotted green line.  \\n The accuracy improves during training on the training data  \\n and the validation data while the loss falls.  \\n Every MLflow run has a unique run ID,  \\n and you can access this programmatically  \\n by calling mlflow.last_active_run().info.run_id,  \\n this is the run ID of the run that we just executed.  \\n Using this run ID, we should be able  \\n to access the model artifacts associated with this run  \\n and then load in those artifacts  \\n to use the train model for prediction.  \\n First, let me set up a batch of the test data  \\n that will pass through the model for predictions.  \\n Test images and test labels contain the data and labels  \\n of one batch in the test data, remember,  \\n this is data the model has not seen during training.  \\n Now let's access the model artifacts  \\n and load the model into our notebook,  \\n the log model is available here at this URL,  \\n runs:/ the run ID and the name of the directory  \\n that we use to save out the model.  \\n Using this URL, you can load the model in as a pyfunc model  \\n using mlflow.pyfunc.load_model.  \\n Once we have the loaded model on line six,  \\n I call model.predict on the batch of test data  \\n that we just accessed and then I print out  \\n the first five predictions.  \\n You can see the predictions  \\n are all in the form of logit scores.  \\n These are the raw unnormalized scores produced by a model.  \\n These can be transformed into probabilities,  \\n but that's not really needed, we can use the logits  \\n to get the predictions from the model as well.  \\n Let's consider the image at index six in our first batch.  \\n The test label at index position six is equal to eight.  \\n Now let's look up the class or category  \\n that's associated with this label eight.  \\n Remember, we have to subtract one since our classes start  \\n at index position zero and the labels start  \\n at index position one, and you can see that this corresponds  \\n to the letter h.  \\n Let's see the predictions from the model  \\n for this index position six to see  \\n whether it matches the actual label in the dataset.  \\n The predictions are in the form of logit scores,  \\n so we have to use argmax to get the actual prediction.  \\n And you can see that the actual prediction is indeed h.  \\n I'll set up a little helper function here to display  \\n the images that we have in our batch of test data,  \\n so we pass in the index position  \\n and view the image at that particular index position.  \\n This show utility method also allows us  \\n to pass in a title for each image.  \\n Now, with this utility method, what I'm going to do  \\n is iterate over all of the images in our test batch,  \\n all 64 of them.  \\n For every image, we'll access the actual label from the data  \\n and also get the prediction from our classification model.  \\n And in the title of each image, we'll print out  \\n the actual label and the model prediction, both of them.  \\n And for each of these 64 images, you'll be able  \\n to see the title to see whether the prediction matches  \\n the actual label associated with the image.  \\n Here is one that does not match  \\n and here is another that actually matches.  \\n Next, we'll get the model to make predictions  \\n on the entire test data, you can see on line seven,  \\n I iterate over the inputs and labels  \\n over the entire test data, on line 16,  \\n we'll compute the accuracy score of the model  \\n on the test data, print that out to screen,  \\n and on lines 20 through 23, I'll plot out a confusion matrix  \\n to see how our model performed on the test data.  \\n Now, this is going to be a rather large confusion matrix  \\n because we have 26 classes or categories.  \\n Notice the accuracy on the test data, 0.853.  \\n You already expected that.  \\n That's the same number we have logged in MLflow.  \\n All of the numbers on the main diagonal from the top left  \\n to the bottom right are predictions  \\n that our model got right, so you can see  \\n that a model got a large number right.  \\n Let's look at an example of a prediction a model got wrong.  \\n Notice in the first row, we have the number 265.  \\n Along the row, we have the letter i,  \\n and along the column for 265, we have the letter l.  \\n This tells us that the model tends to get confused  \\n between the letters i and l, which is understandable.  \\n At this point, you've successfully used MLflow  \\n to track the parameters and metrics  \\n for the training of your dense neural network.  \\n In the next demo, we'll see how we can train  \\n a convolutional neural network for image classification  \\n and use MLflow for tracking.  \\n \\n\\n\"}],\"name\":\"2. Model Training and Evaluation Using MLflow\",\"size\":68845304,\"urn\":\"urn:li:learningContentChapter:3855191\"},{\"duration\":2160,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3856119\",\"duration\":242,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Preparing data for image classification using CNN\",\"fileName\":\"3095447_en_US_03_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":242,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to prepare data for image classification using CNNs.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8923631,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Here I am on a brand new colab notebook,  \\n EMNIST classification using convolutional neural networks.  \\n Now, a new notebook implies a new colab runtime,  \\n which means we need to restart mlflow  \\n on this local machine.  \\n You can see from the message at the bottom  \\n that I've set up this runtime to run on a GPU.  \\n That's what we'll use to train our image classification  \\n convolutional neural network on the EMNIST data.  \\n Now, we need to install the libraries once again  \\n because we are on a new runtime.  \\n Torch, matplotlib, numpy, and pandas, we need all of these.  \\n We also need pytorch lightning  \\n because we'll be setting up our model  \\n using pytorch lightning, and we also need mlflow.  \\n Once again, this will get us mlflow 2.9.1,  \\n the latest version at the time of this recording.  \\n In order to ensure there are no breaking changes  \\n that mess up your demo, you might want  \\n to specifically install mlflow 2.9.1 yourself.  \\n And we also need pyngrok so that we can set up a tunnel  \\n from the colab machine to the external internet using ngrok.  \\n Because we're on a new notebook  \\n that sets up a new colab runtime,  \\n which means when I run the mlflow development server,  \\n this will be an entirely new instance.  \\n So all of our old experiments and runs  \\n will no longer be available.  \\n This is the same code that we've seen earlier  \\n to run mlflow on Port 5000 and then set up a tunnel  \\n to expose the local development server to the internet.  \\n Here is the mlflow tracking URL on ngrok.  \\n And if you click on this, you should be able  \\n to see the mlflow UI.  \\n Let's click through the warning message  \\n and here is the familiar mlflow user interface.  \\n Once again, we are set up with the default experiment  \\n and there are no additional experiments or runs.  \\n No matter, we are going to be creating one very soon.  \\n Back in our notebook, the initial part of the code  \\n where we import all of the libraries  \\n that we are going to use and where we access  \\n and set up our training data is going to be the same.  \\n I mount drive to the runtime of my current notebook  \\n because Google Drive is where we've stored  \\n the EMNIST training and test data.  \\n As soon as you allow your colab notebook access  \\n to Google Drive, the drive contents will be mounted  \\n at this path, /content/drive.  \\n Let's read in the training  \\n and test data using pd.readcsv.  \\n The data is under Mydrive emnist_data  \\n and then we have the two CSV files.  \\n Header is equal to none  \\n because neither of these files have headers.  \\n Now that we have the data, let's set up  \\n the dataset class EMNIST dataset.  \\n The structure of this is exactly the same  \\n as in the previous demo.  \\n For both the training and test data,  \\n we convert the images and labels to tensors,  \\n this is on lines 4 and 5.  \\n And on 14, when we access a single image,  \\n we divide image pixel values by 255.  \\n This expresses all pixel intensities  \\n in the range zero through one.  \\n We'll shuffle the test data as we did before,  \\n using the sample function frac equal to one,  \\n preserves the entire test data set,  \\n but now it'll be shuffled.  \\n Next, let's instantiate the data set  \\n for both the training data as well as the test data.  \\n Once again, I'm going to use random split  \\n to split the training dataset so that a fraction  \\n of this data, 10%, is kept aside for validation  \\n during the training process.  \\n Again, this is code you've seen before.  \\n Next, we'll instantiate our data loaders.  \\n We use the same batch size of 64 as before,  \\n and have set up three data loaders:  \\n train, validation, and test.  \\n num workers is always equal to one  \\n because we have just one GPU.  \\n The train data set has shuffle equal to true  \\n so that the data set is shuffled while training our model  \\n and drop last is equal to true  \\n so that all batches have 64 records.  \\n So if you have a batch with fewer than 64 records,  \\n that batch will be dropped.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3858094\",\"duration\":379,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Configuring and training the model using MLflow runs\",\"fileName\":\"3095447_en_US_03_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":379,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to track model runs using MLflow auto logging.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14730985,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We can now  \\n for image classification.  \\n Once again, I import some PyTorch libraries  \\n and classes that we'll need to set up our network.  \\n And next we'll create a new experiment within which we'll  \\n have a training run for our cnn.  \\n I call mlflow.create.experiment.  \\n The name of the experiment  \\n is emnist_letters_prediction_using_cnn,  \\n and I also set this as the active experiment  \\n so that any run that we create will be  \\n within this experiment.  \\n Next, we set up a class  \\n that inherits from a lightning module  \\n where we'll define our convolutional neural network, as well  \\n as the training and validation steps for our model.  \\n As before, for our classification model,  \\n we'll use the cross entropy loss function.  \\n Next, we define our convolutional neural network  \\n with multiple blocks of convolutional  \\n and polling layers.  \\n On line 8, we have our first convolutional layer.  \\n It takes in a single channel image as an input notice.  \\n The first input value is 1,  \\n and then it produces 32 feature maps at the output.  \\n It's followed by a ReLU activation layer,  \\n and we have the second convolutional layer on line 10,  \\n followed by a ReLU activation layer again.  \\n And then on line 12 we have a pooling layer  \\n to subsample the feature maps generated  \\n by the convolutional layers.  \\n Then on lines 14 through 18, we have another block  \\n with two convolutional layers with ReLU activation  \\n and a max pool layer again.  \\n On lines 12 and 18,  \\n notice I've specified in the comments the shape  \\n of the output feature maps from each of the max pool layers.  \\n This is something that you can compute by knowing the size  \\n of the input, knowing the kernel that you're using,  \\n the stride that you're using for the kernel,  \\n and of course the padding  \\n that you're using on the input image.  \\n The convolutional and pooling layer blocks are followed  \\n by a linear layer on line 21.  \\n We flatten the feature maps generated  \\n by the convolutional and pooling blocks  \\n and pass the flattened image through the linear layer,  \\n followed by ReLU activation.  \\n Observe the input dimensions  \\n of the linear layer on line 21 matches the size  \\n of the output feature maps  \\n from the max pooling layer on line 18.  \\n We have two linear layers  \\n with ReLU activation on line 21, 22, 24, and 25.  \\n And then we have our last linear layer on line 27.  \\n The output dimension for the last linear layer is 26 equal  \\n to the number of classes or categories  \\n into which the images can be classified.  \\n We've overridden the forward function,  \\n which defines a forward pass through the network.  \\n The input batch of images xb will be flattened  \\n and it needs to be reshaped into a form  \\n that can be fed into a convolutional neural network.  \\n The reshape function on line 31 converts the images  \\n to a form that is acceptable to PyTorch.  \\n That is the batch size is the first dimension.  \\n Then the number of channels  \\n and then the heightened width of each image.  \\n In configure optimizers, we specify the Adam optimizer  \\n with a learning rate of 0.0001.  \\n The code for the training step line 36  \\n through 49 is identical to the code  \\n that we had in the previous model.  \\n We make a forward pass through the model,  \\n get the predictions, compute the accuracy and the loss,  \\n and we return the loss on line 49.  \\n The code for the validation step is the same  \\n as the training step, again, same  \\n as in the previous dense neural network that we built.  \\n So the code on line 51 to 64 needs no explanation.  \\n Again, note that on line 64,  \\n the validation step returns the accuracy  \\n of the model on the validation data.  \\n Here is the method for the test step.  \\n This works on the test data,  \\n and on line 79, you can see  \\n that this returns the accuracy on the test data.  \\n And finally, we have the predict step.  \\n All this involves is a forward pass through the model  \\n to get predictions.  \\n Once again, we use the PyTorch Lightning Trainer  \\n to train the model,  \\n and we use MLflow runs to track the models,  \\n parameters and metrics during the training process.  \\n We instantiate our convolutional neural network  \\n on line 5, we use the CSV logger instantiated on line 7  \\n for the trainer to track out the model metrics  \\n during the training run.  \\n We'll instantiate the trainer on line 9.  \\n Again, we train for 10 epoch.  \\n You can train for longer if you want to,  \\n and we pass in the CSV logger so that the trainer  \\n can log out metrics.  \\n Once again on line 11, I turn on MLflow PyTorch autolog,  \\n so all metrics, parameters, tags,  \\n et cetera are logged out automatically except  \\n for the model artifacts.  \\n Notice I've set log models to false.  \\n I start the MLflow run on line 13.  \\n This run will automatically be ended once we are out  \\n of the width block.  \\n We start the training process on line 15  \\n by calling trainer.fit.  \\n We evaluate the trained model on the test data on line 16.  \\n Observe the input schema is now different  \\n because we're expecting input in the form of images  \\n and not flattened images,  \\n so the input is expected in the form of batch, number  \\n of channels, height and width  \\n of the image, minus 1, 1, 28 28.  \\n The output schema, again, has a dimension of 26  \\n because there are 26 classes  \\n or categories into which we can classify images.  \\n We instantiate a model signature object on line 20,  \\n and I explicitly call MLflow PyTorch log model.  \\n This will log out the model artifacts in the folder,  \\n emnist-letters-classifier-cnn-model, along  \\n with the model signature in the ML model file.  \\n I'm going to kickstart the training process,  \\n and for about 10 epochs of training, it took maybe 20  \\n to 25 minutes for this convolutional neural network.  \\n You can see on the first line here,  \\n GPU available: True, used: True.  \\n The PyTorch Lightning Trainer realized  \\n that a GPU was available  \\n and used that for training.  \\n Now this convolutional neural network does  \\n so much better on our data than the dense neural network.  \\n Notice the accuracy on the data is 0.907.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3858095\",\"duration\":412,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualizing charts, metrics, and parameters on MLflow\",\"fileName\":\"3095447_en_US_03_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":412,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to view and interpret charts generated by MLflow.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14796997,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now that we've  \\n and tracked its metrics  \\n and parameters using MLflow runs, let's head over  \\n to the MLflow UI  \\n and here you can see the experiment that we've just created  \\n and used the emnist_letters_prediction_using_cnn.  \\n You can see this exactly one run here  \\n within this experiment, mysterious-fly-874.  \\n That is our current training run.  \\n Let's take a look at the chart first so that we can see how  \\n training accuracy and validation accuracy changed over time.  \\n The very first chart that we get,  \\n let me expand the view first is of the test accuracy.  \\n This is a single value  \\n and you can see that it's rather high, 0.91.  \\n If you scroll down, you'll see  \\n how the training loss changed over time.  \\n You can see the loss was high  \\n and it gradually fell over the 10 epochs of training.  \\n Maybe we could have trained further to get a better model.  \\n You can see the validation accuracy here.  \\n It arose up to about 0.91.  \\n Here is the validation loss also fallen over the 10 epochs  \\n of training, and here below is the training accuracy,  \\n which went up to about 0.95.  \\n These charts are automatically generated when we use auto  \\n log with MLflow while training in PyTorch.  \\n Let's click through to the run  \\n and view the details of the run here.  \\n Here we can see the parameters, metrics, tags,  \\n and artifacts that are logged.  \\n Let's expand the parameters here first.  \\n You can see epochs of training.  \\n Then the optimizer name is Adam  \\n and a bunch of other details.  \\n What I'm interested in is the model metrics, so I'm going  \\n to expand the metrics section.  \\n Notice the test accuracy 0.907.  \\n We've seen that training accuracy went up to 0.948,  \\n and validation accuracy was at 0.9181, so all in all,  \\n a pretty decent model.  \\n Once again, since we explicitly logged the model,  \\n your other model artifacts, all  \\n of the artifacts are placed in this folder structure  \\n inside the top level folder that we had specified  \\n by name emnist-letters-classifier-cnn-model.  \\n In a production environment, MLflow would be part  \\n of a shared workspace  \\n and you'd be able to share this run with your team members.  \\n They'd be able to look at the model schema, know the schema  \\n of the input that needs to be fed into this model  \\n and the format of the output  \\n that will be received from this model.  \\n Once again, we have the MLmodel file,  \\n which defines the environment used to train your model  \\n and what flavors of the model are available.  \\n These are the flavors that you can use  \\n to load in the train model and use it for predictions.  \\n Once again, all of this is typically used  \\n by deployment tools.  \\n Here below you can see the signature  \\n of the model specifying the input and output schema.  \\n We're familiar with the other files here,  \\n nothing really interesting there.  \\n Let's head straight over to model summary.  \\n What I found interesting here was observe that every detail  \\n of our model is logged out, all of the layers  \\n in our convolutional neural network  \\n and the trainable params.  \\n Let's head back to the notebook.  \\n Let's visualize the training and validation loss  \\n and accuracy using the metrics that was logged out  \\n by the PyTorch Lightning Trainer.  \\n Here's the metrics file  \\n that I read in from the log directory.  \\n I'll now use some simple seaborn auto visualize the metrics  \\n for our training and validation data.  \\n The data available in metrics do CSV allows us  \\n to visualize these metrics right within our notebook.  \\n We've already seen these charts in MLflow.  \\n Those were auto logged.  \\n Let's get the run ID of the last active run so  \\n that we can then access the model artifact  \\n and load it into our notebook and use it for predictions.  \\n Here is the ID of the last active run.  \\n As we did before, I'm going to get one batch  \\n of data from our test dataset.  \\n I have the images and labels.  \\n We have 64 images and labels in this batch.  \\n Next, I'm going to load the serialized model  \\n into my notebook.  \\n I use the run ID to get a path  \\n to the model artifacts on our local machine.  \\n That is the URL runs /run_id  \\n and then the folder for the model.  \\n Pyfunc here stands  \\n for python function, which is a flexible  \\n and general way to package  \\n and deploy machine learning models as Python functions.  \\n When we use mlflow.pyfunc.load_model,  \\n we are essentially loading our model as a Python function  \\n that we can then invoke.  \\n We call loaded_model.predict pass in the test data  \\n to get the predictions on the test data.  \\n The predictions are in the form of raw logics scores.  \\n This gives us predictions for a single batch of test images,  \\n so if you look at the shape of the predictions,  \\n you should see that we have 64 different predictions.  \\n Now, the prediction  \\n for the image at index 2 in our test data  \\n is the label 4.  \\n Now let's look at what class this label 4 corresponds to.  \\n So I'll set up the classes list here  \\n and I'll look up the classes list by using the label  \\n for index 2.  \\n I have to subtract 1  \\n because remember, our classes list is indexed starting  \\n at 0, whereas the labels in our dataset are indexed  \\n starting at 1.  \\n The image at index 2 corresponds to the letter D.  \\n Let's see what the prediction from our model says.  \\n Let's look at predictions at index 2.  \\n This will give us the raw logics scores, so we need  \\n to use argmax to compute what prediction this actually was,  \\n and you can see that the model also predicted the letter D.  \\n I'm going to set up a utility function show as  \\n before, which will look up an image  \\n by index in the test images  \\n and display a title for that image.  \\n We'll then iterate over all 64 images in one batch  \\n of the test dataset  \\n and see what the predictions from the model were.  \\n The title will display both the predicted  \\n and the actual values from the dataset,  \\n and here are the results.  \\n You can see that in most cases,  \\n the model predictions were spot on.  \\n You'll find very few errors.  \\n That's because our model here has an accuracy  \\n of 91% on the test data.  \\n I use the same code as in the previous demo  \\n to get predictions for the entire test data  \\n and view the results in the form of a confusion matrix.  \\n Notice the accuracy on the test data, 0.907,  \\n that's what we saw logged in MLflow.  \\n The main dial from the top left to the bottom right,  \\n those are the correct predictions of the model,  \\n but our model does get confused with a few letters.  \\n The 246 corresponding to row I  \\n and column L tells us that the model still tends  \\n to mix up I and L.  \\n The 181 on row Q and column G tells us  \\n that Q and G are also confusing for the model.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3853105\",\"duration\":335,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Setting up the objective function for hyperparameter tuning\",\"fileName\":\"3095447_en_US_03_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":335,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to implement an objective function for hyperparameter training with Hyperopt.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11990693,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] An important part of model development  \\n in the full stack development workflow is  \\n hyper parameter tuning,  \\n and in this demo we'll see how  \\n to use the hyperop Python library for hyper parameter tuning  \\n and optimization of our convolutional neural network.  \\n Notice I'm on the same notebook  \\n as before EMNIST classification using CNN.  \\n This is the same GPU runtime as  \\n before with ML flow running on Port 5,000.  \\n Hyperopt is an open source python library  \\n for hyper parameter optimization.  \\n The process of finding the best set of hyper parameters  \\n for your machine learning model.  \\n Hyperop is designed to automate  \\n and streamline the search for optimal hyper parameters,  \\n making it easier to fine tune models  \\n and improve their performance.  \\n Now, the reason we select hyperop  \\n for hyper parameter tuning is  \\n because it has great integration with ML flow.  \\n You can combine hyperop  \\n and ML flow to perform hyper parameter tuning  \\n within the context of ML flow experiments and runs.  \\n Now, the first thing I'm going to do is import a bunch  \\n of libraries from the HYPEROP framework, fmin, tpe, hp,  \\n and trials.  \\n I'll explain each of these libraries as we use them.  \\n What I've done here is set up the search space  \\n for hyper parameter optimization.  \\n I've defined the third space  \\n for the learning rate LR in the long space.  \\n HP log uniform is used  \\n to specify a continuous distribution over positive values  \\n in the logarithmic space.  \\n Other number of neurons in the two linear layers at the end  \\n of the convolutional blocks, L one and L two.  \\n We'll try values of 32, 64, and 128 for the first layer  \\n and 64, 128, and 256 for the second layer.  \\n Next, I'll set up our lightning module.  \\n The EM model, the code for the neural network,  \\n and this class is exactly the same  \\n On line two, observe that the init method takes an hparams,  \\n that is the hyper parameters used to configure the model.  \\n Now on line five, self-taught save hyper parameters hparams  \\n will save the hyper parameters past so  \\n that it can be referenced elsewhere in this class.  \\n If you scroll down below  \\n in our convolutional neural network architecture on line 23,  \\n we set up the number  \\n of neurons in the first linear layer  \\n using self hparams L one.  \\n On line 25, we configure the second linear layer  \\n using hparams L two,  \\n and on line 34,  \\n when we instantiate the atom optimizer,  \\n we specify its learning read using our hyper parameter  \\n self-taught hparam lr.  \\n There is absolutely no other change to the code  \\n of this model.  \\n For hyper parameter tuning with hyperop,  \\n you need to define an objective function  \\n and that's what I'm going to do next.  \\n This train emnist function,  \\n which takes in the hyper parameters  \\n for this particular iteration  \\n of training is the objective function.  \\n The objective function  \\n usually contains the model training code  \\n that you want to perform for different runs of training,  \\n and this function represents the performance metric  \\n or evaluation criteria that you want to minimize  \\n during the hyper parameter optimization process.  \\n The objective function is at the core  \\n of hyper parameter tuning with hyperop,  \\n and it aims to find the set of hyper parameters that leads  \\n to the best value of this function.  \\n You'll find that our entire model training code is  \\n encapsulated within this objective function.  \\n This objective function will be repeatedly called  \\n during the hyper parameter optimization process  \\n with a different set of hyper parameters  \\n to build up the model.  \\n Notice the input argument  \\n to this objective function is the params dictionary  \\n containing the set of hyper parameters  \\n to use for this model.  \\n For each model that's trained, we start an ML flow run.  \\n This is on line six.  \\n Observe the fact that we pass in nested equal to true.  \\n Every run part  \\n of this hyper parametal tuning process will be nested  \\n within a parent run.  \\n All of the different trial runs  \\n of the process will be child runs of the parent run.  \\n Now within ML flow start run,  \\n we instantiate the emnist model  \\n and pass in the parameters for this particular model.  \\n On line 10, we instantiate the PyTorch lightning trainer  \\n and on line 12 we start the training process  \\n by passing in the training and validation data.  \\n We are not using ML flow auto logging here, so on lines 14  \\n through 17, we compute the metrics for the training process  \\n of this model on the training and validation data  \\n and on lines 21 through 24,  \\n we log those metrics out explicitly.  \\n On line 19, we explicitly log out the hyper parameters  \\n to construct this model.  \\n We have the metrics and parameters logged.  \\n Then as usual, we compute the input schema  \\n and the output schema for the model,  \\n set up the model signature, and log the model on line 31.  \\n This will be done for every model that's built  \\n and trained as a part  \\n of the hyper parameter optimization process.  \\n Notice what we return here on line 33, that is the core  \\n of this objective function.  \\n Now the objective function returns the metric  \\n that will be minimized  \\n during the hyper parameter optimization process,  \\n and I return the negative value of the validation accuracy.  \\n Minimizing the negative  \\n of the validation accuracy is equivalent  \\n to maximizing the validation accuracy.  \\n So the best model will be the one with the highest accuracy  \\n on the validation data.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3855190\",\"duration\":381,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Hyperparameter optimization with Hyperopt and MLflow\",\"fileName\":\"3095447_en_US_03_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":381,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to perform hyperparameter optimization with Hyperopt and MLflow.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12938387,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now that we've set up the objective function,  \\n which returns the metrics that has to be minimized  \\n during the process of hyper parameter optimization,  \\n we can start the hyper parameter tuning process.  \\n Notice that we perform hyper parameter tuning  \\n within an outer ML Flow run.  \\n I call with mlflow.start_run.  \\n All of the runs used to track the individual  \\n model trainings metrics and parameters  \\n will be nested within this outer run.  \\n Now the way you perform hyper parameter tuning  \\n using hyperop is by using this fmin function.  \\n The fmin function is responsible for executing  \\n the actual hyper parameter optimization process  \\n and searching for the best set of hyper parameters  \\n that minimize our objective function.  \\n The function that fmin will run over and over again  \\n to train the different models, which are part  \\n of this optimization process, is the function  \\n that we pass in as an input argument on line three,  \\n the train_emnist function.  \\n That is the objective function that we just defined.  \\n Fmin also needs the third space for the hyper parameters,  \\n which I specify on line four,  \\n and the maximum number of evaluations  \\n or model trainings that we run, I specify on line six.  \\n That is max evals equal to 10.  \\n The fmin function also allows you to specify  \\n the algorithm to use for finding the model  \\n with the best set of hyper parameters.  \\n On line five, you can see that I've asked fmin  \\n to use the tpe.suggest algorithm.  \\n The tpe here stands for tree of parzan estimators,  \\n which is an optimization algorithm  \\n used for Bayesian hyper parameter optimization.  \\n Bayesian optimization is a technique  \\n that uses a probabilistic model  \\n to approximate the objective function  \\n and then make informed decisions, about where  \\n to sample the next set of hyper parameters.  \\n This is a popular choice over other optimization algorithms  \\n because it can find the optimal set of hyper parameters,  \\n while minimizing the number of evaluations required.  \\n And the model with the best results  \\n will be stored in the best result variable.  \\n Let's kick start our hyper parameter optimization process  \\n by hitting shift enter and executing the fmin function.  \\n Now, remember that there'll be 10 different trials.  \\n That is 10 different models will be trained,  \\n so this is going to take a while.  \\n Now, while the training is in progress,  \\n and the entire training is going to take  \\n about 60 minutes to run through,  \\n let's head over to ML Flow.  \\n And notice we have a new run  \\n within our emnist_letters experiment,  \\n the hilarious-shad-653.  \\n Observe the little plus next to that top level run.  \\n This indicates that this is a parent run  \\n and nested within it, there are several child runs.  \\n Exactly one at this point in time,  \\n but this is going to increase.  \\n All of the runs nested within the parent run  \\n are child runs, each representing  \\n a separate model that has been trained.  \\n Let's click through to the first run here  \\n and here you'll be able to view the parameters  \\n and metrics of this first model that's being trained.  \\n The model training for the first model seems to be complete.  \\n If you look at the metrics, you can see  \\n that the validation accuracy is .8989.  \\n Let's go back to our experiment page here  \\n where all of the runs are listed.  \\n Now the second model is currently being trained  \\n and you can click on this little refresh icon  \\n to get additional details.  \\n When a run is complete, that should show up.  \\n Let me click on the experiment once again,  \\n and notice that we have two child runs under the parent run.  \\n You can click through and view the details  \\n of this particular run if you want to.  \\n But what I'm going to show you instead  \\n is an interesting way you can configure  \\n this particular page, to make it more useful.  \\n If you click on the Columns option here,  \\n you'll see additional columns  \\n that you can view on this Experiment page.  \\n Observe that it's possible to view  \\n the metrics of the various models  \\n right here on this page, so I'm going  \\n to select a few metrics that I want to see.  \\n The test accuracy, then maybe the training accuracy,  \\n and the validation accuracy.  \\n If there are other columns that you want to view,  \\n you should go ahead and select those here.  \\n These are the three that I'm interested in,  \\n so I'm going to just stick with that.  \\n And these should be now visible as columns  \\n here in this experiment view.  \\n I'm not showing them to you yet,  \\n because they are not as interesting.  \\n I'll show them to you in just a bit.  \\n Meanwhile, I'm waiting for more child runs to complete.  \\n Remember, we'll be running a total of 10 trials,  \\n so we still have a ways to go.  \\n What I'm going to do now  \\n is show you how you can compare runs.  \\n I'm going to hide all of the runs  \\n that I'm not interested in, by clicking  \\n on the little eye icon next to the run.  \\n So I'm going to click on this eye icon,  \\n that'll hide this particular run.  \\n And the only runs I have selected are the last two runs,  \\n that is the last two model executions.  \\n You can see the popup that says  \\n switch to the chart view to compare runs.  \\n Click on \\\"Got It,\\\" and that will take you  \\n immediately to the chart view.  \\n And this will show you the two runs that we have selected.  \\n It will compare all of the metrics for those two runs.  \\n Let's take a look at the charts here.  \\n Here is the training loss for the two runs.  \\n The colors in the chart correspond  \\n to the colors for each of the runs,  \\n specified in the little table off to the left.  \\n The yellow color is for unequaled-cub-63,  \\n and the red color is for aged-hawk-722.  \\n You can see that the second run, the one  \\n denoted in yellow, is still in progress.  \\n Only three epochs are done of training,  \\n and that's why you don't see a complete line.  \\n Let's scroll down a little bit further.  \\n Here you can see how the validation accuracy  \\n of the two runs compare.  \\n And again, the second run isn't complete yet.  \\n And scrolling down further, here we have  \\n the train accuracy for the two runs.  \\n You can see that the run in red  \\n is a better model overall than the run  \\n in yellow, at least so far.  \\n I'll now go back to the main able view here.  \\n There is nothing much we can do  \\n except wait for this hyper parameter  \\n optimization process to complete.  \\n All 10 evaluations need to go through.  \\n So I've been waiting patiently, but at some time,  \\n model training will indeed complete.  \\n And you can see here in the notebook  \\n that we've reached end of the optimization process.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3856120\",\"duration\":219,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Identifying the best model\",\"fileName\":\"3095447_en_US_03_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":219,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to identify the best model parameters found using hyperparameter optimization.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7632266,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now that the hyper parameter  \\n optimization process is complete, the variable best result  \\n should hold the hyper parameters for the model  \\n with the best accuracy score on the validation data.  \\n So let's take a look at best result here.  \\n And this was the result where we chose the first option  \\n for l1 the second option for l2,  \\n and the learning rate was around 0.000285.  \\n Now, in order to know what the model parameters were  \\n for this best possible model, you can print out the results  \\n of the hyper opt of space eval function,  \\n pass in the search space, pass in the best result,  \\n and you'll get the hyper parameters for the best model.  \\n l1 has a value of 64, l2, a value of 256,  \\n and lr is 0.0002855.  \\n Now, let's go back to our ml flow experiment.  \\n Now, if you remember, we had added to this page,  \\n three different columns: the training accuracy,  \\n validation accuracy, and the test accuracy.  \\n If you scroll over to the right here, you'll see  \\n that these three columns are available here in this view,  \\n there is no test accuracy metric, but we do have the train  \\n and validation accuracy metrics.  \\n And you can actually sort your runs  \\n based on the validation accuracy.  \\n Let's do exactly that.  \\n I'm sorting by the descending order of validation accuracy  \\n and you can see that the run  \\n that had the best model was aged hawk 722.  \\n This is the run that produced the model  \\n with the best parameters that we saw in our notebook.  \\n Let's click through to this run  \\n and take a look at the parameters of the model  \\n that was trained in this run.  \\n If you expand the parameters, notice l1 is 64, l2 is 256,  \\n and lr is 0.0002855.  \\n You'll be able to view all of the metrics  \\n for this model on the training and validation data.  \\n If you expand the metrics here,  \\n you can see the training accuracy was very high, 0.968,  \\n the validation accuracy was 9324.  \\n For every model that we trained,  \\n we also log the model artifacts, and they're available here  \\n in the artifacts section at the bottom.  \\n You can select the individual details here  \\n in the model artifacts and view them.  \\n Now, I want to load and use this model artifact  \\n for predictions on the test data.  \\n So I'm going to copy over the URL for the logged model  \\n and use that within my notebook.  \\n The logged model references the best model  \\n that was found using hyper parameter tuning  \\n based on the validation accuracy, remember that.  \\n I'll now use this model  \\n for predictions on one batch of the test data.  \\n On line 5, I use mlflow.pyfunc.load model  \\n to load the model in as a Python function  \\n from the artifacts directory.  \\n And on line 9, we make predictions on test images,  \\n that is one batch of the test data.  \\n And we'll print out the first five predictions,  \\n and these are in the form of raw logic scores.  \\n Now, with this loaded model,  \\n let's make predictions on the entire test dataset.  \\n We run a for loop over the entire test data on line 7.  \\n On line 8, we call loaded model.predict  \\n on the current patch of the test data.  \\n We get the predictions on line 10, we add the predictions  \\n to the y pred list on line 11,  \\n and we add the true values to the y true list on line 14.  \\n On line 16, we compute the accuracy  \\n of the model on the test data, and you can see  \\n that this is also fairly high, 0.9256 or 92.56%.  \\n So hyper parameter tuning with ml flow allowed us  \\n to identify the best model.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3857097\",\"duration\":192,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Registering a model with the MLflow registry\",\"fileName\":\"3095447_en_US_03_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":192,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to register your model with the MLflow model registry and manage the model's environment.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5551710,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Here I am back in the ML Flow page  \\n for the run that produced the best model on our data.  \\n Now, I did tell you early on that we won't be  \\n working at all with the model registry,  \\n but I'll show you one little thing here.  \\n I'll show you how you can register a model  \\n with the ML flow model registry,  \\n and this will help you keep track  \\n of the different versions of your model  \\n that you create over time.  \\n Now, here on this run page,  \\n notice you have a button for register model.  \\n Just click on that button.  \\n This will bring up a dialogue, select create new model,  \\n and specify a name for your model.  \\n I've called this emnist_cnn_classification.  \\n Click on register,  \\n and your model is now registered  \\n with the ML Flow Centralized Model Registry.  \\n And a version is associated with this model.  \\n If you head over to the Models tab,  \\n you should see our newly registered model in there.  \\n You can see that this is version one of the model.  \\n As you create new versions of the model,  \\n you can use that to replace this older version.  \\n And the centralized model registry will keep track  \\n of your different model versions.  \\n Let's click through and take a look.  \\n You can see we have just one version  \\n of the model, version one.  \\n If you had multiple versions, they'd be listed here.  \\n I'll click through to version one,  \\n and here you'll be able to see  \\n the details of the registered model.  \\n Notice that the input and output schema  \\n are both available here for you to view.  \\n You can see the source run  \\n that created this model, aged-hawk-722.  \\n And you can see that the current stage  \\n of the model is set to none by default.  \\n Now let's say you've tested this model  \\n thoroughly in the development environment  \\n and you're ready to transition this model.  \\n You can choose to transition  \\n to the staging or production environments.  \\n The next step would likely be the staging environment.  \\n So I'm going to go ahead and transition this model  \\n to the staging environment.  \\n Because this is our locally running ML flow server,  \\n this transition does not require any approval.  \\n If you're working in a production environment  \\n as a part of a shared workspace,  \\n these transitions usually require approval  \\n from someone higher up in your team.  \\n Observe that the model is currently  \\n in the staging environment, and if you head back  \\n to the model, you can see that version one  \\n is in staging, and you can have  \\n multiple versions in different environments,  \\n staging, production, and even archive.  \\n At some point, you've likely tested this model  \\n in the staging environment and you are satisfied with it.  \\n That's when you'd transition the model  \\n to the production environment.  \\n And, this is where you indicate that this model  \\n is ready to be deployed to production.  \\n If you work with ML Flow integrated as a part  \\n of a cloud platform such as Databricks,  \\n well, you can manage all of these transitions  \\n within a Databricks workspace, and you can also  \\n deploy your model once you've registered it  \\n to be part of the model registry.  \\n Because you're working with a locally running  \\n ML flow in a CoLab environment,  \\n we can't really deploy our model here.  \\n In the next demo, we'll work with model deployment  \\n and we'll use a slight workaround,  \\n in order to see how exactly ML Flow  \\n serving and deployment work.  \\n \\n\\n\"}],\"name\":\"3. Model Training and Hyperparameter Tuning\",\"size\":76564669,\"urn\":\"urn:li:learningContentChapter:3855192\"},{\"duration\":828,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3852092\",\"duration\":296,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Setting up MLflow on the local machine\",\"fileName\":\"3095447_en_US_04_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":296,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, learn how to install and set up MLflow on your local machine.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7998830,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] While working on Colab,  \\n we don't have access to the underlying virtual machine  \\n that hosts our cloud notebook,  \\n which is why we had to use Ngrok  \\n to access MLflow running on Colab.  \\n Now, if you want to be able to serve  \\n and deploy our models using MLflow,  \\n well Colab is just not going to work.  \\n So I'm going to use a little work around  \\n and set up MLflow on my local machine  \\n and we'll serve the model  \\n that we trained on Colab on our local machine.  \\n This will involve downloading the artifacts  \\n trained on Colab, setting up the same directory structure  \\n on our local machine, and then using MLflow to deploy  \\n and serve the model artifacts.  \\n First thing here on my local machine,  \\n let's make sure I have Python installed.  \\n Python --version tells me I have Python 3.10.9,  \\n the same version of Python that we use  \\n to train our model.  \\n We'll be using a locally running Jupyter Notebook,  \\n so I'm going to check whether I have Jupyter installed.  \\n Jupyter --version tells me that I have a new version  \\n of Jupyter available that I can use.  \\n I'm going to create a folder called full_stack_deep_learning  \\n that will serve as my working directory.  \\n I create this folder and I'm going to CD into this folder.  \\n We'll need to install MLflow on our local machine,  \\n and for that I'm going  \\n to set up a Python virtual environment.  \\n It's always a good practice  \\n to run MLflow within an isolated virtual environment.  \\n I've created an environment called fsdl_venv,  \\n and if you run an ls -l here in the current folder,  \\n you should see there is a folder  \\n that corresponds to this virtual.  \\n Since I'm working on a Mac machine,  \\n I use this source command in order  \\n to activate this virtual environment source fsdl_venv  \\n /bin/activate.  \\n If you're on a Windows machine, this is the command  \\n that you'd use to activate the virtual environment.  \\n You'd run the activate.bat batch script  \\n within the bin folder.  \\n Notice that our prompt has changed indicating  \\n that we are now within the fsdl_venv virtual environment.  \\n Next, I'm going to install IPyKernel.  \\n IPyKernel is a Python package  \\n that provides the communication  \\n between the Jupyter Notebook interface  \\n and the Python kernel.  \\n And this is the Python package that will allow us  \\n to run our Jupyter notebook  \\n using this virtual environment that we've created.  \\n Once IPyKernel is installed, run jupyter kernelspec list  \\n to see what kernels are currently available  \\n to your Jupyter Notebook.  \\n You can see there's exactly one Python 3 kernel  \\n that's available.  \\n Let's install the kernel associated  \\n with our virtual environment, python -m ipykernel install.  \\n The name of the kernel to install is fsdl_venv.  \\n That is our virtual environment,  \\n and I'm installing this for the current user.  \\n So this kernel specification has now been installed  \\n for Jupyter Notebook.  \\n If you're on Jupyter Kernel spec list, you can see  \\n that we have a new kernel available, fsdl_venv kernel.  \\n We'll be using the Python package manager pip  \\n to install packages.  \\n So let's upgrade pip so that we  \\n have the latest version, 22.3.1,  \\n at the time of my recording.  \\n I'll now install the different Python packages  \\n that we need within this virtual environment,  \\n torch, matplotlib, numpy, pandas, and mlflow.  \\n We install all of these.  \\n Once the installation has run through,  \\n let's confirm that MLflow is indeed available  \\n on our local machine, run mlflow --version.  \\n And this should give you a version number.  \\n You can see that we are using MLflow version 2.9.1.  \\n If you are building and training your models locally  \\n using MLflow, the way you bring  \\n up the MLflow development environment  \\n is by running MLflow UI in your terminal.  \\n I'll show you that we'll see the same MLflow page  \\n as before when we worked on Colab.  \\n So let's head over to 127.0.0.1:5000.  \\n And here is our familiar MLflow user interface experiments  \\n and models.  \\n We won't really be using the MLflow development server  \\n on our local machine except to deploy  \\n and serve our MLflow model.  \\n So let's go ahead and heal the MLflow UI.  \\n Within this virtual environment,  \\n I'll now bring up the Jupyter Notebook server  \\n on my local machine, and this is what we'll use  \\n to send request to our deployed model  \\n to get predictions.  \\n Let's open up the Jupyter Notebook server here.  \\n I've created a new notebook called ModelDeployment  \\n on our locally running Jupyter Notebook server.  \\n And you can see on the top right that the Python kernel  \\n that this notebook is using  \\n is our virtual environment, fsdl_venv.  \\n I'm just going to toggle the header here under view so  \\n that we have more room on our notebook for our code.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3854104\",\"duration\":139,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Workaround to get model artifacts on the local machine\",\"fileName\":\"3095447_en_US_04_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":139,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, discover how to download MLflow artifacts.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4107144,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now early on in this course  \\n I had mentioned that because we needed to use a GPU,  \\n we couldn't train this convolutional neural network on our  \\n local machine and instead had to use Colab.  \\n We then set up ML flow on the Colab runtime  \\n and then used ML Flow to track our models, metrics,  \\n parameters, and artifacts.  \\n Now, here are our models,  \\n artifacts available on the Colab runtime,  \\n but they're not present on my local machine.  \\n I want to show you how we can deploy  \\n and serve this model using ML flow  \\n and we'll do that on our local machine.  \\n So what I need to do is download all of the artifacts  \\n that ML Flow has saved into this emnist letters classifier,  \\n CNN model, sub folder to the local machine.  \\n I've selected the model.path file under data,  \\n and I'm going to click on this download button,  \\n and this will download this into my downloads folder.  \\n And I'm going do this for every file under artifacts here.  \\n I'm going to download this text file as well,  \\n and I'm going to download all of the other files one by one.  \\n So this is not something that you'd have  \\n to do if you are essentially serving the model exactly  \\n where you train the model.  \\n It just so happened that we train the model on CoLab  \\n and we are going to be deploying  \\n and serving the model on our local machine.  \\n So I need all of these artifacts here on my local machine.  \\n On my local machine, I use all of the downloaded files  \\n and set up the structure  \\n of a folder called Best underscore Model.  \\n This has the same structure as the ML flow artifacts folder,  \\n and I've placed this best model sub folder under the ML runs  \\n directory that ML Flow created when we  \\n ran the ML flow server.  \\n So under best model, I have a conda YML file  \\n for the conda environment, an ML model file, a Python  \\n underscore ENV YML file, and a requirements.text file.  \\n And in the data sub folder, I have the model artifact,  \\n the model.path file, and the module info text file.  \\n So this best model folder exactly mimics the model artifacts  \\n folder that ML flow creates.  \\n Again, we've had to do this as kind of a hack  \\n or a workaround because we are going to be deploying  \\n and serving our model locally while the model was trained  \\n and tracked in ML flow in a CoLab environment.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3854105\",\"duration\":393,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Deploying and serving the model locally\",\"fileName\":\"3095447_en_US_04_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":393,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"In this video, discover how to use MLflow serving to deploy your model on your local machine.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12574703,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Before we deploy  \\n and serve the model using MLflow,  \\n let's load the model in from the best model folder  \\n as a Python function  \\n and use it for predictions right here  \\n in this Jupyter Notebook.  \\n I'm going to install PyTorch Lightning  \\n because I'm going to set up a data set  \\n and a data loader for the test data.  \\n Let's quickly import the libraries  \\n that we need to work with PyTorch Torch, NumPy, matplotlib,  \\n pandas, and so on, and also the dataset and the data loader.  \\n I have the CSV file  \\n of the test data in my local machine  \\n in the current working directory,  \\n so I'm going to use Pandas to read in this data.  \\n Once we have the test data, I'm going to shuffle it  \\n by using the sample function in Pandas.  \\n We now have the test data in the form of a data frame.  \\n I create a dataset to represent this test data.  \\n This is the same emnist dataset  \\n that we've seen in earlier demos.  \\n I'm just setting this up in this notebook.  \\n Next, I instantiate the test data set  \\n by passing in the test data frame  \\n and I also instantiate a test data loader  \\n with a batch size of 64.  \\n This is the batch size we've been  \\n consistently using with this data.  \\n Next, let's set up an iterator over this test data loader  \\n to get the first batch of test images.  \\n The shape of the test images is 64 by 784.  \\n This is one batch. We have the images as well as the labels.  \\n Let's now access the trained model to use for predictions.  \\n We'll reference the artifacts  \\n that we downloaded from Colab.  \\n The best model sub folder  \\n that we set up is in the ML runs directory.  \\n That is in the current working directory of this notebook,  \\n so the log model is at mlruns/best_model.  \\n We load the model using mlflow.pyfunc.load_model,  \\n and then called loaded_model.predict on the test images.  \\n This will give us 64 predictions on the first batch  \\n of the test data.  \\n It's likely that you'll encounter some warnings  \\n when you do this.  \\n That's because all of the packages  \\n and versions for the various Python libraries  \\n that we've used on Colab may not exactly match the packages  \\n and versions that we have in our local environment.  \\n This is because we are using a workaround to deploy a model  \\n that we trained on Colab on our local machine.  \\n This is not a trick you're likely  \\n to be using in a production environment.  \\n Well, now that we have the predictions from our model,  \\n let's take a look at one of the test images,  \\n the one at Index 5,  \\n and this seems to be an image of an I or maybe an L.  \\n Let's take a look at the test label associated with 5,  \\n and you can see that it's 12 indicating  \\n that this is likely an L.  \\n We need a list of the classes  \\n or categories into which the images  \\n are classified, A through Z.  \\n Let's see the actual label associated  \\n with the test image 5, and you can see  \\n that it corresponds to an L.  \\n This is the actual label from the test data.  \\n Now let's get the prediction from our model,  \\n but rather than use the model loaded into this notebook  \\n to get the prediction, let's deploy this model  \\n using MLflow to a local endpoint.  \\n And deploying a model with MLflow is very straightforward.  \\n Simply call mlflow models serve, specify the path  \\n to the model artifacts folder.  \\n That is mlruns/best_model.  \\n This is a locally running MLflow, so env-manager is local  \\n and the host at which the model endpoint  \\n will be available is 127.0.0.1:1234.  \\n Executing this command will deploy  \\n and serve your package model locally available  \\n at the endpoint that we have specified.  \\n Now, back to our notebook.  \\n Let me show you how we can hit this locally running endpoint  \\n with some image data.  \\n I'm going to take the test image at index 5.  \\n I need the data in a particular format so I can use  \\n that data to hit the model endpoint.  \\n I reshape the image to be of a form that the model accepts.  \\n Bat size is the first dimension, the number of channels,  \\n and then height and width.  \\n And I convert the image data to a list  \\n and I set up a JSON structure  \\n where this list is the value corresponding  \\n to the key instances.  \\n This is the structure that we need to use  \\n to send this image data to our prediction endpoint.  \\n I'm just going to copy this entire structure over so  \\n that I can use it to make the curl request  \\n to the prediction endpoint in the next code cell.  \\n Go ahead and copy this, and here is my curl request.  \\n I use the curl utility to make a get request  \\n to the URL http://127.0.0.1:1234/invocations.  \\n This is the rest endpoint of the locally deployed model.  \\n I have a request body in this HTTP request that I make  \\n and the request body specified using -d is the JSON data  \\n representing a single test image  \\n that I copied over from the previous code cells output.  \\n Let's go ahead and hit shift enter,  \\n and you can see the predictions  \\n from our locally running endpoint.  \\n These other raw logics scores.  \\n The responses in the form of a JSON string will need  \\n to extract the actual predictions from this response.  \\n Going to copy this JSON string over  \\n and use json.loads to get it in the JSON format.  \\n We'll then look up the value for the predictions key  \\n and we'll index into it at index 0.  \\n That'll give us the array of the actual predictions.  \\n You can see the predictions variable here  \\n contains the predictions in the list format.  \\n Let's now get the predicted label  \\n from the raw logics scores.  \\n I use np.argmax and index into the classes list,  \\n and you can see here that the model has predicted L.  \\n The predicted label is equal to the actual label.  \\n Now here's some practice that you can do on your own.  \\n Simply pick a different test image converted  \\n to the JSON format that are locally  \\n hosted prediction endpoint expects,  \\n and then use the curl command  \\n to get predictions for the test image.  \\n Pass the predictions response  \\n and see what the predicted label is.  \\n All of the code is available for you to use  \\n right in this notebook.  \\n \\n\\n\"}],\"name\":\"4. Model Deployment and Predictions\",\"size\":24680677,\"urn\":\"urn:li:learningContentChapter:3857098\"},{\"duration\":104,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3853106\",\"duration\":104,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Summary and next steps\",\"fileName\":\"3095447_en_US_05_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"rawDurationSeconds\":104,\"solutionVideo\":false,\"handoutGraphicsIncluded\":false,\"assignedBy\":\"urn:li:member:-1\",\"includesPickups\":false,\"challengeVideo\":false,\"hasSlides\":false,\"graphicsIncluded\":false,\"assignedTo\":\"urn:li:member:-1\",\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2513834,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] This demo brings us  \\n to the very end of this course.  \\n Let's quickly review what we've covered so far.  \\n We started off with an overview of full-stack deep learning,  \\n and we saw that this covers the complete lifecycle  \\n of a deep learning model,  \\n from prototyping to production.  \\n we understood the role of MLOps in full-stack deep learning,  \\n and we were introduced to the MLflow tool that streamlines  \\n and automates the machine learning lifecycle.  \\n We then got hands-on with MLflow  \\n and saw how we could track logs  \\n and metrics using MLflow experiments and runs.  \\n We trained two different image classification models  \\n against neural network  \\n and a convolutional neural network,  \\n and we trained them to classify images  \\n from the EMNIST dataset.  \\n We performed hyperparameter tuning  \\n on our convolutional neural network  \\n using the Hyperopt Python library,  \\n which integrates very well with MLflow.  \\n And finally, we used MLflow serving  \\n to deploy our model on the local machine.  \\n Here, at the end of this course,  \\n if you're still interested in learning more  \\n about deep learning systems,  \\n here are some other courses on LinkedIn Learning  \\n that you might find interesting,  \\n AI Text Summarization Using Hugging Face,  \\n Self-Supervised Machine Learning,  \\n and GANs and Diffusion Models in Machine Learning.  \\n The AI Text Summarization course will show you  \\n how you can use Hugging Face transformers  \\n for abstractive text summarization.  \\n Self-Supervised Machine Learning will show you  \\n how you can leverage unlabeled data to build models.  \\n And the third course on GANs  \\n and Diffusion models will introduce you  \\n to early generative AI models.  \\n It's time for me to say goodbye.  \\n That's it from me here today.  \\n Thank you for listening.  \\n \\n\\n\"}],\"name\":\"Conclusion\",\"size\":2513834,\"urn\":\"urn:li:learningContentChapter:3857099\"}],\"size\":226567729,\"duration\":7081,\"zeroBased\":false}]}"